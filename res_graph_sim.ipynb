{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9668e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from statannot import add_stat_annotation\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "import dgl.function as fn\n",
    "from dgl.nn.pytorch import GraphConv, SAGEConv, TAGConv, GINConv\n",
    "\n",
    "from deepsurv_utils import c_index, adjust_learning_rate\n",
    "# from loss import NegativeLogLikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e8ef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# load SHPH data\n",
    "all_patient_info = pd.read_csv(\"\")\n",
    "all_patient_info = all_patient_info[['folder_name', 'Sex_1_male_2_female', 'Age',\n",
    "       'Location_1_LUL_2_LLL_3_RUL_4_RML_5_RLL','Histology_1_Adenocarcinoma_2_SquamousCellCarcinoma_3_Others',\n",
    "        'pT_Stage', 'pN_Stage', 'pM_Stage', 'pTNM', 'RFS_Status', 'RFS_Month',\n",
    "       'OS_Status', 'OS_Month']]\n",
    "stage1 = list(np.load(\"/home/jielian/lung-graph-project/data/seg_image/labels/name_stage1.npy\"))\n",
    "stage2 = list(np.load(\"/home/jielian/lung-graph-project/data/seg_image/labels/name_stage2.npy\"))\n",
    "patint_list = [*stage1, *stage2]\n",
    "patient_info = all_patient_info[all_patient_info['folder_name'].isin(patint_list)]\n",
    "\n",
    "patient_info['pT_Stage']=patient_info['pT_Stage'].replace({\"T1a\":0, \"T1b\":0, \"T1c\":0, \"T2a\":1,\"T2b\":1,\"T3\":2})\n",
    "patient_info['pM_Stage']=patient_info['pM_Stage'].replace({\"M1a\":1})\n",
    "\n",
    "feature_files = os.listdir(\"trans_feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782cd24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_feature = \"/home/jielian/lung-graph-project/data/seg_image/medicalnet/tumor_medical/\"\n",
    "\n",
    "data = []\n",
    "name = []\n",
    "for feature_name in feature_files:\n",
    "#     print(feature_name)\n",
    "    path = trans_feature +feature_name[:-4] + \"_d64.npy\"\n",
    "    name.append(int(feature_name[:-4]))\n",
    "    feature = np.load(path, allow_pickle=True)\n",
    "    af = feature.reshape((-1, 8, 8, 8))\n",
    "    af_mean = af.mean(axis=(1, 2, 3))\n",
    "    data.append(af_mean)\n",
    "feature_data = pd.DataFrame(data)\n",
    "feature_data = (feature_data-feature_data.min())/(feature_data.max()-feature_data.min())\n",
    "feature_data['folder_name']=name\n",
    "all_data = patient_info.merge(feature_data, how='left', on='folder_name')\n",
    "print(len(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a94f2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['Location_1_LUL_2_LLL_3_RUL_4_RML_5_RLL'] = all_data['Location_1_LUL_2_LLL_3_RUL_4_RML_5_RLL'].replace({4:3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aa6bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load external information\n",
    "external_info = pd.read_csv(\"/home/jielian/lung-graph-project/Tumor_tranformer/data_ind/External_label.csv\")\n",
    "external_patint_list = external_info['Patient']\n",
    "external_info=external_info.rename(columns={\"Patient\":\"folder_name\"})\n",
    "external_info=external_info.rename(columns={\"Histology\":\"Histology_1_Adenocarcinoma_2_SquamousCellCarcinoma_3_Others\"})\n",
    "external_info['pT_Stage']=external_info['pT_Stage'].replace({\"Tis\":0,\"T1a\":0, \"T1b\":0, \"T1c\":0, \"T2a\":1,\"T2b\":1,\"T3\":2,\"T4\":3 })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f3c43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_feature_files = os.listdir(\"trans_feature_val\")\n",
    "external_info['Location_1_LUL_2_LLL_3_RUL_4_RML_5_RLL'] = external_info['Location_1_LUL_2_LLL_3_RUL_4_RML_5_RLL'].replace({4:3})\n",
    "external_data = []\n",
    "for feature_name in external_patint_list:\n",
    "    path = \"/tumor/\"+feature_name+\"_d64.npy\"\n",
    "    feature = np.load(path, allow_pickle=True)\n",
    "    af = feature.reshape((-1, 8, 8, 8))\n",
    "    af_mean = af.mean(axis=(1, 2, 3))\n",
    "    external_data.append(af_mean)\n",
    "external_feature_data = pd.DataFrame(external_data)\n",
    "external_feature_data = (external_feature_data-external_feature_data.min())/(external_feature_data.max()-external_feature_data.min())\n",
    "external_feature_data['folder_name']=external_patint_list\n",
    "external_all_data = external_info.merge(external_feature_data, how='left', on='folder_name')\n",
    "print(len(external_all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186cfc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #merge the dataset\n",
    "frames = [all_data, external_all_data]\n",
    "final_data = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae9c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = np.load(\"data_ind/train_index.npy\",allow_pickle=True)\n",
    "val_id = np.load(\"data_ind/val_index.npy\", allow_pickle=True)\n",
    "test_id = np.load(\"data_ind/test_index.npy\",allow_pickle=True)\n",
    "external_id = np.array(range(len(all_data),len(final_data)))\n",
    "idx_train = torch.LongTensor(train_id)\n",
    "idx_val = torch.LongTensor(val_id)\n",
    "idx_test = torch.LongTensor(test_id)\n",
    "idx_external_val = torch.LongTensor(external_id)\n",
    "\n",
    "print(\"training OS distribution:\")\n",
    "print(all_data.iloc[train_id,:]['OS_Status'].value_counts())\n",
    "print(\"validation OS distribution:\")\n",
    "print(all_data.iloc[val_id,:]['OS_Status'].value_counts())\n",
    "print(\"test OS distribution:\")\n",
    "print(all_data.iloc[test_id,:]['OS_Status'].value_counts())\n",
    "print(\"External OS distribution:\")\n",
    "print(final_data.iloc[external_id,:]['OS_Status'].value_counts())\n",
    "\n",
    "\n",
    "print(\"training RFS_Status distribution:\")\n",
    "print(all_data.iloc[train_id,:]['RFS_Status'].value_counts())\n",
    "print(\"validation RFS_Status distribution:\")\n",
    "print(all_data.iloc[val_id,:]['RFS_Status'].value_counts())\n",
    "print(\"test RFS_Status distribution:\")\n",
    "print(all_data.iloc[test_id,:]['RFS_Status'].value_counts())\n",
    "print(\"External RFS_Status distribution:\")\n",
    "print(final_data.iloc[external_id,:]['RFS_Status'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e657d71f",
   "metadata": {},
   "source": [
    "# Start Graph Building!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e35452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define similarity of two patient\n",
    "def SimScore(a1,a2,s1,s2,l1,l2,h1,h2,t1,t2,n1,n2,m1,m2,tnm1,tnm2): \n",
    "    c_score = 0\n",
    "    h_score = 0\n",
    "    t_score = 0\n",
    "    l_score = 0\n",
    "    # sex and age\n",
    "    if s1 == s2:\n",
    "        c_score +=1\n",
    "    if abs(a1-a2) <= 5:\n",
    "        c_score +=1\n",
    "    \n",
    "    if l1 == l2:\n",
    "        l_score +=1\n",
    "        \n",
    "    if h1 == h2:\n",
    "        h_score +=1\n",
    "    \n",
    "#     if (t1 == t2) and (n1 == n2) and (m1 == m2):\n",
    "#         t_score +=1\n",
    "    \n",
    "    if t1 == t2:\n",
    "        t_score +=1\n",
    "    if n1 == n2:\n",
    "        t_score +=1\n",
    "    if m1 == m2:\n",
    "        t_score +=1\n",
    "\n",
    "\n",
    "\n",
    "    return c_score*t_score*h_score*l_score\n",
    "\n",
    "\n",
    "def adj_matrix(patient_info):\n",
    "    age = patient_info['Age'].to_list()\n",
    "    sex = patient_info['Sex_1_male_2_female'].to_list()\n",
    "    loc = patient_info['Location_1_LUL_2_LLL_3_RUL_4_RML_5_RLL'].to_list()\n",
    "    his = patient_info['Histology_1_Adenocarcinoma_2_SquamousCellCarcinoma_3_Others'].to_list()\n",
    "    pts = patient_info['pT_Stage'].to_list()\n",
    "    pns = patient_info['pN_Stage'].to_list()\n",
    "    pms = patient_info['pM_Stage'].to_list()\n",
    "    tnm = patient_info['pTNM'].to_list()\n",
    "\n",
    "    edge_list=[]\n",
    "    edge_wight=[]\n",
    "    n_sample = len(age)\n",
    "    adj = np.zeros((n_sample, n_sample))\n",
    "    for i in range(n_sample):\n",
    "        for j in range(n_sample):\n",
    "            adj[i,j] = SimScore(age[i],age[j],sex[i],sex[j],loc[i],loc[j],his[i],his[j],\n",
    "                                pts[i],pts[j],pns[i],pns[j], pms[i],pms[j],tnm[i],tnm[j])\n",
    "            if adj[i,j] != 0:\n",
    "                edge_list.append([i,j])\n",
    "                edge_wight.append(adj[i,j])\n",
    "    return adj, edge_list,edge_wight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c77c681",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def graph_bulider(all_data, start_cloumn = 13, event = \"OS_Status\", label = \"OS_Month\"):\n",
    "\n",
    "    # save the labels\n",
    "    norm_label_sh = all_data[label]\n",
    "    # norm_label = (final_data['OS_Month']-np.min(final_data['OS_Month']))/(np.max(final_data['OS_Month'])-np.min(final_data['OS_Month']))\n",
    "    labels_sh = torch.from_numpy(norm_label_sh.to_numpy())\n",
    "    \n",
    "    events_sh = torch.from_numpy(all_data[event].to_numpy())\n",
    "    \n",
    "    adj_sh, edge_list_sh, edge_wight_sh = adj_matrix(all_data)\n",
    "    print(\"the number of nodes in this graph:\",len(norm_label_sh))\n",
    "    print(\"the number of edges in this graph:\",len(edge_list_sh))\n",
    "    print(\"Number of average degree: \",len(edge_list_sh)/len(norm_label_sh) )\n",
    "    \n",
    "    # build graph struture data\n",
    "    g_sh = dgl.DGLGraph()\n",
    "    g_sh.add_nodes(len(labels_sh))\n",
    "    # add nodes\n",
    "    # node_feature = (all_data.iloc[:, 15:]-all_data.iloc[:, 15:].min())/(all_data.iloc[:, 15:].max()- all_data.iloc[:, 15:].min())\n",
    "    node_feature_sh = all_data.iloc[:, start_cloumn:]\n",
    "    # print(node_feature)\n",
    "    node_feature_norm_sh = node_feature_sh.to_numpy()\n",
    "    g_sh.ndata['h'] = torch.from_numpy(node_feature_norm_sh).float()\n",
    "    g_sh.ndata['event'] = events_sh\n",
    "    g_sh.ndata['label'] = labels_sh\n",
    "    g_sh.ndata\n",
    "    # g.adj = adj\n",
    "    # add edges\n",
    "    src, dst = tuple(zip(*edge_list_sh))\n",
    "    g_sh.add_edges(src, dst)\n",
    "    # add edge weight\n",
    "    edge_wight_sh = np.array(edge_wight_sh)\n",
    "    g_sh.edata['w'] = torch.from_numpy(edge_wight_sh).float()\n",
    "    return adj_sh, g_sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7758f5f8",
   "metadata": {},
   "source": [
    "# Network and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ede423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats, dropout=0, activation = None,aggregator_type='mean'):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_feats, hid_feats) \n",
    "        self.conv1 = SAGEConv(in_feats=hid_feats, out_feats=hid_feats, aggregator_type=aggregator_type, activation=activation, feat_drop=dropout)\n",
    "        self.conv2 = SAGEConv(in_feats=hid_feats, out_feats= out_feats, aggregator_type=aggregator_type, activation=activation, feat_drop=dropout)\n",
    "        self.fc2 = nn.Linear(out_feats, 1) \n",
    "    def forward(self, graph, inputs, w_input):\n",
    "        # inputs are features of nodes\n",
    "        h = self.fc1(inputs)\n",
    "        h = self.conv1(graph, h, w_input)\n",
    "        h = self.conv2(graph,h)\n",
    "#         print(h.size())\n",
    "#         output=F.relu(self.fc2(h))\n",
    "        output= self.fc2(h)\n",
    "\n",
    "        return output\n",
    "    \n",
    "class TAG(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats, activation = F.softmax):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_feats, hid_feats) \n",
    "        self.conv1 = TAGConv(in_feats=hid_feats, out_feats= hid_feats, activation=activation)\n",
    "        self.conv2 = TAGConv(in_feats=hid_feats, out_feats= out_feats,  activation=activation)\n",
    "        self.fc2 = nn.Linear(out_feats, 1) \n",
    "        \n",
    "    def forward(self, graph, inputs, w_input):\n",
    "        # inputs are features of nodes\n",
    "        h= self.fc1(inputs)\n",
    "        h = self.conv1(graph, h)\n",
    "        h = self.conv2(graph,h)\n",
    "#         output=F.relu(self.fc2(h))\n",
    "        h=self.fc2(h)\n",
    "        \n",
    "        return h\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats, activation = F.softmax, norm =\"both\"):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_feats, hid_feats) \n",
    "        self.conv1 = GraphConv(in_feats=hid_feats, out_feats= 32, activation=activation, norm=norm)\n",
    "        self.conv2 = GraphConv(in_feats=32, out_feats= out_feats,  activation=activation, norm=norm)\n",
    "        self.fc2 = nn.Linear(out_feats, 1) \n",
    "        \n",
    "    def forward(self, graph, inputs, w_input):\n",
    "        # inputs are features of nodes\n",
    "        h= self.fc1(inputs)\n",
    "        h = self.conv1(graph, h)\n",
    "        h = self.conv2(graph,h)\n",
    "#         output=F.relu(self.fc2(h))\n",
    "        h=self.fc2(h)\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    \n",
    "class SAGE1L(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats, dropout=0, activation = None,aggregator_type='mean'):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_feats, hid_feats) \n",
    "        self.conv1 = SAGEConv(in_feats=hid_feats, out_feats=out_feats, aggregator_type=aggregator_type, activation=activation, feat_drop=dropout)\n",
    "        self.fc2 = nn.Linear(out_feats, 1) \n",
    "    def forward(self, graph, inputs, w_input):\n",
    "        # inputs are features of nodes\n",
    "        h = self.fc1(inputs)\n",
    "        h = self.conv1(graph, h, w_input)\n",
    "\n",
    "        output= self.fc2(h)\n",
    "\n",
    "        return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18751f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regularization(object):\n",
    "    def __init__(self, order, weight_decay):\n",
    "        ''' The initialization of Regularization class\n",
    "        :param order: (int) norm order number\n",
    "        :param weight_decay: (float) weight decay rate\n",
    "        '''\n",
    "        super(Regularization, self).__init__()\n",
    "        self.order = order\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def __call__(self, model):\n",
    "        ''' Performs calculates regularization(self.order) loss for model.\n",
    "        :param model: (torch.nn.Module object)\n",
    "        :return reg_loss: (torch.Tensor) the regularization(self.order) loss\n",
    "        '''\n",
    "        reg_loss = 0\n",
    "        for name, w in model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                reg_loss = reg_loss + torch.norm(w, p=self.order)\n",
    "        reg_loss = self.weight_decay * reg_loss\n",
    "        return reg_loss\n",
    "\n",
    "    \n",
    "class NegativeLogLikelihood(nn.Module):\n",
    "    def __init__(self, l2_reg, device):\n",
    "        super(NegativeLogLikelihood, self).__init__()\n",
    "        self.L2_reg = l2_reg\n",
    "        self.device = device\n",
    "        self.reg = Regularization(order=2, weight_decay=self.L2_reg)\n",
    "\n",
    "    def forward(self, risk_pred, y, e, model):\n",
    "        mask = torch.ones(y.shape[0], y.shape[0]).to(self.device)\n",
    "        mask[(y.T - y) > 0] = 0\n",
    "        log_loss = torch.exp(risk_pred) * mask\n",
    "        log_loss = torch.sum(log_loss, dim=0) / torch.sum(mask, dim=0)\n",
    "        log_loss = torch.log(log_loss).reshape(-1, 1)\n",
    "        neg_log_loss = -torch.sum((risk_pred-log_loss) * e) / torch.sum(e)\n",
    "        l2_loss = self.reg(model)\n",
    "        return neg_log_loss + l2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ebd2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_all, g_all = graph_bulider(final_data)\n",
    "adj_sh, g_sh = graph_bulider(all_data)\n",
    "# g_sh = dgl.node_subgraph(g_all, list(range(len(all_data))))\n",
    "g_external = dgl.node_subgraph(g_all, list(range(len(all_data),len(final_data))))\n",
    "# g_val = graph_bulider(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d4e1cf",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a4aad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g, g_all, model,device, save_dic, idx_train,idx_val, idx_test, total_epoch=100, patience=5, lr=0.001, reg_l2=0, weight_decay=0.0001):\n",
    "    model_name = save_dic['model']+str(save_dic['hid_feats'])+str(save_dic['out_feats'])+str(save_dic['reg_l2'])+save_dic[\"aggregator_type\"]\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr, weight_decay=weight_decay)\n",
    "    best_cindex = 0\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.5, patience=patience, min_lr = 0.0001, verbose=True)\n",
    "    criterion = NegativeLogLikelihood(reg_l2, device).to(device) \n",
    "    model = model.to(device) \n",
    "    features = g.ndata['h'].to(device) \n",
    "    e_feature = g.edata['w'].to(device) \n",
    "    labels = g.ndata['label'].to(device) \n",
    "    events = g.ndata['event'].to(device) \n",
    "    g_all= g_all.to(device)  \n",
    "    g = g.to(device) \n",
    "    t_total = time.time()\n",
    "    with tqdm(range(total_epoch)) as t:\n",
    "        for epoch in t:\n",
    "            t.set_description('Epoch %d' % epoch)\n",
    "            start = time.time()\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(g, features,e_feature)\n",
    "            # Compute loss\n",
    "            # Note that you should only compute the losses of the nodes in the training set.\n",
    "            loss_train = criterion(output[idx_train], labels[idx_train],events[idx_train], model).clone()\n",
    "            auc_train = c_index(-output[idx_train], labels[idx_train],events[idx_train])\n",
    "            \n",
    "            loss_train.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            \n",
    "            model.eval()\n",
    "            val_output = model(g, features,e_feature)\n",
    "            loss_val = criterion(val_output[idx_val], labels[idx_val],events[idx_val], model).clone()\n",
    "            scheduler.step(loss_val)\n",
    "            \n",
    "            auc_val = c_index(-val_output[idx_val], labels[idx_val],events[idx_val])\n",
    "            auc_test = c_index(-val_output[idx_test], labels[idx_test], events[idx_test])\n",
    "            exter_val_output = model(g_all, g_all.ndata['h'],g_all.edata['w'])\n",
    "\n",
    "            t.set_postfix(\n",
    "                  {\"train_loss\":loss_train.item(), \"val_loss\":loss_val.item(),\n",
    "                  \"train_cindex\":auc_train.item(), \"val_auc\":auc_val.item(),\n",
    "                \"lr\":optimizer.param_groups[0]['lr']}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8caa870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
