{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9668e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from statannot import add_stat_annotation\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "import dgl.function as fn\n",
    "from dgl.nn.pytorch import GraphConv, SAGEConv, TAGConv\n",
    "\n",
    "from deepsurv_utils import c_index, adjust_learning_rate\n",
    "# from loss import NegativeLogLikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "782cd24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_190523/2336714941.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_info['pT_Stage']=patient_info['pT_Stage'].replace({\"T1a\":0, \"T1b\":0, \"T1c\":0, \"T2a\":1,\"T2b\":1,\"T3\":2})\n",
      "/tmp/ipykernel_190523/2336714941.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_info['pM_Stage']=patient_info['pM_Stage'].replace({\"M1a\":1})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1705\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# load SHPH data\n",
    "all_patient_info = pd.read_csv(\"/home/jielian/lung-graph-project/data/csv/SPH0812.csv\")\n",
    "all_patient_info = all_patient_info[['folder_name', 'Sex_1_male_2_female', 'Age',\n",
    "       'Location_1_LUL_2_LLL_3_RUL_4_RML_5_RLL','Histology_1_Adenocarcinoma_2_SquamousCellCarcinoma_3_Others',\n",
    "        'pT_Stage', 'pN_Stage', 'pM_Stage', 'pTNM', 'RFS_Status', 'RFS_Month',\n",
    "       'OS_Status', 'OS_Month']]\n",
    "stage1 = list(np.load(\"/home/jielian/lung-graph-project/data/seg_image/labels/name_stage1.npy\"))\n",
    "stage2 = list(np.load(\"/home/jielian/lung-graph-project/data/seg_image/labels/name_stage2.npy\"))\n",
    "patint_list = [*stage1, *stage2]\n",
    "patient_info = all_patient_info[all_patient_info['folder_name'].isin(patint_list)]\n",
    "\n",
    "patient_info['pT_Stage']=patient_info['pT_Stage'].replace({\"T1a\":0, \"T1b\":0, \"T1c\":0, \"T2a\":1,\"T2b\":1,\"T3\":2})\n",
    "patient_info['pM_Stage']=patient_info['pM_Stage'].replace({\"M1a\":1})\n",
    "\n",
    "feature_files = os.listdir(\"trans_feature\")\n",
    "\n",
    "data = []\n",
    "name = []\n",
    "for feature_name in feature_files:\n",
    "    path = \"trans_feature/\"+feature_name\n",
    "    name.append(int(feature_name[:-4]))\n",
    "    feature = list(np.load(path, allow_pickle=True))\n",
    "    data.append(feature)\n",
    "feature_data = pd.DataFrame(data)\n",
    "feature_data['folder_name']=name\n",
    "all_data = patient_info.merge(feature_data, how='left', on='folder_name')\n",
    "print(len(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80aa6bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n"
     ]
    }
   ],
   "source": [
    "# load external information\n",
    "external_info = pd.read_csv(\"/home/jielian/lung-graph-project/Tumor_tranformer/data_ind/External_label.csv\")\n",
    "external_patint_list = external_info['Patient']\n",
    "external_info=external_info.rename(columns={\"Patient\":\"folder_name\"})\n",
    "external_info=external_info.rename(columns={\"Histology\":\"Histology_1_Adenocarcinoma_2_SquamousCellCarcinoma_3_Others\"})\n",
    "external_info['pT_Stage']=external_info['pT_Stage'].replace({\"Tis\":0,\"T1a\":0, \"T1b\":0, \"T1c\":0, \"T2a\":1,\"T2b\":1,\"T3\":2,\"T4\":3 })\n",
    "\n",
    "external_feature_files = os.listdir(\"trans_feature_val\")\n",
    "\n",
    "external_data = []\n",
    "for feature_name in external_patint_list:\n",
    "    path = \"trans_feature_val/\"+feature_name+\".npy\"\n",
    "    feature = list(np.load(path, allow_pickle=True))\n",
    "    external_data.append(feature)\n",
    "external_feature_data = pd.DataFrame(external_data)\n",
    "external_feature_data['folder_name']=external_patint_list\n",
    "external_all_data = external_info.merge(external_feature_data, how='left', on='folder_name')\n",
    "print(len(external_all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "186cfc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #merge the dataset\n",
    "frames = [all_data, external_all_data]\n",
    "final_data = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6eae9c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training OS distribution:\n",
      "0    991\n",
      "1    287\n",
      "Name: OS_Status, dtype: int64\n",
      "validation OS distribution:\n",
      "0    173\n",
      "1     41\n",
      "Name: OS_Status, dtype: int64\n",
      "test OS distribution:\n",
      "0    169\n",
      "1     44\n",
      "Name: OS_Status, dtype: int64\n",
      "External OS distribution:\n",
      "0    102\n",
      "1     57\n",
      "Name: OS_Status, dtype: int64\n",
      "training RFS_Status distribution:\n",
      "0    931\n",
      "1    347\n",
      "Name: RFS_Status, dtype: int64\n",
      "validation RFS_Status distribution:\n",
      "0    158\n",
      "1     56\n",
      "Name: RFS_Status, dtype: int64\n",
      "test RFS_Status distribution:\n",
      "0    154\n",
      "1     59\n",
      "Name: RFS_Status, dtype: int64\n",
      "External RFS_Status distribution:\n",
      "0    113\n",
      "1     46\n",
      "Name: RFS_Status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_id = np.load(\"data_ind/train_index.npy\",allow_pickle=True)\n",
    "val_id = np.load(\"data_ind/val_index.npy\", allow_pickle=True)\n",
    "test_id = np.load(\"data_ind/test_index.npy\",allow_pickle=True)\n",
    "external_id = np.array(range(len(all_data),len(final_data)))\n",
    "idx_train = torch.LongTensor(train_id)\n",
    "idx_val = torch.LongTensor(val_id)\n",
    "idx_test = torch.LongTensor(test_id)\n",
    "idx_external_val = torch.LongTensor(external_id)\n",
    "\n",
    "print(\"training OS distribution:\")\n",
    "print(all_data.iloc[train_id,:]['OS_Status'].value_counts())\n",
    "print(\"validation OS distribution:\")\n",
    "print(all_data.iloc[val_id,:]['OS_Status'].value_counts())\n",
    "print(\"test OS distribution:\")\n",
    "print(all_data.iloc[test_id,:]['OS_Status'].value_counts())\n",
    "print(\"External OS distribution:\")\n",
    "print(final_data.iloc[external_id,:]['OS_Status'].value_counts())\n",
    "\n",
    "\n",
    "print(\"training RFS_Status distribution:\")\n",
    "print(all_data.iloc[train_id,:]['RFS_Status'].value_counts())\n",
    "print(\"validation RFS_Status distribution:\")\n",
    "print(all_data.iloc[val_id,:]['RFS_Status'].value_counts())\n",
    "print(\"test RFS_Status distribution:\")\n",
    "print(all_data.iloc[test_id,:]['RFS_Status'].value_counts())\n",
    "print(\"External RFS_Status distribution:\")\n",
    "print(final_data.iloc[external_id,:]['RFS_Status'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e657d71f",
   "metadata": {},
   "source": [
    "# Start Graph Building!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6e35452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define similarity of two patient\n",
    "def SimScore(a1,a2,s1,s2,l1,l2,h1,h2,t1,t2,n1,n2,m1,m2,tnm1,tnm2): \n",
    "    c_score = 0\n",
    "    h_score = 0\n",
    "    t_score = 0\n",
    "    # sex and age\n",
    "    if s1 == s2:\n",
    "        c_score +=1\n",
    "    if abs(a1-a2) <= 5:\n",
    "        c_score +=1\n",
    "    \n",
    "    if l1 == l2:\n",
    "        h_score +=1\n",
    "    if h1 == h2:\n",
    "        h_score +=1\n",
    "    \n",
    "    if t1 == t2:\n",
    "        t_score +=1\n",
    "    if n1 == n2:\n",
    "        t_score +=1\n",
    "    if m1 == m2:\n",
    "        t_score +=1\n",
    "#     if tnm1 == tnm2:\n",
    "#         t_score +=1\n",
    "\n",
    "    return c_score*t_score*h_score\n",
    "\n",
    "# def SimScore(a1,a2,s1,s2,l1,l2,h1,h2,t1,t2,n1,n2,m1,m2,tnm1,tnm2): \n",
    "\n",
    "#     return c_score*t_score*h_score\n",
    "\n",
    "\n",
    "def adj_matrix(patient_info):\n",
    "    age = patient_info['Age'].to_list()\n",
    "    sex = patient_info['Sex_1_male_2_female'].to_list()\n",
    "    loc = patient_info['Location_1_LUL_2_LLL_3_RUL_4_RML_5_RLL'].to_list()\n",
    "    his = patient_info['Histology_1_Adenocarcinoma_2_SquamousCellCarcinoma_3_Others'].to_list()\n",
    "    pts = patient_info['pT_Stage'].to_list()\n",
    "    pns = patient_info['pN_Stage'].to_list()\n",
    "    pms = patient_info['pM_Stage'].to_list()\n",
    "    tnm = patient_info['pTNM'].to_list()\n",
    "\n",
    "    edge_list=[]\n",
    "    edge_wight=[]\n",
    "    n_sample = len(age)\n",
    "    adj = np.zeros((n_sample, n_sample))\n",
    "    for i in range(n_sample):\n",
    "        for j in range(n_sample):\n",
    "            adj[i,j] = SimScore(age[i],age[j],sex[i],sex[j],loc[i],loc[j],his[i],his[j],\n",
    "                                pts[i],pts[j],pns[i],pns[j], pms[i],pms[j],tnm[i],tnm[j])\n",
    "            if adj[i,j] != 0:\n",
    "                edge_list.append([i,j])\n",
    "                edge_wight.append(adj[i,j])\n",
    "    return adj, edge_list,edge_wight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c77c681",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def graph_bulider(all_data, start_cloumn = 13, event = \"OS_Status\", label = \"OS_Month\"):\n",
    "\n",
    "    # save the labels\n",
    "    norm_label_sh = all_data[label]\n",
    "    # norm_label = (final_data['OS_Month']-np.min(final_data['OS_Month']))/(np.max(final_data['OS_Month'])-np.min(final_data['OS_Month']))\n",
    "    labels_sh = torch.from_numpy(norm_label_sh.to_numpy())\n",
    "    \n",
    "    events_sh = torch.from_numpy(all_data[event].to_numpy())\n",
    "    \n",
    "    adj_sh, edge_list_sh, edge_wight_sh = adj_matrix(all_data)\n",
    "    print(\"the number of nodes in this graph:\",len(norm_label_sh))\n",
    "    print(\"the number of edges in this graph:\",len(edge_list_sh))\n",
    "    print(\"Number of average degree: \",len(edge_list_sh)/len(norm_label_sh) )\n",
    "    \n",
    "    # build graph struture data\n",
    "    g_sh = dgl.DGLGraph()\n",
    "    g_sh.add_nodes(len(labels_sh))\n",
    "    # add nodes\n",
    "    # node_feature = (all_data.iloc[:, 15:]-all_data.iloc[:, 15:].min())/(all_data.iloc[:, 15:].max()- all_data.iloc[:, 15:].min())\n",
    "    node_feature_sh = all_data.iloc[:, start_cloumn:]\n",
    "    # print(node_feature)\n",
    "    node_feature_norm_sh = node_feature_sh.to_numpy()\n",
    "    g_sh.ndata['h'] = torch.from_numpy(node_feature_norm_sh).float()\n",
    "    g_sh.ndata['event'] = events_sh\n",
    "    g_sh.ndata['label'] = labels_sh\n",
    "    g_sh.ndata\n",
    "    # g.adj = adj\n",
    "    # add edges\n",
    "    src, dst = tuple(zip(*edge_list_sh))\n",
    "    g_sh.add_edges(src, dst)\n",
    "    # add edge weight\n",
    "    edge_wight_sh = np.array(edge_wight_sh)\n",
    "    g_sh.edata['w'] = torch.from_numpy(edge_wight_sh).float()\n",
    "    return adj_sh, g_sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7758f5f8",
   "metadata": {},
   "source": [
    "# Network and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ede423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats, dropout=0, activation = None,aggregator_type='mean'):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_feats, hid_feats) \n",
    "        self.conv1 = SAGEConv(in_feats=hid_feats, out_feats=hid_feats, aggregator_type=aggregator_type, activation=activation, feat_drop=dropout)\n",
    "        self.conv2 = SAGEConv(in_feats=hid_feats, out_feats= out_feats, aggregator_type=aggregator_type, activation=activation, feat_drop=dropout)\n",
    "        self.fc2 = nn.Linear(out_feats, 1) \n",
    "    def forward(self, graph, inputs, w_input):\n",
    "        # inputs are features of nodes\n",
    "        h = self.fc1(inputs)\n",
    "        h = self.conv1(graph, h, w_input)\n",
    "        h = self.conv2(graph,h)\n",
    "#         print(h.size())\n",
    "#         output=F.relu(self.fc2(h))\n",
    "        output= self.fc2(h)\n",
    "\n",
    "        return output\n",
    "\n",
    "class TAG(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats, activation = F.softmax):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_feats, hid_feats) \n",
    "        self.conv1 = TAGConv(in_feats=hid_feats, out_feats= hid_feats, activation=activation)\n",
    "        self.conv2 = TAGConv(in_feats=hid_feats, out_feats= out_feats,  activation=activation)\n",
    "        self.fc2 = nn.Linear(out_feats, 1) \n",
    "        \n",
    "    def forward(self, graph, inputs, w_input):\n",
    "        # inputs are features of nodes\n",
    "        h= self.fc1(inputs)\n",
    "        h = self.conv1(graph, h)\n",
    "        h = self.conv2(graph,h)\n",
    "#         output=F.relu(self.fc2(h))\n",
    "        h=self.fc2(h)\n",
    "        \n",
    "        return h\n",
    "\n",
    "    \n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats, activation = F.softmax, norm =\"both\"):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_feats, hid_feats) \n",
    "        self.conv1 = GraphConv(in_feats=hid_feats, out_feats= 32, activation=activation, norm=norm)\n",
    "        self.conv2 = GraphConv(in_feats=32, out_feats= out_feats,  activation=activation, norm=norm)\n",
    "        self.fc2 = nn.Linear(out_feats, 1) \n",
    "        \n",
    "    def forward(self, graph, inputs, w_input):\n",
    "        # inputs are features of nodes\n",
    "        h= self.fc1(inputs)\n",
    "        h = self.conv1(graph, h)\n",
    "        h = self.conv2(graph,h)\n",
    "#         output=F.relu(self.fc2(h))\n",
    "        h=self.fc2(h)\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    \n",
    "class SAGE1L(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats, dropout=0, activation = None,aggregator_type='mean'):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_feats, hid_feats) \n",
    "        self.conv1 = SAGEConv(in_feats=hid_feats, out_feats=out_feats, aggregator_type=aggregator_type, activation=activation, feat_drop=dropout)\n",
    "        self.fc2 = nn.Linear(out_feats, 1) \n",
    "    def forward(self, graph, inputs, w_input):\n",
    "        # inputs are features of nodes\n",
    "        h = self.fc1(inputs)\n",
    "        h = self.conv1(graph, h, w_input)\n",
    "\n",
    "        output= self.fc2(h)\n",
    "\n",
    "        return output\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d18751f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regularization(object):\n",
    "    def __init__(self, order, weight_decay):\n",
    "        ''' The initialization of Regularization class\n",
    "        :param order: (int) norm order number\n",
    "        :param weight_decay: (float) weight decay rate\n",
    "        '''\n",
    "        super(Regularization, self).__init__()\n",
    "        self.order = order\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def __call__(self, model):\n",
    "        ''' Performs calculates regularization(self.order) loss for model.\n",
    "        :param model: (torch.nn.Module object)\n",
    "        :return reg_loss: (torch.Tensor) the regularization(self.order) loss\n",
    "        '''\n",
    "        reg_loss = 0\n",
    "        for name, w in model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                reg_loss = reg_loss + torch.norm(w, p=self.order)\n",
    "        reg_loss = self.weight_decay * reg_loss\n",
    "        return reg_loss\n",
    "\n",
    "    \n",
    "class NegativeLogLikelihood(nn.Module):\n",
    "    def __init__(self, l2_reg, device):\n",
    "        super(NegativeLogLikelihood, self).__init__()\n",
    "        self.L2_reg = l2_reg\n",
    "        self.device = device\n",
    "        self.reg = Regularization(order=2, weight_decay=self.L2_reg)\n",
    "\n",
    "    def forward(self, risk_pred, y, e, model):\n",
    "        mask = torch.ones(y.shape[0], y.shape[0]).to(self.device)\n",
    "        mask[(y.T - y) > 0] = 0\n",
    "        log_loss = torch.exp(risk_pred) * mask\n",
    "        log_loss = torch.sum(log_loss, dim=0) / torch.sum(mask, dim=0)\n",
    "        log_loss = torch.log(log_loss).reshape(-1, 1)\n",
    "        neg_log_loss = -torch.sum((risk_pred-log_loss) * e) / torch.sum(e)\n",
    "        l2_loss = self.reg(model)\n",
    "        return neg_log_loss + l2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80ebd2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of nodes in this graph: 1864\n",
      "the number of edges in this graph: 1564278\n",
      "Number of average degree:  839.2049356223176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jielian/anaconda3/envs/gnn/lib/python3.8/site-packages/dgl/base.py:45: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  return warnings.warn(message, category=category, stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of nodes in this graph: 1705\n",
      "the number of edges in this graph: 1343795\n",
      "Number of average degree:  788.149560117302\n"
     ]
    }
   ],
   "source": [
    "adj_all, g_all = graph_bulider(final_data)\n",
    "adj_sh, g_sh = graph_bulider(all_data)\n",
    "# g_sh = dgl.node_subgraph(g_all, list(range(len(all_data))))\n",
    "g_external = dgl.node_subgraph(g_all, list(range(len(all_data),len(final_data))))\n",
    "# g_val = graph_bulider(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04b814b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(g, g_all, model, save_dic, idx_train,idx_val, idx_test, total_epoch=100, patience=5, lr=0.001, reg_l2=0, weight_decay=0.0001):\n",
    "#     model_name = save_dic['model']+str(save_dic['hid_feats'])+str(save_dic['out_feats'])+str(save_dic['reg_l2'])+save_dic[\"aggregator_type\"]\n",
    "#     optimizer = torch.optim.Adam(model.parameters(),lr=lr, weight_decay=weight_decay)\n",
    "#     best_cindex = 0\n",
    "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.5, patience=patience, min_lr = 0.0001, verbose=True)\n",
    "#     criterion = NegativeLogLikelihood(reg_l2)\n",
    "#     features = g.ndata['h']\n",
    "#     e_feature = g.edata['w']\n",
    "#     labels = g.ndata['label']\n",
    "#     events = g.ndata['event']\n",
    "#     t_total = time.time()\n",
    "#     with tqdm(range(total_epoch)) as t:\n",
    "#         for epoch in t:\n",
    "#             t.set_description('Epoch %d' % epoch)\n",
    "#             start = time.time()\n",
    "#             model.train()\n",
    "#             optimizer.zero_grad()\n",
    "#             output = model(g, features,e_feature)\n",
    "#             # Compute loss\n",
    "#             # Note that you should only compute the losses of the nodes in the training set.\n",
    "#             loss_train = criterion(output[idx_train], labels[idx_train],events[idx_train], model).clone()\n",
    "#             auc_train = c_index(-output[idx_train], labels[idx_train],events[idx_train])\n",
    "            \n",
    "#             loss_train.backward(retain_graph=True)\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             model.eval()\n",
    "#             val_output = model(g, features,e_feature)\n",
    "#             loss_val = criterion(val_output[idx_val], labels[idx_val],events[idx_val], model).clone()\n",
    "#             scheduler.step(loss_val)\n",
    "            \n",
    "#             auc_val = c_index(-val_output[idx_val], labels[idx_val],events[idx_val])\n",
    "#             auc_test = c_index(-val_output[idx_test], labels[idx_test], events[idx_test])\n",
    "            \n",
    "#             model.eval()\n",
    "#             exter_val_output = model(g_all, g_all.ndata['h'],g_all.edata['w'])\n",
    "            \n",
    "#             if auc_test>best_cindex:\n",
    "#                 print(\"Curent best Test AUC:\",auc_test)\n",
    "#                 best_cindex = auc_test.item()\n",
    "#                 print(\"Its' val AUC :\", auc_val)\n",
    "#                 auc_external = c_index(-exter_val_output, g_all.ndata['label'] ,g_all.ndata['event'])\n",
    "#                 print(\"Its external  AUC:\", auc_external) \n",
    "                \n",
    "#                 if best_cindex > 0.7 and auc_external>0.65:\n",
    "#                     torch.save(model.state_dict(), os.path.join(save_dic['save_path'], \n",
    "#                                 \"{}_ep{}_val{}_test{}_exte{}.pth.gz\".format(model_name,epoch, np.around(auc_val,3),np.around(auc_test,3),np.around(auc_external,3))))\n",
    "#                     result_path = os.path.join(save_dic['save_path'], \"{}_ep{}_val{}_test{}_exte{}.npy\".format(model_name,epoch, np.around(auc_val,3),np.around(auc_test,3),np.around(auc_external,3)))\n",
    "#                     output_cpu = val_output.cpu().detach().numpy()\n",
    "#                     np.save(result_path, output_cpu)\n",
    "                    \n",
    "\n",
    "#             t.set_postfix(\n",
    "#                   {\"train_loss\":loss_train.item(), \"val_loss\":loss_val.item(),\n",
    "#                   \"train_cindex\":auc_train.item(), \"val_auc\":auc_val.item(),\n",
    "#                 \"lr\":optimizer.param_groups[0]['lr']}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d4e1cf",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25a4aad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g, g_all, model,device, save_dic, idx_train,idx_val, idx_test, total_epoch=100, patience=5, lr=0.001, reg_l2=0, weight_decay=0.0001):\n",
    "    model_name = save_dic['model']+str(save_dic['hid_feats'])+str(save_dic['out_feats'])+str(save_dic['reg_l2'])+save_dic[\"aggregator_type\"]\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr, weight_decay=weight_decay)\n",
    "    best_cindex = 0\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.5, patience=patience, min_lr = 0.0001, verbose=True)\n",
    "    criterion = NegativeLogLikelihood(reg_l2, device).to(device) \n",
    "    model = model.to(device) \n",
    "    features = g.ndata['h'].to(device) \n",
    "    e_feature = g.edata['w'].to(device) \n",
    "    labels = g.ndata['label'].to(device) \n",
    "    events = g.ndata['event'].to(device) \n",
    "    g_all= g_all.to(device)  \n",
    "    g = g.to(device) \n",
    "    t_total = time.time()\n",
    "    with tqdm(range(total_epoch)) as t:\n",
    "        for epoch in t:\n",
    "            t.set_description('Epoch %d' % epoch)\n",
    "            start = time.time()\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(g, features,e_feature)\n",
    "            # Compute loss\n",
    "            # Note that you should only compute the losses of the nodes in the training set.\n",
    "            loss_train = criterion(output[idx_train], labels[idx_train],events[idx_train], model).clone()\n",
    "            auc_train = c_index(-output[idx_train], labels[idx_train],events[idx_train])\n",
    "            \n",
    "            loss_train.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            \n",
    "            model.eval()\n",
    "            val_output = model(g, features,e_feature)\n",
    "            loss_val = criterion(val_output[idx_val], labels[idx_val],events[idx_val], model).clone()\n",
    "            scheduler.step(loss_val)\n",
    "            \n",
    "            auc_val = c_index(-val_output[idx_val], labels[idx_val],events[idx_val])\n",
    "            auc_test = c_index(-val_output[idx_test], labels[idx_test], events[idx_test])\n",
    "            exter_val_output = model(g_all, g_all.ndata['h'],g_all.edata['w'])\n",
    "            \n",
    "            if auc_test>best_cindex:\n",
    "                print(\"Curent best Test AUC:\",auc_test)\n",
    "                best_cindex = auc_test.item()\n",
    "                print(\"Its' val AUC :\", auc_val)\n",
    "                auc_external = c_index(-exter_val_output, g_all.ndata['label'] ,g_all.ndata['event'])\n",
    "                print(\"Its external  AUC:\", auc_external) \n",
    "                \n",
    "                if best_cindex > 0.72 and auc_external>0.65:\n",
    "                    torch.save(model.state_dict(), os.path.join(save_dic['save_path'], \n",
    "                                \"{}_ep{}_val{}_test{}_exte{}.pth.gz\".format(model_name,epoch, np.around(auc_val,3),np.around(auc_test,3),np.around(auc_external,3))))\n",
    "                    result_path = os.path.join(save_dic['save_path'], \"{}_ep{}_val{}_test{}_exte{}.npy\".format(model_name,epoch, np.around(auc_val,3),np.around(auc_test,3),np.around(auc_external,3)))\n",
    "                    output_cpu = val_output.cpu().detach().numpy()\n",
    "                    val_cpu = exter_val_output.cpu().detach().numpy()\n",
    "                    np.save(result_path, [output_cpu, val_cpu])\n",
    "                    \n",
    "\n",
    "            t.set_postfix(\n",
    "                  {\"train_loss\":loss_train.item(), \"val_loss\":loss_val.item(),\n",
    "                  \"train_cindex\":auc_train.item(), \"val_auc\":auc_val.item(),\n",
    "                \"lr\":optimizer.param_groups[0]['lr']}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "269f9d42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range(100):\n",
    "#     save_dic = {\"model\":\"GCN\", \n",
    "#                 \"hid_feats\":256, \n",
    "#                 'out_feats':64, \n",
    "#                 'reg_l2':0.00005,\n",
    "#                 \"aggregator_type\":'mean',\n",
    "#                 \"save_path\":\"/home/jielian/lung-graph-project/Tumor_tranformer/logs/OS_new_data/\"}\n",
    "#     sage96_norm = GCN(g_sh.ndata['h'].shape[1],hid_feats=save_dic[\"hid_feats\"],out_feats=save_dic['out_feats'], \n",
    "#                        activation = F.relu)\n",
    "#     train(g_sh, g_external, sage96_norm, save_dic, idx_train,idx_val, idx_test, 100, patience=10, reg_l2=save_dic[\"reg_l2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a811f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for i in tqdm(range(100)):  \n",
    "# save_dic = {\"model\":\"SAGE1L\", \n",
    "#             \"hid_feats\":64, \n",
    "#             'out_feats':16, \n",
    "#             'reg_l2':0.00001,\n",
    "#             \"aggregator_type\":'lstm',\n",
    "#             \"save_path\":\"/home/jielian/lung-graph-project/Tumor_tranformer/logs/OS_new_data/\"}\n",
    "# model = SAGE1L(g_sh.ndata['h'].shape[1],hid_feats=save_dic[\"hid_feats\"],out_feats=save_dic['out_feats'], \n",
    "#                    activation = F.leaky_relu, aggregator_type=save_dic['aggregator_type'])\n",
    "# train(g_sh, g_external, model, device, save_dic, idx_train,idx_val, idx_test, 50, patience=10, reg_l2=save_dic[\"reg_l2\"])\n",
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cebde23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=2.5, val_loss=27.3, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=2.5, val_loss=27.3, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=145, val_loss=14.3, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=145, val_loss=14.3, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=92.3, val_loss=51.5, train_cin\u001b[A\n",
      "Epoch 2:   6%| | 3/50 [00:00<00:02, 22.10it/s, train_loss=92.3, val_loss=51.5, t\u001b[A\n",
      "Epoch 3:   6%| | 3/50 [00:00<00:02, 22.10it/s, train_loss=92.3, val_loss=51.5, t\u001b[A\n",
      "Epoch 3:   6%| | 3/50 [00:00<00:02, 22.10it/s, train_loss=258, val_loss=5.5, tra\u001b[A\n",
      "Epoch 4:   6%| | 3/50 [00:00<00:02, 22.10it/s, train_loss=258, val_loss=5.5, tra\u001b[A\n",
      "Epoch 4:   6%| | 3/50 [00:00<00:02, 22.10it/s, train_loss=29, val_loss=30.3, tra\u001b[A\n",
      "Epoch 5:   6%| | 3/50 [00:00<00:02, 22.10it/s, train_loss=29, val_loss=30.3, tra\u001b[A\n",
      "Epoch 5:   6%| | 3/50 [00:00<00:02, 22.10it/s, train_loss=176, val_loss=0.669, t\u001b[A\n",
      "Epoch 6:   6%| | 3/50 [00:00<00:02, 22.10it/s, train_loss=176, val_loss=0.669, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.3636819035691922\n",
      "Its' val AUC : 0.31847220334375426\n",
      "Its external  AUC: 0.40743658421301404\n",
      "Curent best Test AUC: 0.7466499686912962\n",
      "Its' val AUC : 0.6683430746228082\n",
      "Its external  AUC: 0.6354183078619821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6:   6%| | 3/50 [00:00<00:02, 22.10it/s, train_loss=4.45, val_loss=4.79, t\u001b[A\n",
      "Epoch 6:  14%|▏| 7/50 [00:00<00:01, 27.33it/s, train_loss=4.45, val_loss=4.79, t\u001b[A\n",
      "Epoch 7:  14%|▏| 7/50 [00:00<00:01, 27.33it/s, train_loss=4.45, val_loss=4.79, t\u001b[A\n",
      "Epoch 7:  14%|▏| 7/50 [00:00<00:01, 27.33it/s, train_loss=24.1, val_loss=4.14, t\u001b[A\n",
      "Epoch 8:  14%|▏| 7/50 [00:00<00:01, 27.33it/s, train_loss=24.1, val_loss=4.14, t\u001b[A\n",
      "Epoch 8:  14%|▏| 7/50 [00:00<00:01, 27.33it/s, train_loss=20.5, val_loss=0.302, \u001b[A\n",
      "Epoch 9:  14%|▏| 7/50 [00:00<00:01, 27.33it/s, train_loss=20.5, val_loss=0.302, \u001b[A\n",
      "Epoch 9:  14%|▏| 7/50 [00:00<00:01, 27.33it/s, train_loss=1.74, val_loss=2.39, t\u001b[A\n",
      "Epoch 10:  14%|▏| 7/50 [00:00<00:01, 27.33it/s, train_loss=1.74, val_loss=2.39, \u001b[A\n",
      "Epoch 10:  14%|▏| 7/50 [00:00<00:01, 27.33it/s, train_loss=12.9, val_loss=1.48, \u001b[A\n",
      "Epoch 10:  22%|▏| 11/50 [00:00<00:01, 29.48it/s, train_loss=12.9, val_loss=1.48,\u001b[A\n",
      "Epoch 11:  22%|▏| 11/50 [00:00<00:01, 29.48it/s, train_loss=12.9, val_loss=1.48,\u001b[A\n",
      "Epoch 11:  22%|▏| 11/50 [00:00<00:01, 29.48it/s, train_loss=7.41, val_loss=0.594\u001b[A\n",
      "Epoch 12:  22%|▏| 11/50 [00:00<00:01, 29.48it/s, train_loss=7.41, val_loss=0.594\u001b[A\n",
      "Epoch 12:  22%|▏| 11/50 [00:00<00:01, 29.48it/s, train_loss=3.45, val_loss=1.34,\u001b[A\n",
      "Epoch 13:  22%|▏| 11/50 [00:00<00:01, 29.48it/s, train_loss=3.45, val_loss=1.34,\u001b[A\n",
      "Epoch 13:  22%|▏| 11/50 [00:00<00:01, 29.48it/s, train_loss=7.34, val_loss=0.136\u001b[A\n",
      "Epoch 14:  22%|▏| 11/50 [00:00<00:01, 29.48it/s, train_loss=7.34, val_loss=0.136\u001b[A\n",
      "Epoch 14:  22%|▏| 11/50 [00:00<00:01, 29.48it/s, train_loss=0.91, val_loss=1.27,\u001b[A\n",
      "Epoch 14:  30%|▎| 15/50 [00:00<00:01, 30.46it/s, train_loss=0.91, val_loss=1.27,\u001b[A\n",
      "Epoch 15:  30%|▎| 15/50 [00:00<00:01, 30.46it/s, train_loss=0.91, val_loss=1.27,\u001b[A\n",
      "Epoch 15:  30%|▎| 15/50 [00:00<00:01, 30.46it/s, train_loss=6.87, val_loss=0.146\u001b[A\n",
      "Epoch 16:  30%|▎| 15/50 [00:00<00:01, 30.46it/s, train_loss=6.87, val_loss=0.146\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.754539762053851\n",
      "Its' val AUC : 0.6827511213809977\n",
      "Its external  AUC: 0.6198203875846857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16:  30%|▎| 15/50 [00:00<00:01, 30.46it/s, train_loss=1.13, val_loss=0.879\u001b[A\n",
      "Epoch 17:  30%|▎| 15/50 [00:00<00:01, 30.46it/s, train_loss=1.13, val_loss=0.879\u001b[A\n",
      "Epoch 17:  30%|▎| 15/50 [00:00<00:01, 30.46it/s, train_loss=4.48, val_loss=0.447\u001b[A\n",
      "Epoch 18:  30%|▎| 15/50 [00:00<00:01, 30.46it/s, train_loss=4.48, val_loss=0.447\u001b[A\n",
      "Epoch 18:  30%|▎| 15/50 [00:00<00:01, 30.46it/s, train_loss=2.43, val_loss=0.443\u001b[A\n",
      "Epoch 18:  38%|▍| 19/50 [00:00<00:00, 31.26it/s, train_loss=2.43, val_loss=0.443\u001b[A\n",
      "Epoch 19:  38%|▍| 19/50 [00:00<00:00, 31.26it/s, train_loss=2.43, val_loss=0.443\u001b[A\n",
      "Epoch 19:  38%|▍| 19/50 [00:00<00:00, 31.26it/s, train_loss=2.63, val_loss=0.523\u001b[A\n",
      "Epoch 20:  38%|▍| 19/50 [00:00<00:00, 31.26it/s, train_loss=2.63, val_loss=0.523\u001b[A\n",
      "Epoch 20:  38%|▍| 19/50 [00:00<00:00, 31.26it/s, train_loss=3, val_loss=0.123, t\u001b[A\n",
      "Epoch 21:  38%|▍| 19/50 [00:00<00:00, 31.26it/s, train_loss=3, val_loss=0.123, t\u001b[A\n",
      "Epoch 21:  38%|▍| 19/50 [00:00<00:00, 31.26it/s, train_loss=0.798, val_loss=0.29\u001b[A\n",
      "Epoch 22:  38%|▍| 19/50 [00:00<00:00, 31.26it/s, train_loss=0.798, val_loss=0.29\u001b[A\n",
      "Epoch 22:  38%|▍| 19/50 [00:00<00:00, 31.26it/s, train_loss=1.53, val_loss=0.46,\u001b[A\n",
      "Epoch 22:  46%|▍| 23/50 [00:00<00:00, 31.75it/s, train_loss=1.53, val_loss=0.46,\u001b[A\n",
      "Epoch 23:  46%|▍| 23/50 [00:00<00:00, 31.75it/s, train_loss=1.53, val_loss=0.46,\u001b[A\n",
      "Epoch 23:  46%|▍| 23/50 [00:00<00:00, 31.75it/s, train_loss=2.31, val_loss=0.146\u001b[A\n",
      "Epoch 24:  46%|▍| 23/50 [00:00<00:00, 31.75it/s, train_loss=2.31, val_loss=0.146\u001b[A\n",
      "Epoch 24:  46%|▍| 23/50 [00:00<00:00, 31.75it/s, train_loss=0.777, val_loss=0.14\u001b[A\n",
      "Epoch 25:  46%|▍| 23/50 [00:00<00:00, 31.75it/s, train_loss=0.777, val_loss=0.14\u001b[A\n",
      "Epoch 25:  46%|▍| 23/50 [00:00<00:00, 31.75it/s, train_loss=0.883, val_loss=0.31\u001b[A\n",
      "Epoch 26:  46%|▍| 23/50 [00:00<00:00, 31.75it/s, train_loss=0.883, val_loss=0.31\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    20: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26:  46%|▍| 23/50 [00:00<00:00, 31.75it/s, train_loss=1.79, val_loss=0.109\u001b[A\n",
      "Epoch 26:  54%|▌| 27/50 [00:00<00:00, 32.08it/s, train_loss=1.79, val_loss=0.109\u001b[A\n",
      "Epoch 27:  54%|▌| 27/50 [00:00<00:00, 32.08it/s, train_loss=1.79, val_loss=0.109\u001b[A\n",
      "Epoch 27:  54%|▌| 27/50 [00:00<00:00, 32.08it/s, train_loss=0.649, val_loss=0.10\u001b[A\n",
      "Epoch 28:  54%|▌| 27/50 [00:00<00:00, 32.08it/s, train_loss=0.649, val_loss=0.10\u001b[A\n",
      "Epoch 28:  54%|▌| 27/50 [00:00<00:00, 32.08it/s, train_loss=0.61, val_loss=0.248\u001b[A\n",
      "Epoch 29:  54%|▌| 27/50 [00:00<00:00, 32.08it/s, train_loss=0.61, val_loss=0.248\u001b[A\n",
      "Epoch 29:  54%|▌| 27/50 [00:00<00:00, 32.08it/s, train_loss=1.33, val_loss=0.103\u001b[A\n",
      "Epoch 30:  54%|▌| 27/50 [00:00<00:00, 32.08it/s, train_loss=1.33, val_loss=0.103\u001b[A\n",
      "Epoch 30:  54%|▌| 27/50 [00:01<00:00, 32.08it/s, train_loss=0.567, val_loss=0.08\u001b[A\n",
      "Epoch 30:  62%|▌| 31/50 [00:01<00:00, 31.99it/s, train_loss=0.567, val_loss=0.08\u001b[A\n",
      "Epoch 31:  62%|▌| 31/50 [00:01<00:00, 31.99it/s, train_loss=0.567, val_loss=0.08\u001b[A\n",
      "Epoch 31:  62%|▌| 31/50 [00:01<00:00, 31.99it/s, train_loss=0.467, val_loss=0.19\u001b[A\n",
      "Epoch 32:  62%|▌| 31/50 [00:01<00:00, 31.99it/s, train_loss=0.467, val_loss=0.19\u001b[A\n",
      "Epoch 32:  62%|▌| 31/50 [00:01<00:00, 31.99it/s, train_loss=1.04, val_loss=0.068\u001b[A\n",
      "Epoch 33:  62%|▌| 31/50 [00:01<00:00, 31.99it/s, train_loss=1.04, val_loss=0.068\u001b[A\n",
      "Epoch 33:  62%|▌| 31/50 [00:01<00:00, 31.99it/s, train_loss=0.4, val_loss=0.0722\u001b[A\n",
      "Epoch 34:  62%|▌| 31/50 [00:01<00:00, 31.99it/s, train_loss=0.4, val_loss=0.0722\u001b[A\n",
      "Epoch 34:  62%|▌| 31/50 [00:01<00:00, 31.99it/s, train_loss=0.368, val_loss=0.15\u001b[A\n",
      "Epoch 34:  70%|▋| 35/50 [00:01<00:00, 32.16it/s, train_loss=0.368, val_loss=0.15\u001b[A\n",
      "Epoch 35:  70%|▋| 35/50 [00:01<00:00, 32.16it/s, train_loss=0.368, val_loss=0.15\u001b[A\n",
      "Epoch 35:  70%|▋| 35/50 [00:01<00:00, 32.16it/s, train_loss=0.74, val_loss=0.062\u001b[A\n",
      "Epoch 36:  70%|▋| 35/50 [00:01<00:00, 32.16it/s, train_loss=0.74, val_loss=0.062\u001b[A\n",
      "Epoch 36:  70%|▋| 35/50 [00:01<00:00, 32.16it/s, train_loss=0.294, val_loss=0.04\u001b[A\n",
      "Epoch 37:  70%|▋| 35/50 [00:01<00:00, 32.16it/s, train_loss=0.294, val_loss=0.04\u001b[A\n",
      "Epoch 37:  70%|▋| 35/50 [00:01<00:00, 32.16it/s, train_loss=0.336, val_loss=0.08\u001b[A\n",
      "Epoch 38:  70%|▋| 35/50 [00:01<00:00, 32.16it/s, train_loss=0.336, val_loss=0.08\u001b[A\n",
      "Epoch 38:  70%|▋| 35/50 [00:01<00:00, 32.16it/s, train_loss=0.572, val_loss=0.03\u001b[A\n",
      "Epoch 38:  78%|▊| 39/50 [00:01<00:00, 32.27it/s, train_loss=0.572, val_loss=0.03\u001b[A\n",
      "Epoch 39:  78%|▊| 39/50 [00:01<00:00, 32.27it/s, train_loss=0.572, val_loss=0.03\u001b[A\n",
      "Epoch 39:  78%|▊| 39/50 [00:01<00:00, 32.27it/s, train_loss=0.253, val_loss=0.06\u001b[A\n",
      "Epoch 40:  78%|▊| 39/50 [00:01<00:00, 32.27it/s, train_loss=0.253, val_loss=0.06\u001b[A\n",
      "Epoch 40:  78%|▊| 39/50 [00:01<00:00, 32.27it/s, train_loss=0.261, val_loss=0.10\u001b[A\n",
      "Epoch 41:  78%|▊| 39/50 [00:01<00:00, 32.27it/s, train_loss=0.261, val_loss=0.10\u001b[A\n",
      "Epoch 41:  78%|▊| 39/50 [00:01<00:00, 32.27it/s, train_loss=0.44, val_loss=0.054\u001b[A\n",
      "Epoch 42:  78%|▊| 39/50 [00:01<00:00, 32.27it/s, train_loss=0.44, val_loss=0.054\u001b[A\n",
      "Epoch 42:  78%|▊| 39/50 [00:01<00:00, 32.27it/s, train_loss=0.228, val_loss=0.04\u001b[A\n",
      "Epoch 42:  86%|▊| 43/50 [00:01<00:00, 32.31it/s, train_loss=0.228, val_loss=0.04\u001b[A\n",
      "Epoch 43:  86%|▊| 43/50 [00:01<00:00, 32.31it/s, train_loss=0.228, val_loss=0.04\u001b[A\n",
      "Epoch 43:  86%|▊| 43/50 [00:01<00:00, 32.31it/s, train_loss=0.256, val_loss=0.05\u001b[A\n",
      "Epoch 44:  86%|▊| 43/50 [00:01<00:00, 32.31it/s, train_loss=0.256, val_loss=0.05\u001b[A\n",
      "Epoch 44:  86%|▊| 43/50 [00:01<00:00, 32.31it/s, train_loss=0.355, val_loss=0.02\u001b[A\n",
      "Epoch 45:  86%|▊| 43/50 [00:01<00:00, 32.31it/s, train_loss=0.355, val_loss=0.02\u001b[A\n",
      "Epoch 45:  86%|▊| 43/50 [00:01<00:00, 32.31it/s, train_loss=0.153, val_loss=0.04\u001b[A\n",
      "Epoch 46:  86%|▊| 43/50 [00:01<00:00, 32.31it/s, train_loss=0.153, val_loss=0.04\u001b[A\n",
      "Epoch 46:  86%|▊| 43/50 [00:01<00:00, 32.31it/s, train_loss=0.207, val_loss=0.06\u001b[A\n",
      "Epoch 46:  94%|▉| 47/50 [00:01<00:00, 31.83it/s, train_loss=0.207, val_loss=0.06\u001b[A\n",
      "Epoch 47:  94%|▉| 47/50 [00:01<00:00, 31.83it/s, train_loss=0.207, val_loss=0.06\u001b[A\n",
      "Epoch 47:  94%|▉| 47/50 [00:01<00:00, 31.83it/s, train_loss=0.269, val_loss=0.02\u001b[A\n",
      "Epoch 48:  94%|▉| 47/50 [00:01<00:00, 31.83it/s, train_loss=0.269, val_loss=0.02\u001b[A\n",
      "Epoch 48:  94%|▉| 47/50 [00:01<00:00, 31.83it/s, train_loss=0.138, val_loss=0.03\u001b[A\n",
      "Epoch 49:  94%|▉| 47/50 [00:01<00:00, 31.83it/s, train_loss=0.138, val_loss=0.03\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 31.33it/s, train_loss=0.208, val_loss=0.03\u001b[A\n",
      "  1%|▍                                          | 1/100 [00:01<02:41,  1.64s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=6.77, val_loss=32.5, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=6.77, val_loss=32.5, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=196, val_loss=3.16, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=196, val_loss=3.16, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=12.6, val_loss=7.23, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=12.6, val_loss=7.23, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=47.5, val_loss=0.805, train_ci\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 30.83it/s, train_loss=47.5, val_loss=0.805, \u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 30.83it/s, train_loss=47.5, val_loss=0.805, \u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 30.83it/s, train_loss=4.68, val_loss=16.1, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 30.83it/s, train_loss=4.68, val_loss=16.1, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 30.83it/s, train_loss=76.9, val_loss=8.69, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 30.83it/s, train_loss=76.9, val_loss=8.69, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 30.83it/s, train_loss=50.6, val_loss=2.88, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 30.83it/s, train_loss=50.6, val_loss=2.88, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.2904195366311835\n",
      "Its' val AUC : 0.3645507679760772\n",
      "Its external  AUC: 0.3782889554120057\n",
      "Curent best Test AUC: 0.4958046336881653\n",
      "Its' val AUC : 0.5740111458474921\n",
      "Its external  AUC: 0.5810619190168583\n",
      "Curent best Test AUC: 0.571571696931747\n",
      "Its' val AUC : 0.5352725295636809\n",
      "Its external  AUC: 0.4365842130140224\n",
      "Curent best Test AUC: 0.6161552911709455\n",
      "Its' val AUC : 0.569661546826152\n",
      "Its external  AUC: 0.5111076098944383\n",
      "Curent best Test AUC: 0.730369442705072\n",
      "Its' val AUC : 0.7023243169770287\n",
      "Its external  AUC: 0.6269103513470932\n",
      "Curent best Test AUC: 0.7496556042579837\n",
      "Its' val AUC : 0.6471387793937746\n",
      "Its external  AUC: 0.6221837088388215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 30.83it/s, train_loss=17.6, val_loss=7.71, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 30.76it/s, train_loss=17.6, val_loss=7.71, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 30.76it/s, train_loss=17.6, val_loss=7.71, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 30.76it/s, train_loss=36.1, val_loss=2.23, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 30.76it/s, train_loss=36.1, val_loss=2.23, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 30.76it/s, train_loss=10.2, val_loss=2.31, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 30.76it/s, train_loss=10.2, val_loss=2.31, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 30.76it/s, train_loss=13.8, val_loss=0.755,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 30.76it/s, train_loss=13.8, val_loss=0.755,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 30.76it/s, train_loss=5.5, val_loss=0.417, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 30.77it/s, train_loss=5.5, val_loss=0.417,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 30.77it/s, train_loss=5.5, val_loss=0.417,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 30.77it/s, train_loss=3.19, val_loss=0.187\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 30.77it/s, train_loss=3.19, val_loss=0.187\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 30.77it/s, train_loss=1.38, val_loss=0.235\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 30.77it/s, train_loss=1.38, val_loss=0.235\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 30.77it/s, train_loss=1.4, val_loss=0.473,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 30.77it/s, train_loss=1.4, val_loss=0.473,\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    10: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Curent best Test AUC: 0.7500313087038196\n",
      "Its' val AUC : 0.655973902405872\n",
      "Its external  AUC: 0.6169844020797227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 30.77it/s, train_loss=2.66, val_loss=0.556\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 30.72it/s, train_loss=2.66, val_loss=0.556\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 30.72it/s, train_loss=2.66, val_loss=0.556\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 30.72it/s, train_loss=3.1, val_loss=0.44, \u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 30.72it/s, train_loss=3.1, val_loss=0.44, \u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 30.72it/s, train_loss=2.37, val_loss=0.283\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 30.72it/s, train_loss=2.37, val_loss=0.283\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 30.72it/s, train_loss=1.45, val_loss=0.124\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 30.72it/s, train_loss=1.45, val_loss=0.124\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 30.72it/s, train_loss=0.638, val_loss=0.11\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 31.40it/s, train_loss=0.638, val_loss=0.11\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 31.40it/s, train_loss=0.638, val_loss=0.11\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 31.40it/s, train_loss=0.673, val_loss=0.22\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 31.40it/s, train_loss=0.673, val_loss=0.22\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 31.40it/s, train_loss=1.41, val_loss=0.223\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 31.40it/s, train_loss=1.41, val_loss=0.223\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 31.40it/s, train_loss=1.5, val_loss=0.144,\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 31.40it/s, train_loss=1.5, val_loss=0.144,\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 31.40it/s, train_loss=1.05, val_loss=0.109\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 31.28it/s, train_loss=1.05, val_loss=0.109\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 31.28it/s, train_loss=1.05, val_loss=0.109\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 31.28it/s, train_loss=0.674, val_loss=0.09\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 31.28it/s, train_loss=0.674, val_loss=0.09\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 31.28it/s, train_loss=0.475, val_loss=0.15\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 31.28it/s, train_loss=0.475, val_loss=0.15\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 31.28it/s, train_loss=0.845, val_loss=0.18\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 31.28it/s, train_loss=0.845, val_loss=0.18\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 31.28it/s, train_loss=1.02, val_loss=0.185\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 31.85it/s, train_loss=1.02, val_loss=0.185\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 31.85it/s, train_loss=1.02, val_loss=0.185\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 31.85it/s, train_loss=0.916, val_loss=0.13\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 31.85it/s, train_loss=0.916, val_loss=0.13\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 31.85it/s, train_loss=0.661, val_loss=0.05\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 31.85it/s, train_loss=0.661, val_loss=0.05\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 31.85it/s, train_loss=0.309, val_loss=0.05\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 31.85it/s, train_loss=0.309, val_loss=0.05\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:01<00:00, 31.85it/s, train_loss=0.371, val_loss=0.07\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:01<00:00, 32.12it/s, train_loss=0.371, val_loss=0.07\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:01<00:00, 32.12it/s, train_loss=0.371, val_loss=0.07\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:01<00:00, 32.12it/s, train_loss=0.474, val_loss=0.07\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 32.12it/s, train_loss=0.474, val_loss=0.07\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 32.12it/s, train_loss=0.563, val_loss=0.06\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 32.12it/s, train_loss=0.563, val_loss=0.06\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 32.12it/s, train_loss=0.46, val_loss=0.038\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 32.12it/s, train_loss=0.46, val_loss=0.038\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 32.12it/s, train_loss=0.261, val_loss=0.03\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 32.32it/s, train_loss=0.261, val_loss=0.03\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 32.32it/s, train_loss=0.261, val_loss=0.03\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 32.32it/s, train_loss=0.207, val_loss=0.04\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 32.32it/s, train_loss=0.207, val_loss=0.04\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 32.32it/s, train_loss=0.249, val_loss=0.05\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 32.32it/s, train_loss=0.249, val_loss=0.05\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 32.32it/s, train_loss=0.305, val_loss=0.05\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 32.32it/s, train_loss=0.305, val_loss=0.05\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 32.32it/s, train_loss=0.295, val_loss=0.03\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 32.45it/s, train_loss=0.295, val_loss=0.03\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 32.45it/s, train_loss=0.295, val_loss=0.03\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 32.45it/s, train_loss=0.207, val_loss=0.02\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 32.45it/s, train_loss=0.207, val_loss=0.02\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 32.45it/s, train_loss=0.153, val_loss=0.02\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 32.45it/s, train_loss=0.153, val_loss=0.02\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 32.45it/s, train_loss=0.17, val_loss=0.035\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 32.45it/s, train_loss=0.17, val_loss=0.035\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 32.45it/s, train_loss=0.231, val_loss=0.03\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 32.52it/s, train_loss=0.231, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 32.52it/s, train_loss=0.231, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 32.52it/s, train_loss=0.236, val_loss=0.03\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 32.52it/s, train_loss=0.236, val_loss=0.03\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 32.52it/s, train_loss=0.193, val_loss=0.02\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 32.52it/s, train_loss=0.193, val_loss=0.02\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 32.52it/s, train_loss=0.153, val_loss=0.03\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 32.52it/s, train_loss=0.153, val_loss=0.03\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 32.52it/s, train_loss=0.154, val_loss=0.03\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 32.94it/s, train_loss=0.154, val_loss=0.03\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 32.94it/s, train_loss=0.154, val_loss=0.03\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 32.94it/s, train_loss=0.164, val_loss=0.03\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 32.94it/s, train_loss=0.164, val_loss=0.03\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 32.07it/s, train_loss=0.164, val_loss=0.03\u001b[A\n",
      "  2%|▊                                          | 2/100 [00:03<02:37,  1.60s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=17, val_loss=18.3, train_cinde\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=17, val_loss=18.3, train_cinde\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=91.4, val_loss=38.4, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=91.4, val_loss=38.4, train_cin\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    47: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Curent best Test AUC: 0.2519724483406387\n",
      "Its' val AUC : 0.3150740791083322\n",
      "Its external  AUC: 0.48306286434536\n",
      "Curent best Test AUC: 0.6460864120225422\n",
      "Its' val AUC : 0.6773141226043224\n",
      "Its external  AUC: 0.6226563730896486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=233, val_loss=10.2, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=233, val_loss=10.2, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=68.7, val_loss=16.4, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.23it/s, train_loss=68.7, val_loss=16.4, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.23it/s, train_loss=68.7, val_loss=16.4, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.23it/s, train_loss=86.3, val_loss=8.83, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.23it/s, train_loss=86.3, val_loss=8.83, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.23it/s, train_loss=60.3, val_loss=8.52, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.23it/s, train_loss=60.3, val_loss=8.52, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.23it/s, train_loss=48.1, val_loss=1.05, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.23it/s, train_loss=48.1, val_loss=1.05, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.23it/s, train_loss=5.32, val_loss=4.08, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 32.84it/s, train_loss=5.32, val_loss=4.08, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.84it/s, train_loss=5.32, val_loss=4.08, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.84it/s, train_loss=22.4, val_loss=4.7, tr\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.84it/s, train_loss=22.4, val_loss=4.7, tr\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.84it/s, train_loss=31.3, val_loss=2.3, tr\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.84it/s, train_loss=31.3, val_loss=2.3, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.84it/s, train_loss=14.9, val_loss=0.671,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.84it/s, train_loss=14.9, val_loss=0.671,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.84it/s, train_loss=4.52, val_loss=2.34, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 32.16it/s, train_loss=4.52, val_loss=2.34,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.16it/s, train_loss=4.52, val_loss=2.34,\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.741014402003757\n",
      "Its' val AUC : 0.7038194916406144\n",
      "Its external  AUC: 0.6475500236332126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.16it/s, train_loss=14, val_loss=1.95, t\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.16it/s, train_loss=14, val_loss=1.95, t\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.16it/s, train_loss=12.1, val_loss=0.376\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.16it/s, train_loss=12.1, val_loss=0.376\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.16it/s, train_loss=2.6, val_loss=0.516,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.16it/s, train_loss=2.6, val_loss=0.516,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.16it/s, train_loss=3.27, val_loss=0.853\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 32.44it/s, train_loss=3.27, val_loss=0.853\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.44it/s, train_loss=3.27, val_loss=0.853\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.44it/s, train_loss=4.8, val_loss=0.694,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.44it/s, train_loss=4.8, val_loss=0.694,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.44it/s, train_loss=4.08, val_loss=0.32,\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.44it/s, train_loss=4.08, val_loss=0.32,\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.44it/s, train_loss=1.69, val_loss=0.262\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.44it/s, train_loss=1.69, val_loss=0.262\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.44it/s, train_loss=1.56, val_loss=0.402\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 32.03it/s, train_loss=1.56, val_loss=0.402\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.03it/s, train_loss=1.56, val_loss=0.402\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.03it/s, train_loss=2.5, val_loss=0.245,\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.03it/s, train_loss=2.5, val_loss=0.245,\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.03it/s, train_loss=1.61, val_loss=0.184\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.03it/s, train_loss=1.61, val_loss=0.184\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.03it/s, train_loss=1.18, val_loss=0.838\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.03it/s, train_loss=1.18, val_loss=0.838\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.03it/s, train_loss=4.5, val_loss=1.69, \u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 31.97it/s, train_loss=4.5, val_loss=1.69, \u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 31.97it/s, train_loss=4.5, val_loss=1.69, \u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 31.97it/s, train_loss=9.61, val_loss=3.05,\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 31.97it/s, train_loss=9.61, val_loss=3.05,\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 31.97it/s, train_loss=16.3, val_loss=1.79,\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 31.97it/s, train_loss=16.3, val_loss=1.79,\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 31.97it/s, train_loss=8.76, val_loss=0.254\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 31.97it/s, train_loss=8.76, val_loss=0.254\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 31.97it/s, train_loss=1.9, val_loss=0.955,\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 32.02it/s, train_loss=1.9, val_loss=0.955,\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 32.02it/s, train_loss=1.9, val_loss=0.955,\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 32.02it/s, train_loss=5.65, val_loss=0.11,\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 32.02it/s, train_loss=5.65, val_loss=0.11,\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 32.02it/s, train_loss=0.722, val_loss=0.90\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 32.02it/s, train_loss=0.722, val_loss=0.90\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 32.02it/s, train_loss=4.69, val_loss=0.099\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 32.02it/s, train_loss=4.69, val_loss=0.099\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 32.02it/s, train_loss=0.615, val_loss=0.81\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 32.20it/s, train_loss=0.615, val_loss=0.81\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 32.20it/s, train_loss=0.615, val_loss=0.81\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.743894802755166\n",
      "Its' val AUC : 0.7074894658148702\n",
      "Its external  AUC: 0.5980778320466362\n",
      "Epoch    28: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32:  64%|▋| 32/50 [00:01<00:00, 32.20it/s, train_loss=3.91, val_loss=0.291\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 32.20it/s, train_loss=3.91, val_loss=0.291\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 32.20it/s, train_loss=1.39, val_loss=0.433\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 32.20it/s, train_loss=1.39, val_loss=0.433\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 32.20it/s, train_loss=2.38, val_loss=0.274\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 32.20it/s, train_loss=2.38, val_loss=0.274\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 32.20it/s, train_loss=1.41, val_loss=0.242\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 32.34it/s, train_loss=1.41, val_loss=0.242\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 32.34it/s, train_loss=1.41, val_loss=0.242\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 32.34it/s, train_loss=1.29, val_loss=0.355\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 32.34it/s, train_loss=1.29, val_loss=0.355\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 32.34it/s, train_loss=2, val_loss=0.101, t\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 32.34it/s, train_loss=2, val_loss=0.101, t\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 32.34it/s, train_loss=0.606, val_loss=0.21\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 32.34it/s, train_loss=0.606, val_loss=0.21\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 32.34it/s, train_loss=0.977, val_loss=0.33\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 32.33it/s, train_loss=0.977, val_loss=0.33\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 32.33it/s, train_loss=0.977, val_loss=0.33\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 32.33it/s, train_loss=1.54, val_loss=0.099\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 32.33it/s, train_loss=1.54, val_loss=0.099\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 32.33it/s, train_loss=0.437, val_loss=0.09\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 32.33it/s, train_loss=0.437, val_loss=0.09\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 32.33it/s, train_loss=0.481, val_loss=0.23\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 32.33it/s, train_loss=0.481, val_loss=0.23\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    37: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 32.33it/s, train_loss=1.2, val_loss=0.117,\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 31.97it/s, train_loss=1.2, val_loss=0.117,\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 31.97it/s, train_loss=1.2, val_loss=0.117,\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 31.97it/s, train_loss=0.616, val_loss=0.08\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 31.97it/s, train_loss=0.616, val_loss=0.08\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 31.97it/s, train_loss=0.446, val_loss=0.20\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 31.97it/s, train_loss=0.446, val_loss=0.20\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 31.97it/s, train_loss=0.985, val_loss=0.11\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 31.97it/s, train_loss=0.985, val_loss=0.11\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 31.97it/s, train_loss=0.513, val_loss=0.04\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 32.15it/s, train_loss=0.513, val_loss=0.04\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 32.15it/s, train_loss=0.513, val_loss=0.04\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 32.15it/s, train_loss=0.253, val_loss=0.11\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 32.15it/s, train_loss=0.253, val_loss=0.11\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 32.17it/s, train_loss=0.68, val_loss=0.075\u001b[A\n",
      "  3%|█▎                                         | 3/100 [00:04<02:34,  1.60s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=13.2, val_loss=99.3, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=13.2, val_loss=99.3, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=530, val_loss=5.78, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=530, val_loss=5.78, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=30.6, val_loss=45.3, train_cin\u001b[A\n",
      "Epoch 2:   6%| | 3/50 [00:00<00:01, 28.59it/s, train_loss=30.6, val_loss=45.3, t\u001b[A\n",
      "Epoch 3:   6%| | 3/50 [00:00<00:01, 28.59it/s, train_loss=30.6, val_loss=45.3, t\u001b[A/home/jielian/anaconda3/envs/gnn/lib/python3.8/site-packages/numpy/core/_asarray.py:171: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n",
      "\n",
      "Epoch 3:   6%| | 3/50 [00:00<00:01, 28.59it/s, train_loss=244, val_loss=1.59, tr\u001b[A\n",
      "Epoch 4:   6%| | 3/50 [00:00<00:01, 28.59it/s, train_loss=244, val_loss=1.59, tr\u001b[A\n",
      "Epoch 4:   6%| | 3/50 [00:00<00:01, 28.59it/s, train_loss=10.2, val_loss=14.6, t\u001b[A\n",
      "Epoch 5:   6%| | 3/50 [00:00<00:01, 28.59it/s, train_loss=10.2, val_loss=14.6, t\u001b[A\n",
      "Epoch 5:   6%| | 3/50 [00:00<00:01, 28.59it/s, train_loss=82, val_loss=15.7, tra\u001b[A\n",
      "Epoch 5:  12%| | 6/50 [00:00<00:01, 27.23it/s, train_loss=82, val_loss=15.7, tra\u001b[A\n",
      "Epoch 6:  12%| | 6/50 [00:00<00:01, 27.23it/s, train_loss=82, val_loss=15.7, tra\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.26086412022542266\n",
      "Its' val AUC : 0.28163653663177923\n",
      "Its external  AUC: 0.38789979517882467\n",
      "Curent best Test AUC: 0.30745147150907953\n",
      "Its' val AUC : 0.28938425988854155\n",
      "Its external  AUC: 0.40712147471246257\n",
      "Curent best Test AUC: 0.7050720100187852\n",
      "Its' val AUC : 0.6885958950659236\n",
      "Its external  AUC: 0.6217110445879943\n",
      "Curent best Test AUC: 0.7336255479023168\n",
      "Its' val AUC : 0.5931765665352725\n",
      "Its external  AUC: 0.6535371041436899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6:  12%| | 6/50 [00:00<00:01, 27.23it/s, train_loss=79.6, val_loss=1.62, t\u001b[A\n",
      "Epoch 7:  12%| | 6/50 [00:00<00:01, 27.23it/s, train_loss=79.6, val_loss=1.62, t\u001b[A\n",
      "Epoch 7:  12%| | 6/50 [00:00<00:01, 27.23it/s, train_loss=8.78, val_loss=9.18, t\u001b[A\n",
      "Epoch 8:  12%| | 6/50 [00:00<00:01, 27.23it/s, train_loss=8.78, val_loss=9.18, t\u001b[A\n",
      "Epoch 8:  12%| | 6/50 [00:00<00:01, 27.23it/s, train_loss=50.4, val_loss=3.38, t\u001b[A\n",
      "Epoch 9:  12%| | 6/50 [00:00<00:01, 27.23it/s, train_loss=50.4, val_loss=3.38, t\u001b[A\n",
      "Epoch 9:  12%| | 6/50 [00:00<00:01, 27.23it/s, train_loss=18.9, val_loss=1.37, t\u001b[A\n",
      "Epoch 9:  20%|▏| 10/50 [00:00<00:01, 29.67it/s, train_loss=18.9, val_loss=1.37, \u001b[A\n",
      "Epoch 10:  20%|▏| 10/50 [00:00<00:01, 29.67it/s, train_loss=18.9, val_loss=1.37,\u001b[A\n",
      "Epoch 10:  20%|▏| 10/50 [00:00<00:01, 29.67it/s, train_loss=7.17, val_loss=4.82,\u001b[A\n",
      "Epoch 11:  20%|▏| 10/50 [00:00<00:01, 29.67it/s, train_loss=7.17, val_loss=4.82,\u001b[A\n",
      "Epoch 11:  20%|▏| 10/50 [00:00<00:01, 29.67it/s, train_loss=25.6, val_loss=1.86,\u001b[A\n",
      "Epoch 12:  20%|▏| 10/50 [00:00<00:01, 29.67it/s, train_loss=25.6, val_loss=1.86,\u001b[A\n",
      "Epoch 12:  20%|▏| 10/50 [00:00<00:01, 29.67it/s, train_loss=10.5, val_loss=0.955\u001b[A\n",
      "Epoch 13:  20%|▏| 10/50 [00:00<00:01, 29.67it/s, train_loss=10.5, val_loss=0.955\u001b[A\n",
      "Epoch 13:  20%|▏| 10/50 [00:00<00:01, 29.67it/s, train_loss=4.05, val_loss=4, tr\u001b[A\n",
      "Epoch 13:  28%|▎| 14/50 [00:00<00:01, 30.23it/s, train_loss=4.05, val_loss=4, tr\u001b[A\n",
      "Epoch 14:  28%|▎| 14/50 [00:00<00:01, 30.23it/s, train_loss=4.05, val_loss=4, tr\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7590482154038822\n",
      "Its' val AUC : 0.6760907978795705\n",
      "Its external  AUC: 0.6228139278399244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14:  28%|▎| 14/50 [00:00<00:01, 30.23it/s, train_loss=18.3, val_loss=1.03,\u001b[A\n",
      "Epoch 15:  28%|▎| 14/50 [00:00<00:01, 30.23it/s, train_loss=18.3, val_loss=1.03,\u001b[A\n",
      "Epoch 15:  28%|▎| 14/50 [00:00<00:01, 30.23it/s, train_loss=4.69, val_loss=1, tr\u001b[A\n",
      "Epoch 16:  28%|▎| 14/50 [00:00<00:01, 30.23it/s, train_loss=4.69, val_loss=1, tr\u001b[A\n",
      "Epoch 16:  28%|▎| 14/50 [00:00<00:01, 30.23it/s, train_loss=6.03, val_loss=2.4, \u001b[A\n",
      "Epoch 17:  28%|▎| 14/50 [00:00<00:01, 30.23it/s, train_loss=6.03, val_loss=2.4, \u001b[A\n",
      "Epoch 17:  28%|▎| 14/50 [00:00<00:01, 30.23it/s, train_loss=12.9, val_loss=0.409\u001b[A\n",
      "Epoch 17:  36%|▎| 18/50 [00:00<00:01, 31.14it/s, train_loss=12.9, val_loss=0.409\u001b[A\n",
      "Epoch 18:  36%|▎| 18/50 [00:00<00:01, 31.14it/s, train_loss=12.9, val_loss=0.409\u001b[A\n",
      "Epoch 18:  36%|▎| 18/50 [00:00<00:01, 31.14it/s, train_loss=2.22, val_loss=1.23,\u001b[A\n",
      "Epoch 19:  36%|▎| 18/50 [00:00<00:01, 31.14it/s, train_loss=2.22, val_loss=1.23,\u001b[A\n",
      "Epoch 19:  36%|▎| 18/50 [00:00<00:01, 31.14it/s, train_loss=6.93, val_loss=1.24,\u001b[A\n",
      "Epoch 20:  36%|▎| 18/50 [00:00<00:01, 31.14it/s, train_loss=6.93, val_loss=1.24,\u001b[A\n",
      "Epoch 20:  36%|▎| 18/50 [00:00<00:01, 31.14it/s, train_loss=6.97, val_loss=0.165\u001b[A\n",
      "Epoch 21:  36%|▎| 18/50 [00:00<00:01, 31.14it/s, train_loss=6.97, val_loss=0.165\u001b[A\n",
      "Epoch 21:  36%|▎| 18/50 [00:00<00:01, 31.14it/s, train_loss=1.17, val_loss=1.19,\u001b[A\n",
      "Epoch 21:  44%|▍| 22/50 [00:00<00:00, 31.62it/s, train_loss=1.17, val_loss=1.19,\u001b[A\n",
      "Epoch 22:  44%|▍| 22/50 [00:00<00:00, 31.62it/s, train_loss=1.17, val_loss=1.19,\u001b[A\n",
      "Epoch 22:  44%|▍| 22/50 [00:00<00:00, 31.62it/s, train_loss=6.71, val_loss=0.423\u001b[A\n",
      "Epoch 23:  44%|▍| 22/50 [00:00<00:00, 31.62it/s, train_loss=6.71, val_loss=0.423\u001b[A\n",
      "Epoch 23:  44%|▍| 22/50 [00:00<00:00, 31.62it/s, train_loss=2.78, val_loss=0.549\u001b[A\n",
      "Epoch 24:  44%|▍| 22/50 [00:00<00:00, 31.62it/s, train_loss=2.78, val_loss=0.549\u001b[A\n",
      "Epoch 24:  44%|▍| 22/50 [00:00<00:00, 31.62it/s, train_loss=2.63, val_loss=1.01,\u001b[A\n",
      "Epoch 25:  44%|▍| 22/50 [00:00<00:00, 31.62it/s, train_loss=2.63, val_loss=1.01,\u001b[A\n",
      "Epoch 25:  44%|▍| 22/50 [00:00<00:00, 31.62it/s, train_loss=4.61, val_loss=0.078\u001b[A\n",
      "Epoch 25:  52%|▌| 26/50 [00:00<00:00, 32.00it/s, train_loss=4.61, val_loss=0.078\u001b[A\n",
      "Epoch 26:  52%|▌| 26/50 [00:00<00:00, 32.00it/s, train_loss=4.61, val_loss=0.078\u001b[A\n",
      "Epoch 26:  52%|▌| 26/50 [00:00<00:00, 32.00it/s, train_loss=0.531, val_loss=0.53\u001b[A\n",
      "Epoch 27:  52%|▌| 26/50 [00:00<00:00, 32.00it/s, train_loss=0.531, val_loss=0.53\u001b[A\n",
      "Epoch 27:  52%|▌| 26/50 [00:00<00:00, 32.00it/s, train_loss=3.24, val_loss=0.298\u001b[A\n",
      "Epoch 28:  52%|▌| 26/50 [00:00<00:00, 32.00it/s, train_loss=3.24, val_loss=0.298\u001b[A\n",
      "Epoch 28:  52%|▌| 26/50 [00:00<00:00, 32.00it/s, train_loss=1.8, val_loss=0.213,\u001b[A\n",
      "Epoch 29:  52%|▌| 26/50 [00:00<00:00, 32.00it/s, train_loss=1.8, val_loss=0.213,\u001b[A\n",
      "Epoch 29:  52%|▌| 26/50 [00:00<00:00, 32.00it/s, train_loss=1.22, val_loss=0.524\u001b[A\n",
      "Epoch 29:  60%|▌| 30/50 [00:00<00:00, 32.32it/s, train_loss=1.22, val_loss=0.524\u001b[A\n",
      "Epoch 30:  60%|▌| 30/50 [00:00<00:00, 32.32it/s, train_loss=1.22, val_loss=0.524\u001b[A\n",
      "Epoch 30:  60%|▌| 30/50 [00:00<00:00, 32.32it/s, train_loss=2.76, val_loss=0.049\u001b[A\n",
      "Epoch 31:  60%|▌| 30/50 [00:00<00:00, 32.32it/s, train_loss=2.76, val_loss=0.049\u001b[A\n",
      "Epoch 31:  60%|▌| 30/50 [00:01<00:00, 32.32it/s, train_loss=0.273, val_loss=0.38\u001b[A\n",
      "Epoch 32:  60%|▌| 30/50 [00:01<00:00, 32.32it/s, train_loss=0.273, val_loss=0.38\u001b[A\n",
      "Epoch 32:  60%|▌| 30/50 [00:01<00:00, 32.32it/s, train_loss=2.16, val_loss=0.161\u001b[A\n",
      "Epoch 33:  60%|▌| 30/50 [00:01<00:00, 32.32it/s, train_loss=2.16, val_loss=0.161\u001b[A\n",
      "Epoch 33:  60%|▌| 30/50 [00:01<00:00, 32.32it/s, train_loss=0.936, val_loss=0.22\u001b[A\n",
      "Epoch 33:  68%|▋| 34/50 [00:01<00:00, 32.25it/s, train_loss=0.936, val_loss=0.22\u001b[A\n",
      "Epoch 34:  68%|▋| 34/50 [00:01<00:00, 32.25it/s, train_loss=0.936, val_loss=0.22\u001b[A\n",
      "Epoch 34:  68%|▋| 34/50 [00:01<00:00, 32.25it/s, train_loss=1.05, val_loss=0.315\u001b[A\n",
      "Epoch 35:  68%|▋| 34/50 [00:01<00:00, 32.25it/s, train_loss=1.05, val_loss=0.315\u001b[A\n",
      "Epoch 35:  68%|▋| 34/50 [00:01<00:00, 32.25it/s, train_loss=1.53, val_loss=0.070\u001b[A\n",
      "Epoch 36:  68%|▋| 34/50 [00:01<00:00, 32.25it/s, train_loss=1.53, val_loss=0.070\u001b[A\n",
      "Epoch 36:  68%|▋| 34/50 [00:01<00:00, 32.25it/s, train_loss=0.377, val_loss=0.31\u001b[A\n",
      "Epoch 37:  68%|▋| 34/50 [00:01<00:00, 32.25it/s, train_loss=0.377, val_loss=0.31\u001b[A\n",
      "Epoch 37:  68%|▋| 34/50 [00:01<00:00, 32.25it/s, train_loss=1.62, val_loss=0.155\u001b[A\n",
      "Epoch 37:  76%|▊| 38/50 [00:01<00:00, 32.35it/s, train_loss=1.62, val_loss=0.155\u001b[A\n",
      "Epoch 38:  76%|▊| 38/50 [00:01<00:00, 32.35it/s, train_loss=1.62, val_loss=0.155\u001b[A\n",
      "Epoch 38:  76%|▊| 38/50 [00:01<00:00, 32.35it/s, train_loss=0.801, val_loss=0.05\u001b[A\n",
      "Epoch 39:  76%|▊| 38/50 [00:01<00:00, 32.35it/s, train_loss=0.801, val_loss=0.05\u001b[A\n",
      "Epoch 39:  76%|▊| 38/50 [00:01<00:00, 32.35it/s, train_loss=0.3, val_loss=0.2, t\u001b[A\n",
      "Epoch 40:  76%|▊| 38/50 [00:01<00:00, 32.35it/s, train_loss=0.3, val_loss=0.2, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7597996242955541\n",
      "Its' val AUC : 0.6875084953105886\n",
      "Its external  AUC: 0.6308492201039861\n",
      "Epoch    37: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40:  76%|▊| 38/50 [00:01<00:00, 32.35it/s, train_loss=1.01, val_loss=0.175\u001b[A\n",
      "Epoch 41:  76%|▊| 38/50 [00:01<00:00, 32.35it/s, train_loss=1.01, val_loss=0.175\u001b[A\n",
      "Epoch 41:  76%|▊| 38/50 [00:01<00:00, 32.35it/s, train_loss=0.835, val_loss=0.04\u001b[A\n",
      "Epoch 41:  84%|▊| 42/50 [00:01<00:00, 32.30it/s, train_loss=0.835, val_loss=0.04\u001b[A\n",
      "Epoch 42:  84%|▊| 42/50 [00:01<00:00, 32.30it/s, train_loss=0.835, val_loss=0.04\u001b[A\n",
      "Epoch 42:  84%|▊| 42/50 [00:01<00:00, 32.30it/s, train_loss=0.184, val_loss=0.09\u001b[A\n",
      "Epoch 43:  84%|▊| 42/50 [00:01<00:00, 32.30it/s, train_loss=0.184, val_loss=0.09\u001b[A\n",
      "Epoch 43:  84%|▊| 42/50 [00:01<00:00, 32.30it/s, train_loss=0.567, val_loss=0.14\u001b[A\n",
      "Epoch 44:  84%|▊| 42/50 [00:01<00:00, 32.30it/s, train_loss=0.567, val_loss=0.14\u001b[A\n",
      "Epoch 44:  84%|▊| 42/50 [00:01<00:00, 32.30it/s, train_loss=0.815, val_loss=0.04\u001b[A\n",
      "Epoch 45:  84%|▊| 42/50 [00:01<00:00, 32.30it/s, train_loss=0.815, val_loss=0.04\u001b[A\n",
      "Epoch 45:  84%|▊| 42/50 [00:01<00:00, 32.30it/s, train_loss=0.275, val_loss=0.06\u001b[A\n",
      "Epoch 45:  92%|▉| 46/50 [00:01<00:00, 32.46it/s, train_loss=0.275, val_loss=0.06\u001b[A\n",
      "Epoch 46:  92%|▉| 46/50 [00:01<00:00, 32.46it/s, train_loss=0.275, val_loss=0.06\u001b[A\n",
      "Epoch 46:  92%|▉| 46/50 [00:01<00:00, 32.46it/s, train_loss=0.252, val_loss=0.13\u001b[A\n",
      "Epoch 47:  92%|▉| 46/50 [00:01<00:00, 32.46it/s, train_loss=0.252, val_loss=0.13\u001b[A\n",
      "Epoch 47:  92%|▉| 46/50 [00:01<00:00, 32.46it/s, train_loss=0.596, val_loss=0.06\u001b[A\n",
      "Epoch 48:  92%|▉| 46/50 [00:01<00:00, 32.46it/s, train_loss=0.596, val_loss=0.06\u001b[A\n",
      "Epoch 48:  92%|▉| 46/50 [00:01<00:00, 32.46it/s, train_loss=0.306, val_loss=0.02\u001b[A\n",
      "Epoch 49:  92%|▉| 46/50 [00:01<00:00, 32.46it/s, train_loss=0.306, val_loss=0.02\u001b[A\n",
      "Epoch 49:  92%|▉| 46/50 [00:01<00:00, 32.46it/s, train_loss=0.139, val_loss=0.02\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 31.71it/s, train_loss=0.139, val_loss=0.02\u001b[A\n",
      "  4%|█▋                                         | 4/100 [00:06<02:33,  1.60s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=17, val_loss=53.3, train_cinde\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=17, val_loss=53.3, train_cinde\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=269, val_loss=3.02, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=269, val_loss=3.02, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=14.8, val_loss=5.15, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=14.8, val_loss=5.15, train_cin\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    48: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Curent best Test AUC: 0.22905447714464622\n",
      "Its' val AUC : 0.28408318608128313\n",
      "Its external  AUC: 0.39530486844178353\n",
      "Curent best Test AUC: 0.2618659987476519\n",
      "Its' val AUC : 0.34443387250237867\n",
      "Its external  AUC: 0.41153300772018275\n",
      "Curent best Test AUC: 0.6955541640576081\n",
      "Its' val AUC : 0.697295093108604\n",
      "Its external  AUC: 0.6412478336221837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=28.3, val_loss=0.813, train_ci\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 30.84it/s, train_loss=28.3, val_loss=0.813, \u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 30.84it/s, train_loss=28.3, val_loss=0.813, \u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 30.84it/s, train_loss=5.7, val_loss=1.95, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 30.84it/s, train_loss=5.7, val_loss=1.95, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 30.84it/s, train_loss=10.6, val_loss=0.305, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 30.84it/s, train_loss=10.6, val_loss=0.305, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 30.84it/s, train_loss=2.24, val_loss=1.78, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 30.84it/s, train_loss=2.24, val_loss=1.78, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 30.84it/s, train_loss=9.74, val_loss=1.43, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 32.04it/s, train_loss=9.74, val_loss=1.43, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.04it/s, train_loss=9.74, val_loss=1.43, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.04it/s, train_loss=8.32, val_loss=0.714, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.04it/s, train_loss=8.32, val_loss=0.714, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.04it/s, train_loss=4.59, val_loss=1.82, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.04it/s, train_loss=4.59, val_loss=1.82, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.04it/s, train_loss=9.41, val_loss=0.308,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.04it/s, train_loss=9.41, val_loss=0.308,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.04it/s, train_loss=2.26, val_loss=0.788,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 32.22it/s, train_loss=2.26, val_loss=0.788\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.22it/s, train_loss=2.26, val_loss=0.788\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.22it/s, train_loss=5.12, val_loss=0.165\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.22it/s, train_loss=5.12, val_loss=0.165\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.22it/s, train_loss=1.11, val_loss=0.733\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.22it/s, train_loss=1.11, val_loss=0.733\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.22it/s, train_loss=3.18, val_loss=0.501\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.22it/s, train_loss=3.18, val_loss=0.501\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.22it/s, train_loss=2.23, val_loss=0.179\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 32.21it/s, train_loss=2.23, val_loss=0.179\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.21it/s, train_loss=2.23, val_loss=0.179\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.6968065122103945\n",
      "Its' val AUC : 0.639798831045263\n",
      "Its external  AUC: 0.44918859303608005\n",
      "Epoch    12: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.21it/s, train_loss=1.07, val_loss=0.552\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.21it/s, train_loss=1.07, val_loss=0.552\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.21it/s, train_loss=3.05, val_loss=0.165\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.21it/s, train_loss=3.05, val_loss=0.165\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.21it/s, train_loss=0.951, val_loss=0.34\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.21it/s, train_loss=0.951, val_loss=0.34\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.21it/s, train_loss=1.74, val_loss=0.368\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 32.07it/s, train_loss=1.74, val_loss=0.368\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.07it/s, train_loss=1.74, val_loss=0.368\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.07it/s, train_loss=1.85, val_loss=0.128\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.07it/s, train_loss=1.85, val_loss=0.128\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.07it/s, train_loss=0.714, val_loss=0.38\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.07it/s, train_loss=0.714, val_loss=0.38\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.07it/s, train_loss=2.02, val_loss=0.156\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.07it/s, train_loss=2.02, val_loss=0.156\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.07it/s, train_loss=0.768, val_loss=0.22\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 31.85it/s, train_loss=0.768, val_loss=0.22\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 31.85it/s, train_loss=0.768, val_loss=0.22\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 31.85it/s, train_loss=1.16, val_loss=0.216\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 31.85it/s, train_loss=1.16, val_loss=0.216\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 31.85it/s, train_loss=1.16, val_loss=0.112\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 31.85it/s, train_loss=1.16, val_loss=0.112\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 31.85it/s, train_loss=0.528, val_loss=0.25\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 31.85it/s, train_loss=0.528, val_loss=0.25\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 31.85it/s, train_loss=1.26, val_loss=0.079\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 31.59it/s, train_loss=1.26, val_loss=0.079\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 31.59it/s, train_loss=1.26, val_loss=0.079\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 31.59it/s, train_loss=0.373, val_loss=0.20\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 31.59it/s, train_loss=0.373, val_loss=0.20\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 31.59it/s, train_loss=0.903, val_loss=0.14\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 31.59it/s, train_loss=0.903, val_loss=0.14\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 31.59it/s, train_loss=0.6, val_loss=0.105,\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 31.59it/s, train_loss=0.6, val_loss=0.105,\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:01<00:00, 31.59it/s, train_loss=0.524, val_loss=0.14\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:01<00:00, 31.43it/s, train_loss=0.524, val_loss=0.14\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:01<00:00, 31.43it/s, train_loss=0.524, val_loss=0.14\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:01<00:00, 31.43it/s, train_loss=0.758, val_loss=0.06\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 31.43it/s, train_loss=0.758, val_loss=0.06\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 31.43it/s, train_loss=0.253, val_loss=0.14\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 31.43it/s, train_loss=0.253, val_loss=0.14\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 31.43it/s, train_loss=0.678, val_loss=0.05\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 31.43it/s, train_loss=0.678, val_loss=0.05\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 31.43it/s, train_loss=0.235, val_loss=0.11\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 31.52it/s, train_loss=0.235, val_loss=0.11\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 31.52it/s, train_loss=0.235, val_loss=0.11\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 31.52it/s, train_loss=0.515, val_loss=0.07\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 31.52it/s, train_loss=0.515, val_loss=0.07\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 31.52it/s, train_loss=0.323, val_loss=0.07\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 31.52it/s, train_loss=0.323, val_loss=0.07\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 31.52it/s, train_loss=0.354, val_loss=0.08\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 31.52it/s, train_loss=0.354, val_loss=0.08\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 31.52it/s, train_loss=0.412, val_loss=0.06\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 31.95it/s, train_loss=0.412, val_loss=0.06\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 31.95it/s, train_loss=0.412, val_loss=0.06\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 31.95it/s, train_loss=0.265, val_loss=0.09\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 31.95it/s, train_loss=0.265, val_loss=0.09\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 31.95it/s, train_loss=0.419, val_loss=0.05\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 31.95it/s, train_loss=0.419, val_loss=0.05\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 31.95it/s, train_loss=0.232, val_loss=0.04\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 31.95it/s, train_loss=0.232, val_loss=0.04\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 31.95it/s, train_loss=0.222, val_loss=0.06\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 32.08it/s, train_loss=0.222, val_loss=0.06\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 32.08it/s, train_loss=0.222, val_loss=0.06\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 32.08it/s, train_loss=0.336, val_loss=0.04\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 32.08it/s, train_loss=0.336, val_loss=0.04\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 32.08it/s, train_loss=0.226, val_loss=0.04\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 32.08it/s, train_loss=0.226, val_loss=0.04\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 32.08it/s, train_loss=0.173, val_loss=0.06\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 32.08it/s, train_loss=0.173, val_loss=0.06\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    41: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 32.08it/s, train_loss=0.268, val_loss=0.05\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 31.94it/s, train_loss=0.268, val_loss=0.05\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 31.94it/s, train_loss=0.268, val_loss=0.05\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 31.94it/s, train_loss=0.214, val_loss=0.03\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 31.94it/s, train_loss=0.214, val_loss=0.03\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 31.84it/s, train_loss=0.149, val_loss=0.05\u001b[A\n",
      "  5%|██▏                                        | 5/100 [00:08<02:32,  1.60s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=79.7, val_loss=11.7, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=79.7, val_loss=11.7, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=64.3, val_loss=1.67, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=64.3, val_loss=1.67, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=8.39, val_loss=0.496, train_ci\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=8.39, val_loss=0.496, train_ci\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=2.73, val_loss=0.281, train_ci\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 31.40it/s, train_loss=2.73, val_loss=0.281, \u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.40it/s, train_loss=2.73, val_loss=0.281, \u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.40it/s, train_loss=1.42, val_loss=0.213, \u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.40it/s, train_loss=1.42, val_loss=0.213, \u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.40it/s, train_loss=1.35, val_loss=0.317, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.40it/s, train_loss=1.35, val_loss=0.317, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.40it/s, train_loss=1.84, val_loss=0.0692,\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.40it/s, train_loss=1.84, val_loss=0.0692,\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.2886662492172824\n",
      "Its' val AUC : 0.291151284490961\n",
      "Its external  AUC: 0.3814400504175201\n",
      "Curent best Test AUC: 0.6986850344395742\n",
      "Its' val AUC : 0.673100448552399\n",
      "Its external  AUC: 0.5867338900267843\n",
      "Curent best Test AUC: 0.7560425798371947\n",
      "Its' val AUC : 0.6929454940872638\n",
      "Its external  AUC: 0.6084764455648338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.40it/s, train_loss=0.399, val_loss=0.207,\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 32.27it/s, train_loss=0.399, val_loss=0.207,\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.27it/s, train_loss=0.399, val_loss=0.207,\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.27it/s, train_loss=1.12, val_loss=0.0231,\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.27it/s, train_loss=1.12, val_loss=0.0231,\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.27it/s, train_loss=0.149, val_loss=0.19, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.27it/s, train_loss=0.149, val_loss=0.19,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.27it/s, train_loss=0.954, val_loss=0.033\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.27it/s, train_loss=0.954, val_loss=0.033\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.27it/s, train_loss=0.189, val_loss=0.187\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 32.63it/s, train_loss=0.189, val_loss=0.18\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.63it/s, train_loss=0.189, val_loss=0.18\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.63it/s, train_loss=1.03, val_loss=0.022\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.63it/s, train_loss=1.03, val_loss=0.022\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.63it/s, train_loss=0.123, val_loss=0.16\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.63it/s, train_loss=0.123, val_loss=0.16\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.63it/s, train_loss=0.9, val_loss=0.0363\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.63it/s, train_loss=0.9, val_loss=0.0363\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.63it/s, train_loss=0.166, val_loss=0.16\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 32.90it/s, train_loss=0.166, val_loss=0.16\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.90it/s, train_loss=0.166, val_loss=0.16\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.90it/s, train_loss=0.78, val_loss=0.026\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.90it/s, train_loss=0.78, val_loss=0.026\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.90it/s, train_loss=0.14, val_loss=0.108\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.90it/s, train_loss=0.14, val_loss=0.108\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.90it/s, train_loss=0.597, val_loss=0.02\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.90it/s, train_loss=0.597, val_loss=0.02\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.90it/s, train_loss=0.0988, val_loss=0.1\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 32.93it/s, train_loss=0.0988, val_loss=0.1\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.93it/s, train_loss=0.0988, val_loss=0.1\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.93it/s, train_loss=0.514, val_loss=0.02\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.93it/s, train_loss=0.514, val_loss=0.02\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.93it/s, train_loss=0.139, val_loss=0.07\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.93it/s, train_loss=0.139, val_loss=0.07\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.93it/s, train_loss=0.406, val_loss=0.03\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.93it/s, train_loss=0.406, val_loss=0.03\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.93it/s, train_loss=0.169, val_loss=0.05\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 33.29it/s, train_loss=0.169, val_loss=0.05\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.29it/s, train_loss=0.169, val_loss=0.05\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.29it/s, train_loss=0.291, val_loss=0.04\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.29it/s, train_loss=0.291, val_loss=0.04\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.29it/s, train_loss=0.235, val_loss=0.03\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.29it/s, train_loss=0.235, val_loss=0.03\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.29it/s, train_loss=0.199, val_loss=0.01\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.29it/s, train_loss=0.199, val_loss=0.01\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.29it/s, train_loss=0.0638, val_loss=0.0\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 33.61it/s, train_loss=0.0638, val_loss=0.0\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.61it/s, train_loss=0.0638, val_loss=0.0\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.61it/s, train_loss=0.2, val_loss=0.0222\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.61it/s, train_loss=0.2, val_loss=0.0222\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.61it/s, train_loss=0.105, val_loss=0.01\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.61it/s, train_loss=0.105, val_loss=0.01\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.61it/s, train_loss=0.0767, val_loss=0.0\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.61it/s, train_loss=0.0767, val_loss=0.0\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    25: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.61it/s, train_loss=0.168, val_loss=0.01\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.02it/s, train_loss=0.168, val_loss=0.01\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.02it/s, train_loss=0.168, val_loss=0.01\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.02it/s, train_loss=0.0638, val_loss=0.0\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.02it/s, train_loss=0.0638, val_loss=0.0\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 34.02it/s, train_loss=0.102, val_loss=0.02\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.02it/s, train_loss=0.102, val_loss=0.02\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.02it/s, train_loss=0.117, val_loss=0.00\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.02it/s, train_loss=0.117, val_loss=0.00\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.02it/s, train_loss=0.0431, val_loss=0.0\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.27it/s, train_loss=0.0431, val_loss=0.0\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.27it/s, train_loss=0.0431, val_loss=0.0\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.27it/s, train_loss=0.107, val_loss=0.01\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.27it/s, train_loss=0.107, val_loss=0.01\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.27it/s, train_loss=0.0678, val_loss=0.0\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.27it/s, train_loss=0.0678, val_loss=0.0\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.27it/s, train_loss=0.0577, val_loss=0.0\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.27it/s, train_loss=0.0577, val_loss=0.0\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.27it/s, train_loss=0.0916, val_loss=0.0\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.48it/s, train_loss=0.0916, val_loss=0.0\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.48it/s, train_loss=0.0916, val_loss=0.0\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.48it/s, train_loss=0.0377, val_loss=0.0\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.48it/s, train_loss=0.0377, val_loss=0.0\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.48it/s, train_loss=0.0728, val_loss=0.0\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.48it/s, train_loss=0.0728, val_loss=0.0\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.48it/s, train_loss=0.0557, val_loss=0.0\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.48it/s, train_loss=0.0557, val_loss=0.0\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.48it/s, train_loss=0.0451, val_loss=0.0\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.63it/s, train_loss=0.0451, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.63it/s, train_loss=0.0451, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.63it/s, train_loss=0.0683, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.63it/s, train_loss=0.0683, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.63it/s, train_loss=0.0347, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.63it/s, train_loss=0.0347, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.63it/s, train_loss=0.0557, val_loss=0.0\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.63it/s, train_loss=0.0557, val_loss=0.0\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.63it/s, train_loss=0.0418, val_loss=0.0\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 33.73it/s, train_loss=0.0418, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 33.73it/s, train_loss=0.0418, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 33.73it/s, train_loss=0.0393, val_loss=0.0\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 33.73it/s, train_loss=0.0393, val_loss=0.0\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 33.52it/s, train_loss=0.0508, val_loss=0.0\u001b[A\n",
      "  6%|██▌                                        | 6/100 [00:09<02:27,  1.57s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=8.15, val_loss=12.2, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=8.15, val_loss=12.2, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=57, val_loss=46.8, train_cinde\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=57, val_loss=46.8, train_cinde\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=211, val_loss=14.3, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=211, val_loss=14.3, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=93.5, val_loss=16.7, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 31.94it/s, train_loss=93.5, val_loss=16.7, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.94it/s, train_loss=93.5, val_loss=16.7, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.94it/s, train_loss=106, val_loss=10.3, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.94it/s, train_loss=106, val_loss=10.3, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.94it/s, train_loss=55.9, val_loss=18.9, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.94it/s, train_loss=55.9, val_loss=18.9, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.94it/s, train_loss=90.4, val_loss=7.15, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.94it/s, train_loss=90.4, val_loss=7.15, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.6628678772698811\n",
      "Its' val AUC : 0.6502650536903629\n",
      "Its external  AUC: 0.5430912242004097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.94it/s, train_loss=41.7, val_loss=8.48, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 32.08it/s, train_loss=41.7, val_loss=8.48, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.08it/s, train_loss=41.7, val_loss=8.48, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.08it/s, train_loss=49.7, val_loss=4.12, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.08it/s, train_loss=49.7, val_loss=4.12, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.08it/s, train_loss=24, val_loss=3.77, tra\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.08it/s, train_loss=24, val_loss=3.77, tr\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.08it/s, train_loss=22.7, val_loss=5.37, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.08it/s, train_loss=22.7, val_loss=5.37, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.08it/s, train_loss=26.1, val_loss=1.67, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 32.18it/s, train_loss=26.1, val_loss=1.67,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.18it/s, train_loss=26.1, val_loss=1.67,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.18it/s, train_loss=10.1, val_loss=1.5, \u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.18it/s, train_loss=10.1, val_loss=1.5, \u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.18it/s, train_loss=8.7, val_loss=0.83, \u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.18it/s, train_loss=8.7, val_loss=0.83, \u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.18it/s, train_loss=4.66, val_loss=0.973\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.18it/s, train_loss=4.66, val_loss=0.973\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.18it/s, train_loss=5.9, val_loss=0.917,\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=5.9, val_loss=0.917,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=5.9, val_loss=0.917,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=5.34, val_loss=0.174\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=5.34, val_loss=0.174\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.6877896055103319\n",
      "Its' val AUC : 0.7091205654478727\n",
      "Its external  AUC: 0.5416732314479281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=0.983, val_loss=0.79\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=0.983, val_loss=0.79\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=4.19, val_loss=0.443\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=4.19, val_loss=0.443\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=2.56, val_loss=0.235\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 33.50it/s, train_loss=2.56, val_loss=0.235\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.50it/s, train_loss=2.56, val_loss=0.235\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.50it/s, train_loss=1.88, val_loss=0.582\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.50it/s, train_loss=1.88, val_loss=0.582\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.50it/s, train_loss=3.96, val_loss=0.25,\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.50it/s, train_loss=3.96, val_loss=0.25,\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.50it/s, train_loss=2.12, val_loss=0.52,\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.50it/s, train_loss=2.12, val_loss=0.52,\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.50it/s, train_loss=3.12, val_loss=0.521\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 33.26it/s, train_loss=3.12, val_loss=0.521\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.26it/s, train_loss=3.12, val_loss=0.521\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.26it/s, train_loss=3.08, val_loss=0.306\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.26it/s, train_loss=3.08, val_loss=0.306\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.26it/s, train_loss=2.11, val_loss=0.259\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.26it/s, train_loss=2.11, val_loss=0.259\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7041953663118347\n",
      "Its' val AUC : 0.609895337773549\n",
      "Its external  AUC: 0.6377816291161178\n",
      "Curent best Test AUC: 0.7337507827175955\n",
      "Its' val AUC : 0.6498572787821123\n",
      "Its external  AUC: 0.6321096581061919\n",
      "Epoch    23: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.26it/s, train_loss=2.04, val_loss=0.342\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.26it/s, train_loss=2.04, val_loss=0.342\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.26it/s, train_loss=2.52, val_loss=0.282\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 33.77it/s, train_loss=2.52, val_loss=0.282\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.77it/s, train_loss=2.52, val_loss=0.282\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.77it/s, train_loss=2.08, val_loss=0.196\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.77it/s, train_loss=2.08, val_loss=0.196\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.77it/s, train_loss=1.41, val_loss=0.206\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.77it/s, train_loss=1.41, val_loss=0.206\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.77it/s, train_loss=1.38, val_loss=0.226\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.77it/s, train_loss=1.38, val_loss=0.226\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.77it/s, train_loss=1.42, val_loss=0.219\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 33.70it/s, train_loss=1.42, val_loss=0.219\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 33.70it/s, train_loss=1.42, val_loss=0.219\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 33.70it/s, train_loss=1.36, val_loss=0.18,\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 33.70it/s, train_loss=1.36, val_loss=0.18,\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 33.70it/s, train_loss=1.16, val_loss=0.14,\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 33.70it/s, train_loss=1.16, val_loss=0.14,\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 33.70it/s, train_loss=0.961, val_loss=0.12\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 33.70it/s, train_loss=0.961, val_loss=0.12\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    29: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 33.70it/s, train_loss=0.887, val_loss=0.12\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 33.66it/s, train_loss=0.887, val_loss=0.12\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.66it/s, train_loss=0.887, val_loss=0.12\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.66it/s, train_loss=0.891, val_loss=0.12\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.66it/s, train_loss=0.891, val_loss=0.12\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.66it/s, train_loss=0.847, val_loss=0.10\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.66it/s, train_loss=0.847, val_loss=0.10\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.66it/s, train_loss=0.717, val_loss=0.08\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.66it/s, train_loss=0.717, val_loss=0.08\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.66it/s, train_loss=0.583, val_loss=0.08\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.00it/s, train_loss=0.583, val_loss=0.08\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.00it/s, train_loss=0.583, val_loss=0.08\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.00it/s, train_loss=0.527, val_loss=0.08\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.00it/s, train_loss=0.527, val_loss=0.08\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.00it/s, train_loss=0.523, val_loss=0.08\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.00it/s, train_loss=0.523, val_loss=0.08\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.00it/s, train_loss=0.49, val_loss=0.069\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.00it/s, train_loss=0.49, val_loss=0.069\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.00it/s, train_loss=0.406, val_loss=0.05\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 33.96it/s, train_loss=0.406, val_loss=0.05\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 33.96it/s, train_loss=0.406, val_loss=0.05\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 33.96it/s, train_loss=0.328, val_loss=0.05\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 33.96it/s, train_loss=0.328, val_loss=0.05\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 33.96it/s, train_loss=0.303, val_loss=0.05\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 33.96it/s, train_loss=0.303, val_loss=0.05\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 33.96it/s, train_loss=0.307, val_loss=0.05\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 33.96it/s, train_loss=0.307, val_loss=0.05\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 33.96it/s, train_loss=0.289, val_loss=0.04\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 33.96it/s, train_loss=0.289, val_loss=0.04\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 33.96it/s, train_loss=0.289, val_loss=0.04\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 33.96it/s, train_loss=0.243, val_loss=0.04\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 33.96it/s, train_loss=0.243, val_loss=0.04\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 33.56it/s, train_loss=0.21, val_loss=0.042\u001b[A\n",
      "  7%|███                                        | 7/100 [00:11<02:24,  1.55s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=17.3, val_loss=71.2, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=17.3, val_loss=71.2, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=376, val_loss=7.65, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=376, val_loss=7.65, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=39.2, val_loss=9.14, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=39.2, val_loss=9.14, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=46.5, val_loss=4.62, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 34.58it/s, train_loss=46.5, val_loss=4.62, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 34.58it/s, train_loss=46.5, val_loss=4.62, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 34.58it/s, train_loss=23.4, val_loss=0.536, \u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 34.58it/s, train_loss=23.4, val_loss=0.536, \u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 34.58it/s, train_loss=3.07, val_loss=2.24, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 34.58it/s, train_loss=3.07, val_loss=2.24, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 34.58it/s, train_loss=12.6, val_loss=3.16, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 34.58it/s, train_loss=12.6, val_loss=3.16, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7635566687539136\n",
      "Its' val AUC : 0.7277422862579855\n",
      "Its external  AUC: 0.6152512998266898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 34.58it/s, train_loss=16.1, val_loss=1.15, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 34.25it/s, train_loss=16.1, val_loss=1.15, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.25it/s, train_loss=16.1, val_loss=1.15, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.25it/s, train_loss=5.27, val_loss=0.132, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.25it/s, train_loss=5.27, val_loss=0.132, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.25it/s, train_loss=0.94, val_loss=0.939, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.25it/s, train_loss=0.94, val_loss=0.939,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.25it/s, train_loss=5.61, val_loss=0.837,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.25it/s, train_loss=5.61, val_loss=0.837,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.25it/s, train_loss=5.15, val_loss=0.288,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 33.53it/s, train_loss=5.15, val_loss=0.288\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.53it/s, train_loss=5.15, val_loss=0.288\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.53it/s, train_loss=1.89, val_loss=0.224\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.53it/s, train_loss=1.89, val_loss=0.224\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.53it/s, train_loss=0.978, val_loss=0.62\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.53it/s, train_loss=0.978, val_loss=0.62\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.53it/s, train_loss=2.87, val_loss=0.156\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.53it/s, train_loss=2.87, val_loss=0.156\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.53it/s, train_loss=0.771, val_loss=0.07\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 32.80it/s, train_loss=0.771, val_loss=0.07\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.80it/s, train_loss=0.771, val_loss=0.07\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.80it/s, train_loss=0.414, val_loss=0.17\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.80it/s, train_loss=0.414, val_loss=0.17\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.80it/s, train_loss=0.851, val_loss=0.24\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.80it/s, train_loss=0.851, val_loss=0.24\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.80it/s, train_loss=1.22, val_loss=0.183\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.80it/s, train_loss=1.22, val_loss=0.183\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.80it/s, train_loss=0.891, val_loss=0.06\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 32.90it/s, train_loss=0.891, val_loss=0.06\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.90it/s, train_loss=0.891, val_loss=0.06\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.90it/s, train_loss=0.332, val_loss=0.06\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.90it/s, train_loss=0.332, val_loss=0.06\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    15: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.90it/s, train_loss=0.339, val_loss=0.12\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.90it/s, train_loss=0.339, val_loss=0.12\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.90it/s, train_loss=0.678, val_loss=0.12\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.90it/s, train_loss=0.678, val_loss=0.12\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.90it/s, train_loss=0.646, val_loss=0.05\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 32.93it/s, train_loss=0.646, val_loss=0.05\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 32.93it/s, train_loss=0.646, val_loss=0.05\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 32.93it/s, train_loss=0.307, val_loss=0.04\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 32.93it/s, train_loss=0.307, val_loss=0.04\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 32.93it/s, train_loss=0.257, val_loss=0.09\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 32.93it/s, train_loss=0.257, val_loss=0.09\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 32.93it/s, train_loss=0.502, val_loss=0.10\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 32.93it/s, train_loss=0.502, val_loss=0.10\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 32.93it/s, train_loss=0.53, val_loss=0.053\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 33.52it/s, train_loss=0.53, val_loss=0.053\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.52it/s, train_loss=0.53, val_loss=0.053\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.52it/s, train_loss=0.282, val_loss=0.03\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.52it/s, train_loss=0.282, val_loss=0.03\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.52it/s, train_loss=0.177, val_loss=0.06\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.52it/s, train_loss=0.177, val_loss=0.06\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.52it/s, train_loss=0.31, val_loss=0.067\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.52it/s, train_loss=0.31, val_loss=0.067\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.52it/s, train_loss=0.337, val_loss=0.03\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 33.40it/s, train_loss=0.337, val_loss=0.03\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 33.40it/s, train_loss=0.337, val_loss=0.03\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 33.40it/s, train_loss=0.185, val_loss=0.02\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 33.40it/s, train_loss=0.185, val_loss=0.02\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 33.40it/s, train_loss=0.141, val_loss=0.04\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 33.40it/s, train_loss=0.141, val_loss=0.04\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 33.40it/s, train_loss=0.25, val_loss=0.050\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 33.40it/s, train_loss=0.25, val_loss=0.050\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 33.40it/s, train_loss=0.258, val_loss=0.02\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 33.19it/s, train_loss=0.258, val_loss=0.02\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.19it/s, train_loss=0.258, val_loss=0.02\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.19it/s, train_loss=0.137, val_loss=0.02\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.19it/s, train_loss=0.137, val_loss=0.02\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.19it/s, train_loss=0.106, val_loss=0.04\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.19it/s, train_loss=0.106, val_loss=0.04\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.19it/s, train_loss=0.178, val_loss=0.04\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.19it/s, train_loss=0.178, val_loss=0.04\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.19it/s, train_loss=0.171, val_loss=0.02\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 32.77it/s, train_loss=0.171, val_loss=0.02\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 32.77it/s, train_loss=0.171, val_loss=0.02\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 32.77it/s, train_loss=0.102, val_loss=0.02\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 32.77it/s, train_loss=0.102, val_loss=0.02\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 32.77it/s, train_loss=0.115, val_loss=0.03\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 32.77it/s, train_loss=0.115, val_loss=0.03\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 32.77it/s, train_loss=0.165, val_loss=0.02\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 32.77it/s, train_loss=0.165, val_loss=0.02\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 32.77it/s, train_loss=0.134, val_loss=0.01\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 32.83it/s, train_loss=0.134, val_loss=0.01\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 32.83it/s, train_loss=0.134, val_loss=0.01\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 32.83it/s, train_loss=0.0811, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 32.83it/s, train_loss=0.0811, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 32.83it/s, train_loss=0.0985, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 32.83it/s, train_loss=0.0985, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 32.83it/s, train_loss=0.119, val_loss=0.02\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 32.83it/s, train_loss=0.119, val_loss=0.02\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 32.83it/s, train_loss=0.0865, val_loss=0.0\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 32.70it/s, train_loss=0.0865, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 32.70it/s, train_loss=0.0865, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 32.70it/s, train_loss=0.0707, val_loss=0.0\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 32.70it/s, train_loss=0.0707, val_loss=0.0\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 33.04it/s, train_loss=0.0964, val_loss=0.0\u001b[A\n",
      "  8%|███▍                                       | 8/100 [00:12<02:21,  1.54s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=44.5, val_loss=5.12, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=44.5, val_loss=5.12, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=24.9, val_loss=19.1, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=24.9, val_loss=19.1, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=90.2, val_loss=4.88, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=90.2, val_loss=4.88, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=32.4, val_loss=11.6, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 32.18it/s, train_loss=32.4, val_loss=11.6, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.18it/s, train_loss=32.4, val_loss=11.6, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.18it/s, train_loss=55.6, val_loss=17.2, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.18it/s, train_loss=55.6, val_loss=17.2, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.18it/s, train_loss=89.5, val_loss=4.13, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.18it/s, train_loss=89.5, val_loss=4.13, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.18it/s, train_loss=21.8, val_loss=14.1, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.18it/s, train_loss=21.8, val_loss=14.1, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.24809016906700063\n",
      "Its' val AUC : 0.27701508767160526\n",
      "Its external  AUC: 0.3782889554120057\n",
      "Curent best Test AUC: 0.7373825923606763\n",
      "Its' val AUC : 0.7184993883376376\n",
      "Its external  AUC: 0.610051993067591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.18it/s, train_loss=69.4, val_loss=1.36, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 32.69it/s, train_loss=69.4, val_loss=1.36, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.69it/s, train_loss=69.4, val_loss=1.36, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.69it/s, train_loss=7.38, val_loss=8.43, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.69it/s, train_loss=7.38, val_loss=8.43, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.69it/s, train_loss=46.1, val_loss=4.69, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.69it/s, train_loss=46.1, val_loss=4.69, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.69it/s, train_loss=25.1, val_loss=2.6, t\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.69it/s, train_loss=25.1, val_loss=2.6, t\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.69it/s, train_loss=13.8, val_loss=5.49, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 32.14it/s, train_loss=13.8, val_loss=5.49,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.14it/s, train_loss=13.8, val_loss=5.49,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.14it/s, train_loss=29.8, val_loss=1.13,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.14it/s, train_loss=29.8, val_loss=1.13,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.14it/s, train_loss=5.66, val_loss=5.88,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.14it/s, train_loss=5.66, val_loss=5.88,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.14it/s, train_loss=29.6, val_loss=0.539\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.14it/s, train_loss=29.6, val_loss=0.539\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.14it/s, train_loss=2.87, val_loss=3.91,\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 32.18it/s, train_loss=2.87, val_loss=3.91,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.18it/s, train_loss=2.87, val_loss=3.91,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.18it/s, train_loss=17.6, val_loss=1.6, \u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.18it/s, train_loss=17.6, val_loss=1.6, \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.747777082028804\n",
      "Its' val AUC : 0.692809569117847\n",
      "Its external  AUC: 0.599338270048842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.18it/s, train_loss=7.63, val_loss=1.71,\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.18it/s, train_loss=7.63, val_loss=1.71,\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.18it/s, train_loss=9.7, val_loss=1.59, \u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.18it/s, train_loss=9.7, val_loss=1.59, \u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.18it/s, train_loss=7.71, val_loss=0.627\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 32.39it/s, train_loss=7.71, val_loss=0.627\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.39it/s, train_loss=7.71, val_loss=0.627\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.39it/s, train_loss=3.68, val_loss=1.65,\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.39it/s, train_loss=3.68, val_loss=1.65,\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.39it/s, train_loss=9.67, val_loss=0.483\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.39it/s, train_loss=9.67, val_loss=0.483\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.39it/s, train_loss=2.64, val_loss=0.814\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.39it/s, train_loss=2.64, val_loss=0.814\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.39it/s, train_loss=3.28, val_loss=1.55,\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 32.58it/s, train_loss=3.28, val_loss=1.55,\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 32.58it/s, train_loss=3.28, val_loss=1.55,\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 32.58it/s, train_loss=7.09, val_loss=0.58,\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 32.58it/s, train_loss=7.09, val_loss=0.58,\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 32.58it/s, train_loss=2.75, val_loss=0.245\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 32.58it/s, train_loss=2.75, val_loss=0.245\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    21: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 32.58it/s, train_loss=1.41, val_loss=0.968\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 32.58it/s, train_loss=1.41, val_loss=0.968\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 32.58it/s, train_loss=4.96, val_loss=0.511\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 31.29it/s, train_loss=4.96, val_loss=0.511\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 31.29it/s, train_loss=4.96, val_loss=0.511\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 31.29it/s, train_loss=2.55, val_loss=0.093\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 31.29it/s, train_loss=2.55, val_loss=0.093\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 31.29it/s, train_loss=0.567, val_loss=0.63\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 31.29it/s, train_loss=0.567, val_loss=0.63\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 31.29it/s, train_loss=3.4, val_loss=0.515,\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 31.29it/s, train_loss=3.4, val_loss=0.515,\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:01<00:00, 31.29it/s, train_loss=2.74, val_loss=0.177\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:01<00:00, 31.50it/s, train_loss=2.74, val_loss=0.177\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:01<00:00, 31.50it/s, train_loss=2.74, val_loss=0.177\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:01<00:00, 31.50it/s, train_loss=0.872, val_loss=0.56\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 31.50it/s, train_loss=0.872, val_loss=0.56\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 31.50it/s, train_loss=2.68, val_loss=0.437\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 31.50it/s, train_loss=2.68, val_loss=0.437\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 31.50it/s, train_loss=2.08, val_loss=0.062\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 31.50it/s, train_loss=2.08, val_loss=0.062\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 31.50it/s, train_loss=0.341, val_loss=0.31\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 32.13it/s, train_loss=0.341, val_loss=0.31\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 32.13it/s, train_loss=0.341, val_loss=0.31\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 32.13it/s, train_loss=1.72, val_loss=0.302\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 32.13it/s, train_loss=1.72, val_loss=0.302\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 32.13it/s, train_loss=1.63, val_loss=0.067\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 32.13it/s, train_loss=1.63, val_loss=0.067\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 32.13it/s, train_loss=0.402, val_loss=0.24\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 32.13it/s, train_loss=0.402, val_loss=0.24\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 32.13it/s, train_loss=1.37, val_loss=0.216\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 32.52it/s, train_loss=1.37, val_loss=0.216\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 32.52it/s, train_loss=1.37, val_loss=0.216\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 32.52it/s, train_loss=1.23, val_loss=0.085\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 32.52it/s, train_loss=1.23, val_loss=0.085\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 32.52it/s, train_loss=0.361, val_loss=0.15\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 32.52it/s, train_loss=0.361, val_loss=0.15\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 32.52it/s, train_loss=0.597, val_loss=0.18\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 32.52it/s, train_loss=0.597, val_loss=0.18\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 32.52it/s, train_loss=0.757, val_loss=0.13\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 32.72it/s, train_loss=0.757, val_loss=0.13\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 32.72it/s, train_loss=0.757, val_loss=0.13\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 32.72it/s, train_loss=0.536, val_loss=0.06\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 32.72it/s, train_loss=0.536, val_loss=0.06\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 32.72it/s, train_loss=0.285, val_loss=0.06\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 32.72it/s, train_loss=0.285, val_loss=0.06\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 32.72it/s, train_loss=0.338, val_loss=0.08\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 32.72it/s, train_loss=0.338, val_loss=0.08\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    41: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 32.72it/s, train_loss=0.462, val_loss=0.06\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 32.77it/s, train_loss=0.462, val_loss=0.06\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 32.77it/s, train_loss=0.462, val_loss=0.06\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 32.77it/s, train_loss=0.366, val_loss=0.03\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 32.77it/s, train_loss=0.366, val_loss=0.03\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 32.35it/s, train_loss=0.223, val_loss=0.05\u001b[A\n",
      "  9%|███▊                                       | 9/100 [00:14<02:20,  1.55s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=7.79, val_loss=111, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=7.79, val_loss=111, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=602, val_loss=0.841, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=602, val_loss=0.841, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=6.8, val_loss=15.7, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=6.8, val_loss=15.7, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=85.2, val_loss=2.43, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 31.36it/s, train_loss=85.2, val_loss=2.43, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.36it/s, train_loss=85.2, val_loss=2.43, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.36it/s, train_loss=13.7, val_loss=0.973, \u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.36it/s, train_loss=13.7, val_loss=0.973, \u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.36it/s, train_loss=6.51, val_loss=0.329, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.36it/s, train_loss=6.51, val_loss=0.329, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.36it/s, train_loss=1.68, val_loss=1.29, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.36it/s, train_loss=1.68, val_loss=1.29, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.3033187226048842\n",
      "Its' val AUC : 0.29917085768655705\n",
      "Its external  AUC: 0.39924373719867656\n",
      "Curent best Test AUC: 0.6351909830932999\n",
      "Its' val AUC : 0.5314666304200082\n",
      "Its external  AUC: 0.6587364109027887\n",
      "Curent best Test AUC: 0.691296180338134\n",
      "Its' val AUC : 0.6880521951882561\n",
      "Its external  AUC: 0.6234441468410272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.36it/s, train_loss=6.42, val_loss=0.406, \u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 32.71it/s, train_loss=6.42, val_loss=0.406, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.71it/s, train_loss=6.42, val_loss=0.406, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.71it/s, train_loss=2.76, val_loss=0.542, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.71it/s, train_loss=2.76, val_loss=0.542, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.71it/s, train_loss=2.88, val_loss=0.368, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.71it/s, train_loss=2.88, val_loss=0.368,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.71it/s, train_loss=1.67, val_loss=0.16, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.71it/s, train_loss=1.67, val_loss=0.16, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.71it/s, train_loss=1.05, val_loss=0.409,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 33.53it/s, train_loss=1.05, val_loss=0.409\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.53it/s, train_loss=1.05, val_loss=0.409\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.53it/s, train_loss=2.84, val_loss=0.404\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.53it/s, train_loss=2.84, val_loss=0.404\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.53it/s, train_loss=2.42, val_loss=0.116\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.53it/s, train_loss=2.42, val_loss=0.116\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.53it/s, train_loss=0.847, val_loss=0.11\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.53it/s, train_loss=0.847, val_loss=0.11\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.53it/s, train_loss=0.661, val_loss=0.11\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=0.661, val_loss=0.11\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=0.661, val_loss=0.11\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=0.838, val_loss=0.16\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=0.838, val_loss=0.16\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=1.06, val_loss=0.259\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=1.06, val_loss=0.259\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=1.41, val_loss=0.108\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=1.41, val_loss=0.108\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=0.677, val_loss=0.09\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 32.81it/s, train_loss=0.677, val_loss=0.09\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.81it/s, train_loss=0.677, val_loss=0.09\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.81it/s, train_loss=0.603, val_loss=0.06\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.81it/s, train_loss=0.603, val_loss=0.06\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.81it/s, train_loss=0.499, val_loss=0.08\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.81it/s, train_loss=0.499, val_loss=0.08\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.81it/s, train_loss=0.542, val_loss=0.09\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.81it/s, train_loss=0.542, val_loss=0.09\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.81it/s, train_loss=0.655, val_loss=0.04\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 33.06it/s, train_loss=0.655, val_loss=0.04\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.06it/s, train_loss=0.655, val_loss=0.04\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.06it/s, train_loss=0.315, val_loss=0.04\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.06it/s, train_loss=0.315, val_loss=0.04\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.06it/s, train_loss=0.258, val_loss=0.02\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.06it/s, train_loss=0.258, val_loss=0.02\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.06it/s, train_loss=0.15, val_loss=0.071\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.06it/s, train_loss=0.15, val_loss=0.071\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.06it/s, train_loss=0.423, val_loss=0.05\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 33.44it/s, train_loss=0.423, val_loss=0.05\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.44it/s, train_loss=0.423, val_loss=0.05\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.44it/s, train_loss=0.375, val_loss=0.05\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.44it/s, train_loss=0.375, val_loss=0.05\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.44it/s, train_loss=0.343, val_loss=0.02\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.44it/s, train_loss=0.343, val_loss=0.02\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.44it/s, train_loss=0.161, val_loss=0.03\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.44it/s, train_loss=0.161, val_loss=0.03\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.44it/s, train_loss=0.234, val_loss=0.04\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 33.82it/s, train_loss=0.234, val_loss=0.04\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 33.82it/s, train_loss=0.234, val_loss=0.04\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 33.82it/s, train_loss=0.265, val_loss=0.04\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 33.82it/s, train_loss=0.265, val_loss=0.04\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 33.82it/s, train_loss=0.247, val_loss=0.03\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 33.82it/s, train_loss=0.247, val_loss=0.03\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 33.82it/s, train_loss=0.216, val_loss=0.02\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 33.82it/s, train_loss=0.216, val_loss=0.02\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 33.82it/s, train_loss=0.134, val_loss=0.01\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 33.99it/s, train_loss=0.134, val_loss=0.01\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.99it/s, train_loss=0.134, val_loss=0.01\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.99it/s, train_loss=0.0964, val_loss=0.0\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.99it/s, train_loss=0.0964, val_loss=0.0\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.99it/s, train_loss=0.126, val_loss=0.02\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.99it/s, train_loss=0.126, val_loss=0.02\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    32: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.99it/s, train_loss=0.138, val_loss=0.01\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.99it/s, train_loss=0.138, val_loss=0.01\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.99it/s, train_loss=0.149, val_loss=0.01\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.24it/s, train_loss=0.149, val_loss=0.01\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.24it/s, train_loss=0.149, val_loss=0.01\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.24it/s, train_loss=0.141, val_loss=0.01\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.24it/s, train_loss=0.141, val_loss=0.01\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.24it/s, train_loss=0.0921, val_loss=0.0\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.24it/s, train_loss=0.0921, val_loss=0.0\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.24it/s, train_loss=0.0704, val_loss=0.0\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.24it/s, train_loss=0.0704, val_loss=0.0\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.24it/s, train_loss=0.0755, val_loss=0.0\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 33.75it/s, train_loss=0.0755, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 33.75it/s, train_loss=0.0755, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 33.75it/s, train_loss=0.082, val_loss=0.01\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 33.75it/s, train_loss=0.082, val_loss=0.01\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 33.75it/s, train_loss=0.101, val_loss=0.01\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 33.75it/s, train_loss=0.101, val_loss=0.01\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 33.75it/s, train_loss=0.098, val_loss=0.00\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 33.75it/s, train_loss=0.098, val_loss=0.00\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 33.75it/s, train_loss=0.0744, val_loss=0.0\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 33.15it/s, train_loss=0.0744, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 33.15it/s, train_loss=0.0744, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 33.15it/s, train_loss=0.065, val_loss=0.00\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 33.15it/s, train_loss=0.065, val_loss=0.00\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 33.31it/s, train_loss=0.0588, val_loss=0.0\u001b[A\n",
      " 10%|████▏                                     | 10/100 [00:15<02:18,  1.54s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=4.05, val_loss=17, train_cinde\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=4.05, val_loss=17, train_cinde\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=82.4, val_loss=6.55, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=82.4, val_loss=6.55, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=34.4, val_loss=0.176, train_ci\u001b[A\n",
      "Epoch 2:   6%| | 3/50 [00:00<00:01, 29.75it/s, train_loss=34.4, val_loss=0.176, \u001b[A\n",
      "Epoch 3:   6%| | 3/50 [00:00<00:01, 29.75it/s, train_loss=34.4, val_loss=0.176, \u001b[A\n",
      "Epoch 3:   6%| | 3/50 [00:00<00:01, 29.75it/s, train_loss=0.988, val_loss=3.5, t\u001b[A\n",
      "Epoch 4:   6%| | 3/50 [00:00<00:01, 29.75it/s, train_loss=0.988, val_loss=3.5, t\u001b[A\n",
      "Epoch 4:   6%| | 3/50 [00:00<00:01, 29.75it/s, train_loss=19.5, val_loss=3.42, t\u001b[A\n",
      "Epoch 5:   6%| | 3/50 [00:00<00:01, 29.75it/s, train_loss=19.5, val_loss=3.42, t\u001b[A\n",
      "Epoch 5:   6%| | 3/50 [00:00<00:01, 29.75it/s, train_loss=18, val_loss=1.01, tra\u001b[A\n",
      "Epoch 5:  12%| | 6/50 [00:00<00:01, 29.05it/s, train_loss=18, val_loss=1.01, tra\u001b[A\n",
      "Epoch 6:  12%| | 6/50 [00:00<00:01, 29.05it/s, train_loss=18, val_loss=1.01, tra\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7599248591108329\n",
      "Its' val AUC : 0.7221693625118935\n",
      "Its external  AUC: 0.6143059713250355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6:  12%| | 6/50 [00:00<00:01, 29.05it/s, train_loss=5.77, val_loss=1.66, t\u001b[A\n",
      "Epoch 7:  12%| | 6/50 [00:00<00:01, 29.05it/s, train_loss=5.77, val_loss=1.66, t\u001b[A\n",
      "Epoch 7:  12%| | 6/50 [00:00<00:01, 29.05it/s, train_loss=7.71, val_loss=1.06, t\u001b[A\n",
      "Epoch 8:  12%| | 6/50 [00:00<00:01, 29.05it/s, train_loss=7.71, val_loss=1.06, t\u001b[A\n",
      "Epoch 8:  12%| | 6/50 [00:00<00:01, 29.05it/s, train_loss=5.01, val_loss=0.495, \u001b[A\n",
      "Epoch 9:  12%| | 6/50 [00:00<00:01, 29.05it/s, train_loss=5.01, val_loss=0.495, \u001b[A\n",
      "Epoch 9:  12%| | 6/50 [00:00<00:01, 29.05it/s, train_loss=2.81, val_loss=0.624, \u001b[A\n",
      "Epoch 9:  20%|▏| 10/50 [00:00<00:01, 30.13it/s, train_loss=2.81, val_loss=0.624,\u001b[A\n",
      "Epoch 10:  20%|▏| 10/50 [00:00<00:01, 30.13it/s, train_loss=2.81, val_loss=0.624\u001b[A\n",
      "Epoch 10:  20%|▏| 10/50 [00:00<00:01, 30.13it/s, train_loss=3.28, val_loss=0.154\u001b[A\n",
      "Epoch 11:  20%|▏| 10/50 [00:00<00:01, 30.13it/s, train_loss=3.28, val_loss=0.154\u001b[A\n",
      "Epoch 11:  20%|▏| 10/50 [00:00<00:01, 30.13it/s, train_loss=0.765, val_loss=0.20\u001b[A\n",
      "Epoch 12:  20%|▏| 10/50 [00:00<00:01, 30.13it/s, train_loss=0.765, val_loss=0.20\u001b[A\n",
      "Epoch 12:  20%|▏| 10/50 [00:00<00:01, 30.13it/s, train_loss=1.17, val_loss=0.492\u001b[A\n",
      "Epoch 13:  20%|▏| 10/50 [00:00<00:01, 30.13it/s, train_loss=1.17, val_loss=0.492\u001b[A\n",
      "Epoch 13:  20%|▏| 10/50 [00:00<00:01, 30.13it/s, train_loss=2.78, val_loss=0.191\u001b[A\n",
      "Epoch 13:  28%|▎| 14/50 [00:00<00:01, 30.52it/s, train_loss=2.78, val_loss=0.191\u001b[A\n",
      "Epoch 14:  28%|▎| 14/50 [00:00<00:01, 30.52it/s, train_loss=2.78, val_loss=0.191\u001b[A\n",
      "Epoch 14:  28%|▎| 14/50 [00:00<00:01, 30.52it/s, train_loss=1.11, val_loss=0.078\u001b[A\n",
      "Epoch 15:  28%|▎| 14/50 [00:00<00:01, 30.52it/s, train_loss=1.11, val_loss=0.078\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     9: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15:  28%|▎| 14/50 [00:00<00:01, 30.52it/s, train_loss=0.339, val_loss=0.36\u001b[A\n",
      "Epoch 16:  28%|▎| 14/50 [00:00<00:01, 30.52it/s, train_loss=0.339, val_loss=0.36\u001b[A\n",
      "Epoch 16:  28%|▎| 14/50 [00:00<00:01, 30.52it/s, train_loss=1.78, val_loss=0.279\u001b[A\n",
      "Epoch 17:  28%|▎| 14/50 [00:00<00:01, 30.52it/s, train_loss=1.78, val_loss=0.279\u001b[A\n",
      "Epoch 17:  28%|▎| 14/50 [00:00<00:01, 30.52it/s, train_loss=1.39, val_loss=0.047\u001b[A\n",
      "Epoch 17:  36%|▎| 18/50 [00:00<00:01, 31.00it/s, train_loss=1.39, val_loss=0.047\u001b[A\n",
      "Epoch 18:  36%|▎| 18/50 [00:00<00:01, 31.00it/s, train_loss=1.39, val_loss=0.047\u001b[A\n",
      "Epoch 18:  36%|▎| 18/50 [00:00<00:01, 31.00it/s, train_loss=0.273, val_loss=0.19\u001b[A\n",
      "Epoch 19:  36%|▎| 18/50 [00:00<00:01, 31.00it/s, train_loss=0.273, val_loss=0.19\u001b[A\n",
      "Epoch 19:  36%|▎| 18/50 [00:00<00:01, 31.00it/s, train_loss=1.04, val_loss=0.227\u001b[A\n",
      "Epoch 20:  36%|▎| 18/50 [00:00<00:01, 31.00it/s, train_loss=1.04, val_loss=0.227\u001b[A\n",
      "Epoch 20:  36%|▎| 18/50 [00:00<00:01, 31.00it/s, train_loss=1.21, val_loss=0.032\u001b[A\n",
      "Epoch 21:  36%|▎| 18/50 [00:00<00:01, 31.00it/s, train_loss=1.21, val_loss=0.032\u001b[A\n",
      "Epoch 21:  36%|▎| 18/50 [00:00<00:01, 31.00it/s, train_loss=0.194, val_loss=0.10\u001b[A\n",
      "Epoch 21:  44%|▍| 22/50 [00:00<00:00, 31.13it/s, train_loss=0.194, val_loss=0.10\u001b[A\n",
      "Epoch 22:  44%|▍| 22/50 [00:00<00:00, 31.13it/s, train_loss=0.194, val_loss=0.10\u001b[A\n",
      "Epoch 22:  44%|▍| 22/50 [00:00<00:00, 31.13it/s, train_loss=0.543, val_loss=0.20\u001b[A\n",
      "Epoch 23:  44%|▍| 22/50 [00:00<00:00, 31.13it/s, train_loss=0.543, val_loss=0.20\u001b[A\n",
      "Epoch 23:  44%|▍| 22/50 [00:00<00:00, 31.13it/s, train_loss=1.05, val_loss=0.060\u001b[A\n",
      "Epoch 24:  44%|▍| 22/50 [00:00<00:00, 31.13it/s, train_loss=1.05, val_loss=0.060\u001b[A\n",
      "Epoch 24:  44%|▍| 22/50 [00:00<00:00, 31.13it/s, train_loss=0.341, val_loss=0.05\u001b[A\n",
      "Epoch 25:  44%|▍| 22/50 [00:00<00:00, 31.13it/s, train_loss=0.341, val_loss=0.05\u001b[A\n",
      "Epoch 25:  44%|▍| 22/50 [00:00<00:00, 31.13it/s, train_loss=0.3, val_loss=0.155,\u001b[A\n",
      "Epoch 25:  52%|▌| 26/50 [00:00<00:00, 31.69it/s, train_loss=0.3, val_loss=0.155,\u001b[A\n",
      "Epoch 26:  52%|▌| 26/50 [00:00<00:00, 31.69it/s, train_loss=0.3, val_loss=0.155,\u001b[A\n",
      "Epoch 26:  52%|▌| 26/50 [00:00<00:00, 31.69it/s, train_loss=0.783, val_loss=0.06\u001b[A\n",
      "Epoch 27:  52%|▌| 26/50 [00:00<00:00, 31.69it/s, train_loss=0.783, val_loss=0.06\u001b[A\n",
      "Epoch 27:  52%|▌| 26/50 [00:00<00:00, 31.69it/s, train_loss=0.325, val_loss=0.01\u001b[A\n",
      "Epoch 28:  52%|▌| 26/50 [00:00<00:00, 31.69it/s, train_loss=0.325, val_loss=0.01\u001b[A\n",
      "Epoch 28:  52%|▌| 26/50 [00:00<00:00, 31.69it/s, train_loss=0.129, val_loss=0.02\u001b[A\n",
      "Epoch 29:  52%|▌| 26/50 [00:00<00:00, 31.69it/s, train_loss=0.129, val_loss=0.02\u001b[A\n",
      "Epoch 29:  52%|▌| 26/50 [00:00<00:00, 31.69it/s, train_loss=0.201, val_loss=0.05\u001b[A\n",
      "Epoch 29:  60%|▌| 30/50 [00:00<00:00, 32.03it/s, train_loss=0.201, val_loss=0.05\u001b[A\n",
      "Epoch 30:  60%|▌| 30/50 [00:00<00:00, 32.03it/s, train_loss=0.201, val_loss=0.05\u001b[A\n",
      "Epoch 30:  60%|▌| 30/50 [00:00<00:00, 32.03it/s, train_loss=0.352, val_loss=0.05\u001b[A\n",
      "Epoch 31:  60%|▌| 30/50 [00:00<00:00, 32.03it/s, train_loss=0.352, val_loss=0.05\u001b[A\n",
      "Epoch 31:  60%|▌| 30/50 [00:01<00:00, 32.03it/s, train_loss=0.299, val_loss=0.02\u001b[A\n",
      "Epoch 32:  60%|▌| 30/50 [00:01<00:00, 32.03it/s, train_loss=0.299, val_loss=0.02\u001b[A\n",
      "Epoch 32:  60%|▌| 30/50 [00:01<00:00, 32.03it/s, train_loss=0.124, val_loss=0.01\u001b[A\n",
      "Epoch 33:  60%|▌| 30/50 [00:01<00:00, 32.03it/s, train_loss=0.124, val_loss=0.01\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    27: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33:  60%|▌| 30/50 [00:01<00:00, 32.03it/s, train_loss=0.0959, val_loss=0.0\u001b[A\n",
      "Epoch 33:  68%|▋| 34/50 [00:01<00:00, 32.51it/s, train_loss=0.0959, val_loss=0.0\u001b[A\n",
      "Epoch 34:  68%|▋| 34/50 [00:01<00:00, 32.51it/s, train_loss=0.0959, val_loss=0.0\u001b[A\n",
      "Epoch 34:  68%|▋| 34/50 [00:01<00:00, 32.51it/s, train_loss=0.216, val_loss=0.04\u001b[A\n",
      "Epoch 35:  68%|▋| 34/50 [00:01<00:00, 32.51it/s, train_loss=0.216, val_loss=0.04\u001b[A\n",
      "Epoch 35:  68%|▋| 34/50 [00:01<00:00, 32.51it/s, train_loss=0.247, val_loss=0.02\u001b[A\n",
      "Epoch 36:  68%|▋| 34/50 [00:01<00:00, 32.51it/s, train_loss=0.247, val_loss=0.02\u001b[A\n",
      "Epoch 36:  68%|▋| 34/50 [00:01<00:00, 32.51it/s, train_loss=0.137, val_loss=0.01\u001b[A\n",
      "Epoch 37:  68%|▋| 34/50 [00:01<00:00, 32.51it/s, train_loss=0.137, val_loss=0.01\u001b[A\n",
      "Epoch 37:  68%|▋| 34/50 [00:01<00:00, 32.51it/s, train_loss=0.0744, val_loss=0.0\u001b[A\n",
      "Epoch 37:  76%|▊| 38/50 [00:01<00:00, 33.08it/s, train_loss=0.0744, val_loss=0.0\u001b[A\n",
      "Epoch 38:  76%|▊| 38/50 [00:01<00:00, 33.08it/s, train_loss=0.0744, val_loss=0.0\u001b[A\n",
      "Epoch 38:  76%|▊| 38/50 [00:01<00:00, 33.08it/s, train_loss=0.137, val_loss=0.03\u001b[A\n",
      "Epoch 39:  76%|▊| 38/50 [00:01<00:00, 33.08it/s, train_loss=0.137, val_loss=0.03\u001b[A\n",
      "Epoch 39:  76%|▊| 38/50 [00:01<00:00, 33.08it/s, train_loss=0.184, val_loss=0.02\u001b[A\n",
      "Epoch 40:  76%|▊| 38/50 [00:01<00:00, 33.08it/s, train_loss=0.184, val_loss=0.02\u001b[A\n",
      "Epoch 40:  76%|▊| 38/50 [00:01<00:00, 33.08it/s, train_loss=0.128, val_loss=0.01\u001b[A\n",
      "Epoch 41:  76%|▊| 38/50 [00:01<00:00, 33.08it/s, train_loss=0.128, val_loss=0.01\u001b[A\n",
      "Epoch 41:  76%|▊| 38/50 [00:01<00:00, 33.08it/s, train_loss=0.0704, val_loss=0.0\u001b[A\n",
      "Epoch 41:  84%|▊| 42/50 [00:01<00:00, 33.10it/s, train_loss=0.0704, val_loss=0.0\u001b[A\n",
      "Epoch 42:  84%|▊| 42/50 [00:01<00:00, 33.10it/s, train_loss=0.0704, val_loss=0.0\u001b[A\n",
      "Epoch 42:  84%|▊| 42/50 [00:01<00:00, 33.10it/s, train_loss=0.101, val_loss=0.02\u001b[A\n",
      "Epoch 43:  84%|▊| 42/50 [00:01<00:00, 33.10it/s, train_loss=0.101, val_loss=0.02\u001b[A\n",
      "Epoch 43:  84%|▊| 42/50 [00:01<00:00, 33.10it/s, train_loss=0.141, val_loss=0.01\u001b[A\n",
      "Epoch 44:  84%|▊| 42/50 [00:01<00:00, 33.10it/s, train_loss=0.141, val_loss=0.01\u001b[A\n",
      "Epoch 44:  84%|▊| 42/50 [00:01<00:00, 33.10it/s, train_loss=0.108, val_loss=0.00\u001b[A\n",
      "Epoch 45:  84%|▊| 42/50 [00:01<00:00, 33.10it/s, train_loss=0.108, val_loss=0.00\u001b[A\n",
      "Epoch 45:  84%|▊| 42/50 [00:01<00:00, 33.10it/s, train_loss=0.0636, val_loss=0.0\u001b[A\n",
      "Epoch 45:  92%|▉| 46/50 [00:01<00:00, 32.92it/s, train_loss=0.0636, val_loss=0.0\u001b[A\n",
      "Epoch 46:  92%|▉| 46/50 [00:01<00:00, 32.92it/s, train_loss=0.0636, val_loss=0.0\u001b[A\n",
      "Epoch 46:  92%|▉| 46/50 [00:01<00:00, 32.92it/s, train_loss=0.0807, val_loss=0.0\u001b[A\n",
      "Epoch 47:  92%|▉| 46/50 [00:01<00:00, 32.92it/s, train_loss=0.0807, val_loss=0.0\u001b[A\n",
      "Epoch 47:  92%|▉| 46/50 [00:01<00:00, 32.92it/s, train_loss=0.11, val_loss=0.016\u001b[A\n",
      "Epoch 48:  92%|▉| 46/50 [00:01<00:00, 32.92it/s, train_loss=0.11, val_loss=0.016\u001b[A\n",
      "Epoch 48:  92%|▉| 46/50 [00:01<00:00, 32.92it/s, train_loss=0.0903, val_loss=0.0\u001b[A\n",
      "Epoch 49:  92%|▉| 46/50 [00:01<00:00, 32.92it/s, train_loss=0.0903, val_loss=0.0\u001b[A\n",
      "Epoch 49:  92%|▉| 46/50 [00:01<00:00, 32.92it/s, train_loss=0.0598, val_loss=0.0\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 32.02it/s, train_loss=0.0598, val_loss=0.0\u001b[A\n",
      " 11%|████▌                                     | 11/100 [00:17<02:18,  1.55s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=20.2, val_loss=42, train_cinde\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=20.2, val_loss=42, train_cinde\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=188, val_loss=11.7, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=188, val_loss=11.7, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=54.7, val_loss=0.894, train_ci\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=54.7, val_loss=0.894, train_ci\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=3.9, val_loss=3.07, train_cind\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 32.93it/s, train_loss=3.9, val_loss=3.07, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.93it/s, train_loss=3.9, val_loss=3.07, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.93it/s, train_loss=16.8, val_loss=0.91, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.93it/s, train_loss=16.8, val_loss=0.91, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.93it/s, train_loss=4.87, val_loss=0.942, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.93it/s, train_loss=4.87, val_loss=0.942, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.93it/s, train_loss=5.2, val_loss=0.233, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.93it/s, train_loss=5.2, val_loss=0.233, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7570444583594239\n",
      "Its' val AUC : 0.7195867880929727\n",
      "Its external  AUC: 0.6248621395935088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.93it/s, train_loss=1.44, val_loss=0.889, \u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 32.72it/s, train_loss=1.44, val_loss=0.889, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.72it/s, train_loss=1.44, val_loss=0.889, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.72it/s, train_loss=5.83, val_loss=0.26, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.72it/s, train_loss=5.83, val_loss=0.26, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.72it/s, train_loss=1.68, val_loss=0.773, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.72it/s, train_loss=1.68, val_loss=0.773,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.72it/s, train_loss=4.02, val_loss=0.423,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.72it/s, train_loss=4.02, val_loss=0.423,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.72it/s, train_loss=2.09, val_loss=0.356,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 32.67it/s, train_loss=2.09, val_loss=0.356\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.67it/s, train_loss=2.09, val_loss=0.356\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.67it/s, train_loss=1.72, val_loss=0.549\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.67it/s, train_loss=1.72, val_loss=0.549\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.67it/s, train_loss=3.07, val_loss=0.171\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.67it/s, train_loss=3.07, val_loss=0.171\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.67it/s, train_loss=0.935, val_loss=0.22\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.67it/s, train_loss=0.935, val_loss=0.22\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.67it/s, train_loss=1.18, val_loss=0.306\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 32.76it/s, train_loss=1.18, val_loss=0.306\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.76it/s, train_loss=1.18, val_loss=0.306\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7792110206637445\n",
      "Its' val AUC : 0.6785374473290744\n",
      "Its external  AUC: 0.6158815188277926\n",
      "Epoch    13: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.76it/s, train_loss=1.7, val_loss=0.099,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.76it/s, train_loss=1.7, val_loss=0.099,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.76it/s, train_loss=0.59, val_loss=0.254\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.76it/s, train_loss=0.59, val_loss=0.254\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.76it/s, train_loss=1.16, val_loss=0.159\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.76it/s, train_loss=1.16, val_loss=0.159\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.76it/s, train_loss=0.691, val_loss=0.09\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 32.73it/s, train_loss=0.691, val_loss=0.09\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.73it/s, train_loss=0.691, val_loss=0.09\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.73it/s, train_loss=0.506, val_loss=0.16\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.73it/s, train_loss=0.506, val_loss=0.16\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.73it/s, train_loss=0.947, val_loss=0.07\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.73it/s, train_loss=0.947, val_loss=0.07\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.73it/s, train_loss=0.359, val_loss=0.16\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.73it/s, train_loss=0.359, val_loss=0.16\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.73it/s, train_loss=0.708, val_loss=0.11\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 31.92it/s, train_loss=0.708, val_loss=0.11\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 31.92it/s, train_loss=0.708, val_loss=0.11\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 31.92it/s, train_loss=0.502, val_loss=0.04\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 31.92it/s, train_loss=0.502, val_loss=0.04\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 31.92it/s, train_loss=0.274, val_loss=0.09\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 31.92it/s, train_loss=0.274, val_loss=0.09\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 31.92it/s, train_loss=0.544, val_loss=0.03\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 31.92it/s, train_loss=0.544, val_loss=0.03\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 31.92it/s, train_loss=0.189, val_loss=0.09\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 32.00it/s, train_loss=0.189, val_loss=0.09\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 32.00it/s, train_loss=0.189, val_loss=0.09\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 32.00it/s, train_loss=0.442, val_loss=0.06\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 32.00it/s, train_loss=0.442, val_loss=0.06\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 32.00it/s, train_loss=0.311, val_loss=0.04\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 32.00it/s, train_loss=0.311, val_loss=0.04\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 32.00it/s, train_loss=0.245, val_loss=0.07\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 32.00it/s, train_loss=0.245, val_loss=0.07\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 32.00it/s, train_loss=0.36, val_loss=0.033\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 32.40it/s, train_loss=0.36, val_loss=0.033\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 32.40it/s, train_loss=0.36, val_loss=0.033\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:01<00:00, 32.40it/s, train_loss=0.147, val_loss=0.06\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 32.40it/s, train_loss=0.147, val_loss=0.06\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 32.40it/s, train_loss=0.332, val_loss=0.05\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 32.40it/s, train_loss=0.332, val_loss=0.05\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 32.40it/s, train_loss=0.234, val_loss=0.03\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 32.40it/s, train_loss=0.234, val_loss=0.03\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 32.40it/s, train_loss=0.138, val_loss=0.05\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 32.71it/s, train_loss=0.138, val_loss=0.05\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 32.71it/s, train_loss=0.138, val_loss=0.05\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 32.71it/s, train_loss=0.228, val_loss=0.05\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 32.71it/s, train_loss=0.228, val_loss=0.05\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 32.71it/s, train_loss=0.229, val_loss=0.03\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 32.71it/s, train_loss=0.229, val_loss=0.03\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 32.71it/s, train_loss=0.124, val_loss=0.03\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 32.71it/s, train_loss=0.124, val_loss=0.03\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    33: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 32.71it/s, train_loss=0.155, val_loss=0.04\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 32.66it/s, train_loss=0.155, val_loss=0.04\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 32.66it/s, train_loss=0.155, val_loss=0.04\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 32.66it/s, train_loss=0.213, val_loss=0.03\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 32.66it/s, train_loss=0.213, val_loss=0.03\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 32.66it/s, train_loss=0.145, val_loss=0.02\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 32.66it/s, train_loss=0.145, val_loss=0.02\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 32.66it/s, train_loss=0.123, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 32.66it/s, train_loss=0.123, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 32.66it/s, train_loss=0.174, val_loss=0.03\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 33.02it/s, train_loss=0.174, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 33.02it/s, train_loss=0.174, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 33.02it/s, train_loss=0.141, val_loss=0.02\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 33.02it/s, train_loss=0.141, val_loss=0.02\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 33.02it/s, train_loss=0.1, val_loss=0.0353\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 33.02it/s, train_loss=0.1, val_loss=0.0353\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 33.02it/s, train_loss=0.14, val_loss=0.034\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 33.02it/s, train_loss=0.14, val_loss=0.034\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 33.02it/s, train_loss=0.136, val_loss=0.02\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 33.47it/s, train_loss=0.136, val_loss=0.02\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 33.47it/s, train_loss=0.136, val_loss=0.02\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 33.47it/s, train_loss=0.0984, val_loss=0.0\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 33.47it/s, train_loss=0.0984, val_loss=0.0\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 32.85it/s, train_loss=0.119, val_loss=0.03\u001b[A\n",
      " 12%|█████                                     | 12/100 [00:18<02:16,  1.55s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=4.47, val_loss=47.7, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=4.47, val_loss=47.7, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=250, val_loss=31.1, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=250, val_loss=31.1, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=171, val_loss=1, train_cindex=\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=171, val_loss=1, train_cindex=\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=6.03, val_loss=11.9, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.39it/s, train_loss=6.03, val_loss=11.9, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.39it/s, train_loss=6.03, val_loss=11.9, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.39it/s, train_loss=56.6, val_loss=0.694, \u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.39it/s, train_loss=56.6, val_loss=0.694, \u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.39it/s, train_loss=4.41, val_loss=3.85, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.39it/s, train_loss=4.41, val_loss=3.85, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.39it/s, train_loss=20.3, val_loss=4.12, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.39it/s, train_loss=20.3, val_loss=4.12, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.26988102692548527\n",
      "Its' val AUC : 0.28014136196819356\n",
      "Its external  AUC: 0.38364581692138017\n",
      "Curent best Test AUC: 0.7269881026925485\n",
      "Its' val AUC : 0.6945765937202664\n",
      "Its external  AUC: 0.6056404600598708\n",
      "Curent best Test AUC: 0.7463994990607389\n",
      "Its' val AUC : 0.7062661410901182\n",
      "Its external  AUC: 0.5908303135339531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.39it/s, train_loss=21.5, val_loss=1.39, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 33.44it/s, train_loss=21.5, val_loss=1.39, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.44it/s, train_loss=21.5, val_loss=1.39, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.44it/s, train_loss=6.99, val_loss=3.1, tr\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.44it/s, train_loss=6.99, val_loss=3.1, tr\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.44it/s, train_loss=17.1, val_loss=0.897, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.44it/s, train_loss=17.1, val_loss=0.897,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.44it/s, train_loss=5.01, val_loss=1.65, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.44it/s, train_loss=5.01, val_loss=1.65, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.44it/s, train_loss=8.4, val_loss=1.25, t\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 33.26it/s, train_loss=8.4, val_loss=1.25, \u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.26it/s, train_loss=8.4, val_loss=1.25, \u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.26it/s, train_loss=6.31, val_loss=0.163\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.26it/s, train_loss=6.31, val_loss=0.163\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.26it/s, train_loss=0.952, val_loss=1.01\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.26it/s, train_loss=0.952, val_loss=1.01\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.26it/s, train_loss=5.37, val_loss=1.04,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.26it/s, train_loss=5.37, val_loss=1.04,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.26it/s, train_loss=5.64, val_loss=0.205\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 33.40it/s, train_loss=5.64, val_loss=0.205\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.40it/s, train_loss=5.64, val_loss=0.205\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.40it/s, train_loss=1.23, val_loss=0.691\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.40it/s, train_loss=1.23, val_loss=0.691\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    11: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Curent best Test AUC: 0.7557921102066374\n",
      "Its' val AUC : 0.7030039418241131\n",
      "Its external  AUC: 0.5942965180400189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.40it/s, train_loss=3.38, val_loss=0.758\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.40it/s, train_loss=3.38, val_loss=0.758\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.40it/s, train_loss=3.74, val_loss=0.136\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.40it/s, train_loss=3.74, val_loss=0.136\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.40it/s, train_loss=0.776, val_loss=0.33\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 33.90it/s, train_loss=0.776, val_loss=0.33\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.90it/s, train_loss=0.776, val_loss=0.33\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.90it/s, train_loss=1.88, val_loss=0.487\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.90it/s, train_loss=1.88, val_loss=0.487\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.90it/s, train_loss=2.47, val_loss=0.086\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.90it/s, train_loss=2.47, val_loss=0.086\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.90it/s, train_loss=0.416, val_loss=0.25\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.90it/s, train_loss=0.416, val_loss=0.25\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.90it/s, train_loss=1.55, val_loss=0.362\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.16it/s, train_loss=1.55, val_loss=0.362\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.16it/s, train_loss=1.55, val_loss=0.362\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.16it/s, train_loss=2.17, val_loss=0.117\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.16it/s, train_loss=2.17, val_loss=0.117\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.16it/s, train_loss=0.642, val_loss=0.31\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.16it/s, train_loss=0.642, val_loss=0.31\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.16it/s, train_loss=1.31, val_loss=0.336\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.16it/s, train_loss=1.31, val_loss=0.336\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.16it/s, train_loss=1.49, val_loss=0.053\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.33it/s, train_loss=1.49, val_loss=0.053\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.33it/s, train_loss=1.49, val_loss=0.053\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.33it/s, train_loss=0.298, val_loss=0.18\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.33it/s, train_loss=0.298, val_loss=0.18\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.33it/s, train_loss=1.05, val_loss=0.177\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.33it/s, train_loss=1.05, val_loss=0.177\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.33it/s, train_loss=1.01, val_loss=0.036\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.33it/s, train_loss=1.01, val_loss=0.036\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.33it/s, train_loss=0.258, val_loss=0.18\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.01it/s, train_loss=0.258, val_loss=0.18\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.01it/s, train_loss=0.258, val_loss=0.18\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.01it/s, train_loss=0.953, val_loss=0.13\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.01it/s, train_loss=0.953, val_loss=0.13\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 34.01it/s, train_loss=0.691, val_loss=0.06\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.01it/s, train_loss=0.691, val_loss=0.06\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.01it/s, train_loss=0.35, val_loss=0.156\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.01it/s, train_loss=0.35, val_loss=0.156\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.01it/s, train_loss=0.859, val_loss=0.06\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.07it/s, train_loss=0.859, val_loss=0.06\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.07it/s, train_loss=0.859, val_loss=0.06\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.07it/s, train_loss=0.374, val_loss=0.05\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.07it/s, train_loss=0.374, val_loss=0.05\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.07it/s, train_loss=0.294, val_loss=0.08\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.07it/s, train_loss=0.294, val_loss=0.08\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.07it/s, train_loss=0.417, val_loss=0.05\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.07it/s, train_loss=0.417, val_loss=0.05\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.07it/s, train_loss=0.271, val_loss=0.02\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.26it/s, train_loss=0.271, val_loss=0.02\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.26it/s, train_loss=0.271, val_loss=0.02\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.26it/s, train_loss=0.15, val_loss=0.041\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.26it/s, train_loss=0.15, val_loss=0.041\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.26it/s, train_loss=0.26, val_loss=0.055\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.26it/s, train_loss=0.26, val_loss=0.055\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.26it/s, train_loss=0.333, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.26it/s, train_loss=0.333, val_loss=0.03\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    37: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.26it/s, train_loss=0.217, val_loss=0.02\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.38it/s, train_loss=0.217, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.38it/s, train_loss=0.217, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.38it/s, train_loss=0.149, val_loss=0.05\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.38it/s, train_loss=0.149, val_loss=0.05\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.38it/s, train_loss=0.235, val_loss=0.05\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.38it/s, train_loss=0.235, val_loss=0.05\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.38it/s, train_loss=0.256, val_loss=0.04\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.38it/s, train_loss=0.256, val_loss=0.04\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.38it/s, train_loss=0.199, val_loss=0.02\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.43it/s, train_loss=0.199, val_loss=0.02\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.43it/s, train_loss=0.199, val_loss=0.02\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.43it/s, train_loss=0.14, val_loss=0.023\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.43it/s, train_loss=0.14, val_loss=0.023\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.07it/s, train_loss=0.137, val_loss=0.02\u001b[A\n",
      " 13%|█████▍                                    | 13/100 [00:20<02:13,  1.53s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=28.6, val_loss=101, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=28.6, val_loss=101, train_cind\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    46: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Curent best Test AUC: 0.7139636819035692\n",
      "Its' val AUC : 0.7084409406007883\n",
      "Its external  AUC: 0.6166692925791712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=553, val_loss=4.56, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=553, val_loss=4.56, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=25.5, val_loss=2.33, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=25.5, val_loss=2.33, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=11.1, val_loss=1.45, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 31.21it/s, train_loss=11.1, val_loss=1.45, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.21it/s, train_loss=11.1, val_loss=1.45, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.21it/s, train_loss=6.86, val_loss=0.618, \u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.21it/s, train_loss=6.86, val_loss=0.618, \u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.21it/s, train_loss=3.46, val_loss=0.892, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.21it/s, train_loss=3.46, val_loss=0.892, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.21it/s, train_loss=4.92, val_loss=0.219, \u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.21it/s, train_loss=4.92, val_loss=0.219, \u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.21it/s, train_loss=1.14, val_loss=0.565, \u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 31.72it/s, train_loss=1.14, val_loss=0.565, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 31.72it/s, train_loss=1.14, val_loss=0.565, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 31.72it/s, train_loss=2.98, val_loss=0.227, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 31.72it/s, train_loss=2.98, val_loss=0.227, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 31.72it/s, train_loss=1.28, val_loss=0.136, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 31.72it/s, train_loss=1.28, val_loss=0.136,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 31.72it/s, train_loss=0.788, val_loss=0.137\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 31.72it/s, train_loss=0.788, val_loss=0.137\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 31.72it/s, train_loss=0.689, val_loss=0.056\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 31.18it/s, train_loss=0.689, val_loss=0.05\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 31.18it/s, train_loss=0.689, val_loss=0.05\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 31.18it/s, train_loss=0.304, val_loss=0.09\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 31.18it/s, train_loss=0.304, val_loss=0.09\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 31.18it/s, train_loss=0.617, val_loss=0.06\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 31.18it/s, train_loss=0.617, val_loss=0.06\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 31.18it/s, train_loss=0.285, val_loss=0.10\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 31.18it/s, train_loss=0.285, val_loss=0.10\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 31.18it/s, train_loss=0.448, val_loss=0.13\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 30.64it/s, train_loss=0.448, val_loss=0.13\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 30.64it/s, train_loss=0.448, val_loss=0.13\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 30.64it/s, train_loss=0.68, val_loss=0.067\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 30.64it/s, train_loss=0.68, val_loss=0.067\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 30.64it/s, train_loss=0.332, val_loss=0.06\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 30.64it/s, train_loss=0.332, val_loss=0.06\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 30.64it/s, train_loss=0.349, val_loss=0.03\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 30.64it/s, train_loss=0.349, val_loss=0.03\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 30.64it/s, train_loss=0.209, val_loss=0.05\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 31.48it/s, train_loss=0.209, val_loss=0.05\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 31.48it/s, train_loss=0.209, val_loss=0.05\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 31.48it/s, train_loss=0.334, val_loss=0.03\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 31.48it/s, train_loss=0.334, val_loss=0.03\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 31.48it/s, train_loss=0.205, val_loss=0.04\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 31.48it/s, train_loss=0.205, val_loss=0.04\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 31.48it/s, train_loss=0.264, val_loss=0.03\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 31.48it/s, train_loss=0.264, val_loss=0.03\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 31.48it/s, train_loss=0.212, val_loss=0.03\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 32.24it/s, train_loss=0.212, val_loss=0.03\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 32.24it/s, train_loss=0.212, val_loss=0.03\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    18: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 32.24it/s, train_loss=0.205, val_loss=0.04\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 32.24it/s, train_loss=0.205, val_loss=0.04\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 32.24it/s, train_loss=0.218, val_loss=0.02\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 32.24it/s, train_loss=0.218, val_loss=0.02\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 32.24it/s, train_loss=0.144, val_loss=0.03\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 32.24it/s, train_loss=0.144, val_loss=0.03\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 32.24it/s, train_loss=0.174, val_loss=0.01\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 32.48it/s, train_loss=0.174, val_loss=0.01\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 32.48it/s, train_loss=0.174, val_loss=0.01\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 32.48it/s, train_loss=0.0981, val_loss=0.0\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 32.48it/s, train_loss=0.0981, val_loss=0.0\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 32.48it/s, train_loss=0.158, val_loss=0.02\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 32.48it/s, train_loss=0.158, val_loss=0.02\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 32.48it/s, train_loss=0.105, val_loss=0.03\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 32.48it/s, train_loss=0.105, val_loss=0.03\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:01<00:00, 32.48it/s, train_loss=0.148, val_loss=0.02\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:01<00:00, 31.68it/s, train_loss=0.148, val_loss=0.02\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:01<00:00, 31.68it/s, train_loss=0.148, val_loss=0.02\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:01<00:00, 31.68it/s, train_loss=0.0968, val_loss=0.0\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 31.68it/s, train_loss=0.0968, val_loss=0.0\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 31.68it/s, train_loss=0.122, val_loss=0.01\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 31.68it/s, train_loss=0.122, val_loss=0.01\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 31.68it/s, train_loss=0.0858, val_loss=0.0\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 31.68it/s, train_loss=0.0858, val_loss=0.0\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 31.68it/s, train_loss=0.0801, val_loss=0.0\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 31.84it/s, train_loss=0.0801, val_loss=0.0\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 31.84it/s, train_loss=0.0801, val_loss=0.0\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 31.84it/s, train_loss=0.0857, val_loss=0.0\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 31.84it/s, train_loss=0.0857, val_loss=0.0\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 31.84it/s, train_loss=0.0736, val_loss=0.0\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 31.84it/s, train_loss=0.0736, val_loss=0.0\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 31.84it/s, train_loss=0.07, val_loss=0.014\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 31.84it/s, train_loss=0.07, val_loss=0.014\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 31.84it/s, train_loss=0.0788, val_loss=0.0\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 32.45it/s, train_loss=0.0788, val_loss=0.0\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 32.45it/s, train_loss=0.0788, val_loss=0.0\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    34: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 32.45it/s, train_loss=0.0743, val_loss=0.0\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 32.45it/s, train_loss=0.0743, val_loss=0.0\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 32.45it/s, train_loss=0.0744, val_loss=0.0\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 32.45it/s, train_loss=0.0744, val_loss=0.0\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 32.45it/s, train_loss=0.0791, val_loss=0.0\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 32.45it/s, train_loss=0.0791, val_loss=0.0\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 32.45it/s, train_loss=0.0718, val_loss=0.0\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 32.23it/s, train_loss=0.0718, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 32.23it/s, train_loss=0.0718, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 32.23it/s, train_loss=0.07, val_loss=0.013\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 32.23it/s, train_loss=0.07, val_loss=0.013\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 32.23it/s, train_loss=0.072, val_loss=0.01\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 32.23it/s, train_loss=0.072, val_loss=0.01\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 32.23it/s, train_loss=0.0665, val_loss=0.0\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 32.23it/s, train_loss=0.0665, val_loss=0.0\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 32.23it/s, train_loss=0.0669, val_loss=0.0\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 31.98it/s, train_loss=0.0669, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 31.98it/s, train_loss=0.0669, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 31.98it/s, train_loss=0.0673, val_loss=0.0\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 31.98it/s, train_loss=0.0673, val_loss=0.0\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 31.81it/s, train_loss=0.0626, val_loss=0.0\u001b[A\n",
      " 14%|█████▉                                    | 14/100 [00:21<02:13,  1.55s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=70.6, val_loss=56.2, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=70.6, val_loss=56.2, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=299, val_loss=0.656, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=299, val_loss=0.656, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=3.69, val_loss=1.07, train_cin\u001b[A\n",
      "Epoch 2:   6%| | 3/50 [00:00<00:01, 27.46it/s, train_loss=3.69, val_loss=1.07, t\u001b[A\n",
      "Epoch 3:   6%| | 3/50 [00:00<00:01, 27.46it/s, train_loss=3.69, val_loss=1.07, t\u001b[A\n",
      "Epoch 3:   6%| | 3/50 [00:00<00:01, 27.46it/s, train_loss=6.09, val_loss=6.01, t\u001b[A\n",
      "Epoch 4:   6%| | 3/50 [00:00<00:01, 27.46it/s, train_loss=6.09, val_loss=6.01, t\u001b[A\n",
      "Epoch 4:   6%| | 3/50 [00:00<00:01, 27.46it/s, train_loss=33, val_loss=2.03, tra\u001b[A\n",
      "Epoch 5:   6%| | 3/50 [00:00<00:01, 27.46it/s, train_loss=33, val_loss=2.03, tra\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    50: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Curent best Test AUC: 0.28340638697557924\n",
      "Its' val AUC : 0.2949571836346337\n",
      "Its external  AUC: 0.38805734992910035\n",
      "Curent best Test AUC: 0.4996869129618034\n",
      "Its' val AUC : 0.4938154138915319\n",
      "Its external  AUC: 0.4800693240901213\n",
      "Curent best Test AUC: 0.723982467125861\n",
      "Its' val AUC : 0.6688867745004757\n",
      "Its external  AUC: 0.6278556798487475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5:   6%| | 3/50 [00:00<00:01, 27.46it/s, train_loss=11.3, val_loss=0.279, \u001b[A\n",
      "Epoch 6:   6%| | 3/50 [00:00<00:01, 27.46it/s, train_loss=11.3, val_loss=0.279, \u001b[A\n",
      "Epoch 6:   6%| | 3/50 [00:00<00:01, 27.46it/s, train_loss=2.04, val_loss=1.59, t\u001b[A\n",
      "Epoch 6:  14%|▏| 7/50 [00:00<00:01, 30.20it/s, train_loss=2.04, val_loss=1.59, t\u001b[A\n",
      "Epoch 7:  14%|▏| 7/50 [00:00<00:01, 30.20it/s, train_loss=2.04, val_loss=1.59, t\u001b[A\n",
      "Epoch 7:  14%|▏| 7/50 [00:00<00:01, 30.20it/s, train_loss=9.73, val_loss=0.377, \u001b[A\n",
      "Epoch 8:  14%|▏| 7/50 [00:00<00:01, 30.20it/s, train_loss=9.73, val_loss=0.377, \u001b[A\n",
      "Epoch 8:  14%|▏| 7/50 [00:00<00:01, 30.20it/s, train_loss=2.15, val_loss=0.838, \u001b[A\n",
      "Epoch 9:  14%|▏| 7/50 [00:00<00:01, 30.20it/s, train_loss=2.15, val_loss=0.838, \u001b[A\n",
      "Epoch 9:  14%|▏| 7/50 [00:00<00:01, 30.20it/s, train_loss=4.42, val_loss=0.353, \u001b[A\n",
      "Epoch 10:  14%|▏| 7/50 [00:00<00:01, 30.20it/s, train_loss=4.42, val_loss=0.353,\u001b[A\n",
      "Epoch 10:  14%|▏| 7/50 [00:00<00:01, 30.20it/s, train_loss=2.05, val_loss=0.566,\u001b[A\n",
      "Epoch 10:  22%|▏| 11/50 [00:00<00:01, 30.58it/s, train_loss=2.05, val_loss=0.566\u001b[A\n",
      "Epoch 11:  22%|▏| 11/50 [00:00<00:01, 30.58it/s, train_loss=2.05, val_loss=0.566\u001b[A\n",
      "Epoch 11:  22%|▏| 11/50 [00:00<00:01, 30.58it/s, train_loss=2.88, val_loss=0.282\u001b[A\n",
      "Epoch 12:  22%|▏| 11/50 [00:00<00:01, 30.58it/s, train_loss=2.88, val_loss=0.282\u001b[A\n",
      "Epoch 12:  22%|▏| 11/50 [00:00<00:01, 30.58it/s, train_loss=1.69, val_loss=0.276\u001b[A\n",
      "Epoch 13:  22%|▏| 11/50 [00:00<00:01, 30.58it/s, train_loss=1.69, val_loss=0.276\u001b[A\n",
      "Epoch 13:  22%|▏| 11/50 [00:00<00:01, 30.58it/s, train_loss=1.59, val_loss=0.078\u001b[A\n",
      "Epoch 14:  22%|▏| 11/50 [00:00<00:01, 30.58it/s, train_loss=1.59, val_loss=0.078\u001b[A\n",
      "Epoch 14:  22%|▏| 11/50 [00:00<00:01, 30.58it/s, train_loss=0.469, val_loss=0.19\u001b[A\n",
      "Epoch 14:  30%|▎| 15/50 [00:00<00:01, 31.21it/s, train_loss=0.469, val_loss=0.19\u001b[A\n",
      "Epoch 15:  30%|▎| 15/50 [00:00<00:01, 31.21it/s, train_loss=0.469, val_loss=0.19\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7569192235441453\n",
      "Its' val AUC : 0.6539350278646188\n",
      "Its external  AUC: 0.6138333070742082\n",
      "Epoch    12: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15:  30%|▎| 15/50 [00:00<00:01, 31.21it/s, train_loss=1.01, val_loss=0.185\u001b[A\n",
      "Epoch 16:  30%|▎| 15/50 [00:00<00:01, 31.21it/s, train_loss=1.01, val_loss=0.185\u001b[A\n",
      "Epoch 16:  30%|▎| 15/50 [00:00<00:01, 31.21it/s, train_loss=1.04, val_loss=0.067\u001b[A\n",
      "Epoch 17:  30%|▎| 15/50 [00:00<00:01, 31.21it/s, train_loss=1.04, val_loss=0.067\u001b[A\n",
      "Epoch 17:  30%|▎| 15/50 [00:00<00:01, 31.21it/s, train_loss=0.436, val_loss=0.17\u001b[A\n",
      "Epoch 18:  30%|▎| 15/50 [00:00<00:01, 31.21it/s, train_loss=0.436, val_loss=0.17\u001b[A\n",
      "Epoch 18:  30%|▎| 15/50 [00:00<00:01, 31.21it/s, train_loss=0.902, val_loss=0.14\u001b[A\n",
      "Epoch 18:  38%|▍| 19/50 [00:00<00:01, 30.66it/s, train_loss=0.902, val_loss=0.14\u001b[A\n",
      "Epoch 19:  38%|▍| 19/50 [00:00<00:01, 30.66it/s, train_loss=0.902, val_loss=0.14\u001b[A\n",
      "Epoch 19:  38%|▍| 19/50 [00:00<00:01, 30.66it/s, train_loss=0.708, val_loss=0.06\u001b[A\n",
      "Epoch 20:  38%|▍| 19/50 [00:00<00:01, 30.66it/s, train_loss=0.708, val_loss=0.06\u001b[A\n",
      "Epoch 20:  38%|▍| 19/50 [00:00<00:01, 30.66it/s, train_loss=0.312, val_loss=0.13\u001b[A\n",
      "Epoch 21:  38%|▍| 19/50 [00:00<00:01, 30.66it/s, train_loss=0.312, val_loss=0.13\u001b[A\n",
      "Epoch 21:  38%|▍| 19/50 [00:00<00:01, 30.66it/s, train_loss=0.706, val_loss=0.10\u001b[A\n",
      "Epoch 22:  38%|▍| 19/50 [00:00<00:01, 30.66it/s, train_loss=0.706, val_loss=0.10\u001b[A\n",
      "Epoch 22:  38%|▍| 19/50 [00:00<00:01, 30.66it/s, train_loss=0.534, val_loss=0.07\u001b[A\n",
      "Epoch 22:  46%|▍| 23/50 [00:00<00:00, 30.06it/s, train_loss=0.534, val_loss=0.07\u001b[A\n",
      "Epoch 23:  46%|▍| 23/50 [00:00<00:00, 30.06it/s, train_loss=0.534, val_loss=0.07\u001b[A\n",
      "Epoch 23:  46%|▍| 23/50 [00:00<00:00, 30.06it/s, train_loss=0.371, val_loss=0.10\u001b[A\n",
      "Epoch 24:  46%|▍| 23/50 [00:00<00:00, 30.06it/s, train_loss=0.371, val_loss=0.10\u001b[A\n",
      "Epoch 24:  46%|▍| 23/50 [00:00<00:00, 30.06it/s, train_loss=0.574, val_loss=0.05\u001b[A\n",
      "Epoch 25:  46%|▍| 23/50 [00:00<00:00, 30.06it/s, train_loss=0.574, val_loss=0.05\u001b[A\n",
      "Epoch 25:  46%|▍| 23/50 [00:00<00:00, 30.06it/s, train_loss=0.288, val_loss=0.06\u001b[A\n",
      "Epoch 26:  46%|▍| 23/50 [00:00<00:00, 30.06it/s, train_loss=0.288, val_loss=0.06\u001b[A\n",
      "Epoch 26:  46%|▍| 23/50 [00:00<00:00, 30.06it/s, train_loss=0.372, val_loss=0.07\u001b[A\n",
      "Epoch 26:  54%|▌| 27/50 [00:00<00:00, 30.33it/s, train_loss=0.372, val_loss=0.07\u001b[A\n",
      "Epoch 27:  54%|▌| 27/50 [00:00<00:00, 30.33it/s, train_loss=0.372, val_loss=0.07\u001b[A\n",
      "Epoch 27:  54%|▌| 27/50 [00:00<00:00, 30.33it/s, train_loss=0.474, val_loss=0.04\u001b[A\n",
      "Epoch 28:  54%|▌| 27/50 [00:00<00:00, 30.33it/s, train_loss=0.474, val_loss=0.04\u001b[A\n",
      "Epoch 28:  54%|▌| 27/50 [00:00<00:00, 30.33it/s, train_loss=0.272, val_loss=0.07\u001b[A\n",
      "Epoch 29:  54%|▌| 27/50 [00:00<00:00, 30.33it/s, train_loss=0.272, val_loss=0.07\u001b[A\n",
      "Epoch 29:  54%|▌| 27/50 [00:00<00:00, 30.33it/s, train_loss=0.408, val_loss=0.04\u001b[A\n",
      "Epoch 30:  54%|▌| 27/50 [00:00<00:00, 30.33it/s, train_loss=0.408, val_loss=0.04\u001b[A\n",
      "Epoch 30:  54%|▌| 27/50 [00:01<00:00, 30.33it/s, train_loss=0.264, val_loss=0.04\u001b[A\n",
      "Epoch 30:  62%|▌| 31/50 [00:01<00:00, 30.35it/s, train_loss=0.264, val_loss=0.04\u001b[A\n",
      "Epoch 31:  62%|▌| 31/50 [00:01<00:00, 30.35it/s, train_loss=0.264, val_loss=0.04\u001b[A\n",
      "Epoch 31:  62%|▌| 31/50 [00:01<00:00, 30.35it/s, train_loss=0.253, val_loss=0.05\u001b[A\n",
      "Epoch 32:  62%|▌| 31/50 [00:01<00:00, 30.35it/s, train_loss=0.253, val_loss=0.05\u001b[A\n",
      "Epoch 32:  62%|▌| 31/50 [00:01<00:00, 30.35it/s, train_loss=0.299, val_loss=0.02\u001b[A\n",
      "Epoch 33:  62%|▌| 31/50 [00:01<00:00, 30.35it/s, train_loss=0.299, val_loss=0.02\u001b[A\n",
      "Epoch 33:  62%|▌| 31/50 [00:01<00:00, 30.35it/s, train_loss=0.16, val_loss=0.043\u001b[A\n",
      "Epoch 34:  62%|▌| 31/50 [00:01<00:00, 30.35it/s, train_loss=0.16, val_loss=0.043\u001b[A\n",
      "Epoch 34:  62%|▌| 31/50 [00:01<00:00, 30.35it/s, train_loss=0.237, val_loss=0.03\u001b[A\n",
      "Epoch 34:  70%|▋| 35/50 [00:01<00:00, 30.57it/s, train_loss=0.237, val_loss=0.03\u001b[A\n",
      "Epoch 35:  70%|▋| 35/50 [00:01<00:00, 30.57it/s, train_loss=0.237, val_loss=0.03\u001b[A\n",
      "Epoch 35:  70%|▋| 35/50 [00:01<00:00, 30.57it/s, train_loss=0.17, val_loss=0.032\u001b[A\n",
      "Epoch 36:  70%|▋| 35/50 [00:01<00:00, 30.57it/s, train_loss=0.17, val_loss=0.032\u001b[A\n",
      "Epoch 36:  70%|▋| 35/50 [00:01<00:00, 30.57it/s, train_loss=0.177, val_loss=0.04\u001b[A\n",
      "Epoch 37:  70%|▋| 35/50 [00:01<00:00, 30.57it/s, train_loss=0.177, val_loss=0.04\u001b[A\n",
      "Epoch 37:  70%|▋| 35/50 [00:01<00:00, 30.57it/s, train_loss=0.217, val_loss=0.02\u001b[A\n",
      "Epoch 38:  70%|▋| 35/50 [00:01<00:00, 30.57it/s, train_loss=0.217, val_loss=0.02\u001b[A\n",
      "Epoch 38:  70%|▋| 35/50 [00:01<00:00, 30.57it/s, train_loss=0.137, val_loss=0.03\u001b[A\n",
      "Epoch 38:  78%|▊| 39/50 [00:01<00:00, 30.68it/s, train_loss=0.137, val_loss=0.03\u001b[A\n",
      "Epoch 39:  78%|▊| 39/50 [00:01<00:00, 30.68it/s, train_loss=0.137, val_loss=0.03\u001b[A\n",
      "Epoch 39:  78%|▊| 39/50 [00:01<00:00, 30.68it/s, train_loss=0.197, val_loss=0.02\u001b[A\n",
      "Epoch 40:  78%|▊| 39/50 [00:01<00:00, 30.68it/s, train_loss=0.197, val_loss=0.02\u001b[A\n",
      "Epoch 40:  78%|▊| 39/50 [00:01<00:00, 30.68it/s, train_loss=0.14, val_loss=0.031\u001b[A\n",
      "Epoch 41:  78%|▊| 39/50 [00:01<00:00, 30.68it/s, train_loss=0.14, val_loss=0.031\u001b[A\n",
      "Epoch 41:  78%|▊| 39/50 [00:01<00:00, 30.68it/s, train_loss=0.154, val_loss=0.03\u001b[A\n",
      "Epoch 42:  78%|▊| 39/50 [00:01<00:00, 30.68it/s, train_loss=0.154, val_loss=0.03\u001b[A\n",
      "Epoch 42:  78%|▊| 39/50 [00:01<00:00, 30.68it/s, train_loss=0.149, val_loss=0.02\u001b[A\n",
      "Epoch 42:  86%|▊| 43/50 [00:01<00:00, 30.64it/s, train_loss=0.149, val_loss=0.02\u001b[A\n",
      "Epoch 43:  86%|▊| 43/50 [00:01<00:00, 30.64it/s, train_loss=0.149, val_loss=0.02\u001b[A\n",
      "Epoch 43:  86%|▊| 43/50 [00:01<00:00, 30.64it/s, train_loss=0.108, val_loss=0.02\u001b[A\n",
      "Epoch 44:  86%|▊| 43/50 [00:01<00:00, 30.64it/s, train_loss=0.108, val_loss=0.02\u001b[A\n",
      "Epoch 44:  86%|▊| 43/50 [00:01<00:00, 30.64it/s, train_loss=0.143, val_loss=0.02\u001b[A\n",
      "Epoch 45:  86%|▊| 43/50 [00:01<00:00, 30.64it/s, train_loss=0.143, val_loss=0.02\u001b[A\n",
      "Epoch 45:  86%|▊| 43/50 [00:01<00:00, 30.64it/s, train_loss=0.102, val_loss=0.02\u001b[A\n",
      "Epoch 46:  86%|▊| 43/50 [00:01<00:00, 30.64it/s, train_loss=0.102, val_loss=0.02\u001b[A\n",
      "Epoch 46:  86%|▊| 43/50 [00:01<00:00, 30.64it/s, train_loss=0.128, val_loss=0.02\u001b[A\n",
      "Epoch 46:  94%|▉| 47/50 [00:01<00:00, 30.37it/s, train_loss=0.128, val_loss=0.02\u001b[A\n",
      "Epoch 47:  94%|▉| 47/50 [00:01<00:00, 30.37it/s, train_loss=0.128, val_loss=0.02\u001b[A\n",
      "Epoch 47:  94%|▉| 47/50 [00:01<00:00, 30.37it/s, train_loss=0.103, val_loss=0.02\u001b[A\n",
      "Epoch 48:  94%|▉| 47/50 [00:01<00:00, 30.37it/s, train_loss=0.103, val_loss=0.02\u001b[A\n",
      "Epoch 48:  94%|▉| 47/50 [00:01<00:00, 30.37it/s, train_loss=0.1, val_loss=0.0232\u001b[A\n",
      "Epoch 49:  94%|▉| 47/50 [00:01<00:00, 30.37it/s, train_loss=0.1, val_loss=0.0232\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 30.28it/s, train_loss=0.107, val_loss=0.01\u001b[A\n",
      " 15%|██████▎                                   | 15/100 [00:23<02:14,  1.59s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=18.4, val_loss=1.99, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=18.4, val_loss=1.99, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=13.5, val_loss=10.9, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=13.5, val_loss=10.9, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=61.4, val_loss=29.6, train_cin\u001b[A\n",
      "Epoch 2:   6%| | 3/50 [00:00<00:01, 28.93it/s, train_loss=61.4, val_loss=29.6, t\u001b[A\n",
      "Epoch 3:   6%| | 3/50 [00:00<00:01, 28.93it/s, train_loss=61.4, val_loss=29.6, t\u001b[A\n",
      "Epoch 3:   6%| | 3/50 [00:00<00:01, 28.93it/s, train_loss=140, val_loss=0.966, t\u001b[A\n",
      "Epoch 4:   6%| | 3/50 [00:00<00:01, 28.93it/s, train_loss=140, val_loss=0.966, t\u001b[A\n",
      "Epoch 4:   6%| | 3/50 [00:00<00:01, 28.93it/s, train_loss=6.37, val_loss=10.3, t\u001b[A\n",
      "Epoch 5:   6%| | 3/50 [00:00<00:01, 28.93it/s, train_loss=6.37, val_loss=10.3, t\u001b[A\n",
      "Epoch 5:   6%| | 3/50 [00:00<00:01, 28.93it/s, train_loss=55.3, val_loss=1.59, t\u001b[A\n",
      "Epoch 5:  12%| | 6/50 [00:00<00:01, 28.89it/s, train_loss=55.3, val_loss=1.59, t\u001b[A\n",
      "Epoch 6:  12%| | 6/50 [00:00<00:01, 28.89it/s, train_loss=55.3, val_loss=1.59, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.46011271133375076\n",
      "Its' val AUC : 0.5169226586924018\n",
      "Its external  AUC: 0.3255081140696392\n",
      "Curent best Test AUC: 0.7435190983093299\n",
      "Its' val AUC : 0.7255674867473155\n",
      "Its external  AUC: 0.5872065542776115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6:  12%| | 6/50 [00:00<00:01, 28.89it/s, train_loss=10.5, val_loss=6.41, t\u001b[A\n",
      "Epoch 7:  12%| | 6/50 [00:00<00:01, 28.89it/s, train_loss=10.5, val_loss=6.41, t\u001b[A\n",
      "Epoch 7:  12%| | 6/50 [00:00<00:01, 28.89it/s, train_loss=33, val_loss=2.22, tra\u001b[A\n",
      "Epoch 8:  12%| | 6/50 [00:00<00:01, 28.89it/s, train_loss=33, val_loss=2.22, tra\u001b[A\n",
      "Epoch 8:  12%| | 6/50 [00:00<00:01, 28.89it/s, train_loss=13.1, val_loss=0.703, \u001b[A\n",
      "Epoch 8:  18%|▏| 9/50 [00:00<00:01, 29.01it/s, train_loss=13.1, val_loss=0.703, \u001b[A\n",
      "Epoch 9:  18%|▏| 9/50 [00:00<00:01, 29.01it/s, train_loss=13.1, val_loss=0.703, \u001b[A\n",
      "Epoch 9:  18%|▏| 9/50 [00:00<00:01, 29.01it/s, train_loss=4.28, val_loss=0.228, \u001b[A\n",
      "Epoch 10:  18%|▏| 9/50 [00:00<00:01, 29.01it/s, train_loss=4.28, val_loss=0.228,\u001b[A\n",
      "Epoch 10:  18%|▏| 9/50 [00:00<00:01, 29.01it/s, train_loss=1.42, val_loss=0.42, \u001b[A\n",
      "Epoch 11:  18%|▏| 9/50 [00:00<00:01, 29.01it/s, train_loss=1.42, val_loss=0.42, \u001b[A\n",
      "Epoch 11:  18%|▏| 9/50 [00:00<00:01, 29.01it/s, train_loss=2.48, val_loss=0.135,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 29.35it/s, train_loss=2.48, val_loss=0.135\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 29.35it/s, train_loss=2.48, val_loss=0.135\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 29.35it/s, train_loss=0.609, val_loss=0.18\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 29.35it/s, train_loss=0.609, val_loss=0.18\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 29.35it/s, train_loss=0.784, val_loss=0.14\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 29.35it/s, train_loss=0.784, val_loss=0.14\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 29.35it/s, train_loss=0.843, val_loss=0.25\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 29.35it/s, train_loss=0.843, val_loss=0.25\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 29.35it/s, train_loss=1.27, val_loss=0.156\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 29.75it/s, train_loss=1.27, val_loss=0.156\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 29.75it/s, train_loss=1.27, val_loss=0.156\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 29.75it/s, train_loss=0.942, val_loss=0.23\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 29.75it/s, train_loss=0.942, val_loss=0.23\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 29.75it/s, train_loss=1.33, val_loss=0.207\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 29.75it/s, train_loss=1.33, val_loss=0.207\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 29.75it/s, train_loss=1.29, val_loss=0.186\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 29.75it/s, train_loss=1.29, val_loss=0.186\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 29.75it/s, train_loss=1.12, val_loss=0.163\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:01, 29.78it/s, train_loss=1.12, val_loss=0.163\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:01, 29.78it/s, train_loss=1.12, val_loss=0.163\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:01, 29.78it/s, train_loss=0.962, val_loss=0.16\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:01, 29.78it/s, train_loss=0.962, val_loss=0.16\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:01, 29.78it/s, train_loss=0.964, val_loss=0.12\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:01, 29.78it/s, train_loss=0.964, val_loss=0.12\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:01, 29.78it/s, train_loss=0.661, val_loss=0.13\u001b[A\n",
      "Epoch 22:  46%|▍| 23/50 [00:00<00:00, 29.30it/s, train_loss=0.661, val_loss=0.13\u001b[A\n",
      "Epoch 23:  46%|▍| 23/50 [00:00<00:00, 29.30it/s, train_loss=0.661, val_loss=0.13\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    18: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23:  46%|▍| 23/50 [00:00<00:00, 29.30it/s, train_loss=0.603, val_loss=0.09\u001b[A\n",
      "Epoch 24:  46%|▍| 23/50 [00:00<00:00, 29.30it/s, train_loss=0.603, val_loss=0.09\u001b[A\n",
      "Epoch 24:  46%|▍| 23/50 [00:00<00:00, 29.30it/s, train_loss=0.435, val_loss=0.07\u001b[A\n",
      "Epoch 25:  46%|▍| 23/50 [00:00<00:00, 29.30it/s, train_loss=0.435, val_loss=0.07\u001b[A\n",
      "Epoch 25:  46%|▍| 23/50 [00:00<00:00, 29.30it/s, train_loss=0.362, val_loss=0.07\u001b[A\n",
      "Epoch 26:  46%|▍| 23/50 [00:00<00:00, 29.30it/s, train_loss=0.362, val_loss=0.07\u001b[A\n",
      "Epoch 26:  46%|▍| 23/50 [00:00<00:00, 29.30it/s, train_loss=0.363, val_loss=0.05\u001b[A\n",
      "Epoch 26:  54%|▌| 27/50 [00:00<00:00, 29.84it/s, train_loss=0.363, val_loss=0.05\u001b[A\n",
      "Epoch 27:  54%|▌| 27/50 [00:00<00:00, 29.84it/s, train_loss=0.363, val_loss=0.05\u001b[A\n",
      "Epoch 27:  54%|▌| 27/50 [00:00<00:00, 29.84it/s, train_loss=0.236, val_loss=0.07\u001b[A\n",
      "Epoch 28:  54%|▌| 27/50 [00:00<00:00, 29.84it/s, train_loss=0.236, val_loss=0.07\u001b[A\n",
      "Epoch 28:  54%|▌| 27/50 [00:00<00:00, 29.84it/s, train_loss=0.283, val_loss=0.04\u001b[A\n",
      "Epoch 29:  54%|▌| 27/50 [00:00<00:00, 29.84it/s, train_loss=0.283, val_loss=0.04\u001b[A\n",
      "Epoch 29:  54%|▌| 27/50 [00:01<00:00, 29.84it/s, train_loss=0.21, val_loss=0.046\u001b[A\n",
      "Epoch 30:  54%|▌| 27/50 [00:01<00:00, 29.84it/s, train_loss=0.21, val_loss=0.046\u001b[A\n",
      "Epoch 30:  54%|▌| 27/50 [00:01<00:00, 29.84it/s, train_loss=0.272, val_loss=0.03\u001b[A\n",
      "Epoch 30:  62%|▌| 31/50 [00:01<00:00, 30.07it/s, train_loss=0.272, val_loss=0.03\u001b[A\n",
      "Epoch 31:  62%|▌| 31/50 [00:01<00:00, 30.07it/s, train_loss=0.272, val_loss=0.03\u001b[A\n",
      "Epoch 31:  62%|▌| 31/50 [00:01<00:00, 30.07it/s, train_loss=0.244, val_loss=0.04\u001b[A\n",
      "Epoch 32:  62%|▌| 31/50 [00:01<00:00, 30.07it/s, train_loss=0.244, val_loss=0.04\u001b[A\n",
      "Epoch 32:  62%|▌| 31/50 [00:01<00:00, 30.07it/s, train_loss=0.265, val_loss=0.04\u001b[A\n",
      "Epoch 33:  62%|▌| 31/50 [00:01<00:00, 30.07it/s, train_loss=0.265, val_loss=0.04\u001b[A\n",
      "Epoch 33:  62%|▌| 31/50 [00:01<00:00, 30.07it/s, train_loss=0.299, val_loss=0.04\u001b[A\n",
      "Epoch 34:  62%|▌| 31/50 [00:01<00:00, 30.07it/s, train_loss=0.299, val_loss=0.04\u001b[A\n",
      "Epoch 34:  62%|▌| 31/50 [00:01<00:00, 30.07it/s, train_loss=0.274, val_loss=0.04\u001b[A\n",
      "Epoch 34:  70%|▋| 35/50 [00:01<00:00, 30.86it/s, train_loss=0.274, val_loss=0.04\u001b[A\n",
      "Epoch 35:  70%|▋| 35/50 [00:01<00:00, 30.86it/s, train_loss=0.274, val_loss=0.04\u001b[A\n",
      "Epoch 35:  70%|▋| 35/50 [00:01<00:00, 30.86it/s, train_loss=0.278, val_loss=0.03\u001b[A\n",
      "Epoch 36:  70%|▋| 35/50 [00:01<00:00, 30.86it/s, train_loss=0.278, val_loss=0.03\u001b[A\n",
      "Epoch 36:  70%|▋| 35/50 [00:01<00:00, 30.86it/s, train_loss=0.214, val_loss=0.04\u001b[A\n",
      "Epoch 37:  70%|▋| 35/50 [00:01<00:00, 30.86it/s, train_loss=0.214, val_loss=0.04\u001b[A\n",
      "Epoch 37:  70%|▋| 35/50 [00:01<00:00, 30.86it/s, train_loss=0.221, val_loss=0.03\u001b[A\n",
      "Epoch 38:  70%|▋| 35/50 [00:01<00:00, 30.86it/s, train_loss=0.221, val_loss=0.03\u001b[A\n",
      "Epoch 38:  70%|▋| 35/50 [00:01<00:00, 30.86it/s, train_loss=0.18, val_loss=0.035\u001b[A\n",
      "Epoch 38:  78%|▊| 39/50 [00:01<00:00, 30.92it/s, train_loss=0.18, val_loss=0.035\u001b[A\n",
      "Epoch 39:  78%|▊| 39/50 [00:01<00:00, 30.92it/s, train_loss=0.18, val_loss=0.035\u001b[A\n",
      "Epoch 39:  78%|▊| 39/50 [00:01<00:00, 30.92it/s, train_loss=0.171, val_loss=0.03\u001b[A\n",
      "Epoch 40:  78%|▊| 39/50 [00:01<00:00, 30.92it/s, train_loss=0.171, val_loss=0.03\u001b[A\n",
      "Epoch 40:  78%|▊| 39/50 [00:01<00:00, 30.92it/s, train_loss=0.144, val_loss=0.03\u001b[A\n",
      "Epoch 41:  78%|▊| 39/50 [00:01<00:00, 30.92it/s, train_loss=0.144, val_loss=0.03\u001b[A\n",
      "Epoch 41:  78%|▊| 39/50 [00:01<00:00, 30.92it/s, train_loss=0.138, val_loss=0.02\u001b[A\n",
      "Epoch 42:  78%|▊| 39/50 [00:01<00:00, 30.92it/s, train_loss=0.138, val_loss=0.02\u001b[A\n",
      "Epoch 42:  78%|▊| 39/50 [00:01<00:00, 30.92it/s, train_loss=0.131, val_loss=0.02\u001b[A\n",
      "Epoch 42:  86%|▊| 43/50 [00:01<00:00, 30.58it/s, train_loss=0.131, val_loss=0.02\u001b[A\n",
      "Epoch 43:  86%|▊| 43/50 [00:01<00:00, 30.58it/s, train_loss=0.131, val_loss=0.02\u001b[A\n",
      "Epoch 43:  86%|▊| 43/50 [00:01<00:00, 30.58it/s, train_loss=0.112, val_loss=0.02\u001b[A\n",
      "Epoch 44:  86%|▊| 43/50 [00:01<00:00, 30.58it/s, train_loss=0.112, val_loss=0.02\u001b[A\n",
      "Epoch 44:  86%|▊| 43/50 [00:01<00:00, 30.58it/s, train_loss=0.115, val_loss=0.02\u001b[A\n",
      "Epoch 45:  86%|▊| 43/50 [00:01<00:00, 30.58it/s, train_loss=0.115, val_loss=0.02\u001b[A\n",
      "Epoch 45:  86%|▊| 43/50 [00:01<00:00, 30.58it/s, train_loss=0.11, val_loss=0.031\u001b[A\n",
      "Epoch 46:  86%|▊| 43/50 [00:01<00:00, 30.58it/s, train_loss=0.11, val_loss=0.031\u001b[A\n",
      "Epoch 46:  86%|▊| 43/50 [00:01<00:00, 30.58it/s, train_loss=0.126, val_loss=0.02\u001b[A\n",
      "Epoch 46:  94%|▉| 47/50 [00:01<00:00, 30.74it/s, train_loss=0.126, val_loss=0.02\u001b[A\n",
      "Epoch 47:  94%|▉| 47/50 [00:01<00:00, 30.74it/s, train_loss=0.126, val_loss=0.02\u001b[A\n",
      "Epoch 47:  94%|▉| 47/50 [00:01<00:00, 30.74it/s, train_loss=0.118, val_loss=0.02\u001b[A\n",
      "Epoch 48:  94%|▉| 47/50 [00:01<00:00, 30.74it/s, train_loss=0.118, val_loss=0.02\u001b[A\n",
      "Epoch 48:  94%|▉| 47/50 [00:01<00:00, 30.74it/s, train_loss=0.129, val_loss=0.02\u001b[A\n",
      "Epoch 49:  94%|▉| 47/50 [00:01<00:00, 30.74it/s, train_loss=0.129, val_loss=0.02\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 30.05it/s, train_loss=0.121, val_loss=0.02\u001b[A\n",
      " 16%|██████▋                                   | 16/100 [00:25<02:15,  1.61s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=40.2, val_loss=71.2, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=40.2, val_loss=71.2, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=370, val_loss=44.1, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=370, val_loss=44.1, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=223, val_loss=12, train_cindex\u001b[A\n",
      "Epoch 2:   6%| | 3/50 [00:00<00:01, 29.93it/s, train_loss=223, val_loss=12, trai\u001b[A\n",
      "Epoch 3:   6%| | 3/50 [00:00<00:01, 29.93it/s, train_loss=223, val_loss=12, trai\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    49: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Curent best Test AUC: 0.7168440826549781\n",
      "Its' val AUC : 0.7044991164876988\n",
      "Its external  AUC: 0.5972900582952576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   6%| | 3/50 [00:00<00:01, 29.93it/s, train_loss=69.1, val_loss=7, trai\u001b[A\n",
      "Epoch 4:   6%| | 3/50 [00:00<00:01, 29.93it/s, train_loss=69.1, val_loss=7, trai\u001b[A\n",
      "Epoch 4:   6%| | 3/50 [00:00<00:01, 29.93it/s, train_loss=41, val_loss=2.05, tra\u001b[A\n",
      "Epoch 5:   6%| | 3/50 [00:00<00:01, 29.93it/s, train_loss=41, val_loss=2.05, tra\u001b[A\n",
      "Epoch 5:   6%| | 3/50 [00:00<00:01, 29.93it/s, train_loss=13.4, val_loss=3.1, tr\u001b[A\n",
      "Epoch 5:  12%| | 6/50 [00:00<00:01, 28.66it/s, train_loss=13.4, val_loss=3.1, tr\u001b[A\n",
      "Epoch 6:  12%| | 6/50 [00:00<00:01, 28.66it/s, train_loss=13.4, val_loss=3.1, tr\u001b[A\n",
      "Epoch 6:  12%| | 6/50 [00:00<00:01, 28.66it/s, train_loss=15, val_loss=4.02, tra\u001b[A\n",
      "Epoch 7:  12%| | 6/50 [00:00<00:01, 28.66it/s, train_loss=15, val_loss=4.02, tra\u001b[A\n",
      "Epoch 7:  12%| | 6/50 [00:00<00:01, 28.66it/s, train_loss=20, val_loss=1.44, tra\u001b[A\n",
      "Epoch 8:  12%| | 6/50 [00:00<00:01, 28.66it/s, train_loss=20, val_loss=1.44, tra\u001b[A\n",
      "Epoch 8:  12%| | 6/50 [00:00<00:01, 28.66it/s, train_loss=8.66, val_loss=2.42, t\u001b[A\n",
      "Epoch 8:  18%|▏| 9/50 [00:00<00:01, 29.14it/s, train_loss=8.66, val_loss=2.42, t\u001b[A\n",
      "Epoch 9:  18%|▏| 9/50 [00:00<00:01, 29.14it/s, train_loss=8.66, val_loss=2.42, t\u001b[A\n",
      "Epoch 9:  18%|▏| 9/50 [00:00<00:01, 29.14it/s, train_loss=13.8, val_loss=1.28, t\u001b[A\n",
      "Epoch 10:  18%|▏| 9/50 [00:00<00:01, 29.14it/s, train_loss=13.8, val_loss=1.28, \u001b[A\n",
      "Epoch 10:  18%|▏| 9/50 [00:00<00:01, 29.14it/s, train_loss=6.91, val_loss=1, tra\u001b[A\n",
      "Epoch 11:  18%|▏| 9/50 [00:00<00:01, 29.14it/s, train_loss=6.91, val_loss=1, tra\u001b[A\n",
      "Epoch 11:  18%|▏| 9/50 [00:00<00:01, 29.14it/s, train_loss=6.07, val_loss=1.23, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 29.05it/s, train_loss=6.07, val_loss=1.23,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 29.05it/s, train_loss=6.07, val_loss=1.23,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 29.05it/s, train_loss=7.7, val_loss=0.333,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 29.05it/s, train_loss=7.7, val_loss=0.333,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 29.05it/s, train_loss=1.71, val_loss=1.4, \u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 29.05it/s, train_loss=1.71, val_loss=1.4, \u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 29.05it/s, train_loss=6.96, val_loss=0.216\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 29.05it/s, train_loss=6.96, val_loss=0.216\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 29.05it/s, train_loss=1.1, val_loss=0.865,\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 29.62it/s, train_loss=1.1, val_loss=0.865,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 29.62it/s, train_loss=1.1, val_loss=0.865,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 29.62it/s, train_loss=4.9, val_loss=0.324,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 29.62it/s, train_loss=4.9, val_loss=0.324,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 29.62it/s, train_loss=1.98, val_loss=0.605\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 29.62it/s, train_loss=1.98, val_loss=0.605\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 29.62it/s, train_loss=3.21, val_loss=0.553\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 29.62it/s, train_loss=3.21, val_loss=0.553\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 29.62it/s, train_loss=3.02, val_loss=0.311\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:01, 29.92it/s, train_loss=3.02, val_loss=0.311\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:01, 29.92it/s, train_loss=3.02, val_loss=0.311\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:01, 29.92it/s, train_loss=2.09, val_loss=0.581\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:01, 29.92it/s, train_loss=2.09, val_loss=0.581\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.719724483406387\n",
      "Its' val AUC : 0.7095283403561234\n",
      "Its external  AUC: 0.6322672128564676\n",
      "Epoch    21: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:01, 29.92it/s, train_loss=3.54, val_loss=0.204\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:01, 29.92it/s, train_loss=3.54, val_loss=0.204\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:01, 29.92it/s, train_loss=1.51, val_loss=0.317\u001b[A\n",
      "Epoch 22:  46%|▍| 23/50 [00:00<00:00, 29.71it/s, train_loss=1.51, val_loss=0.317\u001b[A\n",
      "Epoch 23:  46%|▍| 23/50 [00:00<00:00, 29.71it/s, train_loss=1.51, val_loss=0.317\u001b[A\n",
      "Epoch 23:  46%|▍| 23/50 [00:00<00:00, 29.71it/s, train_loss=1.95, val_loss=0.519\u001b[A\n",
      "Epoch 24:  46%|▍| 23/50 [00:00<00:00, 29.71it/s, train_loss=1.95, val_loss=0.519\u001b[A\n",
      "Epoch 24:  46%|▍| 23/50 [00:00<00:00, 29.71it/s, train_loss=2.93, val_loss=0.233\u001b[A\n",
      "Epoch 25:  46%|▍| 23/50 [00:00<00:00, 29.71it/s, train_loss=2.93, val_loss=0.233\u001b[A\n",
      "Epoch 25:  46%|▍| 23/50 [00:00<00:00, 29.71it/s, train_loss=1.49, val_loss=0.187\u001b[A\n",
      "Epoch 26:  46%|▍| 23/50 [00:00<00:00, 29.71it/s, train_loss=1.49, val_loss=0.187\u001b[A\n",
      "Epoch 26:  46%|▍| 23/50 [00:00<00:00, 29.71it/s, train_loss=1.25, val_loss=0.381\u001b[A\n",
      "Epoch 26:  54%|▌| 27/50 [00:00<00:00, 30.18it/s, train_loss=1.25, val_loss=0.381\u001b[A\n",
      "Epoch 27:  54%|▌| 27/50 [00:00<00:00, 30.18it/s, train_loss=1.25, val_loss=0.381\u001b[A\n",
      "Epoch 27:  54%|▌| 27/50 [00:00<00:00, 30.18it/s, train_loss=2.18, val_loss=0.192\u001b[A\n",
      "Epoch 28:  54%|▌| 27/50 [00:00<00:00, 30.18it/s, train_loss=2.18, val_loss=0.192\u001b[A\n",
      "Epoch 28:  54%|▌| 27/50 [00:00<00:00, 30.18it/s, train_loss=1.17, val_loss=0.118\u001b[A\n",
      "Epoch 29:  54%|▌| 27/50 [00:00<00:00, 30.18it/s, train_loss=1.17, val_loss=0.118\u001b[A\n",
      "Epoch 29:  54%|▌| 27/50 [00:01<00:00, 30.18it/s, train_loss=0.753, val_loss=0.26\u001b[A\n",
      "Epoch 30:  54%|▌| 27/50 [00:01<00:00, 30.18it/s, train_loss=0.753, val_loss=0.26\u001b[A\n",
      "Epoch 30:  54%|▌| 27/50 [00:01<00:00, 30.18it/s, train_loss=1.47, val_loss=0.148\u001b[A\n",
      "Epoch 30:  62%|▌| 31/50 [00:01<00:00, 30.07it/s, train_loss=1.47, val_loss=0.148\u001b[A\n",
      "Epoch 31:  62%|▌| 31/50 [00:01<00:00, 30.07it/s, train_loss=1.47, val_loss=0.148\u001b[A\n",
      "Epoch 31:  62%|▌| 31/50 [00:01<00:00, 30.07it/s, train_loss=0.834, val_loss=0.07\u001b[A\n",
      "Epoch 32:  62%|▌| 31/50 [00:01<00:00, 30.07it/s, train_loss=0.834, val_loss=0.07\u001b[A\n",
      "Epoch 32:  62%|▌| 31/50 [00:01<00:00, 30.07it/s, train_loss=0.441, val_loss=0.20\u001b[A\n",
      "Epoch 33:  62%|▌| 31/50 [00:01<00:00, 30.07it/s, train_loss=0.441, val_loss=0.20\u001b[A\n",
      "Epoch 33:  62%|▌| 31/50 [00:01<00:00, 30.07it/s, train_loss=1.05, val_loss=0.117\u001b[A\n",
      "Epoch 34:  62%|▌| 31/50 [00:01<00:00, 30.07it/s, train_loss=1.05, val_loss=0.117\u001b[A\n",
      "Epoch 34:  62%|▌| 31/50 [00:01<00:00, 30.07it/s, train_loss=0.62, val_loss=0.068\u001b[A\n",
      "Epoch 34:  70%|▋| 35/50 [00:01<00:00, 30.30it/s, train_loss=0.62, val_loss=0.068\u001b[A\n",
      "Epoch 35:  70%|▋| 35/50 [00:01<00:00, 30.30it/s, train_loss=0.62, val_loss=0.068\u001b[A\n",
      "Epoch 35:  70%|▋| 35/50 [00:01<00:00, 30.30it/s, train_loss=0.37, val_loss=0.16,\u001b[A\n",
      "Epoch 36:  70%|▋| 35/50 [00:01<00:00, 30.30it/s, train_loss=0.37, val_loss=0.16,\u001b[A\n",
      "Epoch 36:  70%|▋| 35/50 [00:01<00:00, 30.30it/s, train_loss=0.845, val_loss=0.09\u001b[A\n",
      "Epoch 37:  70%|▋| 35/50 [00:01<00:00, 30.30it/s, train_loss=0.845, val_loss=0.09\u001b[A\n",
      "Epoch 37:  70%|▋| 35/50 [00:01<00:00, 30.30it/s, train_loss=0.517, val_loss=0.07\u001b[A\n",
      "Epoch 38:  70%|▋| 35/50 [00:01<00:00, 30.30it/s, train_loss=0.517, val_loss=0.07\u001b[A\n",
      "Epoch 38:  70%|▋| 35/50 [00:01<00:00, 30.30it/s, train_loss=0.384, val_loss=0.15\u001b[A\n",
      "Epoch 38:  78%|▊| 39/50 [00:01<00:00, 30.25it/s, train_loss=0.384, val_loss=0.15\u001b[A\n",
      "Epoch 39:  78%|▊| 39/50 [00:01<00:00, 30.25it/s, train_loss=0.384, val_loss=0.15\u001b[A\n",
      "Epoch 39:  78%|▊| 39/50 [00:01<00:00, 30.25it/s, train_loss=0.746, val_loss=0.08\u001b[A\n",
      "Epoch 40:  78%|▊| 39/50 [00:01<00:00, 30.25it/s, train_loss=0.746, val_loss=0.08\u001b[A\n",
      "Epoch 40:  78%|▊| 39/50 [00:01<00:00, 30.25it/s, train_loss=0.447, val_loss=0.07\u001b[A\n",
      "Epoch 41:  78%|▊| 39/50 [00:01<00:00, 30.25it/s, train_loss=0.447, val_loss=0.07\u001b[A\n",
      "Epoch 41:  78%|▊| 39/50 [00:01<00:00, 30.25it/s, train_loss=0.423, val_loss=0.09\u001b[A\n",
      "Epoch 42:  78%|▊| 39/50 [00:01<00:00, 30.25it/s, train_loss=0.423, val_loss=0.09\u001b[A\n",
      "Epoch 42:  78%|▊| 39/50 [00:01<00:00, 30.25it/s, train_loss=0.517, val_loss=0.08\u001b[A\n",
      "Epoch 42:  86%|▊| 43/50 [00:01<00:00, 30.48it/s, train_loss=0.517, val_loss=0.08\u001b[A\n",
      "Epoch 43:  86%|▊| 43/50 [00:01<00:00, 30.48it/s, train_loss=0.517, val_loss=0.08\u001b[A\n",
      "Epoch 43:  86%|▊| 43/50 [00:01<00:00, 30.48it/s, train_loss=0.436, val_loss=0.06\u001b[A\n",
      "Epoch 44:  86%|▊| 43/50 [00:01<00:00, 30.48it/s, train_loss=0.436, val_loss=0.06\u001b[A\n",
      "Epoch 44:  86%|▊| 43/50 [00:01<00:00, 30.48it/s, train_loss=0.33, val_loss=0.071\u001b[A\n",
      "Epoch 45:  86%|▊| 43/50 [00:01<00:00, 30.48it/s, train_loss=0.33, val_loss=0.071\u001b[A\n",
      "Epoch 45:  86%|▊| 43/50 [00:01<00:00, 30.48it/s, train_loss=0.366, val_loss=0.08\u001b[A\n",
      "Epoch 46:  86%|▊| 43/50 [00:01<00:00, 30.48it/s, train_loss=0.366, val_loss=0.08\u001b[A\n",
      "Epoch 46:  86%|▊| 43/50 [00:01<00:00, 30.48it/s, train_loss=0.432, val_loss=0.07\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    41: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46:  94%|▉| 47/50 [00:01<00:00, 30.49it/s, train_loss=0.432, val_loss=0.07\u001b[A\n",
      "Epoch 47:  94%|▉| 47/50 [00:01<00:00, 30.49it/s, train_loss=0.432, val_loss=0.07\u001b[A\n",
      "Epoch 47:  94%|▉| 47/50 [00:01<00:00, 30.49it/s, train_loss=0.377, val_loss=0.05\u001b[A\n",
      "Epoch 48:  94%|▉| 47/50 [00:01<00:00, 30.49it/s, train_loss=0.377, val_loss=0.05\u001b[A\n",
      "Epoch 48:  94%|▉| 47/50 [00:01<00:00, 30.49it/s, train_loss=0.295, val_loss=0.06\u001b[A\n",
      "Epoch 49:  94%|▉| 47/50 [00:01<00:00, 30.49it/s, train_loss=0.295, val_loss=0.06\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 30.00it/s, train_loss=0.311, val_loss=0.06\u001b[A\n",
      " 17%|███████▏                                  | 17/100 [00:26<02:15,  1.64s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=4.06, val_loss=66.4, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=4.06, val_loss=66.4, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=356, val_loss=1.55, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=356, val_loss=1.55, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=7.79, val_loss=1.57, train_cin\u001b[A\n",
      "Epoch 2:   6%| | 3/50 [00:00<00:01, 27.83it/s, train_loss=7.79, val_loss=1.57, t\u001b[A\n",
      "Epoch 3:   6%| | 3/50 [00:00<00:01, 27.83it/s, train_loss=7.79, val_loss=1.57, t\u001b[A\n",
      "Epoch 3:   6%| | 3/50 [00:00<00:01, 27.83it/s, train_loss=7.85, val_loss=0.482, \u001b[A\n",
      "Epoch 4:   6%| | 3/50 [00:00<00:01, 27.83it/s, train_loss=7.85, val_loss=0.482, \u001b[A\n",
      "Epoch 4:   6%| | 3/50 [00:00<00:01, 27.83it/s, train_loss=2.86, val_loss=0.156, \u001b[A\n",
      "Epoch 5:   6%| | 3/50 [00:00<00:01, 27.83it/s, train_loss=2.86, val_loss=0.156, \u001b[A\n",
      "Epoch 5:   6%| | 3/50 [00:00<00:01, 27.83it/s, train_loss=0.744, val_loss=0.234,\u001b[A\n",
      "Epoch 5:  12%| | 6/50 [00:00<00:01, 27.90it/s, train_loss=0.744, val_loss=0.234,\u001b[A\n",
      "Epoch 6:  12%| | 6/50 [00:00<00:01, 27.90it/s, train_loss=0.744, val_loss=0.234,\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.26261740763932373\n",
      "Its' val AUC : 0.27742286257985593\n",
      "Its external  AUC: 0.38600913817551596\n",
      "Curent best Test AUC: 0.3557921102066374\n",
      "Its' val AUC : 0.31779257849666986\n",
      "Its external  AUC: 0.40775169371356546\n",
      "Curent best Test AUC: 0.728616155291171\n",
      "Its' val AUC : 0.6865570205246704\n",
      "Its external  AUC: 0.5976051677958091\n",
      "Curent best Test AUC: 0.7458985597996243\n",
      "Its' val AUC : 0.6592361016718771\n",
      "Its external  AUC: 0.633212541358122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6:  12%| | 6/50 [00:00<00:01, 27.90it/s, train_loss=1.19, val_loss=0.104, \u001b[A\n",
      "Epoch 7:  12%| | 6/50 [00:00<00:01, 27.90it/s, train_loss=1.19, val_loss=0.104, \u001b[A\n",
      "Epoch 7:  12%| | 6/50 [00:00<00:01, 27.90it/s, train_loss=0.706, val_loss=0.0917\u001b[A\n",
      "Epoch 8:  12%| | 6/50 [00:00<00:01, 27.90it/s, train_loss=0.706, val_loss=0.0917\u001b[A\n",
      "Epoch 8:  12%| | 6/50 [00:00<00:01, 27.90it/s, train_loss=0.731, val_loss=0.0584\u001b[A\n",
      "Epoch 9:  12%| | 6/50 [00:00<00:01, 27.90it/s, train_loss=0.731, val_loss=0.0584\u001b[A\n",
      "Epoch 9:  12%| | 6/50 [00:00<00:01, 27.90it/s, train_loss=0.349, val_loss=0.073,\u001b[A\n",
      "Epoch 9:  20%|▏| 10/50 [00:00<00:01, 29.30it/s, train_loss=0.349, val_loss=0.073\u001b[A\n",
      "Epoch 10:  20%|▏| 10/50 [00:00<00:01, 29.30it/s, train_loss=0.349, val_loss=0.07\u001b[A\n",
      "Epoch 10:  20%|▏| 10/50 [00:00<00:01, 29.30it/s, train_loss=0.382, val_loss=0.07\u001b[A\n",
      "Epoch 11:  20%|▏| 10/50 [00:00<00:01, 29.30it/s, train_loss=0.382, val_loss=0.07\u001b[A\n",
      "Epoch 11:  20%|▏| 10/50 [00:00<00:01, 29.30it/s, train_loss=0.314, val_loss=0.03\u001b[A\n",
      "Epoch 12:  20%|▏| 10/50 [00:00<00:01, 29.30it/s, train_loss=0.314, val_loss=0.03\u001b[A\n",
      "Epoch 12:  20%|▏| 10/50 [00:00<00:01, 29.30it/s, train_loss=0.146, val_loss=0.01\u001b[A\n",
      "Epoch 13:  20%|▏| 10/50 [00:00<00:01, 29.30it/s, train_loss=0.146, val_loss=0.01\u001b[A\n",
      "Epoch 13:  20%|▏| 10/50 [00:00<00:01, 29.30it/s, train_loss=0.102, val_loss=0.01\u001b[A\n",
      "Epoch 13:  28%|▎| 14/50 [00:00<00:01, 30.79it/s, train_loss=0.102, val_loss=0.01\u001b[A\n",
      "Epoch 14:  28%|▎| 14/50 [00:00<00:01, 30.79it/s, train_loss=0.102, val_loss=0.01\u001b[A\n",
      "Epoch 14:  28%|▎| 14/50 [00:00<00:01, 30.79it/s, train_loss=0.124, val_loss=0.01\u001b[A\n",
      "Epoch 15:  28%|▎| 14/50 [00:00<00:01, 30.79it/s, train_loss=0.124, val_loss=0.01\u001b[A\n",
      "Epoch 15:  28%|▎| 14/50 [00:00<00:01, 30.79it/s, train_loss=0.125, val_loss=0.01\u001b[A\n",
      "Epoch 16:  28%|▎| 14/50 [00:00<00:01, 30.79it/s, train_loss=0.125, val_loss=0.01\u001b[A\n",
      "Epoch 16:  28%|▎| 14/50 [00:00<00:01, 30.79it/s, train_loss=0.113, val_loss=0.02\u001b[A\n",
      "Epoch 17:  28%|▎| 14/50 [00:00<00:01, 30.79it/s, train_loss=0.113, val_loss=0.02\u001b[A\n",
      "Epoch 17:  28%|▎| 14/50 [00:00<00:01, 30.79it/s, train_loss=0.125, val_loss=0.02\u001b[A\n",
      "Epoch 17:  36%|▎| 18/50 [00:00<00:01, 30.42it/s, train_loss=0.125, val_loss=0.02\u001b[A\n",
      "Epoch 18:  36%|▎| 18/50 [00:00<00:01, 30.42it/s, train_loss=0.125, val_loss=0.02\u001b[A\n",
      "Epoch 18:  36%|▎| 18/50 [00:00<00:01, 30.42it/s, train_loss=0.126, val_loss=0.01\u001b[A\n",
      "Epoch 19:  36%|▎| 18/50 [00:00<00:01, 30.42it/s, train_loss=0.126, val_loss=0.01\u001b[A\n",
      "Epoch 19:  36%|▎| 18/50 [00:00<00:01, 30.42it/s, train_loss=0.0976, val_loss=0.0\u001b[A\n",
      "Epoch 20:  36%|▎| 18/50 [00:00<00:01, 30.42it/s, train_loss=0.0976, val_loss=0.0\u001b[A\n",
      "Epoch 20:  36%|▎| 18/50 [00:00<00:01, 30.42it/s, train_loss=0.0738, val_loss=0.0\u001b[A\n",
      "Epoch 21:  36%|▎| 18/50 [00:00<00:01, 30.42it/s, train_loss=0.0738, val_loss=0.0\u001b[A\n",
      "Epoch 21:  36%|▎| 18/50 [00:00<00:01, 30.42it/s, train_loss=0.0681, val_loss=0.0\u001b[A\n",
      "Epoch 21:  44%|▍| 22/50 [00:00<00:00, 30.25it/s, train_loss=0.0681, val_loss=0.0\u001b[A\n",
      "Epoch 22:  44%|▍| 22/50 [00:00<00:00, 30.25it/s, train_loss=0.0681, val_loss=0.0\u001b[A\n",
      "Epoch 22:  44%|▍| 22/50 [00:00<00:00, 30.25it/s, train_loss=0.0721, val_loss=0.0\u001b[A\n",
      "Epoch 23:  44%|▍| 22/50 [00:00<00:00, 30.25it/s, train_loss=0.0721, val_loss=0.0\u001b[A\n",
      "Epoch 23:  44%|▍| 22/50 [00:00<00:00, 30.25it/s, train_loss=0.0782, val_loss=0.0\u001b[A\n",
      "Epoch 24:  44%|▍| 22/50 [00:00<00:00, 30.25it/s, train_loss=0.0782, val_loss=0.0\u001b[A\n",
      "Epoch 24:  44%|▍| 22/50 [00:00<00:00, 30.25it/s, train_loss=0.0781, val_loss=0.0\u001b[A\n",
      "Epoch 25:  44%|▍| 22/50 [00:00<00:00, 30.25it/s, train_loss=0.0781, val_loss=0.0\u001b[A\n",
      "Epoch 25:  44%|▍| 22/50 [00:00<00:00, 30.25it/s, train_loss=0.0695, val_loss=0.0\u001b[A\n",
      "Epoch 25:  52%|▌| 26/50 [00:00<00:00, 30.02it/s, train_loss=0.0695, val_loss=0.0\u001b[A\n",
      "Epoch 26:  52%|▌| 26/50 [00:00<00:00, 30.02it/s, train_loss=0.0695, val_loss=0.0\u001b[A\n",
      "Epoch 26:  52%|▌| 26/50 [00:00<00:00, 30.02it/s, train_loss=0.0623, val_loss=0.0\u001b[A\n",
      "Epoch 27:  52%|▌| 26/50 [00:00<00:00, 30.02it/s, train_loss=0.0623, val_loss=0.0\u001b[A\n",
      "Epoch 27:  52%|▌| 26/50 [00:00<00:00, 30.02it/s, train_loss=0.0917, val_loss=0.0\u001b[A\n",
      "Epoch 28:  52%|▌| 26/50 [00:00<00:00, 30.02it/s, train_loss=0.0917, val_loss=0.0\u001b[A\n",
      "Epoch 28:  52%|▌| 26/50 [00:00<00:00, 30.02it/s, train_loss=0.0481, val_loss=0.0\u001b[A\n",
      "Epoch 29:  52%|▌| 26/50 [00:00<00:00, 30.02it/s, train_loss=0.0481, val_loss=0.0\u001b[A\n",
      "Epoch 29:  52%|▌| 26/50 [00:00<00:00, 30.02it/s, train_loss=0.0726, val_loss=0.0\u001b[A\n",
      "Epoch 29:  60%|▌| 30/50 [00:00<00:00, 30.75it/s, train_loss=0.0726, val_loss=0.0\u001b[A\n",
      "Epoch 30:  60%|▌| 30/50 [00:00<00:00, 30.75it/s, train_loss=0.0726, val_loss=0.0\u001b[A\n",
      "Epoch 30:  60%|▌| 30/50 [00:01<00:00, 30.75it/s, train_loss=0.0492, val_loss=0.0\u001b[A\n",
      "Epoch 31:  60%|▌| 30/50 [00:01<00:00, 30.75it/s, train_loss=0.0492, val_loss=0.0\u001b[A\n",
      "Epoch 31:  60%|▌| 30/50 [00:01<00:00, 30.75it/s, train_loss=0.0628, val_loss=0.0\u001b[A\n",
      "Epoch 32:  60%|▌| 30/50 [00:01<00:00, 30.75it/s, train_loss=0.0628, val_loss=0.0\u001b[A\n",
      "Epoch 32:  60%|▌| 30/50 [00:01<00:00, 30.75it/s, train_loss=0.0463, val_loss=0.0\u001b[A\n",
      "Epoch 33:  60%|▌| 30/50 [00:01<00:00, 30.75it/s, train_loss=0.0463, val_loss=0.0\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    27: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33:  60%|▌| 30/50 [00:01<00:00, 30.75it/s, train_loss=0.0517, val_loss=0.0\u001b[A\n",
      "Epoch 33:  68%|▋| 34/50 [00:01<00:00, 31.33it/s, train_loss=0.0517, val_loss=0.0\u001b[A\n",
      "Epoch 34:  68%|▋| 34/50 [00:01<00:00, 31.33it/s, train_loss=0.0517, val_loss=0.0\u001b[A\n",
      "Epoch 34:  68%|▋| 34/50 [00:01<00:00, 31.33it/s, train_loss=0.0429, val_loss=0.0\u001b[A\n",
      "Epoch 35:  68%|▋| 34/50 [00:01<00:00, 31.33it/s, train_loss=0.0429, val_loss=0.0\u001b[A\n",
      "Epoch 35:  68%|▋| 34/50 [00:01<00:00, 31.33it/s, train_loss=0.0429, val_loss=0.0\u001b[A\n",
      "Epoch 36:  68%|▋| 34/50 [00:01<00:00, 31.33it/s, train_loss=0.0429, val_loss=0.0\u001b[A\n",
      "Epoch 36:  68%|▋| 34/50 [00:01<00:00, 31.33it/s, train_loss=0.041, val_loss=0.00\u001b[A\n",
      "Epoch 37:  68%|▋| 34/50 [00:01<00:00, 31.33it/s, train_loss=0.041, val_loss=0.00\u001b[A\n",
      "Epoch 37:  68%|▋| 34/50 [00:01<00:00, 31.33it/s, train_loss=0.036, val_loss=0.00\u001b[A\n",
      "Epoch 37:  76%|▊| 38/50 [00:01<00:00, 30.93it/s, train_loss=0.036, val_loss=0.00\u001b[A\n",
      "Epoch 38:  76%|▊| 38/50 [00:01<00:00, 30.93it/s, train_loss=0.036, val_loss=0.00\u001b[A\n",
      "Epoch 38:  76%|▊| 38/50 [00:01<00:00, 30.93it/s, train_loss=0.0402, val_loss=0.0\u001b[A\n",
      "Epoch 39:  76%|▊| 38/50 [00:01<00:00, 30.93it/s, train_loss=0.0402, val_loss=0.0\u001b[A\n",
      "Epoch 39:  76%|▊| 38/50 [00:01<00:00, 30.93it/s, train_loss=0.032, val_loss=0.00\u001b[A\n",
      "Epoch 40:  76%|▊| 38/50 [00:01<00:00, 30.93it/s, train_loss=0.032, val_loss=0.00\u001b[A\n",
      "Epoch 40:  76%|▊| 38/50 [00:01<00:00, 30.93it/s, train_loss=0.0396, val_loss=0.0\u001b[A\n",
      "Epoch 41:  76%|▊| 38/50 [00:01<00:00, 30.93it/s, train_loss=0.0396, val_loss=0.0\u001b[A\n",
      "Epoch 41:  76%|▊| 38/50 [00:01<00:00, 30.93it/s, train_loss=0.0279, val_loss=0.0\u001b[A\n",
      "Epoch 41:  84%|▊| 42/50 [00:01<00:00, 30.91it/s, train_loss=0.0279, val_loss=0.0\u001b[A\n",
      "Epoch 42:  84%|▊| 42/50 [00:01<00:00, 30.91it/s, train_loss=0.0279, val_loss=0.0\u001b[A\n",
      "Epoch 42:  84%|▊| 42/50 [00:01<00:00, 30.91it/s, train_loss=0.037, val_loss=0.00\u001b[A\n",
      "Epoch 43:  84%|▊| 42/50 [00:01<00:00, 30.91it/s, train_loss=0.037, val_loss=0.00\u001b[A\n",
      "Epoch 43:  84%|▊| 42/50 [00:01<00:00, 30.91it/s, train_loss=0.025, val_loss=0.00\u001b[A\n",
      "Epoch 44:  84%|▊| 42/50 [00:01<00:00, 30.91it/s, train_loss=0.025, val_loss=0.00\u001b[A\n",
      "Epoch 44:  84%|▊| 42/50 [00:01<00:00, 30.91it/s, train_loss=0.0347, val_loss=0.0\u001b[A\n",
      "Epoch 45:  84%|▊| 42/50 [00:01<00:00, 30.91it/s, train_loss=0.0347, val_loss=0.0\u001b[A\n",
      "Epoch 45:  84%|▊| 42/50 [00:01<00:00, 30.91it/s, train_loss=0.0249, val_loss=0.0\u001b[A\n",
      "Epoch 45:  92%|▉| 46/50 [00:01<00:00, 30.48it/s, train_loss=0.0249, val_loss=0.0\u001b[A\n",
      "Epoch 46:  92%|▉| 46/50 [00:01<00:00, 30.48it/s, train_loss=0.0249, val_loss=0.0\u001b[A\n",
      "Epoch 46:  92%|▉| 46/50 [00:01<00:00, 30.48it/s, train_loss=0.0309, val_loss=0.0\u001b[A\n",
      "Epoch 47:  92%|▉| 46/50 [00:01<00:00, 30.48it/s, train_loss=0.0309, val_loss=0.0\u001b[A\n",
      "Epoch 47:  92%|▉| 46/50 [00:01<00:00, 30.48it/s, train_loss=0.0253, val_loss=0.0\u001b[A\n",
      "Epoch 48:  92%|▉| 46/50 [00:01<00:00, 30.48it/s, train_loss=0.0253, val_loss=0.0\u001b[A\n",
      "Epoch 48:  92%|▉| 46/50 [00:01<00:00, 30.48it/s, train_loss=0.0263, val_loss=0.0\u001b[A\n",
      "Epoch 49:  92%|▉| 46/50 [00:01<00:00, 30.48it/s, train_loss=0.0263, val_loss=0.0\u001b[A\n",
      "Epoch 49:  92%|▉| 46/50 [00:01<00:00, 30.48it/s, train_loss=0.0257, val_loss=0.0\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 30.52it/s, train_loss=0.0257, val_loss=0.0\u001b[A\n",
      " 18%|███████▌                                  | 18/100 [00:28<02:14,  1.64s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=32.1, val_loss=29.4, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=32.1, val_loss=29.4, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=136, val_loss=9.47, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=136, val_loss=9.47, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=51.8, val_loss=0.808, train_ci\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=51.8, val_loss=0.808, train_ci\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=4.59, val_loss=7.95, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.46it/s, train_loss=4.59, val_loss=7.95, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.46it/s, train_loss=4.59, val_loss=7.95, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.46it/s, train_loss=44, val_loss=1.1, trai\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.46it/s, train_loss=44, val_loss=1.1, trai\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.46it/s, train_loss=5.35, val_loss=3.49, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.46it/s, train_loss=5.35, val_loss=3.49, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.46it/s, train_loss=17.4, val_loss=0.297, \u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.46it/s, train_loss=17.4, val_loss=0.297, \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7329993738259236\n",
      "Its' val AUC : 0.7095283403561234\n",
      "Its external  AUC: 0.6037498030565621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.46it/s, train_loss=1.88, val_loss=3.59, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 33.20it/s, train_loss=1.88, val_loss=3.59, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.20it/s, train_loss=1.88, val_loss=3.59, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.20it/s, train_loss=16.4, val_loss=0.269, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.20it/s, train_loss=16.4, val_loss=0.269, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.20it/s, train_loss=1.88, val_loss=1.12, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.20it/s, train_loss=1.88, val_loss=1.12, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.20it/s, train_loss=5.84, val_loss=1.03, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.20it/s, train_loss=5.84, val_loss=1.03, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.20it/s, train_loss=5.63, val_loss=0.35, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 33.17it/s, train_loss=5.63, val_loss=0.35,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.17it/s, train_loss=5.63, val_loss=0.35,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.17it/s, train_loss=2.23, val_loss=1.11,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.17it/s, train_loss=2.23, val_loss=1.11,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.17it/s, train_loss=6.06, val_loss=0.123\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.17it/s, train_loss=6.06, val_loss=0.123\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7437695679398872\n",
      "Its' val AUC : 0.6914503194236782\n",
      "Its external  AUC: 0.61855994958248\n",
      "Curent best Test AUC: 0.7457733249843457\n",
      "Its' val AUC : 0.6411580807394318\n",
      "Its external  AUC: 0.6171419568299984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.17it/s, train_loss=0.777, val_loss=0.93\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.17it/s, train_loss=0.777, val_loss=0.93\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.17it/s, train_loss=4.65, val_loss=0.077\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 33.22it/s, train_loss=4.65, val_loss=0.077\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.22it/s, train_loss=4.65, val_loss=0.077\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.22it/s, train_loss=0.555, val_loss=0.45\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.22it/s, train_loss=0.555, val_loss=0.45\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.22it/s, train_loss=2.39, val_loss=0.129\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.22it/s, train_loss=2.39, val_loss=0.129\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.22it/s, train_loss=0.754, val_loss=0.25\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.22it/s, train_loss=0.754, val_loss=0.25\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.22it/s, train_loss=1.53, val_loss=0.169\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 32.75it/s, train_loss=1.53, val_loss=0.169\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.75it/s, train_loss=1.53, val_loss=0.169\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.75it/s, train_loss=1.05, val_loss=0.176\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.75it/s, train_loss=1.05, val_loss=0.176\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.75it/s, train_loss=0.869, val_loss=0.25\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.75it/s, train_loss=0.869, val_loss=0.25\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.75it/s, train_loss=1.22, val_loss=0.089\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.75it/s, train_loss=1.22, val_loss=0.089\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.75it/s, train_loss=0.469, val_loss=0.11\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 32.27it/s, train_loss=0.469, val_loss=0.11\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 32.27it/s, train_loss=0.469, val_loss=0.11\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 32.27it/s, train_loss=0.648, val_loss=0.18\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 32.27it/s, train_loss=0.648, val_loss=0.18\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 32.27it/s, train_loss=1.01, val_loss=0.090\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 32.27it/s, train_loss=1.01, val_loss=0.090\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 32.27it/s, train_loss=0.5, val_loss=0.0821\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 32.27it/s, train_loss=0.5, val_loss=0.0821\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 32.27it/s, train_loss=0.453, val_loss=0.15\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 32.80it/s, train_loss=0.453, val_loss=0.15\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 32.80it/s, train_loss=0.453, val_loss=0.15\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    22: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch    28: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 32.80it/s, train_loss=0.827, val_loss=0.11\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 32.80it/s, train_loss=0.827, val_loss=0.11\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 32.80it/s, train_loss=0.616, val_loss=0.06\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 32.80it/s, train_loss=0.616, val_loss=0.06\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 32.80it/s, train_loss=0.321, val_loss=0.07\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 32.80it/s, train_loss=0.321, val_loss=0.07\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 32.80it/s, train_loss=0.351, val_loss=0.11\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 32.92it/s, train_loss=0.351, val_loss=0.11\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 32.92it/s, train_loss=0.351, val_loss=0.11\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:01<00:00, 32.92it/s, train_loss=0.547, val_loss=0.10\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 32.92it/s, train_loss=0.547, val_loss=0.10\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 32.92it/s, train_loss=0.527, val_loss=0.06\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 32.92it/s, train_loss=0.527, val_loss=0.06\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 32.92it/s, train_loss=0.303, val_loss=0.04\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 32.92it/s, train_loss=0.303, val_loss=0.04\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 32.92it/s, train_loss=0.216, val_loss=0.07\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 33.25it/s, train_loss=0.216, val_loss=0.07\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.25it/s, train_loss=0.216, val_loss=0.07\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.25it/s, train_loss=0.342, val_loss=0.07\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.25it/s, train_loss=0.342, val_loss=0.07\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.25it/s, train_loss=0.387, val_loss=0.05\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.25it/s, train_loss=0.387, val_loss=0.05\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.25it/s, train_loss=0.253, val_loss=0.03\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.25it/s, train_loss=0.253, val_loss=0.03\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.25it/s, train_loss=0.158, val_loss=0.04\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 33.32it/s, train_loss=0.158, val_loss=0.04\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 33.32it/s, train_loss=0.158, val_loss=0.04\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 33.32it/s, train_loss=0.22, val_loss=0.060\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 33.32it/s, train_loss=0.22, val_loss=0.060\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 33.32it/s, train_loss=0.283, val_loss=0.04\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 33.32it/s, train_loss=0.283, val_loss=0.04\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 33.32it/s, train_loss=0.218, val_loss=0.02\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 33.32it/s, train_loss=0.218, val_loss=0.02\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 33.32it/s, train_loss=0.136, val_loss=0.03\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 33.23it/s, train_loss=0.136, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 33.23it/s, train_loss=0.136, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 33.23it/s, train_loss=0.161, val_loss=0.04\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 33.23it/s, train_loss=0.161, val_loss=0.04\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 33.23it/s, train_loss=0.216, val_loss=0.03\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 33.23it/s, train_loss=0.216, val_loss=0.03\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 33.23it/s, train_loss=0.186, val_loss=0.02\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 33.23it/s, train_loss=0.186, val_loss=0.02\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 33.23it/s, train_loss=0.125, val_loss=0.02\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 32.96it/s, train_loss=0.125, val_loss=0.02\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 32.96it/s, train_loss=0.125, val_loss=0.02\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 32.96it/s, train_loss=0.134, val_loss=0.03\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 32.96it/s, train_loss=0.134, val_loss=0.03\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 32.97it/s, train_loss=0.175, val_loss=0.03\u001b[A\n",
      " 19%|███████▉                                  | 19/100 [00:30<02:10,  1.61s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=8.45, val_loss=101, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=8.45, val_loss=101, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=532, val_loss=2.56, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=532, val_loss=2.56, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=11.3, val_loss=11.2, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=11.3, val_loss=11.2, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=59.3, val_loss=5.73, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.24it/s, train_loss=59.3, val_loss=5.73, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.24it/s, train_loss=59.3, val_loss=5.73, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.24it/s, train_loss=27.1, val_loss=4.15, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.24it/s, train_loss=27.1, val_loss=4.15, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.24it/s, train_loss=20.8, val_loss=3.55, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.24it/s, train_loss=20.8, val_loss=3.55, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.24it/s, train_loss=18, val_loss=1.86, tra\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.24it/s, train_loss=18, val_loss=1.86, tra\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.24470882905447713\n",
      "Its' val AUC : 0.27742286257985593\n",
      "Its external  AUC: 0.3934142114384749\n",
      "Curent best Test AUC: 0.39085785848465876\n",
      "Its' val AUC : 0.3915998368900367\n",
      "Its external  AUC: 0.606113124310698\n",
      "Curent best Test AUC: 0.7519098309329993\n",
      "Its' val AUC : 0.713470164469213\n",
      "Its external  AUC: 0.6195052780841342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.24it/s, train_loss=10.3, val_loss=4.58, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 33.89it/s, train_loss=10.3, val_loss=4.58, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.89it/s, train_loss=10.3, val_loss=4.58, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.89it/s, train_loss=26.5, val_loss=1.66, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.89it/s, train_loss=26.5, val_loss=1.66, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.89it/s, train_loss=11.4, val_loss=3.79, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.89it/s, train_loss=11.4, val_loss=3.79, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.89it/s, train_loss=20.3, val_loss=2.92, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.89it/s, train_loss=20.3, val_loss=2.92, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.89it/s, train_loss=14, val_loss=1.1, tra\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.05it/s, train_loss=14, val_loss=1.1, tr\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.05it/s, train_loss=14, val_loss=1.1, tr\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.05it/s, train_loss=6.92, val_loss=1.66,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.05it/s, train_loss=6.92, val_loss=1.66,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.05it/s, train_loss=11.3, val_loss=2.04,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.05it/s, train_loss=11.3, val_loss=2.04,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.05it/s, train_loss=13, val_loss=0.959, \u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.05it/s, train_loss=13, val_loss=0.959, \u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.05it/s, train_loss=6.32, val_loss=0.699\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.21it/s, train_loss=6.32, val_loss=0.699\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.21it/s, train_loss=6.32, val_loss=0.699\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.21it/s, train_loss=4.91, val_loss=1.03,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.21it/s, train_loss=4.91, val_loss=1.03,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.21it/s, train_loss=5.95, val_loss=0.862\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.21it/s, train_loss=5.95, val_loss=0.862\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.21it/s, train_loss=5.43, val_loss=0.61,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.21it/s, train_loss=5.43, val_loss=0.61,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.21it/s, train_loss=3.5, val_loss=0.88, \u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.20it/s, train_loss=3.5, val_loss=0.88, \u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.20it/s, train_loss=3.5, val_loss=0.88, \u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.20it/s, train_loss=4.36, val_loss=0.357\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.20it/s, train_loss=4.36, val_loss=0.357\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.20it/s, train_loss=1.94, val_loss=0.265\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.20it/s, train_loss=1.94, val_loss=0.265\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.20it/s, train_loss=1.49, val_loss=0.354\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.20it/s, train_loss=1.49, val_loss=0.354\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.20it/s, train_loss=1.84, val_loss=0.23,\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.28it/s, train_loss=1.84, val_loss=0.23,\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.28it/s, train_loss=1.84, val_loss=0.23,\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.28it/s, train_loss=1.25, val_loss=0.203\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.28it/s, train_loss=1.25, val_loss=0.203\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.28it/s, train_loss=1.19, val_loss=0.239\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.28it/s, train_loss=1.19, val_loss=0.239\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.28it/s, train_loss=1.33, val_loss=0.243\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.28it/s, train_loss=1.33, val_loss=0.243\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.28it/s, train_loss=1.23, val_loss=0.315\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.29it/s, train_loss=1.23, val_loss=0.315\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.29it/s, train_loss=1.23, val_loss=0.315\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.29it/s, train_loss=1.52, val_loss=0.2, \u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.29it/s, train_loss=1.52, val_loss=0.2, \u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.29it/s, train_loss=1.07, val_loss=0.152\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.29it/s, train_loss=1.07, val_loss=0.152\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.29it/s, train_loss=1.02, val_loss=0.155\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.29it/s, train_loss=1.02, val_loss=0.155\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.29it/s, train_loss=1.07, val_loss=0.102\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 33.74it/s, train_loss=1.07, val_loss=0.102\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 33.74it/s, train_loss=1.07, val_loss=0.102\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 33.74it/s, train_loss=0.717, val_loss=0.12\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 33.74it/s, train_loss=0.717, val_loss=0.12\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 33.74it/s, train_loss=0.776, val_loss=0.14\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 33.74it/s, train_loss=0.776, val_loss=0.14\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 33.74it/s, train_loss=0.887, val_loss=1.54\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 33.74it/s, train_loss=0.887, val_loss=1.54\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 33.74it/s, train_loss=8.29, val_loss=0.754\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 33.19it/s, train_loss=8.29, val_loss=0.754\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.19it/s, train_loss=8.29, val_loss=0.754\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.19it/s, train_loss=3.97, val_loss=0.646\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.19it/s, train_loss=3.97, val_loss=0.646\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.19it/s, train_loss=3.34, val_loss=0.076\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.19it/s, train_loss=3.34, val_loss=0.076\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.19it/s, train_loss=0.446, val_loss=0.69\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.19it/s, train_loss=0.446, val_loss=0.69\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.19it/s, train_loss=3.59, val_loss=0.106\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 33.18it/s, train_loss=3.59, val_loss=0.106\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 33.18it/s, train_loss=3.59, val_loss=0.106\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 33.18it/s, train_loss=0.768, val_loss=0.18\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 33.18it/s, train_loss=0.768, val_loss=0.18\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 33.18it/s, train_loss=0.916, val_loss=0.39\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 33.18it/s, train_loss=0.916, val_loss=0.39\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 33.18it/s, train_loss=1.71, val_loss=0.429\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 33.18it/s, train_loss=1.71, val_loss=0.429\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 33.18it/s, train_loss=1.94, val_loss=0.328\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 32.92it/s, train_loss=1.94, val_loss=0.328\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 32.92it/s, train_loss=1.94, val_loss=0.328\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 32.92it/s, train_loss=1.71, val_loss=0.324\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 32.92it/s, train_loss=1.71, val_loss=0.324\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 32.92it/s, train_loss=1.75, val_loss=0.324\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 32.92it/s, train_loss=1.75, val_loss=0.324\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 32.92it/s, train_loss=1.72, val_loss=0.297\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 32.92it/s, train_loss=1.72, val_loss=0.297\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 32.92it/s, train_loss=1.52, val_loss=0.268\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 33.19it/s, train_loss=1.52, val_loss=0.268\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 33.19it/s, train_loss=1.52, val_loss=0.268\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 33.19it/s, train_loss=1.33, val_loss=0.258\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 33.19it/s, train_loss=1.33, val_loss=0.258\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 33.56it/s, train_loss=1.26, val_loss=0.252\u001b[A\n",
      " 20%|████████▍                                 | 20/100 [00:31<02:06,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    44: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch    50: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=8.14, val_loss=2.84, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=8.14, val_loss=2.84, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=17.2, val_loss=61.9, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=17.2, val_loss=61.9, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=322, val_loss=22.9, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=322, val_loss=22.9, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=120, val_loss=33, train_cindex\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 32.92it/s, train_loss=120, val_loss=33, trai\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.92it/s, train_loss=120, val_loss=33, trai\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.92it/s, train_loss=180, val_loss=5.12, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.92it/s, train_loss=180, val_loss=5.12, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.92it/s, train_loss=28.9, val_loss=9.43, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.92it/s, train_loss=28.9, val_loss=9.43, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.92it/s, train_loss=45.4, val_loss=15, tra\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.92it/s, train_loss=45.4, val_loss=15, tra\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.3264871634314339\n",
      "Its' val AUC : 0.3293462008971048\n",
      "Its external  AUC: 0.4376870962659524\n",
      "Curent best Test AUC: 0.715466499686913\n",
      "Its' val AUC : 0.7077613157537039\n",
      "Its external  AUC: 0.6144635260753112\n",
      "Curent best Test AUC: 0.7406386975579211\n",
      "Its' val AUC : 0.7106157401114584\n",
      "Its external  AUC: 0.6187175043327556\n",
      "Epoch     7: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.92it/s, train_loss=73.5, val_loss=5.18, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 32.53it/s, train_loss=73.5, val_loss=5.18, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.53it/s, train_loss=73.5, val_loss=5.18, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.53it/s, train_loss=24.9, val_loss=0.237, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.53it/s, train_loss=24.9, val_loss=0.237, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.53it/s, train_loss=1.18, val_loss=2.25, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.53it/s, train_loss=1.18, val_loss=2.25, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.53it/s, train_loss=12.1, val_loss=5.34, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.53it/s, train_loss=12.1, val_loss=5.34, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.53it/s, train_loss=28.3, val_loss=5.38, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 33.43it/s, train_loss=28.3, val_loss=5.38,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.43it/s, train_loss=28.3, val_loss=5.38,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.43it/s, train_loss=28.7, val_loss=2.67,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.43it/s, train_loss=28.7, val_loss=2.67,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.43it/s, train_loss=14.3, val_loss=0.367\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.43it/s, train_loss=14.3, val_loss=0.367\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.43it/s, train_loss=2.06, val_loss=0.511\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.43it/s, train_loss=2.06, val_loss=0.511\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.43it/s, train_loss=2.41, val_loss=1.23,\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 33.67it/s, train_loss=2.41, val_loss=1.23,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.67it/s, train_loss=2.41, val_loss=1.23,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.67it/s, train_loss=5.92, val_loss=1.6, \u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.67it/s, train_loss=5.92, val_loss=1.6, \u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.67it/s, train_loss=7.8, val_loss=1.36, \u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.67it/s, train_loss=7.8, val_loss=1.36, \u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.67it/s, train_loss=6.64, val_loss=0.741\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.67it/s, train_loss=6.64, val_loss=0.741\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.67it/s, train_loss=3.64, val_loss=0.208\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 33.73it/s, train_loss=3.64, val_loss=0.208\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.73it/s, train_loss=3.64, val_loss=0.208\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.73it/s, train_loss=1.05, val_loss=0.071\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.73it/s, train_loss=1.05, val_loss=0.071\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    15: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.73it/s, train_loss=0.445, val_loss=0.29\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.73it/s, train_loss=0.445, val_loss=0.29\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.73it/s, train_loss=1.65, val_loss=0.601\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.73it/s, train_loss=1.65, val_loss=0.601\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.73it/s, train_loss=3.28, val_loss=0.724\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 33.46it/s, train_loss=3.28, val_loss=0.724\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.46it/s, train_loss=3.28, val_loss=0.724\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.46it/s, train_loss=3.95, val_loss=0.578\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.46it/s, train_loss=3.95, val_loss=0.578\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.46it/s, train_loss=3.2, val_loss=0.289,\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.46it/s, train_loss=3.2, val_loss=0.289,\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.46it/s, train_loss=1.67, val_loss=0.075\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.46it/s, train_loss=1.67, val_loss=0.075\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.46it/s, train_loss=0.47, val_loss=0.047\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 33.03it/s, train_loss=0.47, val_loss=0.047\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.03it/s, train_loss=0.47, val_loss=0.047\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.03it/s, train_loss=0.257, val_loss=0.07\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.03it/s, train_loss=0.257, val_loss=0.07\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.03it/s, train_loss=0.317, val_loss=0.13\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.03it/s, train_loss=0.317, val_loss=0.13\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.03it/s, train_loss=0.559, val_loss=0.19\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.03it/s, train_loss=0.559, val_loss=0.19\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.03it/s, train_loss=0.831, val_loss=0.23\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 33.05it/s, train_loss=0.831, val_loss=0.23\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 33.05it/s, train_loss=0.831, val_loss=0.23\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 33.05it/s, train_loss=0.993, val_loss=0.22\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 33.05it/s, train_loss=0.993, val_loss=0.22\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    27: reducing learning rate of group 0 to 1.2500e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 33.05it/s, train_loss=0.976, val_loss=0.18\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 33.05it/s, train_loss=0.976, val_loss=0.18\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 33.05it/s, train_loss=0.796, val_loss=0.13\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 33.05it/s, train_loss=0.796, val_loss=0.13\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 33.05it/s, train_loss=0.585, val_loss=0.08\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 32.71it/s, train_loss=0.585, val_loss=0.08\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 32.71it/s, train_loss=0.585, val_loss=0.08\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 32.71it/s, train_loss=0.378, val_loss=0.05\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 32.71it/s, train_loss=0.378, val_loss=0.05\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 32.71it/s, train_loss=0.235, val_loss=0.03\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 32.71it/s, train_loss=0.235, val_loss=0.03\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 32.71it/s, train_loss=0.185, val_loss=0.03\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 32.71it/s, train_loss=0.185, val_loss=0.03\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 32.71it/s, train_loss=0.22, val_loss=0.043\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 32.67it/s, train_loss=0.22, val_loss=0.043\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 32.67it/s, train_loss=0.22, val_loss=0.043\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 32.67it/s, train_loss=0.304, val_loss=0.05\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 32.67it/s, train_loss=0.304, val_loss=0.05\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 32.67it/s, train_loss=0.392, val_loss=0.06\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 32.67it/s, train_loss=0.392, val_loss=0.06\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 32.67it/s, train_loss=0.444, val_loss=0.06\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 32.67it/s, train_loss=0.444, val_loss=0.06\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 32.67it/s, train_loss=0.441, val_loss=0.05\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 32.36it/s, train_loss=0.441, val_loss=0.05\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 32.36it/s, train_loss=0.441, val_loss=0.05\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 32.36it/s, train_loss=0.388, val_loss=0.04\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 32.36it/s, train_loss=0.388, val_loss=0.04\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 32.36it/s, train_loss=0.307, val_loss=0.03\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 32.36it/s, train_loss=0.307, val_loss=0.03\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 32.36it/s, train_loss=0.229, val_loss=0.02\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 32.36it/s, train_loss=0.229, val_loss=0.02\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 32.36it/s, train_loss=0.177, val_loss=0.02\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 32.59it/s, train_loss=0.177, val_loss=0.02\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 32.59it/s, train_loss=0.177, val_loss=0.02\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 32.59it/s, train_loss=0.163, val_loss=0.03\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 32.59it/s, train_loss=0.163, val_loss=0.03\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 32.93it/s, train_loss=0.181, val_loss=0.04\u001b[A\n",
      " 21%|████████▊                                 | 21/100 [00:33<02:03,  1.57s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=91.6, val_loss=9.36, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=91.6, val_loss=9.36, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=53.8, val_loss=17.4, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=53.8, val_loss=17.4, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=86.5, val_loss=0.307, train_ci\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=86.5, val_loss=0.307, train_ci\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=1.95, val_loss=3.51, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 30.96it/s, train_loss=1.95, val_loss=3.51, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 30.96it/s, train_loss=1.95, val_loss=3.51, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 30.96it/s, train_loss=19.5, val_loss=1.79, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 30.96it/s, train_loss=19.5, val_loss=1.79, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 30.96it/s, train_loss=9.75, val_loss=0.963, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 30.96it/s, train_loss=9.75, val_loss=0.963, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 30.96it/s, train_loss=5.18, val_loss=1.89, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 30.96it/s, train_loss=5.18, val_loss=1.89, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.2855353788353162\n",
      "Its' val AUC : 0.3540845453309773\n",
      "Its external  AUC: 0.3905782259335119\n",
      "Curent best Test AUC: 0.6587351283656857\n",
      "Its' val AUC : 0.6856055457387522\n",
      "Its external  AUC: 0.6108397668189696\n",
      "Curent best Test AUC: 0.6616155291170945\n",
      "Its' val AUC : 0.5793122196547505\n",
      "Its external  AUC: 0.5758626122577596\n",
      "Curent best Test AUC: 0.7283656856606137\n",
      "Its' val AUC : 0.685877395677586\n",
      "Its external  AUC: 0.5810619190168583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 30.96it/s, train_loss=9.85, val_loss=0.487, \u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 31.62it/s, train_loss=9.85, val_loss=0.487, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 31.62it/s, train_loss=9.85, val_loss=0.487, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 31.62it/s, train_loss=2.71, val_loss=1.94, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 31.62it/s, train_loss=2.71, val_loss=1.94, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 31.62it/s, train_loss=9.79, val_loss=0.53, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 31.62it/s, train_loss=9.79, val_loss=0.53, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 31.62it/s, train_loss=2.78, val_loss=0.245,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 31.62it/s, train_loss=2.78, val_loss=0.245,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 31.62it/s, train_loss=1.31, val_loss=1.2, t\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 32.27it/s, train_loss=1.31, val_loss=1.2, \u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.27it/s, train_loss=1.31, val_loss=1.2, \u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.27it/s, train_loss=5.81, val_loss=0.641\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.27it/s, train_loss=5.81, val_loss=0.641\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.27it/s, train_loss=2.99, val_loss=0.612\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.27it/s, train_loss=2.99, val_loss=0.612\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.27it/s, train_loss=4.17, val_loss=0.48,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.27it/s, train_loss=4.17, val_loss=0.48,\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     9: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Curent best Test AUC: 0.7396368190356919\n",
      "Its' val AUC : 0.7117031398667936\n",
      "Its external  AUC: 0.592563415786986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.27it/s, train_loss=3, val_loss=0.524, t\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 31.86it/s, train_loss=3, val_loss=0.524, t\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 31.86it/s, train_loss=3, val_loss=0.524, t\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 31.86it/s, train_loss=3.37, val_loss=0.208\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 31.86it/s, train_loss=3.37, val_loss=0.208\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 31.86it/s, train_loss=1.47, val_loss=0.224\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 31.86it/s, train_loss=1.47, val_loss=0.224\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 31.86it/s, train_loss=1.26, val_loss=0.289\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 31.86it/s, train_loss=1.26, val_loss=0.289\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 31.86it/s, train_loss=1.62, val_loss=0.103\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 32.09it/s, train_loss=1.62, val_loss=0.103\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.09it/s, train_loss=1.62, val_loss=0.103\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.09it/s, train_loss=0.68, val_loss=0.202\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.09it/s, train_loss=0.68, val_loss=0.202\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.09it/s, train_loss=1.12, val_loss=0.356\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.09it/s, train_loss=1.12, val_loss=0.356\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.09it/s, train_loss=1.83, val_loss=0.196\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.09it/s, train_loss=1.83, val_loss=0.196\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.09it/s, train_loss=1.06, val_loss=0.073\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 32.77it/s, train_loss=1.06, val_loss=0.073\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 32.77it/s, train_loss=1.06, val_loss=0.073\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7448966812773952\n",
      "Its' val AUC : 0.7195867880929727\n",
      "Its external  AUC: 0.5949267370411218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 32.77it/s, train_loss=0.512, val_loss=0.14\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 32.77it/s, train_loss=0.512, val_loss=0.14\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 32.77it/s, train_loss=0.82, val_loss=0.088\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 32.77it/s, train_loss=0.82, val_loss=0.088\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 32.77it/s, train_loss=0.508, val_loss=0.07\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 32.77it/s, train_loss=0.508, val_loss=0.07\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 32.77it/s, train_loss=0.484, val_loss=0.16\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 32.73it/s, train_loss=0.484, val_loss=0.16\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 32.73it/s, train_loss=0.484, val_loss=0.16\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 32.73it/s, train_loss=1.01, val_loss=0.11,\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 32.73it/s, train_loss=1.01, val_loss=0.11,\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 32.73it/s, train_loss=0.694, val_loss=0.05\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 32.73it/s, train_loss=0.694, val_loss=0.05\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 32.73it/s, train_loss=0.321, val_loss=0.08\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 32.73it/s, train_loss=0.321, val_loss=0.08\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 32.73it/s, train_loss=0.508, val_loss=0.05\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 32.81it/s, train_loss=0.508, val_loss=0.05\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 32.81it/s, train_loss=0.508, val_loss=0.05\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:01<00:00, 32.81it/s, train_loss=0.341, val_loss=0.06\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 32.81it/s, train_loss=0.341, val_loss=0.06\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 32.81it/s, train_loss=0.357, val_loss=0.12\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 32.81it/s, train_loss=0.357, val_loss=0.12\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 32.81it/s, train_loss=0.629, val_loss=0.06\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 32.81it/s, train_loss=0.629, val_loss=0.06\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 32.81it/s, train_loss=0.369, val_loss=0.03\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 32.94it/s, train_loss=0.369, val_loss=0.03\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 32.94it/s, train_loss=0.369, val_loss=0.03\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 32.94it/s, train_loss=0.24, val_loss=0.062\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 32.94it/s, train_loss=0.24, val_loss=0.062\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.747777082028804\n",
      "Its' val AUC : 0.700829142313443\n",
      "Its external  AUC: 0.5991807152985662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 32.94it/s, train_loss=0.336, val_loss=0.03\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 32.94it/s, train_loss=0.336, val_loss=0.03\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 32.94it/s, train_loss=0.2, val_loss=0.0488\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 32.94it/s, train_loss=0.2, val_loss=0.0488\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 32.94it/s, train_loss=0.307, val_loss=0.05\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 32.93it/s, train_loss=0.307, val_loss=0.05\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 32.93it/s, train_loss=0.307, val_loss=0.05\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 32.93it/s, train_loss=0.37, val_loss=0.028\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 32.93it/s, train_loss=0.37, val_loss=0.028\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 32.93it/s, train_loss=0.177, val_loss=0.03\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 32.93it/s, train_loss=0.177, val_loss=0.03\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 32.93it/s, train_loss=0.204, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 32.93it/s, train_loss=0.204, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 32.93it/s, train_loss=0.193, val_loss=0.02\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 33.16it/s, train_loss=0.193, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 33.16it/s, train_loss=0.193, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 33.16it/s, train_loss=0.154, val_loss=0.04\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 33.16it/s, train_loss=0.154, val_loss=0.04\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 33.16it/s, train_loss=0.258, val_loss=0.02\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 33.16it/s, train_loss=0.258, val_loss=0.02\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 33.16it/s, train_loss=0.178, val_loss=0.02\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 33.16it/s, train_loss=0.178, val_loss=0.02\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 33.16it/s, train_loss=0.13, val_loss=0.029\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 33.29it/s, train_loss=0.13, val_loss=0.029\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 33.29it/s, train_loss=0.13, val_loss=0.029\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 33.29it/s, train_loss=0.161, val_loss=0.01\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 33.29it/s, train_loss=0.161, val_loss=0.01\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 32.78it/s, train_loss=0.115, val_loss=0.02\u001b[A\n",
      " 22%|█████████▏                                | 22/100 [00:34<02:01,  1.56s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=28.6, val_loss=78.3, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=28.6, val_loss=78.3, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=401, val_loss=43.4, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=401, val_loss=43.4, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=243, val_loss=7.56, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=243, val_loss=7.56, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=39.9, val_loss=3.48, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.91it/s, train_loss=39.9, val_loss=3.48, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.91it/s, train_loss=39.9, val_loss=3.48, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.91it/s, train_loss=19.1, val_loss=5.99, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.91it/s, train_loss=19.1, val_loss=5.99, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.91it/s, train_loss=32.2, val_loss=2.17, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.91it/s, train_loss=32.2, val_loss=2.17, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.91it/s, train_loss=11.8, val_loss=4.6, tr\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.91it/s, train_loss=11.8, val_loss=4.6, tr\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7505322479649342\n",
      "Its' val AUC : 0.711159439989126\n",
      "Its external  AUC: 0.6217110445879943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.91it/s, train_loss=23.5, val_loss=0.375, \u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 34.09it/s, train_loss=23.5, val_loss=0.375, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.09it/s, train_loss=23.5, val_loss=0.375, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.09it/s, train_loss=2.48, val_loss=3.17, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.09it/s, train_loss=2.48, val_loss=3.17, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.09it/s, train_loss=17.9, val_loss=0.935, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.09it/s, train_loss=17.9, val_loss=0.935,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.09it/s, train_loss=5.9, val_loss=1.84, t\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.09it/s, train_loss=5.9, val_loss=1.84, t\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.09it/s, train_loss=10.6, val_loss=0.25, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 33.63it/s, train_loss=10.6, val_loss=0.25,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.63it/s, train_loss=10.6, val_loss=0.25,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.63it/s, train_loss=1.58, val_loss=1.08,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.63it/s, train_loss=1.58, val_loss=1.08,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.63it/s, train_loss=6.89, val_loss=0.289\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.63it/s, train_loss=6.89, val_loss=0.289\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.63it/s, train_loss=2.23, val_loss=1.19,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.63it/s, train_loss=2.23, val_loss=1.19,\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.758797745773325\n",
      "Its' val AUC : 0.6880521951882561\n",
      "Its external  AUC: 0.5769654955096896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.63it/s, train_loss=6.11, val_loss=0.302\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 33.85it/s, train_loss=6.11, val_loss=0.302\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.85it/s, train_loss=6.11, val_loss=0.302\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.85it/s, train_loss=1.81, val_loss=0.827\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.85it/s, train_loss=1.81, val_loss=0.827\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.85it/s, train_loss=4.44, val_loss=0.264\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.85it/s, train_loss=4.44, val_loss=0.264\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.85it/s, train_loss=1.41, val_loss=0.252\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.85it/s, train_loss=1.41, val_loss=0.252\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.85it/s, train_loss=1.65, val_loss=0.313\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 33.62it/s, train_loss=1.65, val_loss=0.313\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.62it/s, train_loss=1.65, val_loss=0.313\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.62it/s, train_loss=2.03, val_loss=0.13,\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.62it/s, train_loss=2.03, val_loss=0.13,\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.62it/s, train_loss=0.863, val_loss=0.11\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.62it/s, train_loss=0.863, val_loss=0.11\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.62it/s, train_loss=0.625, val_loss=0.20\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.62it/s, train_loss=0.625, val_loss=0.20\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.62it/s, train_loss=1.13, val_loss=0.094\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 33.68it/s, train_loss=1.13, val_loss=0.094\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.68it/s, train_loss=1.13, val_loss=0.094\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    18: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.68it/s, train_loss=0.645, val_loss=0.13\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.68it/s, train_loss=0.645, val_loss=0.13\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.68it/s, train_loss=0.809, val_loss=0.20\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.68it/s, train_loss=0.809, val_loss=0.20\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.68it/s, train_loss=1.11, val_loss=0.087\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.68it/s, train_loss=1.11, val_loss=0.087\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.68it/s, train_loss=0.502, val_loss=0.09\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 33.51it/s, train_loss=0.502, val_loss=0.09\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.51it/s, train_loss=0.502, val_loss=0.09\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.51it/s, train_loss=0.552, val_loss=0.13\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.51it/s, train_loss=0.552, val_loss=0.13\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.51it/s, train_loss=0.772, val_loss=0.08\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.51it/s, train_loss=0.772, val_loss=0.08\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.51it/s, train_loss=0.465, val_loss=0.11\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.51it/s, train_loss=0.465, val_loss=0.11\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.51it/s, train_loss=0.683, val_loss=0.11\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 33.33it/s, train_loss=0.683, val_loss=0.11\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 33.33it/s, train_loss=0.683, val_loss=0.11\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 33.33it/s, train_loss=0.664, val_loss=0.04\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 33.33it/s, train_loss=0.664, val_loss=0.04\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 33.33it/s, train_loss=0.319, val_loss=0.07\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 33.33it/s, train_loss=0.319, val_loss=0.07\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 33.33it/s, train_loss=0.497, val_loss=0.06\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 33.33it/s, train_loss=0.497, val_loss=0.06\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 33.33it/s, train_loss=0.443, val_loss=0.05\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 33.54it/s, train_loss=0.443, val_loss=0.05\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.54it/s, train_loss=0.443, val_loss=0.05\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.54it/s, train_loss=0.37, val_loss=0.096\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.54it/s, train_loss=0.37, val_loss=0.096\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.54it/s, train_loss=0.537, val_loss=0.05\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.54it/s, train_loss=0.537, val_loss=0.05\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.54it/s, train_loss=0.325, val_loss=0.04\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.54it/s, train_loss=0.325, val_loss=0.04\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.54it/s, train_loss=0.287, val_loss=0.05\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 33.57it/s, train_loss=0.287, val_loss=0.05\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 33.57it/s, train_loss=0.287, val_loss=0.05\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 33.57it/s, train_loss=0.335, val_loss=0.03\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 33.57it/s, train_loss=0.335, val_loss=0.03\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 33.57it/s, train_loss=0.217, val_loss=0.05\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 33.57it/s, train_loss=0.217, val_loss=0.05\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 33.57it/s, train_loss=0.326, val_loss=0.04\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 33.57it/s, train_loss=0.326, val_loss=0.04\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 33.57it/s, train_loss=0.265, val_loss=0.03\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 33.66it/s, train_loss=0.265, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 33.66it/s, train_loss=0.265, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 33.66it/s, train_loss=0.197, val_loss=0.04\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 33.66it/s, train_loss=0.197, val_loss=0.04\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 33.66it/s, train_loss=0.257, val_loss=0.03\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 33.66it/s, train_loss=0.257, val_loss=0.03\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 33.66it/s, train_loss=0.187, val_loss=0.04\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 33.66it/s, train_loss=0.187, val_loss=0.04\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 33.66it/s, train_loss=0.242, val_loss=0.03\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 33.24it/s, train_loss=0.242, val_loss=0.03\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 33.24it/s, train_loss=0.242, val_loss=0.03\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 33.24it/s, train_loss=0.229, val_loss=0.02\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 33.24it/s, train_loss=0.229, val_loss=0.02\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 33.46it/s, train_loss=0.171, val_loss=0.03\u001b[A\n",
      " 23%|█████████▋                                | 23/100 [00:36<01:58,  1.55s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=6.56, val_loss=52.7, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=6.56, val_loss=52.7, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=277, val_loss=1.41, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=277, val_loss=1.41, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=7.64, val_loss=2.46, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=7.64, val_loss=2.46, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=13, val_loss=0.117, train_cind\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 30.74it/s, train_loss=13, val_loss=0.117, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 30.74it/s, train_loss=13, val_loss=0.117, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 30.74it/s, train_loss=0.822, val_loss=0.925,\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 30.74it/s, train_loss=0.822, val_loss=0.925,\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 30.74it/s, train_loss=5.27, val_loss=2.22, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 30.74it/s, train_loss=5.27, val_loss=2.22, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 30.74it/s, train_loss=12.2, val_loss=0.222, \u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 30.74it/s, train_loss=12.2, val_loss=0.222, \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.31221039448966814\n",
      "Its' val AUC : 0.30297675683022973\n",
      "Its external  AUC: 0.38711202142744605\n",
      "Curent best Test AUC: 0.5085785848465874\n",
      "Its' val AUC : 0.3887454125322822\n",
      "Its external  AUC: 0.41058767921852846\n",
      "Curent best Test AUC: 0.7079524107701941\n",
      "Its' val AUC : 0.7040913415794482\n",
      "Its external  AUC: 0.6076886718134552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 30.74it/s, train_loss=1.22, val_loss=2.16, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 30.86it/s, train_loss=1.22, val_loss=2.16, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 30.86it/s, train_loss=1.22, val_loss=2.16, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 30.86it/s, train_loss=10.4, val_loss=0.539, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 30.86it/s, train_loss=10.4, val_loss=0.539, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 30.86it/s, train_loss=2.66, val_loss=0.517, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 30.86it/s, train_loss=2.66, val_loss=0.517,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 30.86it/s, train_loss=3.12, val_loss=0.649,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 30.86it/s, train_loss=3.12, val_loss=0.649,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 30.86it/s, train_loss=3.78, val_loss=0.31, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 31.92it/s, train_loss=3.78, val_loss=0.31,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 31.92it/s, train_loss=3.78, val_loss=0.31,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 31.92it/s, train_loss=1.89, val_loss=0.142\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 31.92it/s, train_loss=1.89, val_loss=0.142\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 31.92it/s, train_loss=0.891, val_loss=0.40\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 31.92it/s, train_loss=0.891, val_loss=0.40\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7143393863494051\n",
      "Its' val AUC : 0.7115672148973766\n",
      "Its external  AUC: 0.579013707263274\n",
      "Epoch    10: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 31.92it/s, train_loss=2.15, val_loss=0.457\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 31.92it/s, train_loss=2.15, val_loss=0.457\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 31.92it/s, train_loss=2.45, val_loss=0.199\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 32.39it/s, train_loss=2.45, val_loss=0.199\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.39it/s, train_loss=2.45, val_loss=0.199\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.39it/s, train_loss=1.17, val_loss=0.108\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.39it/s, train_loss=1.17, val_loss=0.108\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.39it/s, train_loss=0.711, val_loss=0.11\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.39it/s, train_loss=0.711, val_loss=0.11\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.39it/s, train_loss=0.719, val_loss=0.17\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.39it/s, train_loss=0.719, val_loss=0.17\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.39it/s, train_loss=1.02, val_loss=0.201\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 32.47it/s, train_loss=1.02, val_loss=0.201\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.47it/s, train_loss=1.02, val_loss=0.201\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.47it/s, train_loss=1.15, val_loss=0.161\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.47it/s, train_loss=1.15, val_loss=0.161\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.47it/s, train_loss=0.937, val_loss=0.09\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.47it/s, train_loss=0.937, val_loss=0.09\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    16: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.47it/s, train_loss=0.579, val_loss=0.05\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.47it/s, train_loss=0.579, val_loss=0.05\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.47it/s, train_loss=0.404, val_loss=0.07\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 32.81it/s, train_loss=0.404, val_loss=0.07\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 32.81it/s, train_loss=0.404, val_loss=0.07\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 32.81it/s, train_loss=0.495, val_loss=0.10\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 32.81it/s, train_loss=0.495, val_loss=0.10\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 32.81it/s, train_loss=0.619, val_loss=0.09\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 32.81it/s, train_loss=0.619, val_loss=0.09\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 32.81it/s, train_loss=0.557, val_loss=0.06\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 32.81it/s, train_loss=0.557, val_loss=0.06\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 32.81it/s, train_loss=0.385, val_loss=0.03\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 33.05it/s, train_loss=0.385, val_loss=0.03\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.05it/s, train_loss=0.385, val_loss=0.03\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.05it/s, train_loss=0.254, val_loss=0.03\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.05it/s, train_loss=0.254, val_loss=0.03\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.05it/s, train_loss=0.237, val_loss=0.04\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.05it/s, train_loss=0.237, val_loss=0.04\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.05it/s, train_loss=0.283, val_loss=0.04\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.05it/s, train_loss=0.283, val_loss=0.04\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.05it/s, train_loss=0.29, val_loss=0.038\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 33.19it/s, train_loss=0.29, val_loss=0.038\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 33.19it/s, train_loss=0.29, val_loss=0.038\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:01<00:00, 33.19it/s, train_loss=0.223, val_loss=0.02\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 33.19it/s, train_loss=0.223, val_loss=0.02\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 33.19it/s, train_loss=0.144, val_loss=0.02\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 33.19it/s, train_loss=0.144, val_loss=0.02\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 33.19it/s, train_loss=0.125, val_loss=0.02\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 33.19it/s, train_loss=0.125, val_loss=0.02\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 33.19it/s, train_loss=0.162, val_loss=0.03\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 33.19it/s, train_loss=0.162, val_loss=0.03\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.19it/s, train_loss=0.162, val_loss=0.03\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.19it/s, train_loss=0.188, val_loss=0.02\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.19it/s, train_loss=0.188, val_loss=0.02\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.19it/s, train_loss=0.161, val_loss=0.01\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.19it/s, train_loss=0.161, val_loss=0.01\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.19it/s, train_loss=0.111, val_loss=0.01\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.19it/s, train_loss=0.111, val_loss=0.01\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.19it/s, train_loss=0.0924, val_loss=0.0\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 32.06it/s, train_loss=0.0924, val_loss=0.0\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 32.06it/s, train_loss=0.0924, val_loss=0.0\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 32.06it/s, train_loss=0.111, val_loss=0.02\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 32.06it/s, train_loss=0.111, val_loss=0.02\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 32.06it/s, train_loss=0.128, val_loss=0.02\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 32.06it/s, train_loss=0.128, val_loss=0.02\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 32.06it/s, train_loss=0.114, val_loss=0.01\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 32.06it/s, train_loss=0.114, val_loss=0.01\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 32.06it/s, train_loss=0.0852, val_loss=0.0\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 31.11it/s, train_loss=0.0852, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 31.11it/s, train_loss=0.0852, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 31.11it/s, train_loss=0.0766, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 31.11it/s, train_loss=0.0766, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 31.11it/s, train_loss=0.0918, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 31.11it/s, train_loss=0.0918, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 31.11it/s, train_loss=0.104, val_loss=0.01\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 31.11it/s, train_loss=0.104, val_loss=0.01\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 31.11it/s, train_loss=0.0932, val_loss=0.0\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 30.83it/s, train_loss=0.0932, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 30.83it/s, train_loss=0.0932, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 30.83it/s, train_loss=0.0746, val_loss=0.0\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 30.83it/s, train_loss=0.0746, val_loss=0.0\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 31.81it/s, train_loss=0.0703, val_loss=0.0\u001b[A\n",
      " 24%|██████████                                | 24/100 [00:37<01:58,  1.56s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=67.4, val_loss=3.29, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=67.4, val_loss=3.29, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=17.1, val_loss=3, train_cindex\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=17.1, val_loss=3, train_cindex\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=15.1, val_loss=15.9, train_cin\u001b[A\n",
      "Epoch 2:   6%| | 3/50 [00:00<00:01, 28.06it/s, train_loss=15.1, val_loss=15.9, t\u001b[A\n",
      "Epoch 3:   6%| | 3/50 [00:00<00:01, 28.06it/s, train_loss=15.1, val_loss=15.9, t\u001b[A\n",
      "Epoch 3:   6%| | 3/50 [00:00<00:01, 28.06it/s, train_loss=83.2, val_loss=3.47, t\u001b[A\n",
      "Epoch 4:   6%| | 3/50 [00:00<00:01, 28.06it/s, train_loss=83.2, val_loss=3.47, t\u001b[A\n",
      "Epoch 4:   6%| | 3/50 [00:00<00:01, 28.06it/s, train_loss=18.7, val_loss=7.32, t\u001b[A\n",
      "Epoch 5:   6%| | 3/50 [00:00<00:01, 28.06it/s, train_loss=18.7, val_loss=7.32, t\u001b[A\n",
      "Epoch 5:   6%| | 3/50 [00:00<00:01, 28.06it/s, train_loss=39.8, val_loss=6.06, t\u001b[A\n",
      "Epoch 6:   6%| | 3/50 [00:00<00:01, 28.06it/s, train_loss=39.8, val_loss=6.06, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.2462116468378209\n",
      "Its' val AUC : 0.2984912328394726\n",
      "Its external  AUC: 0.4000315109500551\n",
      "Curent best Test AUC: 0.7564182842830307\n",
      "Its' val AUC : 0.7187712382764714\n",
      "Its external  AUC: 0.6351031983614306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6:   6%| | 3/50 [00:00<00:01, 28.06it/s, train_loss=29.9, val_loss=4.55, t\u001b[A\n",
      "Epoch 6:  14%|▏| 7/50 [00:00<00:01, 30.33it/s, train_loss=29.9, val_loss=4.55, t\u001b[A\n",
      "Epoch 7:  14%|▏| 7/50 [00:00<00:01, 30.33it/s, train_loss=29.9, val_loss=4.55, t\u001b[A\n",
      "Epoch 7:  14%|▏| 7/50 [00:00<00:01, 30.33it/s, train_loss=22.4, val_loss=1.63, t\u001b[A\n",
      "Epoch 8:  14%|▏| 7/50 [00:00<00:01, 30.33it/s, train_loss=22.4, val_loss=1.63, t\u001b[A\n",
      "Epoch 8:  14%|▏| 7/50 [00:00<00:01, 30.33it/s, train_loss=10.1, val_loss=3.75, t\u001b[A\n",
      "Epoch 9:  14%|▏| 7/50 [00:00<00:01, 30.33it/s, train_loss=10.1, val_loss=3.75, t\u001b[A\n",
      "Epoch 9:  14%|▏| 7/50 [00:00<00:01, 30.33it/s, train_loss=20, val_loss=1.02, tra\u001b[A\n",
      "Epoch 10:  14%|▏| 7/50 [00:00<00:01, 30.33it/s, train_loss=20, val_loss=1.02, tr\u001b[A\n",
      "Epoch 10:  14%|▏| 7/50 [00:00<00:01, 30.33it/s, train_loss=6.69, val_loss=2.71, \u001b[A\n",
      "Epoch 10:  22%|▏| 11/50 [00:00<00:01, 31.96it/s, train_loss=6.69, val_loss=2.71,\u001b[A\n",
      "Epoch 11:  22%|▏| 11/50 [00:00<00:01, 31.96it/s, train_loss=6.69, val_loss=2.71,\u001b[A\n",
      "Epoch 11:  22%|▏| 11/50 [00:00<00:01, 31.96it/s, train_loss=14.8, val_loss=0.168\u001b[A\n",
      "Epoch 12:  22%|▏| 11/50 [00:00<00:01, 31.96it/s, train_loss=14.8, val_loss=0.168\u001b[A\n",
      "Epoch 12:  22%|▏| 11/50 [00:00<00:01, 31.96it/s, train_loss=0.989, val_loss=2.05\u001b[A\n",
      "Epoch 13:  22%|▏| 11/50 [00:00<00:01, 31.96it/s, train_loss=0.989, val_loss=2.05\u001b[A\n",
      "Epoch 13:  22%|▏| 11/50 [00:00<00:01, 31.96it/s, train_loss=10.3, val_loss=0.39,\u001b[A\n",
      "Epoch 14:  22%|▏| 11/50 [00:00<00:01, 31.96it/s, train_loss=10.3, val_loss=0.39,\u001b[A\n",
      "Epoch 14:  22%|▏| 11/50 [00:00<00:01, 31.96it/s, train_loss=1.97, val_loss=0.976\u001b[A\n",
      "Epoch 14:  30%|▎| 15/50 [00:00<00:01, 33.01it/s, train_loss=1.97, val_loss=0.976\u001b[A\n",
      "Epoch 15:  30%|▎| 15/50 [00:00<00:01, 33.01it/s, train_loss=1.97, val_loss=0.976\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7704445835942392\n",
      "Its' val AUC : 0.6824792714421639\n",
      "Its external  AUC: 0.615723964077517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15:  30%|▎| 15/50 [00:00<00:01, 33.01it/s, train_loss=5.03, val_loss=0.127\u001b[A\n",
      "Epoch 16:  30%|▎| 15/50 [00:00<00:01, 33.01it/s, train_loss=5.03, val_loss=0.127\u001b[A\n",
      "Epoch 16:  30%|▎| 15/50 [00:00<00:01, 33.01it/s, train_loss=0.672, val_loss=0.80\u001b[A\n",
      "Epoch 17:  30%|▎| 15/50 [00:00<00:01, 33.01it/s, train_loss=0.672, val_loss=0.80\u001b[A\n",
      "Epoch 17:  30%|▎| 15/50 [00:00<00:01, 33.01it/s, train_loss=3.87, val_loss=0.092\u001b[A\n",
      "Epoch 18:  30%|▎| 15/50 [00:00<00:01, 33.01it/s, train_loss=3.87, val_loss=0.092\u001b[A\n",
      "Epoch 18:  30%|▎| 15/50 [00:00<00:01, 33.01it/s, train_loss=0.606, val_loss=0.55\u001b[A\n",
      "Epoch 18:  38%|▍| 19/50 [00:00<00:00, 33.57it/s, train_loss=0.606, val_loss=0.55\u001b[A\n",
      "Epoch 19:  38%|▍| 19/50 [00:00<00:00, 33.57it/s, train_loss=0.606, val_loss=0.55\u001b[A\n",
      "Epoch 19:  38%|▍| 19/50 [00:00<00:00, 33.57it/s, train_loss=3.09, val_loss=0.098\u001b[A\n",
      "Epoch 20:  38%|▍| 19/50 [00:00<00:00, 33.57it/s, train_loss=3.09, val_loss=0.098\u001b[A\n",
      "Epoch 20:  38%|▍| 19/50 [00:00<00:00, 33.57it/s, train_loss=0.689, val_loss=0.53\u001b[A\n",
      "Epoch 21:  38%|▍| 19/50 [00:00<00:00, 33.57it/s, train_loss=0.689, val_loss=0.53\u001b[A\n",
      "Epoch 21:  38%|▍| 19/50 [00:00<00:00, 33.57it/s, train_loss=2.82, val_loss=0.088\u001b[A\n",
      "Epoch 22:  38%|▍| 19/50 [00:00<00:00, 33.57it/s, train_loss=2.82, val_loss=0.088\u001b[A\n",
      "Epoch 22:  38%|▍| 19/50 [00:00<00:00, 33.57it/s, train_loss=0.574, val_loss=0.34\u001b[A\n",
      "Epoch 22:  46%|▍| 23/50 [00:00<00:00, 34.02it/s, train_loss=0.574, val_loss=0.34\u001b[A\n",
      "Epoch 23:  46%|▍| 23/50 [00:00<00:00, 34.02it/s, train_loss=0.574, val_loss=0.34\u001b[A\n",
      "Epoch 23:  46%|▍| 23/50 [00:00<00:00, 34.02it/s, train_loss=1.89, val_loss=0.094\u001b[A\n",
      "Epoch 24:  46%|▍| 23/50 [00:00<00:00, 34.02it/s, train_loss=1.89, val_loss=0.094\u001b[A\n",
      "Epoch 24:  46%|▍| 23/50 [00:00<00:00, 34.02it/s, train_loss=0.625, val_loss=0.30\u001b[A\n",
      "Epoch 25:  46%|▍| 23/50 [00:00<00:00, 34.02it/s, train_loss=0.625, val_loss=0.30\u001b[A\n",
      "Epoch 25:  46%|▍| 23/50 [00:00<00:00, 34.02it/s, train_loss=1.78, val_loss=0.113\u001b[A\n",
      "Epoch 26:  46%|▍| 23/50 [00:00<00:00, 34.02it/s, train_loss=1.78, val_loss=0.113\u001b[A\n",
      "Epoch 26:  46%|▍| 23/50 [00:00<00:00, 34.02it/s, train_loss=0.655, val_loss=0.23\u001b[A\n",
      "Epoch 26:  54%|▌| 27/50 [00:00<00:00, 34.26it/s, train_loss=0.655, val_loss=0.23\u001b[A\n",
      "Epoch 27:  54%|▌| 27/50 [00:00<00:00, 34.26it/s, train_loss=0.655, val_loss=0.23\u001b[A\n",
      "Epoch 27:  54%|▌| 27/50 [00:00<00:00, 34.26it/s, train_loss=1.25, val_loss=0.106\u001b[A\n",
      "Epoch 28:  54%|▌| 27/50 [00:00<00:00, 34.26it/s, train_loss=1.25, val_loss=0.106\u001b[A\n",
      "Epoch 28:  54%|▌| 27/50 [00:00<00:00, 34.26it/s, train_loss=0.627, val_loss=0.11\u001b[A\n",
      "Epoch 29:  54%|▌| 27/50 [00:00<00:00, 34.26it/s, train_loss=0.627, val_loss=0.11\u001b[A\n",
      "Epoch 29:  54%|▌| 27/50 [00:00<00:00, 34.26it/s, train_loss=0.693, val_loss=0.05\u001b[A\n",
      "Epoch 30:  54%|▌| 27/50 [00:00<00:00, 34.26it/s, train_loss=0.693, val_loss=0.05\u001b[A\n",
      "Epoch 30:  54%|▌| 27/50 [00:00<00:00, 34.26it/s, train_loss=0.32, val_loss=0.11,\u001b[A\n",
      "Epoch 30:  62%|▌| 31/50 [00:00<00:00, 34.38it/s, train_loss=0.32, val_loss=0.11,\u001b[A\n",
      "Epoch 31:  62%|▌| 31/50 [00:00<00:00, 34.38it/s, train_loss=0.32, val_loss=0.11,\u001b[A\n",
      "Epoch 31:  62%|▌| 31/50 [00:00<00:00, 34.38it/s, train_loss=0.566, val_loss=0.08\u001b[A\n",
      "Epoch 32:  62%|▌| 31/50 [00:00<00:00, 34.38it/s, train_loss=0.566, val_loss=0.08\u001b[A\n",
      "Epoch 32:  62%|▌| 31/50 [00:00<00:00, 34.38it/s, train_loss=0.423, val_loss=0.04\u001b[A\n",
      "Epoch 33:  62%|▌| 31/50 [00:00<00:00, 34.38it/s, train_loss=0.423, val_loss=0.04\u001b[A\n",
      "Epoch 33:  62%|▌| 31/50 [00:01<00:00, 34.38it/s, train_loss=0.261, val_loss=0.07\u001b[A\n",
      "Epoch 34:  62%|▌| 31/50 [00:01<00:00, 34.38it/s, train_loss=0.261, val_loss=0.07\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    28: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34:  62%|▌| 31/50 [00:01<00:00, 34.38it/s, train_loss=0.461, val_loss=0.03\u001b[A\n",
      "Epoch 34:  70%|▋| 35/50 [00:01<00:00, 34.48it/s, train_loss=0.461, val_loss=0.03\u001b[A\n",
      "Epoch 35:  70%|▋| 35/50 [00:01<00:00, 34.48it/s, train_loss=0.461, val_loss=0.03\u001b[A\n",
      "Epoch 35:  70%|▋| 35/50 [00:01<00:00, 34.48it/s, train_loss=0.219, val_loss=0.05\u001b[A\n",
      "Epoch 36:  70%|▋| 35/50 [00:01<00:00, 34.48it/s, train_loss=0.219, val_loss=0.05\u001b[A\n",
      "Epoch 36:  70%|▋| 35/50 [00:01<00:00, 34.48it/s, train_loss=0.257, val_loss=0.06\u001b[A\n",
      "Epoch 37:  70%|▋| 35/50 [00:01<00:00, 34.48it/s, train_loss=0.257, val_loss=0.06\u001b[A\n",
      "Epoch 37:  70%|▋| 35/50 [00:01<00:00, 34.48it/s, train_loss=0.296, val_loss=0.02\u001b[A\n",
      "Epoch 38:  70%|▋| 35/50 [00:01<00:00, 34.48it/s, train_loss=0.296, val_loss=0.02\u001b[A\n",
      "Epoch 38:  70%|▋| 35/50 [00:01<00:00, 34.48it/s, train_loss=0.13, val_loss=0.041\u001b[A\n",
      "Epoch 38:  78%|▊| 39/50 [00:01<00:00, 34.52it/s, train_loss=0.13, val_loss=0.041\u001b[A\n",
      "Epoch 39:  78%|▊| 39/50 [00:01<00:00, 34.52it/s, train_loss=0.13, val_loss=0.041\u001b[A\n",
      "Epoch 39:  78%|▊| 39/50 [00:01<00:00, 34.52it/s, train_loss=0.255, val_loss=0.02\u001b[A\n",
      "Epoch 40:  78%|▊| 39/50 [00:01<00:00, 34.52it/s, train_loss=0.255, val_loss=0.02\u001b[A\n",
      "Epoch 40:  78%|▊| 39/50 [00:01<00:00, 34.52it/s, train_loss=0.182, val_loss=0.02\u001b[A\n",
      "Epoch 41:  78%|▊| 39/50 [00:01<00:00, 34.52it/s, train_loss=0.182, val_loss=0.02\u001b[A\n",
      "Epoch 41:  78%|▊| 39/50 [00:01<00:00, 34.52it/s, train_loss=0.14, val_loss=0.047\u001b[A\n",
      "Epoch 42:  78%|▊| 39/50 [00:01<00:00, 34.52it/s, train_loss=0.14, val_loss=0.047\u001b[A\n",
      "Epoch 42:  78%|▊| 39/50 [00:01<00:00, 34.52it/s, train_loss=0.223, val_loss=0.02\u001b[A\n",
      "Epoch 42:  86%|▊| 43/50 [00:01<00:00, 34.59it/s, train_loss=0.223, val_loss=0.02\u001b[A\n",
      "Epoch 43:  86%|▊| 43/50 [00:01<00:00, 34.59it/s, train_loss=0.223, val_loss=0.02\u001b[A\n",
      "Epoch 43:  86%|▊| 43/50 [00:01<00:00, 34.59it/s, train_loss=0.119, val_loss=0.02\u001b[A\n",
      "Epoch 44:  86%|▊| 43/50 [00:01<00:00, 34.59it/s, train_loss=0.119, val_loss=0.02\u001b[A\n",
      "Epoch 44:  86%|▊| 43/50 [00:01<00:00, 34.59it/s, train_loss=0.177, val_loss=0.02\u001b[A\n",
      "Epoch 45:  86%|▊| 43/50 [00:01<00:00, 34.59it/s, train_loss=0.177, val_loss=0.02\u001b[A\n",
      "Epoch 45:  86%|▊| 43/50 [00:01<00:00, 34.59it/s, train_loss=0.171, val_loss=0.02\u001b[A\n",
      "Epoch 46:  86%|▊| 43/50 [00:01<00:00, 34.59it/s, train_loss=0.171, val_loss=0.02\u001b[A\n",
      "Epoch 46:  86%|▊| 43/50 [00:01<00:00, 34.59it/s, train_loss=0.118, val_loss=0.02\u001b[A\n",
      "Epoch 46:  94%|▉| 47/50 [00:01<00:00, 34.61it/s, train_loss=0.118, val_loss=0.02\u001b[A\n",
      "Epoch 47:  94%|▉| 47/50 [00:01<00:00, 34.61it/s, train_loss=0.118, val_loss=0.02\u001b[A\n",
      "Epoch 47:  94%|▉| 47/50 [00:01<00:00, 34.61it/s, train_loss=0.125, val_loss=0.03\u001b[A\n",
      "Epoch 48:  94%|▉| 47/50 [00:01<00:00, 34.61it/s, train_loss=0.125, val_loss=0.03\u001b[A\n",
      "Epoch 48:  94%|▉| 47/50 [00:01<00:00, 34.61it/s, train_loss=0.157, val_loss=0.02\u001b[A\n",
      "Epoch 49:  94%|▉| 47/50 [00:01<00:00, 34.61it/s, train_loss=0.157, val_loss=0.02\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 33.89it/s, train_loss=0.132, val_loss=0.01\u001b[A\n",
      " 25%|██████████▌                               | 25/100 [00:39<01:55,  1.54s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    44: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=14.8, val_loss=50.4, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=14.8, val_loss=50.4, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=260, val_loss=27.4, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=260, val_loss=27.4, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=168, val_loss=2.63, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=168, val_loss=2.63, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=17, val_loss=8.07, train_cinde\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 32.82it/s, train_loss=17, val_loss=8.07, tra\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.82it/s, train_loss=17, val_loss=8.07, tra\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.82it/s, train_loss=41.2, val_loss=5.91, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.82it/s, train_loss=41.2, val_loss=5.91, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.82it/s, train_loss=30.8, val_loss=6, trai\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.82it/s, train_loss=30.8, val_loss=6, trai\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.82it/s, train_loss=27.6, val_loss=0.724, \u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.82it/s, train_loss=27.6, val_loss=0.724, \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.2395742016280526\n",
      "Its' val AUC : 0.26804403969009105\n",
      "Its external  AUC: 0.3904206711832362\n",
      "Curent best Test AUC: 0.6341891045710708\n",
      "Its' val AUC : 0.6611390512437134\n",
      "Its external  AUC: 0.6130455333228297\n",
      "Curent best Test AUC: 0.7481527864746399\n",
      "Its' val AUC : 0.6957999184450183\n",
      "Its external  AUC: 0.630376555853159\n",
      "Curent best Test AUC: 0.7487789605510332\n",
      "Its' val AUC : 0.7011009922522767\n",
      "Its external  AUC: 0.5777532692610682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.82it/s, train_loss=4.03, val_loss=1.16, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 33.27it/s, train_loss=4.03, val_loss=1.16, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.27it/s, train_loss=4.03, val_loss=1.16, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.27it/s, train_loss=7.59, val_loss=1.33, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.27it/s, train_loss=7.59, val_loss=1.33, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.27it/s, train_loss=8.9, val_loss=1.72, tr\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.27it/s, train_loss=8.9, val_loss=1.72, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.27it/s, train_loss=11.3, val_loss=1.29, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.27it/s, train_loss=11.3, val_loss=1.29, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.27it/s, train_loss=8.22, val_loss=0.55, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.02it/s, train_loss=8.22, val_loss=0.55,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.02it/s, train_loss=8.22, val_loss=0.55,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.02it/s, train_loss=3.34, val_loss=0.784\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.02it/s, train_loss=3.34, val_loss=0.784\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.02it/s, train_loss=4.65, val_loss=0.925\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.02it/s, train_loss=4.65, val_loss=0.925\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.02it/s, train_loss=5.32, val_loss=0.557\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.02it/s, train_loss=5.32, val_loss=0.557\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.02it/s, train_loss=3.09, val_loss=0.373\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.31it/s, train_loss=3.09, val_loss=0.373\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.31it/s, train_loss=3.09, val_loss=0.373\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.31it/s, train_loss=2.15, val_loss=0.344\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.31it/s, train_loss=2.15, val_loss=0.344\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.31it/s, train_loss=1.91, val_loss=0.351\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.31it/s, train_loss=1.91, val_loss=0.351\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.31it/s, train_loss=1.93, val_loss=0.278\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.31it/s, train_loss=1.93, val_loss=0.278\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.31it/s, train_loss=1.73, val_loss=0.124\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.51it/s, train_loss=1.73, val_loss=0.124\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.51it/s, train_loss=1.73, val_loss=0.124\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.51it/s, train_loss=0.698, val_loss=0.13\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.51it/s, train_loss=0.698, val_loss=0.13\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.51it/s, train_loss=0.681, val_loss=0.11\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.51it/s, train_loss=0.681, val_loss=0.11\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.51it/s, train_loss=0.745, val_loss=0.17\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.51it/s, train_loss=0.745, val_loss=0.17\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.51it/s, train_loss=1.1, val_loss=0.166,\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.55it/s, train_loss=1.1, val_loss=0.166,\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.55it/s, train_loss=1.1, val_loss=0.166,\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.55it/s, train_loss=1.06, val_loss=0.112\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.55it/s, train_loss=1.06, val_loss=0.112\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.55it/s, train_loss=0.817, val_loss=0.17\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.55it/s, train_loss=0.817, val_loss=0.17\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.55it/s, train_loss=1.3, val_loss=0.189,\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.55it/s, train_loss=1.3, val_loss=0.189,\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.55it/s, train_loss=1.2, val_loss=0.124,\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.62it/s, train_loss=1.2, val_loss=0.124,\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.62it/s, train_loss=1.2, val_loss=0.124,\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.62it/s, train_loss=0.817, val_loss=0.10\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.62it/s, train_loss=0.817, val_loss=0.10\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.62it/s, train_loss=0.723, val_loss=0.09\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.62it/s, train_loss=0.723, val_loss=0.09\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.62it/s, train_loss=0.665, val_loss=0.08\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.62it/s, train_loss=0.665, val_loss=0.08\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.62it/s, train_loss=0.647, val_loss=0.07\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.64it/s, train_loss=0.647, val_loss=0.07\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.64it/s, train_loss=0.647, val_loss=0.07\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.64it/s, train_loss=0.423, val_loss=0.04\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.64it/s, train_loss=0.423, val_loss=0.04\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.64it/s, train_loss=0.27, val_loss=0.082\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.64it/s, train_loss=0.27, val_loss=0.082\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.64it/s, train_loss=0.426, val_loss=0.07\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.64it/s, train_loss=0.426, val_loss=0.07\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.64it/s, train_loss=0.41, val_loss=0.046\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.69it/s, train_loss=0.41, val_loss=0.046\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.69it/s, train_loss=0.41, val_loss=0.046\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.69it/s, train_loss=0.246, val_loss=0.05\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.69it/s, train_loss=0.246, val_loss=0.05\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.69it/s, train_loss=0.325, val_loss=0.08\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.69it/s, train_loss=0.325, val_loss=0.08\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.69it/s, train_loss=0.446, val_loss=0.06\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.69it/s, train_loss=0.446, val_loss=0.06\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.69it/s, train_loss=0.344, val_loss=0.05\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.60it/s, train_loss=0.344, val_loss=0.05\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.60it/s, train_loss=0.344, val_loss=0.05\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.60it/s, train_loss=0.26, val_loss=0.057\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.60it/s, train_loss=0.26, val_loss=0.057\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.60it/s, train_loss=0.308, val_loss=0.05\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.60it/s, train_loss=0.308, val_loss=0.05\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.60it/s, train_loss=0.285, val_loss=0.04\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.60it/s, train_loss=0.285, val_loss=0.04\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.60it/s, train_loss=0.219, val_loss=0.03\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.60it/s, train_loss=0.219, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.60it/s, train_loss=0.219, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.60it/s, train_loss=0.17, val_loss=0.032\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.60it/s, train_loss=0.17, val_loss=0.032\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.60it/s, train_loss=0.15, val_loss=0.032\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.60it/s, train_loss=0.15, val_loss=0.032\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.60it/s, train_loss=0.156, val_loss=0.03\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.60it/s, train_loss=0.156, val_loss=0.03\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.60it/s, train_loss=0.172, val_loss=0.02\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.72it/s, train_loss=0.172, val_loss=0.02\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.72it/s, train_loss=0.172, val_loss=0.02\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    42: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.72it/s, train_loss=0.135, val_loss=0.02\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.72it/s, train_loss=0.135, val_loss=0.02\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.47it/s, train_loss=0.118, val_loss=0.02\u001b[A\n",
      " 26%|██████████▉                               | 26/100 [00:40<01:52,  1.52s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=38.8, val_loss=102, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=38.8, val_loss=102, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=551, val_loss=20.3, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=551, val_loss=20.3, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=108, val_loss=8.21, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=108, val_loss=8.21, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=41.1, val_loss=7.16, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 32.74it/s, train_loss=41.1, val_loss=7.16, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.74it/s, train_loss=41.1, val_loss=7.16, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.74it/s, train_loss=35.2, val_loss=0.6, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.74it/s, train_loss=35.2, val_loss=0.6, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.74it/s, train_loss=3.78, val_loss=0.618, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.74it/s, train_loss=3.78, val_loss=0.618, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.74it/s, train_loss=2.95, val_loss=2.45, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.74it/s, train_loss=2.95, val_loss=2.45, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.28002504696305575\n",
      "Its' val AUC : 0.2921027592768792\n",
      "Its external  AUC: 0.3904206711832362\n",
      "Curent best Test AUC: 0.30745147150907953\n",
      "Its' val AUC : 0.3142585292918309\n",
      "Its external  AUC: 0.40223727745391524\n",
      "Curent best Test AUC: 0.7343769567939887\n",
      "Its' val AUC : 0.7035476417017806\n",
      "Its external  AUC: 0.6286434536001261\n",
      "Curent best Test AUC: 0.7381340012523482\n",
      "Its' val AUC : 0.7153731140410493\n",
      "Its external  AUC: 0.624704584843233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.74it/s, train_loss=11.7, val_loss=1.62, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 33.73it/s, train_loss=11.7, val_loss=1.62, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.73it/s, train_loss=11.7, val_loss=1.62, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.73it/s, train_loss=7.78, val_loss=0.191, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.73it/s, train_loss=7.78, val_loss=0.191, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.73it/s, train_loss=1.05, val_loss=0.812, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.73it/s, train_loss=1.05, val_loss=0.812,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.73it/s, train_loss=4.58, val_loss=0.803,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.73it/s, train_loss=4.58, val_loss=0.803,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.73it/s, train_loss=4.36, val_loss=0.0942\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.18it/s, train_loss=4.36, val_loss=0.094\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.18it/s, train_loss=4.36, val_loss=0.094\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.18it/s, train_loss=0.594, val_loss=0.38\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.18it/s, train_loss=0.594, val_loss=0.38\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.18it/s, train_loss=2.25, val_loss=0.342\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.18it/s, train_loss=2.25, val_loss=0.342\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.18it/s, train_loss=2.07, val_loss=0.147\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.18it/s, train_loss=2.07, val_loss=0.147\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.18it/s, train_loss=0.664, val_loss=0.54\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.36it/s, train_loss=0.664, val_loss=0.54\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.36it/s, train_loss=0.664, val_loss=0.54\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.36it/s, train_loss=2.27, val_loss=0.098\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.36it/s, train_loss=2.27, val_loss=0.098\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.36it/s, train_loss=0.391, val_loss=0.26\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.36it/s, train_loss=0.391, val_loss=0.26\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.36it/s, train_loss=1.74, val_loss=0.182\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.36it/s, train_loss=1.74, val_loss=0.182\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.36it/s, train_loss=1.17, val_loss=0.108\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.46it/s, train_loss=1.17, val_loss=0.108\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.46it/s, train_loss=1.17, val_loss=0.108\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.46it/s, train_loss=0.392, val_loss=0.29\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.46it/s, train_loss=0.392, val_loss=0.29\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.46it/s, train_loss=0.946, val_loss=0.35\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.46it/s, train_loss=0.946, val_loss=0.35\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.46it/s, train_loss=1.24, val_loss=0.17,\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.46it/s, train_loss=1.24, val_loss=0.17,\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.46it/s, train_loss=0.531, val_loss=0.08\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.51it/s, train_loss=0.531, val_loss=0.08\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.51it/s, train_loss=0.531, val_loss=0.08\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    18: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.51it/s, train_loss=0.419, val_loss=0.14\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.51it/s, train_loss=0.419, val_loss=0.14\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.51it/s, train_loss=0.882, val_loss=0.11\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.51it/s, train_loss=0.882, val_loss=0.11\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.51it/s, train_loss=0.69, val_loss=0.058\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.51it/s, train_loss=0.69, val_loss=0.058\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.51it/s, train_loss=0.251, val_loss=0.12\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.50it/s, train_loss=0.251, val_loss=0.12\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.50it/s, train_loss=0.251, val_loss=0.12\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.50it/s, train_loss=0.451, val_loss=0.15\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.50it/s, train_loss=0.451, val_loss=0.15\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.50it/s, train_loss=0.623, val_loss=0.07\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.50it/s, train_loss=0.623, val_loss=0.07\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.50it/s, train_loss=0.285, val_loss=0.04\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.50it/s, train_loss=0.285, val_loss=0.04\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.50it/s, train_loss=0.219, val_loss=0.07\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.47it/s, train_loss=0.219, val_loss=0.07\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.47it/s, train_loss=0.219, val_loss=0.07\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.47it/s, train_loss=0.454, val_loss=0.06\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.47it/s, train_loss=0.454, val_loss=0.06\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.47it/s, train_loss=0.346, val_loss=0.03\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.47it/s, train_loss=0.346, val_loss=0.03\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.47it/s, train_loss=0.159, val_loss=0.06\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.47it/s, train_loss=0.159, val_loss=0.06\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.47it/s, train_loss=0.297, val_loss=0.07\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.51it/s, train_loss=0.297, val_loss=0.07\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.51it/s, train_loss=0.297, val_loss=0.07\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.51it/s, train_loss=0.326, val_loss=0.03\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.51it/s, train_loss=0.326, val_loss=0.03\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.51it/s, train_loss=0.149, val_loss=0.03\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.51it/s, train_loss=0.149, val_loss=0.03\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.51it/s, train_loss=0.189, val_loss=0.04\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.51it/s, train_loss=0.189, val_loss=0.04\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.51it/s, train_loss=0.286, val_loss=0.02\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.31it/s, train_loss=0.286, val_loss=0.02\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.31it/s, train_loss=0.286, val_loss=0.02\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.31it/s, train_loss=0.175, val_loss=0.02\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.31it/s, train_loss=0.175, val_loss=0.02\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.31it/s, train_loss=0.134, val_loss=0.04\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.31it/s, train_loss=0.134, val_loss=0.04\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.31it/s, train_loss=0.218, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.31it/s, train_loss=0.218, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.31it/s, train_loss=0.163, val_loss=0.02\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.18it/s, train_loss=0.163, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.18it/s, train_loss=0.163, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.18it/s, train_loss=0.105, val_loss=0.02\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.18it/s, train_loss=0.105, val_loss=0.02\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.18it/s, train_loss=0.173, val_loss=0.02\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.18it/s, train_loss=0.173, val_loss=0.02\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.18it/s, train_loss=0.168, val_loss=0.02\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.18it/s, train_loss=0.168, val_loss=0.02\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.18it/s, train_loss=0.109, val_loss=0.03\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.33it/s, train_loss=0.109, val_loss=0.03\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.33it/s, train_loss=0.109, val_loss=0.03\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.33it/s, train_loss=0.145, val_loss=0.03\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.33it/s, train_loss=0.145, val_loss=0.03\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.30it/s, train_loss=0.151, val_loss=0.02\u001b[A\n",
      " 27%|███████████▎                              | 27/100 [00:42<01:49,  1.50s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=18, val_loss=30.5, train_cinde\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=18, val_loss=30.5, train_cinde\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=163, val_loss=64.9, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=163, val_loss=64.9, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=318, val_loss=0.575, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=318, val_loss=0.575, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=3.9, val_loss=22, train_cindex\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 32.99it/s, train_loss=3.9, val_loss=22, trai\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.99it/s, train_loss=3.9, val_loss=22, trai\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.99it/s, train_loss=112, val_loss=16, trai\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.99it/s, train_loss=112, val_loss=16, trai\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    50: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Curent best Test AUC: 0.2419536631183469\n",
      "Its' val AUC : 0.28095691178469484\n",
      "Its external  AUC: 0.39231132818654485\n",
      "Curent best Test AUC: 0.7185973700688791\n",
      "Its' val AUC : 0.7040913415794482\n",
      "Its external  AUC: 0.6065857885615251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.99it/s, train_loss=79.7, val_loss=0.964, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.99it/s, train_loss=79.7, val_loss=0.964, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.99it/s, train_loss=5.93, val_loss=13.9, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.99it/s, train_loss=5.93, val_loss=13.9, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.99it/s, train_loss=74.7, val_loss=2.89, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 33.57it/s, train_loss=74.7, val_loss=2.89, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.57it/s, train_loss=74.7, val_loss=2.89, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.57it/s, train_loss=15.9, val_loss=3.4, tr\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.57it/s, train_loss=15.9, val_loss=3.4, tr\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.57it/s, train_loss=15.9, val_loss=4.98, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.57it/s, train_loss=15.9, val_loss=4.98, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.57it/s, train_loss=23.8, val_loss=1.97, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.57it/s, train_loss=23.8, val_loss=1.97, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.57it/s, train_loss=9.09, val_loss=0.179,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.14it/s, train_loss=9.09, val_loss=0.179\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.14it/s, train_loss=9.09, val_loss=0.179\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.756668753913588\n",
      "Its' val AUC : 0.6448280549136877\n",
      "Its external  AUC: 0.6387269576177722\n",
      "Epoch     9: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.14it/s, train_loss=0.785, val_loss=1.02\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.14it/s, train_loss=0.785, val_loss=1.02\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.14it/s, train_loss=5.59, val_loss=2.07,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.14it/s, train_loss=5.59, val_loss=2.07,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.14it/s, train_loss=11, val_loss=0.959, \u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.14it/s, train_loss=11, val_loss=0.959, \u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.14it/s, train_loss=5, val_loss=0.121, t\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.32it/s, train_loss=5, val_loss=0.121, t\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.32it/s, train_loss=5, val_loss=0.121, t\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.32it/s, train_loss=0.728, val_loss=0.68\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.32it/s, train_loss=0.728, val_loss=0.68\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.32it/s, train_loss=3.8, val_loss=1.21, \u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.32it/s, train_loss=3.8, val_loss=1.21, \u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.32it/s, train_loss=6.51, val_loss=0.705\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.32it/s, train_loss=6.51, val_loss=0.705\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.32it/s, train_loss=3.8, val_loss=0.0858\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.48it/s, train_loss=3.8, val_loss=0.0858\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.48it/s, train_loss=3.8, val_loss=0.0858\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.48it/s, train_loss=0.504, val_loss=0.36\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.48it/s, train_loss=0.504, val_loss=0.36\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.48it/s, train_loss=1.93, val_loss=0.814\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.48it/s, train_loss=1.93, val_loss=0.814\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.48it/s, train_loss=4.36, val_loss=0.484\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.48it/s, train_loss=4.36, val_loss=0.484\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.48it/s, train_loss=2.72, val_loss=0.081\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.45it/s, train_loss=2.72, val_loss=0.081\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.45it/s, train_loss=2.72, val_loss=0.081\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.45it/s, train_loss=0.491, val_loss=0.33\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.45it/s, train_loss=0.491, val_loss=0.33\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.45it/s, train_loss=1.55, val_loss=0.638\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.45it/s, train_loss=1.55, val_loss=0.638\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.45it/s, train_loss=2.97, val_loss=0.379\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.45it/s, train_loss=2.97, val_loss=0.379\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.45it/s, train_loss=1.74, val_loss=0.064\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.31it/s, train_loss=1.74, val_loss=0.064\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.31it/s, train_loss=1.74, val_loss=0.064\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.31it/s, train_loss=0.326, val_loss=0.22\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.31it/s, train_loss=0.326, val_loss=0.22\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.31it/s, train_loss=1.26, val_loss=0.396\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.31it/s, train_loss=1.26, val_loss=0.396\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.31it/s, train_loss=2.18, val_loss=0.184\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.31it/s, train_loss=2.18, val_loss=0.184\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.31it/s, train_loss=1.06, val_loss=0.046\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.17it/s, train_loss=1.06, val_loss=0.046\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.17it/s, train_loss=1.06, val_loss=0.046\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.17it/s, train_loss=0.252, val_loss=0.23\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.17it/s, train_loss=0.252, val_loss=0.23\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.17it/s, train_loss=1.06, val_loss=0.302\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.17it/s, train_loss=1.06, val_loss=0.302\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.17it/s, train_loss=1.38, val_loss=0.11,\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.17it/s, train_loss=1.38, val_loss=0.11,\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.17it/s, train_loss=0.505, val_loss=0.04\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.19it/s, train_loss=0.505, val_loss=0.04\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.19it/s, train_loss=0.505, val_loss=0.04\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.19it/s, train_loss=0.312, val_loss=0.17\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.19it/s, train_loss=0.312, val_loss=0.17\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.19it/s, train_loss=0.99, val_loss=0.145\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.19it/s, train_loss=0.99, val_loss=0.145\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.19it/s, train_loss=0.829, val_loss=0.06\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.19it/s, train_loss=0.829, val_loss=0.06\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.19it/s, train_loss=0.407, val_loss=0.02\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 33.93it/s, train_loss=0.407, val_loss=0.02\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 33.93it/s, train_loss=0.407, val_loss=0.02\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 33.93it/s, train_loss=0.155, val_loss=0.06\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 33.93it/s, train_loss=0.155, val_loss=0.06\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 33.93it/s, train_loss=0.301, val_loss=0.11\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 33.93it/s, train_loss=0.301, val_loss=0.11\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 33.93it/s, train_loss=0.564, val_loss=0.11\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 33.93it/s, train_loss=0.564, val_loss=0.11\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    38: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 33.93it/s, train_loss=0.574, val_loss=0.06\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 32.17it/s, train_loss=0.574, val_loss=0.06\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 32.17it/s, train_loss=0.574, val_loss=0.06\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 32.17it/s, train_loss=0.336, val_loss=0.02\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 32.17it/s, train_loss=0.336, val_loss=0.02\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 32.17it/s, train_loss=0.157, val_loss=0.03\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 32.17it/s, train_loss=0.157, val_loss=0.03\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 32.17it/s, train_loss=0.224, val_loss=0.04\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 32.17it/s, train_loss=0.224, val_loss=0.04\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 32.17it/s, train_loss=0.292, val_loss=0.05\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 30.23it/s, train_loss=0.292, val_loss=0.05\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 30.23it/s, train_loss=0.292, val_loss=0.05\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 30.23it/s, train_loss=0.308, val_loss=0.04\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 30.23it/s, train_loss=0.308, val_loss=0.04\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 32.79it/s, train_loss=0.263, val_loss=0.03\u001b[A\n",
      " 28%|███████████▊                              | 28/100 [00:43<01:49,  1.52s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    46: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Curent best Test AUC: 0.7618033813400126\n",
      "Its' val AUC : 0.7178197634905532\n",
      "Its external  AUC: 0.5788561525129983\n",
      "Curent best Test AUC: 0.7665623043206011\n",
      "Its' val AUC : 0.7300530107380726\n",
      "Its external  AUC: 0.5861036710256814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=15, val_loss=50.1, train_cinde\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=15, val_loss=50.1, train_cinde\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=294, val_loss=29.6, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=294, val_loss=29.6, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=158, val_loss=33.8, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=158, val_loss=33.8, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=175, val_loss=11.7, train_cind\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 32.84it/s, train_loss=175, val_loss=11.7, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.84it/s, train_loss=175, val_loss=11.7, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.84it/s, train_loss=58.2, val_loss=14.5, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.84it/s, train_loss=58.2, val_loss=14.5, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.84it/s, train_loss=85.8, val_loss=11.2, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.84it/s, train_loss=85.8, val_loss=11.2, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.84it/s, train_loss=76.3, val_loss=9.39, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.84it/s, train_loss=76.3, val_loss=9.39, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.6440826549780839\n",
      "Its' val AUC : 0.6690226994698926\n",
      "Its external  AUC: 0.5983929415471877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.84it/s, train_loss=63.5, val_loss=7.4, tr\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 32.51it/s, train_loss=63.5, val_loss=7.4, tr\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.51it/s, train_loss=63.5, val_loss=7.4, tr\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.51it/s, train_loss=47.7, val_loss=6.35, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.51it/s, train_loss=47.7, val_loss=6.35, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.51it/s, train_loss=37.3, val_loss=5.45, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.51it/s, train_loss=37.3, val_loss=5.45, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.51it/s, train_loss=32.8, val_loss=3.76, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.51it/s, train_loss=32.8, val_loss=3.76, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.51it/s, train_loss=23.4, val_loss=1.76, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 32.07it/s, train_loss=23.4, val_loss=1.76,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.07it/s, train_loss=23.4, val_loss=1.76,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.07it/s, train_loss=10.8, val_loss=1.25,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.07it/s, train_loss=10.8, val_loss=1.25,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.07it/s, train_loss=8.02, val_loss=3.66,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.07it/s, train_loss=8.02, val_loss=3.66,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.07it/s, train_loss=16.9, val_loss=1.03,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.07it/s, train_loss=16.9, val_loss=1.03,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.07it/s, train_loss=6.91, val_loss=1.21,\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 31.59it/s, train_loss=6.91, val_loss=1.21,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 31.59it/s, train_loss=6.91, val_loss=1.21,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 31.59it/s, train_loss=7.67, val_loss=2.13,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 31.59it/s, train_loss=7.67, val_loss=2.13,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 31.59it/s, train_loss=13.3, val_loss=1.74,\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 31.59it/s, train_loss=13.3, val_loss=1.74,\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 31.59it/s, train_loss=11.8, val_loss=2.26,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 31.59it/s, train_loss=11.8, val_loss=2.26,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 31.59it/s, train_loss=14, val_loss=1.41, t\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 31.28it/s, train_loss=14, val_loss=1.41, t\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 31.28it/s, train_loss=14, val_loss=1.41, t\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 31.28it/s, train_loss=9.38, val_loss=1.4, \u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 31.28it/s, train_loss=9.38, val_loss=1.4, \u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 31.28it/s, train_loss=8.96, val_loss=1.09,\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 31.28it/s, train_loss=8.96, val_loss=1.09,\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.6834063869755792\n",
      "Its' val AUC : 0.5843414435231752\n",
      "Its external  AUC: 0.62076571608634\n",
      "Curent best Test AUC: 0.697683155917345\n",
      "Its' val AUC : 0.5878754927280141\n",
      "Its external  AUC: 0.6333700961083977\n",
      "Epoch    21: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 31.28it/s, train_loss=7.21, val_loss=1.13,\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 31.28it/s, train_loss=7.21, val_loss=1.13,\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 31.28it/s, train_loss=6.84, val_loss=0.6, \u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 31.34it/s, train_loss=6.84, val_loss=0.6, \u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 31.34it/s, train_loss=6.84, val_loss=0.6, \u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 31.34it/s, train_loss=3.65, val_loss=0.618\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 31.34it/s, train_loss=3.65, val_loss=0.618\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 31.34it/s, train_loss=3.85, val_loss=0.466\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 31.34it/s, train_loss=3.85, val_loss=0.466\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 31.34it/s, train_loss=2.8, val_loss=0.582,\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 31.34it/s, train_loss=2.8, val_loss=0.582,\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 31.34it/s, train_loss=3.01, val_loss=0.458\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 31.36it/s, train_loss=3.01, val_loss=0.458\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 31.36it/s, train_loss=3.01, val_loss=0.458\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 31.36it/s, train_loss=2.43, val_loss=0.216\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 31.36it/s, train_loss=2.43, val_loss=0.216\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 31.36it/s, train_loss=1.45, val_loss=0.255\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 31.36it/s, train_loss=1.45, val_loss=0.255\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 31.36it/s, train_loss=1.6, val_loss=0.156,\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 31.36it/s, train_loss=1.6, val_loss=0.156,\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:01<00:00, 31.36it/s, train_loss=1.08, val_loss=0.299\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:01<00:00, 31.73it/s, train_loss=1.08, val_loss=0.299\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:01<00:00, 31.73it/s, train_loss=1.08, val_loss=0.299\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:01<00:00, 31.73it/s, train_loss=1.98, val_loss=0.236\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 31.73it/s, train_loss=1.98, val_loss=0.236\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 31.73it/s, train_loss=1.6, val_loss=0.318,\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 31.73it/s, train_loss=1.6, val_loss=0.318,\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 31.73it/s, train_loss=1.93, val_loss=0.245\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 31.73it/s, train_loss=1.93, val_loss=0.245\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 31.73it/s, train_loss=1.56, val_loss=0.278\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 32.09it/s, train_loss=1.56, val_loss=0.278\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 32.09it/s, train_loss=1.56, val_loss=0.278\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 32.09it/s, train_loss=1.68, val_loss=0.278\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 32.09it/s, train_loss=1.68, val_loss=0.278\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 32.09it/s, train_loss=1.62, val_loss=0.219\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 32.09it/s, train_loss=1.62, val_loss=0.219\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 32.09it/s, train_loss=1.36, val_loss=0.205\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 32.09it/s, train_loss=1.36, val_loss=0.205\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 32.09it/s, train_loss=1.34, val_loss=0.182\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 32.50it/s, train_loss=1.34, val_loss=0.182\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 32.50it/s, train_loss=1.34, val_loss=0.182\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 32.50it/s, train_loss=1.2, val_loss=0.131,\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 32.50it/s, train_loss=1.2, val_loss=0.131,\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 32.50it/s, train_loss=0.852, val_loss=0.13\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 32.50it/s, train_loss=0.852, val_loss=0.13\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 32.50it/s, train_loss=0.816, val_loss=0.14\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 32.50it/s, train_loss=0.816, val_loss=0.14\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    37: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 32.50it/s, train_loss=0.843, val_loss=0.11\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 32.71it/s, train_loss=0.843, val_loss=0.11\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 32.71it/s, train_loss=0.843, val_loss=0.11\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 32.71it/s, train_loss=0.677, val_loss=0.11\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 32.71it/s, train_loss=0.677, val_loss=0.11\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 32.71it/s, train_loss=0.666, val_loss=0.11\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 32.71it/s, train_loss=0.666, val_loss=0.11\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 32.71it/s, train_loss=0.655, val_loss=0.08\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 32.71it/s, train_loss=0.655, val_loss=0.08\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 32.71it/s, train_loss=0.489, val_loss=0.08\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 32.67it/s, train_loss=0.489, val_loss=0.08\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 32.67it/s, train_loss=0.489, val_loss=0.08\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 32.67it/s, train_loss=0.458, val_loss=0.09\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 32.67it/s, train_loss=0.458, val_loss=0.09\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 32.13it/s, train_loss=0.478, val_loss=0.07\u001b[A\n",
      " 29%|████████████▏                             | 29/100 [00:45<01:49,  1.54s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=21.5, val_loss=65.8, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=21.5, val_loss=65.8, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=313, val_loss=56, train_cindex\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=313, val_loss=56, train_cindex\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=305, val_loss=7.15, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=305, val_loss=7.15, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=35.3, val_loss=6.4, train_cind\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 31.30it/s, train_loss=35.3, val_loss=6.4, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.30it/s, train_loss=35.3, val_loss=6.4, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.30it/s, train_loss=37.7, val_loss=2.05, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.30it/s, train_loss=37.7, val_loss=2.05, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.30it/s, train_loss=12, val_loss=1.18, tra\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.30it/s, train_loss=12, val_loss=1.18, tra\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.30it/s, train_loss=6.64, val_loss=0.828, \u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.30it/s, train_loss=6.64, val_loss=0.828, \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7378835316217909\n",
      "Its' val AUC : 0.712382764713878\n",
      "Its external  AUC: 0.6152512998266898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.30it/s, train_loss=5.07, val_loss=0.645, \u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 31.87it/s, train_loss=5.07, val_loss=0.645, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 31.87it/s, train_loss=5.07, val_loss=0.645, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 31.87it/s, train_loss=3.46, val_loss=0.154, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 31.87it/s, train_loss=3.46, val_loss=0.154, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 31.87it/s, train_loss=0.85, val_loss=0.307, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 31.87it/s, train_loss=0.85, val_loss=0.307,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 31.87it/s, train_loss=1.71, val_loss=0.372,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 31.87it/s, train_loss=1.71, val_loss=0.372,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 31.87it/s, train_loss=2.05, val_loss=0.381,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 31.93it/s, train_loss=2.05, val_loss=0.381\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 31.93it/s, train_loss=2.05, val_loss=0.381\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 31.93it/s, train_loss=1.79, val_loss=0.338\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 31.93it/s, train_loss=1.79, val_loss=0.338\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 31.93it/s, train_loss=1.57, val_loss=0.2, \u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 31.93it/s, train_loss=1.57, val_loss=0.2, \u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 31.93it/s, train_loss=1.09, val_loss=0.103\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 31.93it/s, train_loss=1.09, val_loss=0.103\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 31.93it/s, train_loss=0.622, val_loss=0.10\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 32.37it/s, train_loss=0.622, val_loss=0.10\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.37it/s, train_loss=0.622, val_loss=0.10\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.37it/s, train_loss=0.828, val_loss=0.11\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.37it/s, train_loss=0.828, val_loss=0.11\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.37it/s, train_loss=0.817, val_loss=0.12\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.37it/s, train_loss=0.817, val_loss=0.12\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.37it/s, train_loss=0.866, val_loss=0.23\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.37it/s, train_loss=0.866, val_loss=0.23\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.37it/s, train_loss=1.19, val_loss=0.22,\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 32.67it/s, train_loss=1.19, val_loss=0.22,\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.67it/s, train_loss=1.19, val_loss=0.22,\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.67it/s, train_loss=1.37, val_loss=0.269\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.67it/s, train_loss=1.37, val_loss=0.269\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.67it/s, train_loss=1.41, val_loss=0.053\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.67it/s, train_loss=1.41, val_loss=0.053\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.67it/s, train_loss=0.384, val_loss=0.21\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.67it/s, train_loss=0.384, val_loss=0.21\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.67it/s, train_loss=1.15, val_loss=0.055\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 32.42it/s, train_loss=1.15, val_loss=0.055\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 32.42it/s, train_loss=1.15, val_loss=0.055\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 32.42it/s, train_loss=0.329, val_loss=0.18\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 32.42it/s, train_loss=0.329, val_loss=0.18\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 32.42it/s, train_loss=0.904, val_loss=0.09\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 32.42it/s, train_loss=0.904, val_loss=0.09\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 32.42it/s, train_loss=0.469, val_loss=0.14\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 32.42it/s, train_loss=0.469, val_loss=0.14\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    21: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Curent best Test AUC: 0.7744520976831559\n",
      "Its' val AUC : 0.6871007204023379\n",
      "Its external  AUC: 0.5218213329131873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 32.42it/s, train_loss=0.737, val_loss=0.08\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 32.00it/s, train_loss=0.737, val_loss=0.08\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 32.00it/s, train_loss=0.737, val_loss=0.08\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 32.00it/s, train_loss=0.438, val_loss=0.04\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 32.00it/s, train_loss=0.438, val_loss=0.04\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 32.00it/s, train_loss=0.25, val_loss=0.076\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 32.00it/s, train_loss=0.25, val_loss=0.076\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 32.00it/s, train_loss=0.441, val_loss=0.06\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 32.00it/s, train_loss=0.441, val_loss=0.06\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 32.00it/s, train_loss=0.396, val_loss=0.03\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 32.01it/s, train_loss=0.396, val_loss=0.03\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 32.01it/s, train_loss=0.396, val_loss=0.03\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:01<00:00, 32.01it/s, train_loss=0.238, val_loss=0.06\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 32.01it/s, train_loss=0.238, val_loss=0.06\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 32.01it/s, train_loss=0.366, val_loss=0.05\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 32.01it/s, train_loss=0.366, val_loss=0.05\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    28: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Curent best Test AUC: 0.7842204132748904\n",
      "Its' val AUC : 0.5918173168411037\n",
      "Its external  AUC: 0.38742713092799747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 32.01it/s, train_loss=0.337, val_loss=0.03\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 32.01it/s, train_loss=0.337, val_loss=0.03\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 32.01it/s, train_loss=0.208, val_loss=0.05\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 31.64it/s, train_loss=0.208, val_loss=0.05\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 31.64it/s, train_loss=0.208, val_loss=0.05\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 31.64it/s, train_loss=0.309, val_loss=0.05\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 31.64it/s, train_loss=0.309, val_loss=0.05\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 31.64it/s, train_loss=0.296, val_loss=0.03\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 31.64it/s, train_loss=0.296, val_loss=0.03\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 31.64it/s, train_loss=0.204, val_loss=0.04\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 31.64it/s, train_loss=0.204, val_loss=0.04\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 31.64it/s, train_loss=0.278, val_loss=0.04\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 31.64it/s, train_loss=0.278, val_loss=0.04\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 31.64it/s, train_loss=0.278, val_loss=0.04\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 31.64it/s, train_loss=0.247, val_loss=0.02\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 31.64it/s, train_loss=0.247, val_loss=0.02\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 31.64it/s, train_loss=0.179, val_loss=0.03\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 31.64it/s, train_loss=0.179, val_loss=0.03\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 31.64it/s, train_loss=0.239, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 31.64it/s, train_loss=0.239, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 31.64it/s, train_loss=0.208, val_loss=0.02\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 31.30it/s, train_loss=0.208, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 31.30it/s, train_loss=0.208, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 31.30it/s, train_loss=0.175, val_loss=0.03\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 31.30it/s, train_loss=0.175, val_loss=0.03\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 31.30it/s, train_loss=0.222, val_loss=0.03\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 31.30it/s, train_loss=0.222, val_loss=0.03\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 31.30it/s, train_loss=0.181, val_loss=0.02\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 31.30it/s, train_loss=0.181, val_loss=0.02\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 31.30it/s, train_loss=0.172, val_loss=0.03\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 31.69it/s, train_loss=0.172, val_loss=0.03\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 31.69it/s, train_loss=0.172, val_loss=0.03\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 31.69it/s, train_loss=0.184, val_loss=0.02\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 31.69it/s, train_loss=0.184, val_loss=0.02\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 31.83it/s, train_loss=0.174, val_loss=0.02\u001b[A\n",
      " 30%|████████████▌                             | 30/100 [00:46<01:48,  1.55s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=48.5, val_loss=42.5, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=48.5, val_loss=42.5, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=225, val_loss=2.21, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=225, val_loss=2.21, train_cind\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    47: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Curent best Test AUC: 0.27188478396994364\n",
      "Its' val AUC : 0.27049068913959495\n",
      "Its external  AUC: 0.3847487001733102\n",
      "Curent best Test AUC: 0.5026925485284909\n",
      "Its' val AUC : 0.4806306918580943\n",
      "Its external  AUC: 0.6321096581061919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=12.3, val_loss=35.4, train_cin\u001b[A\n",
      "Epoch 2:   6%| | 3/50 [00:00<00:01, 29.34it/s, train_loss=12.3, val_loss=35.4, t\u001b[A\n",
      "Epoch 3:   6%| | 3/50 [00:00<00:01, 29.34it/s, train_loss=12.3, val_loss=35.4, t\u001b[A\n",
      "Epoch 3:   6%| | 3/50 [00:00<00:01, 29.34it/s, train_loss=189, val_loss=35.5, tr\u001b[A\n",
      "Epoch 4:   6%| | 3/50 [00:00<00:01, 29.34it/s, train_loss=189, val_loss=35.5, tr\u001b[A\n",
      "Epoch 4:   6%| | 3/50 [00:00<00:01, 29.34it/s, train_loss=185, val_loss=26.1, tr\u001b[A\n",
      "Epoch 5:   6%| | 3/50 [00:00<00:01, 29.34it/s, train_loss=185, val_loss=26.1, tr\u001b[A\n",
      "Epoch 5:   6%| | 3/50 [00:00<00:01, 29.34it/s, train_loss=132, val_loss=5.35, tr\u001b[A\n",
      "Epoch 6:   6%| | 3/50 [00:00<00:01, 29.34it/s, train_loss=132, val_loss=5.35, tr\u001b[A\n",
      "Epoch 6:   6%| | 3/50 [00:00<00:01, 29.34it/s, train_loss=31.6, val_loss=2.76, t\u001b[A\n",
      "Epoch 6:  14%|▏| 7/50 [00:00<00:01, 29.84it/s, train_loss=31.6, val_loss=2.76, t\u001b[A\n",
      "Epoch 7:  14%|▏| 7/50 [00:00<00:01, 29.84it/s, train_loss=31.6, val_loss=2.76, t\u001b[A\n",
      "Epoch 7:  14%|▏| 7/50 [00:00<00:01, 29.84it/s, train_loss=16, val_loss=2.48, tra\u001b[A\n",
      "Epoch 8:  14%|▏| 7/50 [00:00<00:01, 29.84it/s, train_loss=16, val_loss=2.48, tra\u001b[A\n",
      "Epoch 8:  14%|▏| 7/50 [00:00<00:01, 29.84it/s, train_loss=11.8, val_loss=1.67, t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7167188478396994\n",
      "Its' val AUC : 0.7174119885823026\n",
      "Its external  AUC: 0.6267527965968174\n",
      "Curent best Test AUC: 0.7704445835942392\n",
      "Its' val AUC : 0.6879162702188392\n",
      "Its external  AUC: 0.6224988183393729\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "Epoch 9:  14%|▏| 7/50 [00:00<00:01, 29.84it/s, train_loss=11.8, val_loss=1.67, t\u001b[A\n",
      "Epoch 9:  14%|▏| 7/50 [00:00<00:01, 29.84it/s, train_loss=8.02, val_loss=0.939, \u001b[A\n",
      "Epoch 10:  14%|▏| 7/50 [00:00<00:01, 29.84it/s, train_loss=8.02, val_loss=0.939,\u001b[A\n",
      "Epoch 10:  14%|▏| 7/50 [00:00<00:01, 29.84it/s, train_loss=5.25, val_loss=1.58, \u001b[A\n",
      "Epoch 10:  22%|▏| 11/50 [00:00<00:01, 30.73it/s, train_loss=5.25, val_loss=1.58,\u001b[A\n",
      "Epoch 11:  22%|▏| 11/50 [00:00<00:01, 30.73it/s, train_loss=5.25, val_loss=1.58,\u001b[A\n",
      "Epoch 11:  22%|▏| 11/50 [00:00<00:01, 30.73it/s, train_loss=8.83, val_loss=0.656\u001b[A\n",
      "Epoch 12:  22%|▏| 11/50 [00:00<00:01, 30.73it/s, train_loss=8.83, val_loss=0.656\u001b[A\n",
      "Epoch 12:  22%|▏| 11/50 [00:00<00:01, 30.73it/s, train_loss=3.99, val_loss=0.46,\u001b[A\n",
      "Epoch 13:  22%|▏| 11/50 [00:00<00:01, 30.73it/s, train_loss=3.99, val_loss=0.46,\u001b[A\n",
      "Epoch 13:  22%|▏| 11/50 [00:00<00:01, 30.73it/s, train_loss=2.53, val_loss=0.968\u001b[A\n",
      "Epoch 14:  22%|▏| 11/50 [00:00<00:01, 30.73it/s, train_loss=2.53, val_loss=0.968\u001b[A\n",
      "Epoch 14:  22%|▏| 11/50 [00:00<00:01, 30.73it/s, train_loss=5.08, val_loss=0.444\u001b[A\n",
      "Epoch 14:  30%|▎| 15/50 [00:00<00:01, 30.24it/s, train_loss=5.08, val_loss=0.444\u001b[A\n",
      "Epoch 15:  30%|▎| 15/50 [00:00<00:01, 30.24it/s, train_loss=5.08, val_loss=0.444\u001b[A\n",
      "Epoch 15:  30%|▎| 15/50 [00:00<00:01, 30.24it/s, train_loss=3.09, val_loss=0.593\u001b[A\n",
      "Epoch 16:  30%|▎| 15/50 [00:00<00:01, 30.24it/s, train_loss=3.09, val_loss=0.593\u001b[A\n",
      "Epoch 16:  30%|▎| 15/50 [00:00<00:01, 30.24it/s, train_loss=4.09, val_loss=0.493\u001b[A\n",
      "Epoch 17:  30%|▎| 15/50 [00:00<00:01, 30.24it/s, train_loss=4.09, val_loss=0.493\u001b[A\n",
      "Epoch 17:  30%|▎| 15/50 [00:00<00:01, 30.24it/s, train_loss=3.16, val_loss=0.142\u001b[A\n",
      "Epoch 18:  30%|▎| 15/50 [00:00<00:01, 30.24it/s, train_loss=3.16, val_loss=0.142\u001b[A\n",
      "Epoch 18:  30%|▎| 15/50 [00:00<00:01, 30.24it/s, train_loss=0.952, val_loss=0.48\u001b[A\n",
      "Epoch 18:  38%|▍| 19/50 [00:00<00:01, 30.57it/s, train_loss=0.952, val_loss=0.48\u001b[A\n",
      "Epoch 19:  38%|▍| 19/50 [00:00<00:01, 30.57it/s, train_loss=0.952, val_loss=0.48\u001b[A\n",
      "Epoch 19:  38%|▍| 19/50 [00:00<00:01, 30.57it/s, train_loss=2.58, val_loss=0.317\u001b[A\n",
      "Epoch 20:  38%|▍| 19/50 [00:00<00:01, 30.57it/s, train_loss=2.58, val_loss=0.317\u001b[A\n",
      "Epoch 20:  38%|▍| 19/50 [00:00<00:01, 30.57it/s, train_loss=1.81, val_loss=0.332\u001b[A\n",
      "Epoch 21:  38%|▍| 19/50 [00:00<00:01, 30.57it/s, train_loss=1.81, val_loss=0.332\u001b[A\n",
      "Epoch 21:  38%|▍| 19/50 [00:00<00:01, 30.57it/s, train_loss=1.94, val_loss=0.333\u001b[A\n",
      "Epoch 22:  38%|▍| 19/50 [00:00<00:01, 30.57it/s, train_loss=1.94, val_loss=0.333\u001b[A\n",
      "Epoch 22:  38%|▍| 19/50 [00:00<00:01, 30.57it/s, train_loss=1.98, val_loss=0.18,\u001b[A\n",
      "Epoch 22:  46%|▍| 23/50 [00:00<00:00, 30.52it/s, train_loss=1.98, val_loss=0.18,\u001b[A\n",
      "Epoch 23:  46%|▍| 23/50 [00:00<00:00, 30.52it/s, train_loss=1.98, val_loss=0.18,\u001b[A\n",
      "Epoch 23:  46%|▍| 23/50 [00:00<00:00, 30.52it/s, train_loss=0.94, val_loss=0.437\u001b[A\n",
      "Epoch 24:  46%|▍| 23/50 [00:00<00:00, 30.52it/s, train_loss=0.94, val_loss=0.437\u001b[A\n",
      "Epoch 24:  46%|▍| 23/50 [00:00<00:00, 30.52it/s, train_loss=1.91, val_loss=0.34,\u001b[A\n",
      "Epoch 25:  46%|▍| 23/50 [00:00<00:00, 30.52it/s, train_loss=1.91, val_loss=0.34,\u001b[A\n",
      "Epoch 25:  46%|▍| 23/50 [00:00<00:00, 30.52it/s, train_loss=1.55, val_loss=0.184\u001b[A\n",
      "Epoch 26:  46%|▍| 23/50 [00:00<00:00, 30.52it/s, train_loss=1.55, val_loss=0.184\u001b[A\n",
      "Epoch 26:  46%|▍| 23/50 [00:00<00:00, 30.52it/s, train_loss=1.04, val_loss=0.196\u001b[A\n",
      "Epoch 26:  54%|▌| 27/50 [00:00<00:00, 30.82it/s, train_loss=1.04, val_loss=0.196\u001b[A\n",
      "Epoch 27:  54%|▌| 27/50 [00:00<00:00, 30.82it/s, train_loss=1.04, val_loss=0.196\u001b[A\n",
      "Epoch 27:  54%|▌| 27/50 [00:00<00:00, 30.82it/s, train_loss=1.28, val_loss=0.192\u001b[A\n",
      "Epoch 28:  54%|▌| 27/50 [00:00<00:00, 30.82it/s, train_loss=1.28, val_loss=0.192\u001b[A\n",
      "Epoch 28:  54%|▌| 27/50 [00:00<00:00, 30.82it/s, train_loss=1.24, val_loss=0.131\u001b[A\n",
      "Epoch 29:  54%|▌| 27/50 [00:00<00:00, 30.82it/s, train_loss=1.24, val_loss=0.131\u001b[A\n",
      "Epoch 29:  54%|▌| 27/50 [00:00<00:00, 30.82it/s, train_loss=0.813, val_loss=0.17\u001b[A\n",
      "Epoch 30:  54%|▌| 27/50 [00:00<00:00, 30.82it/s, train_loss=0.813, val_loss=0.17\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    24: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30:  54%|▌| 27/50 [00:01<00:00, 30.82it/s, train_loss=0.996, val_loss=0.19\u001b[A\n",
      "Epoch 30:  62%|▌| 31/50 [00:01<00:00, 30.86it/s, train_loss=0.996, val_loss=0.19\u001b[A\n",
      "Epoch 31:  62%|▌| 31/50 [00:01<00:00, 30.86it/s, train_loss=0.996, val_loss=0.19\u001b[A\n",
      "Epoch 31:  62%|▌| 31/50 [00:01<00:00, 30.86it/s, train_loss=1.12, val_loss=0.138\u001b[A\n",
      "Epoch 32:  62%|▌| 31/50 [00:01<00:00, 30.86it/s, train_loss=1.12, val_loss=0.138\u001b[A\n",
      "Epoch 32:  62%|▌| 31/50 [00:01<00:00, 30.86it/s, train_loss=0.834, val_loss=0.10\u001b[A\n",
      "Epoch 33:  62%|▌| 31/50 [00:01<00:00, 30.86it/s, train_loss=0.834, val_loss=0.10\u001b[A\n",
      "Epoch 33:  62%|▌| 31/50 [00:01<00:00, 30.86it/s, train_loss=0.669, val_loss=0.12\u001b[A\n",
      "Epoch 34:  62%|▌| 31/50 [00:01<00:00, 30.86it/s, train_loss=0.669, val_loss=0.12\u001b[A\n",
      "Epoch 34:  62%|▌| 31/50 [00:01<00:00, 30.86it/s, train_loss=0.748, val_loss=0.09\u001b[A\n",
      "Epoch 34:  70%|▋| 35/50 [00:01<00:00, 31.01it/s, train_loss=0.748, val_loss=0.09\u001b[A\n",
      "Epoch 35:  70%|▋| 35/50 [00:01<00:00, 31.01it/s, train_loss=0.748, val_loss=0.09\u001b[A\n",
      "Epoch 35:  70%|▋| 35/50 [00:01<00:00, 31.01it/s, train_loss=0.584, val_loss=0.08\u001b[A\n",
      "Epoch 36:  70%|▋| 35/50 [00:01<00:00, 31.01it/s, train_loss=0.584, val_loss=0.08\u001b[A\n",
      "Epoch 36:  70%|▋| 35/50 [00:01<00:00, 31.01it/s, train_loss=0.451, val_loss=0.12\u001b[A\n",
      "Epoch 37:  70%|▋| 35/50 [00:01<00:00, 31.01it/s, train_loss=0.451, val_loss=0.12\u001b[A\n",
      "Epoch 37:  70%|▋| 35/50 [00:01<00:00, 31.01it/s, train_loss=0.595, val_loss=0.11\u001b[A\n",
      "Epoch 38:  70%|▋| 35/50 [00:01<00:00, 31.01it/s, train_loss=0.595, val_loss=0.11\u001b[A\n",
      "Epoch 38:  70%|▋| 35/50 [00:01<00:00, 31.01it/s, train_loss=0.575, val_loss=0.08\u001b[A\n",
      "Epoch 38:  78%|▊| 39/50 [00:01<00:00, 30.96it/s, train_loss=0.575, val_loss=0.08\u001b[A\n",
      "Epoch 39:  78%|▊| 39/50 [00:01<00:00, 30.96it/s, train_loss=0.575, val_loss=0.08\u001b[A\n",
      "Epoch 39:  78%|▊| 39/50 [00:01<00:00, 30.96it/s, train_loss=0.432, val_loss=0.08\u001b[A\n",
      "Epoch 40:  78%|▊| 39/50 [00:01<00:00, 30.96it/s, train_loss=0.432, val_loss=0.08\u001b[A\n",
      "Epoch 40:  78%|▊| 39/50 [00:01<00:00, 30.96it/s, train_loss=0.487, val_loss=0.09\u001b[A\n",
      "Epoch 41:  78%|▊| 39/50 [00:01<00:00, 30.96it/s, train_loss=0.487, val_loss=0.09\u001b[A\n",
      "Epoch 41:  78%|▊| 39/50 [00:01<00:00, 30.96it/s, train_loss=0.486, val_loss=0.07\u001b[A\n",
      "Epoch 42:  78%|▊| 39/50 [00:01<00:00, 30.96it/s, train_loss=0.486, val_loss=0.07\u001b[A\n",
      "Epoch 42:  78%|▊| 39/50 [00:01<00:00, 30.96it/s, train_loss=0.393, val_loss=0.09\u001b[A\n",
      "Epoch 42:  86%|▊| 43/50 [00:01<00:00, 30.93it/s, train_loss=0.393, val_loss=0.09\u001b[A\n",
      "Epoch 43:  86%|▊| 43/50 [00:01<00:00, 30.93it/s, train_loss=0.393, val_loss=0.09\u001b[A\n",
      "Epoch 43:  86%|▊| 43/50 [00:01<00:00, 30.93it/s, train_loss=0.462, val_loss=0.09\u001b[A\n",
      "Epoch 44:  86%|▊| 43/50 [00:01<00:00, 30.93it/s, train_loss=0.462, val_loss=0.09\u001b[A\n",
      "Epoch 44:  86%|▊| 43/50 [00:01<00:00, 30.93it/s, train_loss=0.463, val_loss=0.07\u001b[A\n",
      "Epoch 45:  86%|▊| 43/50 [00:01<00:00, 30.93it/s, train_loss=0.463, val_loss=0.07\u001b[A\n",
      "Epoch 45:  86%|▊| 43/50 [00:01<00:00, 30.93it/s, train_loss=0.37, val_loss=0.075\u001b[A\n",
      "Epoch 46:  86%|▊| 43/50 [00:01<00:00, 30.93it/s, train_loss=0.37, val_loss=0.075\u001b[A\n",
      "Epoch 46:  86%|▊| 43/50 [00:01<00:00, 30.93it/s, train_loss=0.391, val_loss=0.07\u001b[A\n",
      "Epoch 46:  94%|▉| 47/50 [00:01<00:00, 31.65it/s, train_loss=0.391, val_loss=0.07\u001b[A\n",
      "Epoch 47:  94%|▉| 47/50 [00:01<00:00, 31.65it/s, train_loss=0.391, val_loss=0.07\u001b[A\n",
      "Epoch 47:  94%|▉| 47/50 [00:01<00:00, 31.65it/s, train_loss=0.383, val_loss=0.06\u001b[A\n",
      "Epoch 48:  94%|▉| 47/50 [00:01<00:00, 31.65it/s, train_loss=0.383, val_loss=0.06\u001b[A\n",
      "Epoch 48:  94%|▉| 47/50 [00:01<00:00, 31.65it/s, train_loss=0.335, val_loss=0.07\u001b[A\n",
      "Epoch 49:  94%|▉| 47/50 [00:01<00:00, 31.65it/s, train_loss=0.335, val_loss=0.07\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 30.96it/s, train_loss=0.37, val_loss=0.068\u001b[A\n",
      " 31%|█████████████                             | 31/100 [00:48<01:48,  1.58s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=14, val_loss=6.66, train_cinde\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=14, val_loss=6.66, train_cinde\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=37.4, val_loss=8.05, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=37.4, val_loss=8.05, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=43.7, val_loss=0.835, train_ci\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=43.7, val_loss=0.835, train_ci\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=5.4, val_loss=4.12, train_cind\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 31.18it/s, train_loss=5.4, val_loss=4.12, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.18it/s, train_loss=5.4, val_loss=4.12, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.18it/s, train_loss=26.5, val_loss=2.42, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.18it/s, train_loss=26.5, val_loss=2.42, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.18it/s, train_loss=11.8, val_loss=2.34, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.18it/s, train_loss=11.8, val_loss=2.34, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.18it/s, train_loss=11.2, val_loss=0.434, \u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.18it/s, train_loss=11.2, val_loss=0.434, \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.736380713838447\n",
      "Its' val AUC : 0.6745956232159848\n",
      "Its external  AUC: 0.6206081613360643\n",
      "Curent best Test AUC: 0.7422667501565435\n",
      "Its' val AUC : 0.6706537991028952\n",
      "Its external  AUC: 0.577910824011344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.18it/s, train_loss=2.2, val_loss=2.04, tr\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 32.55it/s, train_loss=2.2, val_loss=2.04, tr\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.55it/s, train_loss=2.2, val_loss=2.04, tr\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.55it/s, train_loss=9.82, val_loss=0.683, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.55it/s, train_loss=9.82, val_loss=0.683, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.55it/s, train_loss=4.32, val_loss=0.439, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.55it/s, train_loss=4.32, val_loss=0.439,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.55it/s, train_loss=2.49, val_loss=0.217,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.55it/s, train_loss=2.49, val_loss=0.217,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.55it/s, train_loss=1.18, val_loss=0.561,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 33.29it/s, train_loss=1.18, val_loss=0.561\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.29it/s, train_loss=1.18, val_loss=0.561\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.29it/s, train_loss=3.15, val_loss=0.173\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.29it/s, train_loss=3.15, val_loss=0.173\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.29it/s, train_loss=1.08, val_loss=0.284\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.29it/s, train_loss=1.08, val_loss=0.284\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7552911709455229\n",
      "Its' val AUC : 0.724887861900231\n",
      "Its external  AUC: 0.6130455333228297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.29it/s, train_loss=1.68, val_loss=0.090\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.29it/s, train_loss=1.68, val_loss=0.090\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.29it/s, train_loss=0.566, val_loss=0.44\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 33.81it/s, train_loss=0.566, val_loss=0.44\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.81it/s, train_loss=0.566, val_loss=0.44\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.81it/s, train_loss=2.17, val_loss=0.137\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.81it/s, train_loss=2.17, val_loss=0.137\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.81it/s, train_loss=0.908, val_loss=0.20\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.81it/s, train_loss=0.908, val_loss=0.20\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.81it/s, train_loss=1.28, val_loss=0.049\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.81it/s, train_loss=1.28, val_loss=0.049\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.81it/s, train_loss=0.281, val_loss=0.25\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.17it/s, train_loss=0.281, val_loss=0.25\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.17it/s, train_loss=0.281, val_loss=0.25\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.17it/s, train_loss=1.36, val_loss=0.127\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.17it/s, train_loss=1.36, val_loss=0.127\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.17it/s, train_loss=0.678, val_loss=0.15\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.17it/s, train_loss=0.678, val_loss=0.15\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.17it/s, train_loss=0.786, val_loss=0.11\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.17it/s, train_loss=0.786, val_loss=0.11\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.17it/s, train_loss=0.471, val_loss=0.15\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.29it/s, train_loss=0.471, val_loss=0.15\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.29it/s, train_loss=0.471, val_loss=0.15\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.29it/s, train_loss=0.693, val_loss=0.11\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.29it/s, train_loss=0.693, val_loss=0.11\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.29it/s, train_loss=0.656, val_loss=0.07\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.29it/s, train_loss=0.656, val_loss=0.07\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.29it/s, train_loss=0.437, val_loss=0.03\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.29it/s, train_loss=0.437, val_loss=0.03\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.29it/s, train_loss=0.164, val_loss=0.07\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.22it/s, train_loss=0.164, val_loss=0.07\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.22it/s, train_loss=0.164, val_loss=0.07\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.22it/s, train_loss=0.45, val_loss=0.043\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.22it/s, train_loss=0.45, val_loss=0.043\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.22it/s, train_loss=0.311, val_loss=0.05\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.22it/s, train_loss=0.311, val_loss=0.05\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.22it/s, train_loss=0.341, val_loss=0.07\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.22it/s, train_loss=0.341, val_loss=0.07\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    25: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.22it/s, train_loss=0.42, val_loss=0.019\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.39it/s, train_loss=0.42, val_loss=0.019\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.39it/s, train_loss=0.42, val_loss=0.019\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.39it/s, train_loss=0.132, val_loss=0.04\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.39it/s, train_loss=0.132, val_loss=0.04\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 34.39it/s, train_loss=0.222, val_loss=0.05\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.39it/s, train_loss=0.222, val_loss=0.05\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.39it/s, train_loss=0.225, val_loss=0.03\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.39it/s, train_loss=0.225, val_loss=0.03\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.39it/s, train_loss=0.166, val_loss=0.04\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.50it/s, train_loss=0.166, val_loss=0.04\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.50it/s, train_loss=0.166, val_loss=0.04\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.50it/s, train_loss=0.282, val_loss=0.02\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.50it/s, train_loss=0.282, val_loss=0.02\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.50it/s, train_loss=0.122, val_loss=0.02\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.50it/s, train_loss=0.122, val_loss=0.02\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.50it/s, train_loss=0.124, val_loss=0.02\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.50it/s, train_loss=0.124, val_loss=0.02\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.50it/s, train_loss=0.133, val_loss=0.01\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.52it/s, train_loss=0.133, val_loss=0.01\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.52it/s, train_loss=0.133, val_loss=0.01\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.52it/s, train_loss=0.0795, val_loss=0.0\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.52it/s, train_loss=0.0795, val_loss=0.0\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.52it/s, train_loss=0.0812, val_loss=0.0\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.52it/s, train_loss=0.0812, val_loss=0.0\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.52it/s, train_loss=0.122, val_loss=0.01\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.52it/s, train_loss=0.122, val_loss=0.01\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.52it/s, train_loss=0.101, val_loss=0.00\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.57it/s, train_loss=0.101, val_loss=0.00\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.57it/s, train_loss=0.101, val_loss=0.00\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    38: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.57it/s, train_loss=0.0581, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.57it/s, train_loss=0.0581, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.57it/s, train_loss=0.0705, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.57it/s, train_loss=0.0705, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.57it/s, train_loss=0.0886, val_loss=0.0\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.57it/s, train_loss=0.0886, val_loss=0.0\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.57it/s, train_loss=0.0663, val_loss=0.0\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.63it/s, train_loss=0.0663, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.63it/s, train_loss=0.0663, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.63it/s, train_loss=0.0595, val_loss=0.0\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.63it/s, train_loss=0.0595, val_loss=0.0\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.19it/s, train_loss=0.0813, val_loss=0.0\u001b[A\n",
      " 32%|█████████████▍                            | 32/100 [00:50<01:45,  1.55s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=8.37, val_loss=219, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=8.37, val_loss=219, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=1.25e+3, val_loss=9.98, train_\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=1.25e+3, val_loss=9.98, train_\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=47.5, val_loss=35.9, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=47.5, val_loss=35.9, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=178, val_loss=41.5, train_cind\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 34.31it/s, train_loss=178, val_loss=41.5, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 34.31it/s, train_loss=178, val_loss=41.5, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 34.31it/s, train_loss=204, val_loss=7.04, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 34.31it/s, train_loss=204, val_loss=7.04, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 34.31it/s, train_loss=34.1, val_loss=16, tra\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 34.31it/s, train_loss=34.1, val_loss=16, tra\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    50: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Curent best Test AUC: 0.7269881026925485\n",
      "Its' val AUC : 0.7183634633682208\n",
      "Its external  AUC: 0.6039073578068379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 34.31it/s, train_loss=97.4, val_loss=13.7, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 34.31it/s, train_loss=97.4, val_loss=13.7, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 34.31it/s, train_loss=80.2, val_loss=9.67, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 31.77it/s, train_loss=80.2, val_loss=9.67, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 31.77it/s, train_loss=80.2, val_loss=9.67, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 31.77it/s, train_loss=55.5, val_loss=11.3, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 31.77it/s, train_loss=55.5, val_loss=11.3, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 31.77it/s, train_loss=61.3, val_loss=1.37, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 31.77it/s, train_loss=61.3, val_loss=1.37, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 31.77it/s, train_loss=8.37, val_loss=6.37, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 31.77it/s, train_loss=8.37, val_loss=6.37, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 31.77it/s, train_loss=38, val_loss=3.38, tr\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 32.82it/s, train_loss=38, val_loss=3.38, t\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.82it/s, train_loss=38, val_loss=3.38, t\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.82it/s, train_loss=22.1, val_loss=3.49,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.82it/s, train_loss=22.1, val_loss=3.49,\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7580463368816531\n",
      "Its' val AUC : 0.6873725703411717\n",
      "Its external  AUC: 0.6554277611469986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.82it/s, train_loss=20.2, val_loss=4.56,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.82it/s, train_loss=20.2, val_loss=4.56,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.82it/s, train_loss=26.4, val_loss=0.819\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.82it/s, train_loss=26.4, val_loss=0.819\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.82it/s, train_loss=5.62, val_loss=5.37,\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 33.31it/s, train_loss=5.62, val_loss=5.37,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.31it/s, train_loss=5.62, val_loss=5.37,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.31it/s, train_loss=24.4, val_loss=2.34,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.31it/s, train_loss=24.4, val_loss=2.34,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.31it/s, train_loss=10.9, val_loss=1.67,\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.31it/s, train_loss=10.9, val_loss=1.67,\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.31it/s, train_loss=10.8, val_loss=2.33,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.31it/s, train_loss=10.8, val_loss=2.33,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.31it/s, train_loss=13.5, val_loss=0.336\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 33.33it/s, train_loss=13.5, val_loss=0.336\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.33it/s, train_loss=13.5, val_loss=0.336\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.33it/s, train_loss=2.34, val_loss=2.68,\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.33it/s, train_loss=2.34, val_loss=2.68,\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.33it/s, train_loss=15.1, val_loss=0.597\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.33it/s, train_loss=15.1, val_loss=0.597\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.33it/s, train_loss=4.1, val_loss=2.07, \u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.33it/s, train_loss=4.1, val_loss=2.07, \u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.33it/s, train_loss=10.2, val_loss=1.43,\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 32.80it/s, train_loss=10.2, val_loss=1.43,\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 32.80it/s, train_loss=10.2, val_loss=1.43,\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 32.80it/s, train_loss=7.23, val_loss=0.966\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 32.80it/s, train_loss=7.23, val_loss=0.966\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 32.80it/s, train_loss=5.89, val_loss=1.67,\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 32.80it/s, train_loss=5.89, val_loss=1.67,\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 32.80it/s, train_loss=9.56, val_loss=0.591\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 32.80it/s, train_loss=9.56, val_loss=0.591\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 32.80it/s, train_loss=3.57, val_loss=0.727\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 33.19it/s, train_loss=3.57, val_loss=0.727\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.19it/s, train_loss=3.57, val_loss=0.727\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.19it/s, train_loss=3.52, val_loss=1.36,\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.19it/s, train_loss=3.52, val_loss=1.36,\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.19it/s, train_loss=6.3, val_loss=0.708,\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.19it/s, train_loss=6.3, val_loss=0.708,\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.19it/s, train_loss=3.16, val_loss=0.254\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.19it/s, train_loss=3.16, val_loss=0.254\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.19it/s, train_loss=1.51, val_loss=0.751\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 33.68it/s, train_loss=1.51, val_loss=0.751\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 33.68it/s, train_loss=1.51, val_loss=0.751\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    26: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 33.68it/s, train_loss=4.45, val_loss=0.513\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 33.68it/s, train_loss=4.45, val_loss=0.513\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 33.68it/s, train_loss=3, val_loss=0.127, t\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 33.68it/s, train_loss=3, val_loss=0.127, t\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 33.68it/s, train_loss=0.701, val_loss=0.49\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 33.68it/s, train_loss=0.701, val_loss=0.49\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 33.68it/s, train_loss=2.63, val_loss=0.481\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 33.95it/s, train_loss=2.63, val_loss=0.481\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.95it/s, train_loss=2.63, val_loss=0.481\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.95it/s, train_loss=2.73, val_loss=0.167\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.95it/s, train_loss=2.73, val_loss=0.167\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.95it/s, train_loss=1.03, val_loss=0.467\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.95it/s, train_loss=1.03, val_loss=0.467\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.95it/s, train_loss=2.27, val_loss=0.56,\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.95it/s, train_loss=2.27, val_loss=0.56,\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.95it/s, train_loss=2.7, val_loss=0.176,\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.02it/s, train_loss=2.7, val_loss=0.176,\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.02it/s, train_loss=2.7, val_loss=0.176,\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.02it/s, train_loss=1, val_loss=0.141, t\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.02it/s, train_loss=1, val_loss=0.141, t\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.02it/s, train_loss=0.92, val_loss=0.201\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.02it/s, train_loss=0.92, val_loss=0.201\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.02it/s, train_loss=1.26, val_loss=0.217\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.02it/s, train_loss=1.26, val_loss=0.217\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.02it/s, train_loss=1.31, val_loss=0.157\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 33.97it/s, train_loss=1.31, val_loss=0.157\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 33.97it/s, train_loss=1.31, val_loss=0.157\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 33.97it/s, train_loss=0.96, val_loss=0.119\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 33.97it/s, train_loss=0.96, val_loss=0.119\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 33.97it/s, train_loss=0.733, val_loss=0.15\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 33.97it/s, train_loss=0.733, val_loss=0.15\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    40: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 33.97it/s, train_loss=0.881, val_loss=0.17\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 33.97it/s, train_loss=0.881, val_loss=0.17\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 33.97it/s, train_loss=0.996, val_loss=0.14\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.09it/s, train_loss=0.996, val_loss=0.14\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.09it/s, train_loss=0.996, val_loss=0.14\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.09it/s, train_loss=0.774, val_loss=0.09\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.09it/s, train_loss=0.774, val_loss=0.09\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 33.61it/s, train_loss=0.518, val_loss=0.11\u001b[A\n",
      " 33%|█████████████▊                            | 33/100 [00:51<01:43,  1.54s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=18.4, val_loss=20.4, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=18.4, val_loss=20.4, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=110, val_loss=2.68, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=110, val_loss=2.68, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=16.5, val_loss=2.6, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=16.5, val_loss=2.6, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=16.3, val_loss=4.28, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 32.27it/s, train_loss=16.3, val_loss=4.28, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.27it/s, train_loss=16.3, val_loss=4.28, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.27it/s, train_loss=22, val_loss=1.86, tra\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.27it/s, train_loss=22, val_loss=1.86, tra\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.27it/s, train_loss=11.3, val_loss=2.05, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.27it/s, train_loss=11.3, val_loss=2.05, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.27it/s, train_loss=12.3, val_loss=1.31, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.27it/s, train_loss=12.3, val_loss=1.31, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.723606762680025\n",
      "Its' val AUC : 0.7119749898056273\n",
      "Its external  AUC: 0.5968173940444305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.27it/s, train_loss=8.47, val_loss=2.08, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 32.84it/s, train_loss=8.47, val_loss=2.08, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.84it/s, train_loss=8.47, val_loss=2.08, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.84it/s, train_loss=12.4, val_loss=0.887, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.84it/s, train_loss=12.4, val_loss=0.887, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.84it/s, train_loss=6.07, val_loss=1.41, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.84it/s, train_loss=6.07, val_loss=1.41, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.84it/s, train_loss=8.55, val_loss=0.581,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.84it/s, train_loss=8.55, val_loss=0.581,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.84it/s, train_loss=3.5, val_loss=1.1, tr\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 33.23it/s, train_loss=3.5, val_loss=1.1, t\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.23it/s, train_loss=3.5, val_loss=1.1, t\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.23it/s, train_loss=5.48, val_loss=0.334\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.23it/s, train_loss=5.48, val_loss=0.334\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.23it/s, train_loss=1.91, val_loss=0.272\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.23it/s, train_loss=1.91, val_loss=0.272\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.23it/s, train_loss=1.5, val_loss=0.147,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.23it/s, train_loss=1.5, val_loss=0.147,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.23it/s, train_loss=0.767, val_loss=0.14\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 33.72it/s, train_loss=0.767, val_loss=0.14\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.72it/s, train_loss=0.767, val_loss=0.14\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.72it/s, train_loss=0.904, val_loss=0.11\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.72it/s, train_loss=0.904, val_loss=0.11\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.72it/s, train_loss=0.782, val_loss=0.19\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.72it/s, train_loss=0.782, val_loss=0.19\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.72it/s, train_loss=1.1, val_loss=0.157,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.72it/s, train_loss=1.1, val_loss=0.157,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.72it/s, train_loss=0.97, val_loss=0.148\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.09it/s, train_loss=0.97, val_loss=0.148\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.09it/s, train_loss=0.97, val_loss=0.148\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.09it/s, train_loss=1.03, val_loss=0.168\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.09it/s, train_loss=1.03, val_loss=0.168\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.09it/s, train_loss=1.13, val_loss=0.184\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.09it/s, train_loss=1.13, val_loss=0.184\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.09it/s, train_loss=1.27, val_loss=0.18,\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.09it/s, train_loss=1.27, val_loss=0.18,\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.09it/s, train_loss=1.24, val_loss=0.147\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.31it/s, train_loss=1.24, val_loss=0.147\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.31it/s, train_loss=1.24, val_loss=0.147\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.31it/s, train_loss=1.02, val_loss=0.143\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.31it/s, train_loss=1.02, val_loss=0.143\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.31it/s, train_loss=0.952, val_loss=0.13\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.31it/s, train_loss=0.952, val_loss=0.13\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.31it/s, train_loss=0.917, val_loss=0.10\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.31it/s, train_loss=0.917, val_loss=0.10\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.31it/s, train_loss=0.728, val_loss=0.10\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.43it/s, train_loss=0.728, val_loss=0.10\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.43it/s, train_loss=0.728, val_loss=0.10\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.43it/s, train_loss=0.688, val_loss=0.09\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.43it/s, train_loss=0.688, val_loss=0.09\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    23: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.43it/s, train_loss=0.62, val_loss=0.068\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.43it/s, train_loss=0.62, val_loss=0.068\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.43it/s, train_loss=0.442, val_loss=0.06\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.43it/s, train_loss=0.442, val_loss=0.06\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.43it/s, train_loss=0.406, val_loss=0.05\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.49it/s, train_loss=0.406, val_loss=0.05\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.49it/s, train_loss=0.406, val_loss=0.05\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.49it/s, train_loss=0.345, val_loss=0.03\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.49it/s, train_loss=0.345, val_loss=0.03\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.49it/s, train_loss=0.243, val_loss=0.04\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.49it/s, train_loss=0.243, val_loss=0.04\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.49it/s, train_loss=0.257, val_loss=0.03\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.49it/s, train_loss=0.257, val_loss=0.03\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.49it/s, train_loss=0.212, val_loss=0.02\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.54it/s, train_loss=0.212, val_loss=0.02\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.54it/s, train_loss=0.212, val_loss=0.02\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.54it/s, train_loss=0.161, val_loss=0.03\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.54it/s, train_loss=0.161, val_loss=0.03\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.54it/s, train_loss=0.186, val_loss=0.02\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.54it/s, train_loss=0.186, val_loss=0.02\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.54it/s, train_loss=0.153, val_loss=0.02\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.54it/s, train_loss=0.153, val_loss=0.02\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.54it/s, train_loss=0.157, val_loss=0.02\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.57it/s, train_loss=0.157, val_loss=0.02\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.57it/s, train_loss=0.157, val_loss=0.02\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.57it/s, train_loss=0.187, val_loss=0.02\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.57it/s, train_loss=0.187, val_loss=0.02\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.57it/s, train_loss=0.163, val_loss=0.02\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.57it/s, train_loss=0.163, val_loss=0.02\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.57it/s, train_loss=0.183, val_loss=0.02\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.57it/s, train_loss=0.183, val_loss=0.02\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.57it/s, train_loss=0.182, val_loss=0.02\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.61it/s, train_loss=0.182, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.61it/s, train_loss=0.182, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.61it/s, train_loss=0.165, val_loss=0.02\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.61it/s, train_loss=0.165, val_loss=0.02\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.61it/s, train_loss=0.18, val_loss=0.024\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.61it/s, train_loss=0.18, val_loss=0.024\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.61it/s, train_loss=0.168, val_loss=0.02\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.61it/s, train_loss=0.168, val_loss=0.02\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.61it/s, train_loss=0.152, val_loss=0.02\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.61it/s, train_loss=0.152, val_loss=0.02\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.61it/s, train_loss=0.152, val_loss=0.02\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.61it/s, train_loss=0.152, val_loss=0.02\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.61it/s, train_loss=0.152, val_loss=0.02\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.25it/s, train_loss=0.149, val_loss=0.02\u001b[A\n",
      " 34%|██████████████▎                           | 34/100 [00:53<01:40,  1.52s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=33.3, val_loss=49.4, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=33.3, val_loss=49.4, train_cin\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    45: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Curent best Test AUC: 0.3271133375078272\n",
      "Its' val AUC : 0.30637488106565175\n",
      "Its external  AUC: 0.38632424767606743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=283, val_loss=24.3, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=283, val_loss=24.3, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=110, val_loss=3.02, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=110, val_loss=3.02, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=21.1, val_loss=1.24, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 31.60it/s, train_loss=21.1, val_loss=1.24, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.60it/s, train_loss=21.1, val_loss=1.24, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.60it/s, train_loss=6.12, val_loss=1.79, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.60it/s, train_loss=6.12, val_loss=1.79, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.60it/s, train_loss=10.6, val_loss=1.82, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.60it/s, train_loss=10.6, val_loss=1.82, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.60it/s, train_loss=11, val_loss=0.868, tr\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.60it/s, train_loss=11, val_loss=0.868, tr\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.60it/s, train_loss=4.9, val_loss=1.24, tr\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 32.98it/s, train_loss=4.9, val_loss=1.24, tr\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.98it/s, train_loss=4.9, val_loss=1.24, tr\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7603005635566688\n",
      "Its' val AUC : 0.7032757917629469\n",
      "Its external  AUC: 0.5879943280289901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.98it/s, train_loss=5.72, val_loss=1.65, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.98it/s, train_loss=5.72, val_loss=1.65, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.98it/s, train_loss=9.63, val_loss=1.19, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.98it/s, train_loss=9.63, val_loss=1.19, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.98it/s, train_loss=5.67, val_loss=0.467,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.98it/s, train_loss=5.67, val_loss=0.467,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.98it/s, train_loss=2.04, val_loss=1.08, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 33.56it/s, train_loss=2.04, val_loss=1.08,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.56it/s, train_loss=2.04, val_loss=1.08,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.56it/s, train_loss=5.12, val_loss=1.01,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.56it/s, train_loss=5.12, val_loss=1.01,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.56it/s, train_loss=6.4, val_loss=0.439,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.56it/s, train_loss=6.4, val_loss=0.439,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.56it/s, train_loss=2.54, val_loss=0.314\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.56it/s, train_loss=2.54, val_loss=0.314\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.56it/s, train_loss=1.91, val_loss=0.975\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 33.93it/s, train_loss=1.91, val_loss=0.975\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.93it/s, train_loss=1.91, val_loss=0.975\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.93it/s, train_loss=5.22, val_loss=0.806\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.93it/s, train_loss=5.22, val_loss=0.806\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.93it/s, train_loss=4.82, val_loss=0.324\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.93it/s, train_loss=4.82, val_loss=0.324\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.93it/s, train_loss=1.92, val_loss=0.204\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.93it/s, train_loss=1.92, val_loss=0.204\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.93it/s, train_loss=1.4, val_loss=0.647,\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.16it/s, train_loss=1.4, val_loss=0.647,\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.16it/s, train_loss=1.4, val_loss=0.647,\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.16it/s, train_loss=3.42, val_loss=1.09,\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.16it/s, train_loss=3.42, val_loss=1.09,\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.16it/s, train_loss=5.94, val_loss=1.46,\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.16it/s, train_loss=5.94, val_loss=1.46,\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.16it/s, train_loss=7.24, val_loss=0.535\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.16it/s, train_loss=7.24, val_loss=0.535\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.16it/s, train_loss=2.97, val_loss=0.157\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.32it/s, train_loss=2.97, val_loss=0.157\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.32it/s, train_loss=2.97, val_loss=0.157\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.32it/s, train_loss=0.97, val_loss=0.732\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.32it/s, train_loss=0.97, val_loss=0.732\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.32it/s, train_loss=3.44, val_loss=0.844\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.32it/s, train_loss=3.44, val_loss=0.844\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.32it/s, train_loss=4.88, val_loss=0.719\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.32it/s, train_loss=4.88, val_loss=0.719\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.32it/s, train_loss=3.22, val_loss=0.118\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.43it/s, train_loss=3.22, val_loss=0.118\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.43it/s, train_loss=3.22, val_loss=0.118\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.43it/s, train_loss=0.663, val_loss=0.34\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.43it/s, train_loss=0.663, val_loss=0.34\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.43it/s, train_loss=1.68, val_loss=0.789\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.43it/s, train_loss=1.68, val_loss=0.789\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.43it/s, train_loss=4.13, val_loss=0.758\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.43it/s, train_loss=4.13, val_loss=0.758\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.43it/s, train_loss=3.27, val_loss=0.21,\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.42it/s, train_loss=3.27, val_loss=0.21,\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.42it/s, train_loss=3.27, val_loss=0.21,\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.42it/s, train_loss=1.04, val_loss=0.138\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.42it/s, train_loss=1.04, val_loss=0.138\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.42it/s, train_loss=0.637, val_loss=0.42\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.42it/s, train_loss=0.637, val_loss=0.42\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.42it/s, train_loss=2.25, val_loss=0.098\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.42it/s, train_loss=2.25, val_loss=0.098\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.42it/s, train_loss=0.481, val_loss=0.39\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.481, val_loss=0.39\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.481, val_loss=0.39\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=1.87, val_loss=0.068\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=1.87, val_loss=0.068\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.309, val_loss=0.30\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.309, val_loss=0.30\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=1.6, val_loss=0.0772\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=1.6, val_loss=0.0772\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.372, val_loss=0.28\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.58it/s, train_loss=0.372, val_loss=0.28\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.58it/s, train_loss=0.372, val_loss=0.28\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    34: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.58it/s, train_loss=1.33, val_loss=0.077\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.58it/s, train_loss=1.33, val_loss=0.077\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.58it/s, train_loss=0.354, val_loss=0.19\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.58it/s, train_loss=0.354, val_loss=0.19\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.58it/s, train_loss=1.09, val_loss=0.061\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.58it/s, train_loss=1.09, val_loss=0.061\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.58it/s, train_loss=0.365, val_loss=0.20\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.60it/s, train_loss=0.365, val_loss=0.20\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.60it/s, train_loss=0.365, val_loss=0.20\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.60it/s, train_loss=0.943, val_loss=0.07\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.60it/s, train_loss=0.943, val_loss=0.07\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.60it/s, train_loss=0.344, val_loss=0.14\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.60it/s, train_loss=0.344, val_loss=0.14\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.60it/s, train_loss=0.771, val_loss=0.06\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.60it/s, train_loss=0.771, val_loss=0.06\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.60it/s, train_loss=0.291, val_loss=0.14\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.71it/s, train_loss=0.291, val_loss=0.14\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.71it/s, train_loss=0.291, val_loss=0.14\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.71it/s, train_loss=0.692, val_loss=0.06\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.71it/s, train_loss=0.692, val_loss=0.06\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.31it/s, train_loss=0.29, val_loss=0.124\u001b[A\n",
      " 35%|██████████████▋                           | 35/100 [00:54<01:38,  1.51s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=9.31, val_loss=0.419, train_ci\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=9.31, val_loss=0.419, train_ci\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=2.29, val_loss=2.49, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=2.29, val_loss=2.49, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=13, val_loss=1.59, train_cinde\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=13, val_loss=1.59, train_cinde\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=9.29, val_loss=2.43, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.32it/s, train_loss=9.29, val_loss=2.43, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.32it/s, train_loss=9.29, val_loss=2.43, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.32it/s, train_loss=14.5, val_loss=2.86, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.32it/s, train_loss=14.5, val_loss=2.86, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.32it/s, train_loss=16.4, val_loss=1.35, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.32it/s, train_loss=16.4, val_loss=1.35, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.32it/s, train_loss=8.3, val_loss=0.961, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.32it/s, train_loss=8.3, val_loss=0.961, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.5922354414527239\n",
      "Its' val AUC : 0.6108468125594672\n",
      "Its external  AUC: 0.5271781944225619\n",
      "Curent best Test AUC: 0.613525360050094\n",
      "Its' val AUC : 0.6653527252956368\n",
      "Its external  AUC: 0.3937293209390263\n",
      "Epoch     7: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.32it/s, train_loss=6.32, val_loss=0.741, \u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 34.02it/s, train_loss=6.32, val_loss=0.741, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.02it/s, train_loss=6.32, val_loss=0.741, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.02it/s, train_loss=4.3, val_loss=0.882, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.02it/s, train_loss=4.3, val_loss=0.882, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.02it/s, train_loss=4.49, val_loss=0.385, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.02it/s, train_loss=4.49, val_loss=0.385,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.02it/s, train_loss=2.14, val_loss=0.217,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.02it/s, train_loss=2.14, val_loss=0.217,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.02it/s, train_loss=1.45, val_loss=0.477,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.08it/s, train_loss=1.45, val_loss=0.477\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.08it/s, train_loss=1.45, val_loss=0.477\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.08it/s, train_loss=2.72, val_loss=0.38,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.08it/s, train_loss=2.72, val_loss=0.38,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.08it/s, train_loss=2.39, val_loss=0.262\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.08it/s, train_loss=2.39, val_loss=0.262\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.08it/s, train_loss=1.86, val_loss=0.188\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.08it/s, train_loss=1.86, val_loss=0.188\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.6478396994364434\n",
      "Its' val AUC : 0.7213538126953921\n",
      "Its external  AUC: 0.4813297620923271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.08it/s, train_loss=1.2, val_loss=0.0549\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.27it/s, train_loss=1.2, val_loss=0.0549\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.27it/s, train_loss=1.2, val_loss=0.0549\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.27it/s, train_loss=0.327, val_loss=0.06\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.27it/s, train_loss=0.327, val_loss=0.06\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.27it/s, train_loss=0.397, val_loss=0.09\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.27it/s, train_loss=0.397, val_loss=0.09\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.27it/s, train_loss=0.642, val_loss=0.13\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.27it/s, train_loss=0.642, val_loss=0.13\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.27it/s, train_loss=0.831, val_loss=0.17\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 33.94it/s, train_loss=0.831, val_loss=0.17\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.94it/s, train_loss=0.831, val_loss=0.17\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.94it/s, train_loss=0.983, val_loss=0.09\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.94it/s, train_loss=0.983, val_loss=0.09\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.94it/s, train_loss=0.599, val_loss=0.04\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.94it/s, train_loss=0.599, val_loss=0.04\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.94it/s, train_loss=0.356, val_loss=0.04\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.94it/s, train_loss=0.356, val_loss=0.04\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.94it/s, train_loss=0.335, val_loss=0.06\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.19it/s, train_loss=0.335, val_loss=0.06\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.19it/s, train_loss=0.335, val_loss=0.06\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.19it/s, train_loss=0.413, val_loss=0.10\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.19it/s, train_loss=0.413, val_loss=0.10\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.19it/s, train_loss=0.679, val_loss=0.09\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.19it/s, train_loss=0.679, val_loss=0.09\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7060738885410144\n",
      "Its' val AUC : 0.6293326084001631\n",
      "Its external  AUC: 0.5631006774854261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.19it/s, train_loss=0.628, val_loss=0.08\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.19it/s, train_loss=0.628, val_loss=0.08\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.19it/s, train_loss=0.477, val_loss=0.05\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.30it/s, train_loss=0.477, val_loss=0.05\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.30it/s, train_loss=0.477, val_loss=0.05\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.30it/s, train_loss=0.319, val_loss=0.03\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.30it/s, train_loss=0.319, val_loss=0.03\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.30it/s, train_loss=0.222, val_loss=0.02\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.30it/s, train_loss=0.222, val_loss=0.02\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.30it/s, train_loss=0.202, val_loss=0.03\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.30it/s, train_loss=0.202, val_loss=0.03\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.30it/s, train_loss=0.246, val_loss=0.03\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.42it/s, train_loss=0.246, val_loss=0.03\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.42it/s, train_loss=0.246, val_loss=0.03\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.42it/s, train_loss=0.254, val_loss=0.03\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.42it/s, train_loss=0.254, val_loss=0.03\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.42it/s, train_loss=0.231, val_loss=0.02\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.42it/s, train_loss=0.231, val_loss=0.02\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    28: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.42it/s, train_loss=0.227, val_loss=0.02\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.42it/s, train_loss=0.227, val_loss=0.02\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.42it/s, train_loss=0.2, val_loss=0.0169\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.56it/s, train_loss=0.2, val_loss=0.0169\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.56it/s, train_loss=0.2, val_loss=0.0169\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.56it/s, train_loss=0.132, val_loss=0.01\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.56it/s, train_loss=0.132, val_loss=0.01\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.56it/s, train_loss=0.0916, val_loss=0.0\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.56it/s, train_loss=0.0916, val_loss=0.0\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.56it/s, train_loss=0.0937, val_loss=0.0\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.56it/s, train_loss=0.0937, val_loss=0.0\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.56it/s, train_loss=0.0903, val_loss=0.0\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.66it/s, train_loss=0.0903, val_loss=0.0\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.66it/s, train_loss=0.0903, val_loss=0.0\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.66it/s, train_loss=0.0869, val_loss=0.0\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.66it/s, train_loss=0.0869, val_loss=0.0\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.66it/s, train_loss=0.104, val_loss=0.01\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.66it/s, train_loss=0.104, val_loss=0.01\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.66it/s, train_loss=0.105, val_loss=0.01\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.66it/s, train_loss=0.105, val_loss=0.01\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.66it/s, train_loss=0.0793, val_loss=0.0\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.65it/s, train_loss=0.0793, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.65it/s, train_loss=0.0793, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.65it/s, train_loss=0.0682, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.65it/s, train_loss=0.0682, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.65it/s, train_loss=0.0612, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.65it/s, train_loss=0.0612, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.65it/s, train_loss=0.0559, val_loss=0.0\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.65it/s, train_loss=0.0559, val_loss=0.0\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.65it/s, train_loss=0.0498, val_loss=0.0\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.73it/s, train_loss=0.0498, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.73it/s, train_loss=0.0498, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.73it/s, train_loss=0.0458, val_loss=0.0\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.73it/s, train_loss=0.0458, val_loss=0.0\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    43: reducing learning rate of group 0 to 1.2500e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.42it/s, train_loss=0.0471, val_loss=0.0\u001b[A\n",
      " 36%|███████████████                           | 36/100 [00:55<01:35,  1.50s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=4.44, val_loss=117, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=4.44, val_loss=117, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=623, val_loss=8.2, train_cinde\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=623, val_loss=8.2, train_cinde\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=39.6, val_loss=11.4, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=39.6, val_loss=11.4, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=61.1, val_loss=1.53, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.97it/s, train_loss=61.1, val_loss=1.53, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.97it/s, train_loss=61.1, val_loss=1.53, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.97it/s, train_loss=8.84, val_loss=0.242, \u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.97it/s, train_loss=8.84, val_loss=0.242, \u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.97it/s, train_loss=1.46, val_loss=0.443, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.97it/s, train_loss=1.46, val_loss=0.443, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.97it/s, train_loss=2.28, val_loss=2.31, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.97it/s, train_loss=2.28, val_loss=2.31, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.24470882905447713\n",
      "Its' val AUC : 0.28000543699877667\n",
      "Its external  AUC: 0.3896328974318576\n",
      "Curent best Test AUC: 0.7079524107701941\n",
      "Its' val AUC : 0.6794889221149926\n",
      "Its external  AUC: 0.603277138805735\n",
      "Curent best Test AUC: 0.719724483406387\n",
      "Its' val AUC : 0.6922658692401794\n",
      "Its external  AUC: 0.6231290373404759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.97it/s, train_loss=11.8, val_loss=1.84, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 34.13it/s, train_loss=11.8, val_loss=1.84, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.13it/s, train_loss=11.8, val_loss=1.84, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.13it/s, train_loss=9.91, val_loss=1.18, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.13it/s, train_loss=9.91, val_loss=1.18, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.13it/s, train_loss=6.6, val_loss=0.406, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.13it/s, train_loss=6.6, val_loss=0.406, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.13it/s, train_loss=1.97, val_loss=0.793,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.13it/s, train_loss=1.97, val_loss=0.793,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.13it/s, train_loss=4.21, val_loss=0.428,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.12it/s, train_loss=4.21, val_loss=0.428\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.12it/s, train_loss=4.21, val_loss=0.428\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.12it/s, train_loss=2.6, val_loss=0.153,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.12it/s, train_loss=2.6, val_loss=0.153,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.12it/s, train_loss=0.825, val_loss=0.30\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.12it/s, train_loss=0.825, val_loss=0.30\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.12it/s, train_loss=1.78, val_loss=0.416\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.12it/s, train_loss=1.78, val_loss=0.416\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.12it/s, train_loss=2.35, val_loss=0.198\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.36it/s, train_loss=2.35, val_loss=0.198\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.36it/s, train_loss=2.35, val_loss=0.198\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7426424546023794\n",
      "Its' val AUC : 0.6910425445154275\n",
      "Its external  AUC: 0.5913029777847802\n",
      "Epoch    11: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.36it/s, train_loss=1.21, val_loss=0.103\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.36it/s, train_loss=1.21, val_loss=0.103\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.36it/s, train_loss=0.744, val_loss=0.25\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.36it/s, train_loss=0.744, val_loss=0.25\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.36it/s, train_loss=1.6, val_loss=0.221,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.36it/s, train_loss=1.6, val_loss=0.221,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.36it/s, train_loss=1.5, val_loss=0.0752\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.51it/s, train_loss=1.5, val_loss=0.0752\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.51it/s, train_loss=1.5, val_loss=0.0752\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.51it/s, train_loss=0.712, val_loss=0.15\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.51it/s, train_loss=0.712, val_loss=0.15\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.51it/s, train_loss=1.02, val_loss=0.245\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.51it/s, train_loss=1.02, val_loss=0.245\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.51it/s, train_loss=1.42, val_loss=0.108\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.51it/s, train_loss=1.42, val_loss=0.108\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.51it/s, train_loss=0.735, val_loss=0.06\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.65it/s, train_loss=0.735, val_loss=0.06\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.65it/s, train_loss=0.735, val_loss=0.06\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.65it/s, train_loss=0.535, val_loss=0.14\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.65it/s, train_loss=0.535, val_loss=0.14\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.65it/s, train_loss=0.921, val_loss=0.09\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.65it/s, train_loss=0.921, val_loss=0.09\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.65it/s, train_loss=0.63, val_loss=0.038\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.65it/s, train_loss=0.63, val_loss=0.038\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.65it/s, train_loss=0.294, val_loss=0.10\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.71it/s, train_loss=0.294, val_loss=0.10\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.71it/s, train_loss=0.294, val_loss=0.10\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.71it/s, train_loss=0.577, val_loss=0.09\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.71it/s, train_loss=0.577, val_loss=0.09\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.71it/s, train_loss=0.532, val_loss=0.03\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.71it/s, train_loss=0.532, val_loss=0.03\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.71it/s, train_loss=0.195, val_loss=0.05\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.71it/s, train_loss=0.195, val_loss=0.05\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.71it/s, train_loss=0.336, val_loss=0.06\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.70it/s, train_loss=0.336, val_loss=0.06\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.70it/s, train_loss=0.336, val_loss=0.06\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.70it/s, train_loss=0.408, val_loss=0.02\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.70it/s, train_loss=0.408, val_loss=0.02\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.70it/s, train_loss=0.152, val_loss=0.04\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.70it/s, train_loss=0.152, val_loss=0.04\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.70it/s, train_loss=0.212, val_loss=0.06\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.70it/s, train_loss=0.212, val_loss=0.06\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.70it/s, train_loss=0.322, val_loss=0.02\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.74it/s, train_loss=0.322, val_loss=0.02\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.74it/s, train_loss=0.322, val_loss=0.02\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.74it/s, train_loss=0.142, val_loss=0.02\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.74it/s, train_loss=0.142, val_loss=0.02\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.74it/s, train_loss=0.153, val_loss=0.05\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.74it/s, train_loss=0.153, val_loss=0.05\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.74it/s, train_loss=0.263, val_loss=0.02\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.74it/s, train_loss=0.263, val_loss=0.02\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.74it/s, train_loss=0.139, val_loss=0.02\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.74it/s, train_loss=0.139, val_loss=0.02\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.74it/s, train_loss=0.139, val_loss=0.02\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.74it/s, train_loss=0.107, val_loss=0.02\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.74it/s, train_loss=0.107, val_loss=0.02\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.74it/s, train_loss=0.137, val_loss=0.03\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.74it/s, train_loss=0.137, val_loss=0.03\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.74it/s, train_loss=0.165, val_loss=0.02\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.74it/s, train_loss=0.165, val_loss=0.02\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.74it/s, train_loss=0.142, val_loss=0.02\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.71it/s, train_loss=0.142, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.71it/s, train_loss=0.142, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.71it/s, train_loss=0.106, val_loss=0.02\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.71it/s, train_loss=0.106, val_loss=0.02\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    39: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.71it/s, train_loss=0.11, val_loss=0.027\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.71it/s, train_loss=0.11, val_loss=0.027\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.71it/s, train_loss=0.132, val_loss=0.02\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.71it/s, train_loss=0.132, val_loss=0.02\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.71it/s, train_loss=0.123, val_loss=0.01\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.72it/s, train_loss=0.123, val_loss=0.01\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.72it/s, train_loss=0.123, val_loss=0.01\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.72it/s, train_loss=0.0952, val_loss=0.0\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.72it/s, train_loss=0.0952, val_loss=0.0\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.59it/s, train_loss=0.0913, val_loss=0.0\u001b[A\n",
      " 37%|███████████████▌                          | 37/100 [00:57<01:33,  1.49s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=19.3, val_loss=1.96, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=19.3, val_loss=1.96, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=12.1, val_loss=91.7, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=12.1, val_loss=91.7, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=450, val_loss=3.97, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=450, val_loss=3.97, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=20.9, val_loss=29, train_cinde\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.94it/s, train_loss=20.9, val_loss=29, tra\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.94it/s, train_loss=20.9, val_loss=29, tra\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.94it/s, train_loss=153, val_loss=3.41, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.94it/s, train_loss=153, val_loss=3.41, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.94it/s, train_loss=16.7, val_loss=4.3, tr\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.94it/s, train_loss=16.7, val_loss=4.3, tr\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.94it/s, train_loss=23.2, val_loss=5.29, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.94it/s, train_loss=23.2, val_loss=5.29, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.33074514715090797\n",
      "Its' val AUC : 0.3043360065243985\n",
      "Its external  AUC: 0.5079565148889239\n",
      "Curent best Test AUC: 0.7655604257983719\n",
      "Its' val AUC : 0.724887861900231\n",
      "Its external  AUC: 0.6031195840554593\n",
      "Epoch     7: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.94it/s, train_loss=25.2, val_loss=2.69, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 33.96it/s, train_loss=25.2, val_loss=2.69, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.96it/s, train_loss=25.2, val_loss=2.69, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.96it/s, train_loss=17.3, val_loss=3.16, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.96it/s, train_loss=17.3, val_loss=3.16, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.96it/s, train_loss=22.3, val_loss=1.38, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.96it/s, train_loss=22.3, val_loss=1.38, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.96it/s, train_loss=11.3, val_loss=0.921,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.96it/s, train_loss=11.3, val_loss=0.921,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.96it/s, train_loss=5.2, val_loss=0.331, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.36it/s, train_loss=5.2, val_loss=0.331,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.36it/s, train_loss=5.2, val_loss=0.331,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.36it/s, train_loss=2.23, val_loss=0.301\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.36it/s, train_loss=2.23, val_loss=0.301\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.36it/s, train_loss=1.7, val_loss=0.64, \u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.36it/s, train_loss=1.7, val_loss=0.64, \u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.36it/s, train_loss=3.2, val_loss=0.671,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.36it/s, train_loss=3.2, val_loss=0.671,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.36it/s, train_loss=3.31, val_loss=0.452\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.62it/s, train_loss=3.31, val_loss=0.452\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.62it/s, train_loss=3.31, val_loss=0.452\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.62it/s, train_loss=2.26, val_loss=0.18,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.62it/s, train_loss=2.26, val_loss=0.18,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.62it/s, train_loss=0.918, val_loss=0.07\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.62it/s, train_loss=0.918, val_loss=0.07\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.62it/s, train_loss=0.399, val_loss=0.22\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.62it/s, train_loss=0.399, val_loss=0.22\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.62it/s, train_loss=1.1, val_loss=0.396,\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.70it/s, train_loss=1.1, val_loss=0.396,\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.70it/s, train_loss=1.1, val_loss=0.396,\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.70it/s, train_loss=1.89, val_loss=0.355\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.70it/s, train_loss=1.89, val_loss=0.355\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.70it/s, train_loss=1.65, val_loss=0.168\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.70it/s, train_loss=1.65, val_loss=0.168\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.70it/s, train_loss=0.76, val_loss=0.070\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.70it/s, train_loss=0.76, val_loss=0.070\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.70it/s, train_loss=0.386, val_loss=0.12\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.58it/s, train_loss=0.386, val_loss=0.12\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.58it/s, train_loss=0.386, val_loss=0.12\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.58it/s, train_loss=0.796, val_loss=0.19\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.58it/s, train_loss=0.796, val_loss=0.19\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.58it/s, train_loss=1.14, val_loss=0.138\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.58it/s, train_loss=1.14, val_loss=0.138\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.58it/s, train_loss=0.828, val_loss=0.04\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.58it/s, train_loss=0.828, val_loss=0.04\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.58it/s, train_loss=0.313, val_loss=0.05\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.70it/s, train_loss=0.313, val_loss=0.05\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.70it/s, train_loss=0.313, val_loss=0.05\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.70it/s, train_loss=0.325, val_loss=0.13\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.70it/s, train_loss=0.325, val_loss=0.13\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.70it/s, train_loss=0.719, val_loss=0.15\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.70it/s, train_loss=0.719, val_loss=0.15\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.70it/s, train_loss=0.779, val_loss=0.07\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.70it/s, train_loss=0.779, val_loss=0.07\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.70it/s, train_loss=0.4, val_loss=0.0289\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.73it/s, train_loss=0.4, val_loss=0.0289\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.73it/s, train_loss=0.4, val_loss=0.0289\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.73it/s, train_loss=0.173, val_loss=0.06\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.73it/s, train_loss=0.173, val_loss=0.06\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.73it/s, train_loss=0.346, val_loss=0.09\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.73it/s, train_loss=0.346, val_loss=0.09\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.73it/s, train_loss=0.511, val_loss=0.06\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.73it/s, train_loss=0.511, val_loss=0.06\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.73it/s, train_loss=0.364, val_loss=0.02\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.71it/s, train_loss=0.364, val_loss=0.02\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.71it/s, train_loss=0.364, val_loss=0.02\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.71it/s, train_loss=0.158, val_loss=0.04\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.71it/s, train_loss=0.158, val_loss=0.04\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.71it/s, train_loss=0.205, val_loss=0.07\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.71it/s, train_loss=0.205, val_loss=0.07\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.71it/s, train_loss=0.36, val_loss=0.063\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.71it/s, train_loss=0.36, val_loss=0.063\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.71it/s, train_loss=0.318, val_loss=0.02\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.49it/s, train_loss=0.318, val_loss=0.02\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.49it/s, train_loss=0.318, val_loss=0.02\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.49it/s, train_loss=0.15, val_loss=0.026\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.49it/s, train_loss=0.15, val_loss=0.026\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.49it/s, train_loss=0.13, val_loss=0.049\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.49it/s, train_loss=0.13, val_loss=0.049\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.49it/s, train_loss=0.24, val_loss=0.052\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.49it/s, train_loss=0.24, val_loss=0.052\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.49it/s, train_loss=0.251, val_loss=0.03\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.63it/s, train_loss=0.251, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.63it/s, train_loss=0.251, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.63it/s, train_loss=0.149, val_loss=0.02\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.63it/s, train_loss=0.149, val_loss=0.02\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.63it/s, train_loss=0.123, val_loss=0.03\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.63it/s, train_loss=0.123, val_loss=0.03\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.63it/s, train_loss=0.193, val_loss=0.03\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.63it/s, train_loss=0.193, val_loss=0.03\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.63it/s, train_loss=0.199, val_loss=0.02\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.72it/s, train_loss=0.199, val_loss=0.02\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.72it/s, train_loss=0.199, val_loss=0.02\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.72it/s, train_loss=0.119, val_loss=0.02\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.72it/s, train_loss=0.119, val_loss=0.02\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.60it/s, train_loss=0.0949, val_loss=0.0\u001b[A\n",
      " 38%|███████████████▉                          | 38/100 [00:58<01:31,  1.48s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=4.78, val_loss=40.5, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=4.78, val_loss=40.5, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=210, val_loss=4.79, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=210, val_loss=4.79, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=22.6, val_loss=17.5, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=22.6, val_loss=17.5, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=83, val_loss=3.25, train_cinde\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.65it/s, train_loss=83, val_loss=3.25, tra\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.65it/s, train_loss=83, val_loss=3.25, tra\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.65it/s, train_loss=17.5, val_loss=7.41, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.65it/s, train_loss=17.5, val_loss=7.41, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.65it/s, train_loss=40.2, val_loss=0.486, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.65it/s, train_loss=40.2, val_loss=0.486, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.65it/s, train_loss=1.95, val_loss=6.05, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.65it/s, train_loss=1.95, val_loss=6.05, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.2670006261740764\n",
      "Its' val AUC : 0.278510262335191\n",
      "Its external  AUC: 0.38805734992910035\n",
      "Curent best Test AUC: 0.7233562930494678\n",
      "Its' val AUC : 0.6729645235829822\n",
      "Its external  AUC: 0.6177721758311013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.65it/s, train_loss=28.5, val_loss=0.253, \u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 34.05it/s, train_loss=28.5, val_loss=0.253, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.05it/s, train_loss=28.5, val_loss=0.253, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.05it/s, train_loss=1.17, val_loss=3.11, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.05it/s, train_loss=1.17, val_loss=3.11, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.05it/s, train_loss=15.2, val_loss=1.02, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.05it/s, train_loss=15.2, val_loss=1.02, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.05it/s, train_loss=4.73, val_loss=2.01, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.05it/s, train_loss=4.73, val_loss=2.01, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.05it/s, train_loss=10.6, val_loss=1.09, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.03it/s, train_loss=10.6, val_loss=1.09,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.03it/s, train_loss=10.6, val_loss=1.09,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.03it/s, train_loss=5.77, val_loss=1.17,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.03it/s, train_loss=5.77, val_loss=1.17,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.03it/s, train_loss=5.12, val_loss=1.62,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.03it/s, train_loss=5.12, val_loss=1.62,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.03it/s, train_loss=7.17, val_loss=0.481\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.03it/s, train_loss=7.17, val_loss=0.481\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.03it/s, train_loss=2.02, val_loss=0.397\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.35it/s, train_loss=2.02, val_loss=0.397\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.35it/s, train_loss=2.02, val_loss=0.397\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.35it/s, train_loss=2.13, val_loss=0.934\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.35it/s, train_loss=2.13, val_loss=0.934\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7577958672510958\n",
      "Its' val AUC : 0.7267908114720674\n",
      "Its external  AUC: 0.6026469198046321\n",
      "Epoch    14: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.35it/s, train_loss=4.75, val_loss=0.449\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.35it/s, train_loss=4.75, val_loss=0.449\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.35it/s, train_loss=2.19, val_loss=0.226\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.35it/s, train_loss=2.19, val_loss=0.226\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.35it/s, train_loss=1.27, val_loss=0.707\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.41it/s, train_loss=1.27, val_loss=0.707\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.41it/s, train_loss=1.27, val_loss=0.707\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.41it/s, train_loss=3.85, val_loss=0.487\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.41it/s, train_loss=3.85, val_loss=0.487\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.41it/s, train_loss=2.64, val_loss=0.124\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.41it/s, train_loss=2.64, val_loss=0.124\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.41it/s, train_loss=0.582, val_loss=0.47\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.41it/s, train_loss=0.582, val_loss=0.47\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.41it/s, train_loss=2.24, val_loss=0.471\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.56it/s, train_loss=2.24, val_loss=0.471\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.56it/s, train_loss=2.24, val_loss=0.471\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.56it/s, train_loss=2.33, val_loss=0.152\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.56it/s, train_loss=2.33, val_loss=0.152\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.56it/s, train_loss=0.746, val_loss=0.34\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.56it/s, train_loss=0.746, val_loss=0.34\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.56it/s, train_loss=1.6, val_loss=0.366,\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.56it/s, train_loss=1.6, val_loss=0.366,\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.56it/s, train_loss=1.71, val_loss=0.090\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.68it/s, train_loss=1.71, val_loss=0.090\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.68it/s, train_loss=1.71, val_loss=0.090\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.68it/s, train_loss=0.466, val_loss=0.20\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.68it/s, train_loss=0.466, val_loss=0.20\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.68it/s, train_loss=1.21, val_loss=0.262\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.68it/s, train_loss=1.21, val_loss=0.262\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.68it/s, train_loss=1.52, val_loss=0.065\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.68it/s, train_loss=1.52, val_loss=0.065\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.68it/s, train_loss=0.41, val_loss=0.19,\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.65it/s, train_loss=0.41, val_loss=0.19,\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.65it/s, train_loss=0.41, val_loss=0.19,\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.65it/s, train_loss=0.834, val_loss=0.26\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.65it/s, train_loss=0.834, val_loss=0.26\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.65it/s, train_loss=1.15, val_loss=0.098\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.65it/s, train_loss=1.15, val_loss=0.098\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.65it/s, train_loss=0.467, val_loss=0.13\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.65it/s, train_loss=0.467, val_loss=0.13\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.65it/s, train_loss=0.759, val_loss=0.14\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.67it/s, train_loss=0.759, val_loss=0.14\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.67it/s, train_loss=0.759, val_loss=0.14\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.67it/s, train_loss=0.816, val_loss=0.04\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.67it/s, train_loss=0.816, val_loss=0.04\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.67it/s, train_loss=0.283, val_loss=0.12\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.67it/s, train_loss=0.283, val_loss=0.12\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.67it/s, train_loss=0.665, val_loss=0.13\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.67it/s, train_loss=0.665, val_loss=0.13\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.67it/s, train_loss=0.679, val_loss=0.04\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.73it/s, train_loss=0.679, val_loss=0.04\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.73it/s, train_loss=0.679, val_loss=0.04\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.73it/s, train_loss=0.246, val_loss=0.10\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.73it/s, train_loss=0.246, val_loss=0.10\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.73it/s, train_loss=0.547, val_loss=0.08\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.73it/s, train_loss=0.547, val_loss=0.08\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.73it/s, train_loss=0.48, val_loss=0.054\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.73it/s, train_loss=0.48, val_loss=0.054\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.73it/s, train_loss=0.263, val_loss=0.10\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.75it/s, train_loss=0.263, val_loss=0.10\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.75it/s, train_loss=0.263, val_loss=0.10\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.75it/s, train_loss=0.502, val_loss=0.07\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.75it/s, train_loss=0.502, val_loss=0.07\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.75it/s, train_loss=0.333, val_loss=0.04\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.75it/s, train_loss=0.333, val_loss=0.04\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.75it/s, train_loss=0.212, val_loss=0.07\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.75it/s, train_loss=0.212, val_loss=0.07\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.75it/s, train_loss=0.412, val_loss=0.04\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.74it/s, train_loss=0.412, val_loss=0.04\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.74it/s, train_loss=0.412, val_loss=0.04\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.74it/s, train_loss=0.245, val_loss=0.04\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.74it/s, train_loss=0.245, val_loss=0.04\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.56it/s, train_loss=0.218, val_loss=0.06\u001b[A\n",
      " 39%|████████████████▍                         | 39/100 [01:00<01:30,  1.48s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=8.33, val_loss=65.5, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=8.33, val_loss=65.5, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=349, val_loss=0.558, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=349, val_loss=0.558, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=2.84, val_loss=2.7, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=2.84, val_loss=2.7, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=11.9, val_loss=0.467, train_ci\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.46it/s, train_loss=11.9, val_loss=0.467, \u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.46it/s, train_loss=11.9, val_loss=0.467, \u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.46it/s, train_loss=2.53, val_loss=1.7, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.46it/s, train_loss=2.53, val_loss=1.7, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.46it/s, train_loss=9.87, val_loss=0.253, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.46it/s, train_loss=9.87, val_loss=0.253, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.46it/s, train_loss=1.64, val_loss=0.935, \u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.46it/s, train_loss=1.64, val_loss=0.935, \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.24608641202254228\n",
      "Its' val AUC : 0.2775587875492728\n",
      "Its external  AUC: 0.37576807940759416\n",
      "Curent best Test AUC: 0.6780212899185973\n",
      "Its' val AUC : 0.6907706945765937\n",
      "Its external  AUC: 0.596502284543879\n",
      "Curent best Test AUC: 0.7107075767063243\n",
      "Its' val AUC : 0.7216256626342259\n",
      "Its external  AUC: 0.6010713723018749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.46it/s, train_loss=4.47, val_loss=0.468, \u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 34.24it/s, train_loss=4.47, val_loss=0.468, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.24it/s, train_loss=4.47, val_loss=0.468, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.24it/s, train_loss=2.55, val_loss=0.372, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.24it/s, train_loss=2.55, val_loss=0.372, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.24it/s, train_loss=2.09, val_loss=0.679, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.24it/s, train_loss=2.09, val_loss=0.679,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.24it/s, train_loss=3.52, val_loss=0.148,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.24it/s, train_loss=3.52, val_loss=0.148,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.24it/s, train_loss=0.887, val_loss=0.52,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.33it/s, train_loss=0.887, val_loss=0.52\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.33it/s, train_loss=0.887, val_loss=0.52\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.33it/s, train_loss=2.94, val_loss=0.045\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.33it/s, train_loss=2.94, val_loss=0.045\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.33it/s, train_loss=0.3, val_loss=0.501,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.33it/s, train_loss=0.3, val_loss=0.501,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.33it/s, train_loss=2.46, val_loss=0.111\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.33it/s, train_loss=2.46, val_loss=0.111\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.33it/s, train_loss=0.57, val_loss=0.437\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.50it/s, train_loss=0.57, val_loss=0.437\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.50it/s, train_loss=0.57, val_loss=0.437\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.50it/s, train_loss=2.18, val_loss=0.128\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.50it/s, train_loss=2.18, val_loss=0.128\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.50it/s, train_loss=0.643, val_loss=0.27\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.50it/s, train_loss=0.643, val_loss=0.27\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7243581715716969\n",
      "Its' val AUC : 0.7119749898056273\n",
      "Its external  AUC: 0.6012289270521506\n",
      "Epoch    19: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.50it/s, train_loss=1.48, val_loss=0.109\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.50it/s, train_loss=1.48, val_loss=0.109\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.50it/s, train_loss=0.634, val_loss=0.02\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.63it/s, train_loss=0.634, val_loss=0.02\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.63it/s, train_loss=0.634, val_loss=0.02\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.63it/s, train_loss=0.178, val_loss=0.12\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.63it/s, train_loss=0.178, val_loss=0.12\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.63it/s, train_loss=0.666, val_loss=0.09\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.63it/s, train_loss=0.666, val_loss=0.09\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.63it/s, train_loss=0.56, val_loss=0.018\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.63it/s, train_loss=0.56, val_loss=0.018\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.63it/s, train_loss=0.172, val_loss=0.08\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.54it/s, train_loss=0.172, val_loss=0.08\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.54it/s, train_loss=0.172, val_loss=0.08\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.54it/s, train_loss=0.512, val_loss=0.07\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.54it/s, train_loss=0.512, val_loss=0.07\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.54it/s, train_loss=0.478, val_loss=0.01\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.54it/s, train_loss=0.478, val_loss=0.01\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.54it/s, train_loss=0.136, val_loss=0.06\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.54it/s, train_loss=0.136, val_loss=0.06\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7556668753913588\n",
      "Its' val AUC : 0.698382492863939\n",
      "Its external  AUC: 0.6013864818024264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.54it/s, train_loss=0.349, val_loss=0.06\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.68it/s, train_loss=0.349, val_loss=0.06\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.68it/s, train_loss=0.349, val_loss=0.06\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.68it/s, train_loss=0.328, val_loss=0.01\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.68it/s, train_loss=0.328, val_loss=0.01\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.68it/s, train_loss=0.0818, val_loss=0.0\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.68it/s, train_loss=0.0818, val_loss=0.0\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.68it/s, train_loss=0.265, val_loss=0.03\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.68it/s, train_loss=0.265, val_loss=0.03\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.68it/s, train_loss=0.24, val_loss=0.014\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.75it/s, train_loss=0.24, val_loss=0.014\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.75it/s, train_loss=0.24, val_loss=0.014\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.75it/s, train_loss=0.0801, val_loss=0.0\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.75it/s, train_loss=0.0801, val_loss=0.0\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.75it/s, train_loss=0.24, val_loss=0.037\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.75it/s, train_loss=0.24, val_loss=0.037\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.75it/s, train_loss=0.179, val_loss=0.01\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.75it/s, train_loss=0.179, val_loss=0.01\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.75it/s, train_loss=0.0772, val_loss=0.0\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.72it/s, train_loss=0.0772, val_loss=0.0\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.72it/s, train_loss=0.0772, val_loss=0.0\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.72it/s, train_loss=0.116, val_loss=0.01\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.72it/s, train_loss=0.116, val_loss=0.01\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.72it/s, train_loss=0.113, val_loss=0.00\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.72it/s, train_loss=0.113, val_loss=0.00\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.72it/s, train_loss=0.0685, val_loss=0.0\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.72it/s, train_loss=0.0685, val_loss=0.0\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.72it/s, train_loss=0.0602, val_loss=0.0\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.79it/s, train_loss=0.0602, val_loss=0.0\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.79it/s, train_loss=0.0602, val_loss=0.0\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.79it/s, train_loss=0.0902, val_loss=0.0\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.79it/s, train_loss=0.0902, val_loss=0.0\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    35: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.79it/s, train_loss=0.0926, val_loss=0.0\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.79it/s, train_loss=0.0926, val_loss=0.0\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.79it/s, train_loss=0.064, val_loss=0.00\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.79it/s, train_loss=0.064, val_loss=0.00\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.79it/s, train_loss=0.0586, val_loss=0.0\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.81it/s, train_loss=0.0586, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.81it/s, train_loss=0.0586, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.81it/s, train_loss=0.0779, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.81it/s, train_loss=0.0779, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.81it/s, train_loss=0.0758, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.81it/s, train_loss=0.0758, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.81it/s, train_loss=0.0542, val_loss=0.0\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.81it/s, train_loss=0.0542, val_loss=0.0\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.81it/s, train_loss=0.0518, val_loss=0.0\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.84it/s, train_loss=0.0518, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.84it/s, train_loss=0.0518, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.84it/s, train_loss=0.0635, val_loss=0.0\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.84it/s, train_loss=0.0635, val_loss=0.0\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.65it/s, train_loss=0.0576, val_loss=0.0\u001b[A\n",
      " 40%|████████████████▊                         | 40/100 [01:01<01:28,  1.47s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=8.45, val_loss=33.2, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=8.45, val_loss=33.2, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=176, val_loss=204, train_cinde\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=176, val_loss=204, train_cinde\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=1.11e+3, val_loss=5.68, train_\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=1.11e+3, val_loss=5.68, train_\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=29.9, val_loss=43, train_cinde\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.84it/s, train_loss=29.9, val_loss=43, tra\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.84it/s, train_loss=29.9, val_loss=43, tra\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.84it/s, train_loss=235, val_loss=4.81, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.84it/s, train_loss=235, val_loss=4.81, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.84it/s, train_loss=28.5, val_loss=27.4, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.84it/s, train_loss=28.5, val_loss=27.4, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.84it/s, train_loss=130, val_loss=5.11, tr\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.84it/s, train_loss=130, val_loss=5.11, tr\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.24596117720726363\n",
      "Its' val AUC : 0.28095691178469484\n",
      "Its external  AUC: 0.36599968489049944\n",
      "Curent best Test AUC: 0.728616155291171\n",
      "Its' val AUC : 0.7157808889493\n",
      "Its external  AUC: 0.6017015913029777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.84it/s, train_loss=27.3, val_loss=3.37, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 34.40it/s, train_loss=27.3, val_loss=3.37, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.40it/s, train_loss=27.3, val_loss=3.37, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.40it/s, train_loss=19.3, val_loss=2.89, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.40it/s, train_loss=19.3, val_loss=2.89, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.40it/s, train_loss=18.4, val_loss=1.7, tr\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.40it/s, train_loss=18.4, val_loss=1.7, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.40it/s, train_loss=10.5, val_loss=2.38, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.40it/s, train_loss=10.5, val_loss=2.38, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.40it/s, train_loss=13, val_loss=1.23, tr\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.67it/s, train_loss=13, val_loss=1.23, t\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.67it/s, train_loss=13, val_loss=1.23, t\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.67it/s, train_loss=7.21, val_loss=1.26,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.67it/s, train_loss=7.21, val_loss=1.26,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.67it/s, train_loss=8.44, val_loss=0.541\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.67it/s, train_loss=8.44, val_loss=0.541\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.67it/s, train_loss=3.4, val_loss=0.877,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.67it/s, train_loss=3.4, val_loss=0.877,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.67it/s, train_loss=4.31, val_loss=0.222\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.83it/s, train_loss=4.31, val_loss=0.222\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.83it/s, train_loss=4.31, val_loss=0.222\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.83it/s, train_loss=1.51, val_loss=0.477\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.83it/s, train_loss=1.51, val_loss=0.477\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.83it/s, train_loss=2.74, val_loss=0.252\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.83it/s, train_loss=2.74, val_loss=0.252\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.83it/s, train_loss=1.9, val_loss=0.37, \u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.83it/s, train_loss=1.9, val_loss=0.37, \u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.83it/s, train_loss=2.88, val_loss=0.416\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.82it/s, train_loss=2.88, val_loss=0.416\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.82it/s, train_loss=2.88, val_loss=0.416\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.82it/s, train_loss=2.68, val_loss=0.364\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.82it/s, train_loss=2.68, val_loss=0.364\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.82it/s, train_loss=2.65, val_loss=0.359\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.82it/s, train_loss=2.65, val_loss=0.359\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.82it/s, train_loss=3.19, val_loss=0.291\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.82it/s, train_loss=3.19, val_loss=0.291\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.82it/s, train_loss=2.67, val_loss=0.332\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.87it/s, train_loss=2.67, val_loss=0.332\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.87it/s, train_loss=2.67, val_loss=0.332\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.87it/s, train_loss=2.46, val_loss=0.411\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.87it/s, train_loss=2.46, val_loss=0.411\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.87it/s, train_loss=2.63, val_loss=0.246\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.87it/s, train_loss=2.63, val_loss=0.246\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.87it/s, train_loss=1.9, val_loss=0.22, \u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.87it/s, train_loss=1.9, val_loss=0.22, \u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.87it/s, train_loss=1.91, val_loss=0.188\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.94it/s, train_loss=1.91, val_loss=0.188\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.94it/s, train_loss=1.91, val_loss=0.188\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    22: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.94it/s, train_loss=1.61, val_loss=0.142\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.94it/s, train_loss=1.61, val_loss=0.142\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.94it/s, train_loss=1.07, val_loss=0.182\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.94it/s, train_loss=1.07, val_loss=0.182\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.94it/s, train_loss=1.13, val_loss=0.093\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.94it/s, train_loss=1.13, val_loss=0.093\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.94it/s, train_loss=0.684, val_loss=0.09\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 35.00it/s, train_loss=0.684, val_loss=0.09\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 35.00it/s, train_loss=0.684, val_loss=0.09\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 35.00it/s, train_loss=0.681, val_loss=0.10\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 35.00it/s, train_loss=0.681, val_loss=0.10\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 35.00it/s, train_loss=0.667, val_loss=0.07\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 35.00it/s, train_loss=0.667, val_loss=0.07\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 35.00it/s, train_loss=0.477, val_loss=0.12\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 35.00it/s, train_loss=0.477, val_loss=0.12\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 35.00it/s, train_loss=0.698, val_loss=0.09\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.96it/s, train_loss=0.698, val_loss=0.09\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.96it/s, train_loss=0.698, val_loss=0.09\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.96it/s, train_loss=0.57, val_loss=0.126\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.96it/s, train_loss=0.57, val_loss=0.126\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.96it/s, train_loss=0.706, val_loss=0.13\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.96it/s, train_loss=0.706, val_loss=0.13\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.96it/s, train_loss=0.736, val_loss=0.12\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.96it/s, train_loss=0.736, val_loss=0.12\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.96it/s, train_loss=0.678, val_loss=0.14\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.95it/s, train_loss=0.678, val_loss=0.14\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.95it/s, train_loss=0.678, val_loss=0.14\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.95it/s, train_loss=0.776, val_loss=0.12\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.95it/s, train_loss=0.776, val_loss=0.12\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.95it/s, train_loss=0.683, val_loss=0.11\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.95it/s, train_loss=0.683, val_loss=0.11\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.95it/s, train_loss=0.636, val_loss=0.12\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.95it/s, train_loss=0.636, val_loss=0.12\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.95it/s, train_loss=0.678, val_loss=0.11\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.96it/s, train_loss=0.678, val_loss=0.11\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.96it/s, train_loss=0.678, val_loss=0.11\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.96it/s, train_loss=0.628, val_loss=0.1,\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.96it/s, train_loss=0.628, val_loss=0.1,\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.96it/s, train_loss=0.531, val_loss=0.10\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.96it/s, train_loss=0.531, val_loss=0.10\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    40: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch    46: reducing learning rate of group 0 to 1.2500e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.96it/s, train_loss=0.526, val_loss=0.10\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.96it/s, train_loss=0.526, val_loss=0.10\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.96it/s, train_loss=0.518, val_loss=0.09\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.92it/s, train_loss=0.518, val_loss=0.09\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.92it/s, train_loss=0.518, val_loss=0.09\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.92it/s, train_loss=0.477, val_loss=0.08\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.92it/s, train_loss=0.477, val_loss=0.08\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.84it/s, train_loss=0.435, val_loss=0.08\u001b[A\n",
      " 41%|█████████████████▏                        | 41/100 [01:03<01:26,  1.47s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=1.79, val_loss=4.42, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=1.79, val_loss=4.42, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=26.1, val_loss=63.3, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=26.1, val_loss=63.3, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=328, val_loss=5.01, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=328, val_loss=5.01, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=25, val_loss=6.49, train_cinde\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 34.39it/s, train_loss=25, val_loss=6.49, tra\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 34.39it/s, train_loss=25, val_loss=6.49, tra\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 34.39it/s, train_loss=34.5, val_loss=5.65, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 34.39it/s, train_loss=34.5, val_loss=5.65, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 34.39it/s, train_loss=29, val_loss=0.447, tr\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 34.39it/s, train_loss=29, val_loss=0.447, tr\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 34.39it/s, train_loss=2.21, val_loss=0.321, \u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 34.39it/s, train_loss=2.21, val_loss=0.321, \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.756293049467752\n",
      "Its' val AUC : 0.6993339676498572\n",
      "Its external  AUC: 0.6407751693713566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 34.39it/s, train_loss=1.6, val_loss=0.239, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 34.47it/s, train_loss=1.6, val_loss=0.239, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.47it/s, train_loss=1.6, val_loss=0.239, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.47it/s, train_loss=1.08, val_loss=0.614, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.47it/s, train_loss=1.08, val_loss=0.614, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.47it/s, train_loss=4.17, val_loss=0.185, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.47it/s, train_loss=4.17, val_loss=0.185,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.47it/s, train_loss=0.804, val_loss=0.201\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.47it/s, train_loss=0.804, val_loss=0.201\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.47it/s, train_loss=0.99, val_loss=0.138,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.60it/s, train_loss=0.99, val_loss=0.138\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.60it/s, train_loss=0.99, val_loss=0.138\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.60it/s, train_loss=0.678, val_loss=0.07\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.60it/s, train_loss=0.678, val_loss=0.07\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.60it/s, train_loss=0.38, val_loss=0.035\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.60it/s, train_loss=0.38, val_loss=0.035\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.60it/s, train_loss=0.198, val_loss=0.01\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.60it/s, train_loss=0.198, val_loss=0.01\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.60it/s, train_loss=0.127, val_loss=0.02\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.75it/s, train_loss=0.127, val_loss=0.02\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.75it/s, train_loss=0.127, val_loss=0.02\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.75it/s, train_loss=0.15, val_loss=0.032\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.75it/s, train_loss=0.15, val_loss=0.032\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.75it/s, train_loss=0.223, val_loss=0.04\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.75it/s, train_loss=0.223, val_loss=0.04\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.75it/s, train_loss=0.298, val_loss=0.04\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.75it/s, train_loss=0.298, val_loss=0.04\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.75it/s, train_loss=0.318, val_loss=0.04\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.85it/s, train_loss=0.318, val_loss=0.04\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.85it/s, train_loss=0.318, val_loss=0.04\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.85it/s, train_loss=0.292, val_loss=0.03\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.85it/s, train_loss=0.292, val_loss=0.03\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.85it/s, train_loss=0.201, val_loss=0.02\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.85it/s, train_loss=0.201, val_loss=0.02\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.85it/s, train_loss=0.173, val_loss=0.03\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.85it/s, train_loss=0.173, val_loss=0.03\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.85it/s, train_loss=0.187, val_loss=0.03\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.89it/s, train_loss=0.187, val_loss=0.03\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.89it/s, train_loss=0.187, val_loss=0.03\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.89it/s, train_loss=0.198, val_loss=0.03\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.89it/s, train_loss=0.198, val_loss=0.03\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.89it/s, train_loss=0.186, val_loss=0.02\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.89it/s, train_loss=0.186, val_loss=0.02\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.89it/s, train_loss=0.155, val_loss=0.02\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.89it/s, train_loss=0.155, val_loss=0.02\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    21: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch    27: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.89it/s, train_loss=0.132, val_loss=0.01\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.90it/s, train_loss=0.132, val_loss=0.01\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.90it/s, train_loss=0.132, val_loss=0.01\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.90it/s, train_loss=0.132, val_loss=0.01\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.90it/s, train_loss=0.132, val_loss=0.01\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.90it/s, train_loss=0.131, val_loss=0.01\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.90it/s, train_loss=0.131, val_loss=0.01\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.90it/s, train_loss=0.128, val_loss=0.01\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.90it/s, train_loss=0.128, val_loss=0.01\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.90it/s, train_loss=0.122, val_loss=0.01\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.90it/s, train_loss=0.122, val_loss=0.01\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.90it/s, train_loss=0.122, val_loss=0.01\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.90it/s, train_loss=0.115, val_loss=0.01\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.90it/s, train_loss=0.115, val_loss=0.01\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.90it/s, train_loss=0.107, val_loss=0.01\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.90it/s, train_loss=0.107, val_loss=0.01\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.90it/s, train_loss=0.0996, val_loss=0.0\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.90it/s, train_loss=0.0996, val_loss=0.0\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.90it/s, train_loss=0.0936, val_loss=0.0\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.88it/s, train_loss=0.0936, val_loss=0.0\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.88it/s, train_loss=0.0936, val_loss=0.0\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.88it/s, train_loss=0.0886, val_loss=0.0\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.88it/s, train_loss=0.0886, val_loss=0.0\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.88it/s, train_loss=0.0847, val_loss=0.0\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.88it/s, train_loss=0.0847, val_loss=0.0\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.88it/s, train_loss=0.0807, val_loss=0.0\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.88it/s, train_loss=0.0807, val_loss=0.0\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.88it/s, train_loss=0.0759, val_loss=0.0\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.93it/s, train_loss=0.0759, val_loss=0.0\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.93it/s, train_loss=0.0759, val_loss=0.0\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.93it/s, train_loss=0.0717, val_loss=0.0\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.93it/s, train_loss=0.0717, val_loss=0.0\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.93it/s, train_loss=0.07, val_loss=0.010\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.93it/s, train_loss=0.07, val_loss=0.010\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.93it/s, train_loss=0.0703, val_loss=0.0\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.93it/s, train_loss=0.0703, val_loss=0.0\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.93it/s, train_loss=0.0718, val_loss=0.0\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.93it/s, train_loss=0.0718, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.93it/s, train_loss=0.0718, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.93it/s, train_loss=0.0732, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.93it/s, train_loss=0.0732, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.93it/s, train_loss=0.0739, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.93it/s, train_loss=0.0739, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.93it/s, train_loss=0.0738, val_loss=0.0\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.93it/s, train_loss=0.0738, val_loss=0.0\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.93it/s, train_loss=0.0728, val_loss=0.0\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.90it/s, train_loss=0.0728, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.90it/s, train_loss=0.0728, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.90it/s, train_loss=0.0719, val_loss=0.0\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.90it/s, train_loss=0.0719, val_loss=0.0\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.83it/s, train_loss=0.0706, val_loss=0.0\u001b[A\n",
      " 42%|█████████████████▋                        | 42/100 [01:04<01:24,  1.46s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=4.71, val_loss=137, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=4.71, val_loss=137, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=733, val_loss=9.72, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=733, val_loss=9.72, train_cind\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    47: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Curent best Test AUC: 0.2706324358171572\n",
      "Its' val AUC : 0.29672420823705314\n",
      "Its external  AUC: 0.39246888293682053\n",
      "Curent best Test AUC: 0.29730745147150905\n",
      "Its' val AUC : 0.3309773005301074\n",
      "Its external  AUC: 0.41830786198203873\n",
      "Curent best Test AUC: 0.7252348152786474\n",
      "Its' val AUC : 0.7004213674051923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=45.8, val_loss=2.22, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=45.8, val_loss=2.22, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=13.3, val_loss=0.7, train_cind\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.12it/s, train_loss=13.3, val_loss=0.7, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.12it/s, train_loss=13.3, val_loss=0.7, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.12it/s, train_loss=5.35, val_loss=1.67, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.12it/s, train_loss=5.35, val_loss=1.67, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.12it/s, train_loss=9.34, val_loss=0.324, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.12it/s, train_loss=9.34, val_loss=0.324, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.12it/s, train_loss=1.71, val_loss=2.08, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.12it/s, train_loss=1.71, val_loss=2.08, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.12it/s, train_loss=10.8, val_loss=0.633, \u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 34.13it/s, train_loss=10.8, val_loss=0.633, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.13it/s, train_loss=10.8, val_loss=0.633, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.13it/s, train_loss=3.77, val_loss=0.452, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.13it/s, train_loss=3.77, val_loss=0.452, \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its external  AUC: 0.6273830155979203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.13it/s, train_loss=2.93, val_loss=1.53, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.13it/s, train_loss=2.93, val_loss=1.53, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.13it/s, train_loss=7.75, val_loss=0.217,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.13it/s, train_loss=7.75, val_loss=0.217,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.13it/s, train_loss=1.58, val_loss=0.783,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.49it/s, train_loss=1.58, val_loss=0.783\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.49it/s, train_loss=1.58, val_loss=0.783\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.49it/s, train_loss=4.71, val_loss=0.255\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.49it/s, train_loss=4.71, val_loss=0.255\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.49it/s, train_loss=1.42, val_loss=0.395\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.49it/s, train_loss=1.42, val_loss=0.395\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.49it/s, train_loss=2.09, val_loss=0.458\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.49it/s, train_loss=2.09, val_loss=0.458\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.49it/s, train_loss=2.36, val_loss=0.136\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.52it/s, train_loss=2.36, val_loss=0.136\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.52it/s, train_loss=2.36, val_loss=0.136\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.52it/s, train_loss=0.731, val_loss=0.52\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.52it/s, train_loss=0.731, val_loss=0.52\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.52it/s, train_loss=2.96, val_loss=0.155\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.52it/s, train_loss=2.96, val_loss=0.155\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.52it/s, train_loss=0.827, val_loss=0.31\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.52it/s, train_loss=0.827, val_loss=0.31\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.52it/s, train_loss=1.55, val_loss=0.323\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.24it/s, train_loss=1.55, val_loss=0.323\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.24it/s, train_loss=1.55, val_loss=0.323\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.24it/s, train_loss=2.03, val_loss=0.079\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.24it/s, train_loss=2.03, val_loss=0.079\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.24it/s, train_loss=0.442, val_loss=0.31\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.24it/s, train_loss=0.442, val_loss=0.31\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.24it/s, train_loss=1.44, val_loss=0.208\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.24it/s, train_loss=1.44, val_loss=0.208\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.24it/s, train_loss=1.32, val_loss=0.032\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.48it/s, train_loss=1.32, val_loss=0.032\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.48it/s, train_loss=1.32, val_loss=0.032\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.48it/s, train_loss=0.269, val_loss=0.15\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.48it/s, train_loss=0.269, val_loss=0.15\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.48it/s, train_loss=0.988, val_loss=0.21\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.48it/s, train_loss=0.988, val_loss=0.21\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7507827175954915\n",
      "Its' val AUC : 0.7247519369308142\n",
      "Its external  AUC: 0.637151410115015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.48it/s, train_loss=0.983, val_loss=0.05\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.48it/s, train_loss=0.983, val_loss=0.05\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.48it/s, train_loss=0.305, val_loss=0.11\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.63it/s, train_loss=0.305, val_loss=0.11\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.63it/s, train_loss=0.305, val_loss=0.11\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.63it/s, train_loss=0.718, val_loss=0.22\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.63it/s, train_loss=0.718, val_loss=0.22\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.63it/s, train_loss=1.12, val_loss=0.149\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.63it/s, train_loss=1.12, val_loss=0.149\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.63it/s, train_loss=0.868, val_loss=0.03\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.63it/s, train_loss=0.868, val_loss=0.03\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.63it/s, train_loss=0.234, val_loss=0.12\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.75it/s, train_loss=0.234, val_loss=0.12\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.75it/s, train_loss=0.234, val_loss=0.12\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.75it/s, train_loss=0.537, val_loss=0.09\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.75it/s, train_loss=0.537, val_loss=0.09\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.75it/s, train_loss=0.42, val_loss=0.045\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.75it/s, train_loss=0.42, val_loss=0.045\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.75it/s, train_loss=0.301, val_loss=0.08\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.75it/s, train_loss=0.301, val_loss=0.08\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.75it/s, train_loss=0.515, val_loss=0.02\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.76it/s, train_loss=0.515, val_loss=0.02\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.76it/s, train_loss=0.515, val_loss=0.02\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    30: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.76it/s, train_loss=0.143, val_loss=0.09\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.76it/s, train_loss=0.143, val_loss=0.09\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.76it/s, train_loss=0.499, val_loss=0.02\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.76it/s, train_loss=0.499, val_loss=0.02\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.76it/s, train_loss=0.137, val_loss=0.07\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.76it/s, train_loss=0.137, val_loss=0.07\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.76it/s, train_loss=0.415, val_loss=0.02\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.85it/s, train_loss=0.415, val_loss=0.02\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.85it/s, train_loss=0.415, val_loss=0.02\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.85it/s, train_loss=0.141, val_loss=0.05\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.85it/s, train_loss=0.141, val_loss=0.05\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.85it/s, train_loss=0.288, val_loss=0.03\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.85it/s, train_loss=0.288, val_loss=0.03\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.85it/s, train_loss=0.165, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.85it/s, train_loss=0.165, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.85it/s, train_loss=0.21, val_loss=0.027\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.85it/s, train_loss=0.21, val_loss=0.027\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.85it/s, train_loss=0.21, val_loss=0.027\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.85it/s, train_loss=0.182, val_loss=0.01\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.85it/s, train_loss=0.182, val_loss=0.01\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.85it/s, train_loss=0.0778, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.85it/s, train_loss=0.0778, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.85it/s, train_loss=0.141, val_loss=0.03\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.85it/s, train_loss=0.141, val_loss=0.03\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.85it/s, train_loss=0.159, val_loss=0.01\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.90it/s, train_loss=0.159, val_loss=0.01\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.90it/s, train_loss=0.159, val_loss=0.01\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.90it/s, train_loss=0.0792, val_loss=0.0\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.90it/s, train_loss=0.0792, val_loss=0.0\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.65it/s, train_loss=0.119, val_loss=0.02\u001b[A\n",
      " 43%|██████████████████                        | 43/100 [01:06<01:23,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    44: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=14, val_loss=27.5, train_cinde\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=14, val_loss=27.5, train_cinde\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=169, val_loss=40.6, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=169, val_loss=40.6, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=210, val_loss=8.82, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=210, val_loss=8.82, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=47.8, val_loss=8.83, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.92it/s, train_loss=47.8, val_loss=8.83, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.92it/s, train_loss=47.8, val_loss=8.83, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.92it/s, train_loss=43.5, val_loss=2.7, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.92it/s, train_loss=43.5, val_loss=2.7, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.92it/s, train_loss=13.2, val_loss=2.13, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.92it/s, train_loss=13.2, val_loss=2.13, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.92it/s, train_loss=13.2, val_loss=2.37, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.92it/s, train_loss=13.2, val_loss=2.37, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.404257983719474\n",
      "Its' val AUC : 0.3553078700557292\n",
      "Its external  AUC: 0.40365527020639674\n",
      "Curent best Test AUC: 0.747777082028804\n",
      "Its' val AUC : 0.7152371890716325\n",
      "Its external  AUC: 0.6048526863084922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.92it/s, train_loss=12.6, val_loss=0.796, \u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 34.41it/s, train_loss=12.6, val_loss=0.796, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.41it/s, train_loss=12.6, val_loss=0.796, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.41it/s, train_loss=4.6, val_loss=1.85, tr\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.41it/s, train_loss=4.6, val_loss=1.85, tr\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.41it/s, train_loss=9.64, val_loss=0.44, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.41it/s, train_loss=9.64, val_loss=0.44, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.41it/s, train_loss=2.49, val_loss=0.845,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.41it/s, train_loss=2.49, val_loss=0.845,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.41it/s, train_loss=4.98, val_loss=0.549,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.32it/s, train_loss=4.98, val_loss=0.549\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.32it/s, train_loss=4.98, val_loss=0.549\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.32it/s, train_loss=3.15, val_loss=0.478\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.32it/s, train_loss=3.15, val_loss=0.478\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.32it/s, train_loss=2.49, val_loss=0.541\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.32it/s, train_loss=2.49, val_loss=0.541\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.32it/s, train_loss=2.77, val_loss=0.41,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.32it/s, train_loss=2.77, val_loss=0.41,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.32it/s, train_loss=2.52, val_loss=0.405\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.55it/s, train_loss=2.52, val_loss=0.405\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.55it/s, train_loss=2.52, val_loss=0.405\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.55it/s, train_loss=2.38, val_loss=0.29,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.55it/s, train_loss=2.38, val_loss=0.29,\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7585472761427677\n",
      "Its' val AUC : 0.6664401250509718\n",
      "Its external  AUC: 0.6272254608476445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.55it/s, train_loss=1.64, val_loss=0.199\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.55it/s, train_loss=1.64, val_loss=0.199\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.55it/s, train_loss=1.19, val_loss=0.218\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.55it/s, train_loss=1.19, val_loss=0.218\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.55it/s, train_loss=1.03, val_loss=0.187\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.54it/s, train_loss=1.03, val_loss=0.187\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.54it/s, train_loss=1.03, val_loss=0.187\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.54it/s, train_loss=0.887, val_loss=0.25\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.54it/s, train_loss=0.887, val_loss=0.25\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.54it/s, train_loss=1.31, val_loss=0.131\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.54it/s, train_loss=1.31, val_loss=0.131\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.54it/s, train_loss=0.71, val_loss=0.198\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.54it/s, train_loss=0.71, val_loss=0.198\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.54it/s, train_loss=1.15, val_loss=0.070\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.08it/s, train_loss=1.15, val_loss=0.070\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.08it/s, train_loss=1.15, val_loss=0.070\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.08it/s, train_loss=0.445, val_loss=0.20\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.08it/s, train_loss=0.445, val_loss=0.20\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.08it/s, train_loss=0.963, val_loss=0.06\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.08it/s, train_loss=0.963, val_loss=0.06\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.08it/s, train_loss=0.351, val_loss=0.13\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.08it/s, train_loss=0.351, val_loss=0.13\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.08it/s, train_loss=0.743, val_loss=0.10\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.30it/s, train_loss=0.743, val_loss=0.10\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.30it/s, train_loss=0.743, val_loss=0.10\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.30it/s, train_loss=0.43, val_loss=0.098\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.30it/s, train_loss=0.43, val_loss=0.098\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.30it/s, train_loss=0.436, val_loss=0.09\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.30it/s, train_loss=0.436, val_loss=0.09\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.30it/s, train_loss=0.519, val_loss=0.03\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.30it/s, train_loss=0.519, val_loss=0.03\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.30it/s, train_loss=0.18, val_loss=0.093\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.45it/s, train_loss=0.18, val_loss=0.093\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.45it/s, train_loss=0.18, val_loss=0.093\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.45it/s, train_loss=0.492, val_loss=0.05\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.45it/s, train_loss=0.492, val_loss=0.05\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.45it/s, train_loss=0.267, val_loss=0.06\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.45it/s, train_loss=0.267, val_loss=0.06\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.45it/s, train_loss=0.318, val_loss=0.08\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.45it/s, train_loss=0.318, val_loss=0.08\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.45it/s, train_loss=0.416, val_loss=0.03\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.416, val_loss=0.03\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.416, val_loss=0.03\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.153, val_loss=0.05\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.153, val_loss=0.05\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.303, val_loss=0.06\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.303, val_loss=0.06\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.272, val_loss=0.03\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.272, val_loss=0.03\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.149, val_loss=0.05\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.52it/s, train_loss=0.149, val_loss=0.05\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.52it/s, train_loss=0.149, val_loss=0.05\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.52it/s, train_loss=0.311, val_loss=0.05\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.52it/s, train_loss=0.311, val_loss=0.05\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.52it/s, train_loss=0.229, val_loss=0.02\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.52it/s, train_loss=0.229, val_loss=0.02\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.52it/s, train_loss=0.11, val_loss=0.042\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.52it/s, train_loss=0.11, val_loss=0.042\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.52it/s, train_loss=0.213, val_loss=0.03\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.64it/s, train_loss=0.213, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.64it/s, train_loss=0.213, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.64it/s, train_loss=0.169, val_loss=0.02\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.64it/s, train_loss=0.169, val_loss=0.02\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.64it/s, train_loss=0.0939, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.64it/s, train_loss=0.0939, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.64it/s, train_loss=0.193, val_loss=0.03\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.64it/s, train_loss=0.193, val_loss=0.03\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.64it/s, train_loss=0.202, val_loss=0.02\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.71it/s, train_loss=0.202, val_loss=0.02\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.71it/s, train_loss=0.202, val_loss=0.02\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.71it/s, train_loss=0.102, val_loss=0.02\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.71it/s, train_loss=0.102, val_loss=0.02\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.50it/s, train_loss=0.121, val_loss=0.03\u001b[A\n",
      " 44%|██████████████████▍                       | 44/100 [01:07<01:22,  1.47s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=35.8, val_loss=262, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=35.8, val_loss=262, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=1.45e+3, val_loss=16.4, train_\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=1.45e+3, val_loss=16.4, train_\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=91.6, val_loss=32.9, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=91.6, val_loss=32.9, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=185, val_loss=9.69, train_cind\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 34.61it/s, train_loss=185, val_loss=9.69, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 34.61it/s, train_loss=185, val_loss=9.69, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 34.61it/s, train_loss=56, val_loss=4.29, tra\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 34.61it/s, train_loss=56, val_loss=4.29, tra\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 34.61it/s, train_loss=22.4, val_loss=6.39, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 34.61it/s, train_loss=22.4, val_loss=6.39, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 34.61it/s, train_loss=33.9, val_loss=3.35, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 34.61it/s, train_loss=33.9, val_loss=3.35, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7073262366938009\n",
      "Its' val AUC : 0.7084409406007883\n",
      "Its external  AUC: 0.6133606428233811\n",
      "Curent best Test AUC: 0.7555416405760802\n",
      "Its' val AUC : 0.726111186624983\n",
      "Its external  AUC: 0.6278556798487475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 34.61it/s, train_loss=18.8, val_loss=0.438, \u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 34.30it/s, train_loss=18.8, val_loss=0.438, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.30it/s, train_loss=18.8, val_loss=0.438, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.30it/s, train_loss=2.66, val_loss=3.77, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.30it/s, train_loss=2.66, val_loss=3.77, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.30it/s, train_loss=18.2, val_loss=4.32, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.30it/s, train_loss=18.2, val_loss=4.32, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.30it/s, train_loss=21, val_loss=1.81, tr\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.30it/s, train_loss=21, val_loss=1.81, tr\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.30it/s, train_loss=8.88, val_loss=0.18, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.63it/s, train_loss=8.88, val_loss=0.18,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.63it/s, train_loss=8.88, val_loss=0.18,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.63it/s, train_loss=1.17, val_loss=1.86,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.63it/s, train_loss=1.17, val_loss=1.86,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.63it/s, train_loss=9.35, val_loss=1.9, \u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.63it/s, train_loss=9.35, val_loss=1.9, \u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.63it/s, train_loss=9.11, val_loss=0.467\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.63it/s, train_loss=9.11, val_loss=0.467\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.63it/s, train_loss=2.2, val_loss=0.471,\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.60it/s, train_loss=2.2, val_loss=0.471,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.60it/s, train_loss=2.2, val_loss=0.471,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.60it/s, train_loss=2.96, val_loss=1.09,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.60it/s, train_loss=2.96, val_loss=1.09,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.60it/s, train_loss=6.19, val_loss=0.526\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.60it/s, train_loss=6.19, val_loss=0.526\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.60it/s, train_loss=2.95, val_loss=0.16,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.60it/s, train_loss=2.95, val_loss=0.16,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.60it/s, train_loss=0.955, val_loss=0.04\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.72it/s, train_loss=0.955, val_loss=0.04\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.72it/s, train_loss=0.955, val_loss=0.04\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.72it/s, train_loss=0.278, val_loss=0.15\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.72it/s, train_loss=0.278, val_loss=0.15\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.72it/s, train_loss=0.792, val_loss=0.30\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.72it/s, train_loss=0.792, val_loss=0.30\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.72it/s, train_loss=1.52, val_loss=0.276\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.72it/s, train_loss=1.52, val_loss=0.276\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.72it/s, train_loss=1.41, val_loss=0.119\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.69it/s, train_loss=1.41, val_loss=0.119\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.69it/s, train_loss=1.41, val_loss=0.119\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    18: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.69it/s, train_loss=0.654, val_loss=0.03\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.69it/s, train_loss=0.654, val_loss=0.03\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.69it/s, train_loss=0.246, val_loss=0.08\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.69it/s, train_loss=0.246, val_loss=0.08\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.69it/s, train_loss=0.543, val_loss=0.16\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.69it/s, train_loss=0.543, val_loss=0.16\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.69it/s, train_loss=0.931, val_loss=0.14\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.83it/s, train_loss=0.931, val_loss=0.14\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.83it/s, train_loss=0.931, val_loss=0.14\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.83it/s, train_loss=0.79, val_loss=0.054\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.83it/s, train_loss=0.79, val_loss=0.054\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.83it/s, train_loss=0.319, val_loss=0.02\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.83it/s, train_loss=0.319, val_loss=0.02\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.83it/s, train_loss=0.155, val_loss=0.09\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.83it/s, train_loss=0.155, val_loss=0.09\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.83it/s, train_loss=0.431, val_loss=0.13\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.90it/s, train_loss=0.431, val_loss=0.13\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.90it/s, train_loss=0.431, val_loss=0.13\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.90it/s, train_loss=0.656, val_loss=0.09\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.90it/s, train_loss=0.656, val_loss=0.09\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.90it/s, train_loss=0.484, val_loss=0.03\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.90it/s, train_loss=0.484, val_loss=0.03\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.90it/s, train_loss=0.192, val_loss=0.03\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.90it/s, train_loss=0.192, val_loss=0.03\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.90it/s, train_loss=0.181, val_loss=0.06\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.88it/s, train_loss=0.181, val_loss=0.06\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.88it/s, train_loss=0.181, val_loss=0.06\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.88it/s, train_loss=0.374, val_loss=0.07\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.88it/s, train_loss=0.374, val_loss=0.07\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.88it/s, train_loss=0.387, val_loss=0.05\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.88it/s, train_loss=0.387, val_loss=0.05\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.88it/s, train_loss=0.278, val_loss=0.02\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.88it/s, train_loss=0.278, val_loss=0.02\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.88it/s, train_loss=0.121, val_loss=0.02\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.90it/s, train_loss=0.121, val_loss=0.02\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.90it/s, train_loss=0.121, val_loss=0.02\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.90it/s, train_loss=0.12, val_loss=0.041\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.90it/s, train_loss=0.12, val_loss=0.041\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.90it/s, train_loss=0.208, val_loss=0.05\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.90it/s, train_loss=0.208, val_loss=0.05\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    36: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.90it/s, train_loss=0.253, val_loss=0.04\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.90it/s, train_loss=0.253, val_loss=0.04\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.90it/s, train_loss=0.212, val_loss=0.02\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.84it/s, train_loss=0.212, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.84it/s, train_loss=0.212, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.84it/s, train_loss=0.135, val_loss=0.01\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.84it/s, train_loss=0.135, val_loss=0.01\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.84it/s, train_loss=0.0993, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.84it/s, train_loss=0.0993, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.84it/s, train_loss=0.132, val_loss=0.03\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.84it/s, train_loss=0.132, val_loss=0.03\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.84it/s, train_loss=0.174, val_loss=0.02\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.90it/s, train_loss=0.174, val_loss=0.02\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.90it/s, train_loss=0.174, val_loss=0.02\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.90it/s, train_loss=0.163, val_loss=0.02\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.90it/s, train_loss=0.163, val_loss=0.02\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.78it/s, train_loss=0.12, val_loss=0.016\u001b[A\n",
      " 45%|██████████████████▉                       | 45/100 [01:09<01:20,  1.46s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=40.3, val_loss=1.58, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=40.3, val_loss=1.58, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=10.8, val_loss=8.44, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=10.8, val_loss=8.44, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=42.6, val_loss=41, train_cinde\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=42.6, val_loss=41, train_cinde\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=214, val_loss=3.88, train_cind\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 34.04it/s, train_loss=214, val_loss=3.88, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 34.04it/s, train_loss=214, val_loss=3.88, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 34.04it/s, train_loss=19, val_loss=22.3, tra\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 34.04it/s, train_loss=19, val_loss=22.3, tra\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 34.04it/s, train_loss=114, val_loss=7.97, tr\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 34.04it/s, train_loss=114, val_loss=7.97, tr\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 34.04it/s, train_loss=43.2, val_loss=2.25, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 34.04it/s, train_loss=43.2, val_loss=2.25, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.6052598622417031\n",
      "Its' val AUC : 0.5286122060622537\n",
      "Its external  AUC: 0.5873641090278872\n",
      "Curent best Test AUC: 0.7056981840951785\n",
      "Its' val AUC : 0.7009650672828599\n",
      "Its external  AUC: 0.6136757523239326\n",
      "Epoch     7: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 34.04it/s, train_loss=12.7, val_loss=1.2, tr\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 34.17it/s, train_loss=12.7, val_loss=1.2, tr\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.17it/s, train_loss=12.7, val_loss=1.2, tr\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.17it/s, train_loss=7.24, val_loss=2.54, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.17it/s, train_loss=7.24, val_loss=2.54, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.17it/s, train_loss=13.8, val_loss=2.44, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.17it/s, train_loss=13.8, val_loss=2.44, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.17it/s, train_loss=13.8, val_loss=0.81, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.17it/s, train_loss=13.8, val_loss=0.81, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.17it/s, train_loss=5.52, val_loss=0.702,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.19it/s, train_loss=5.52, val_loss=0.702\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.19it/s, train_loss=5.52, val_loss=0.702\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.19it/s, train_loss=4.54, val_loss=1.58,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.19it/s, train_loss=4.54, val_loss=1.58,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.19it/s, train_loss=8.57, val_loss=1.48,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.19it/s, train_loss=8.57, val_loss=1.48,\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7355040701314965\n",
      "Its' val AUC : 0.5965746907706946\n",
      "Its external  AUC: 0.6204506065857885\n",
      "Curent best Test AUC: 0.7592986850344395\n",
      "Its' val AUC : 0.6699741742558107\n",
      "Its external  AUC: 0.6226563730896486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.19it/s, train_loss=7.98, val_loss=0.563\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.19it/s, train_loss=7.98, val_loss=0.563\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.19it/s, train_loss=3.44, val_loss=0.261\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.48it/s, train_loss=3.44, val_loss=0.261\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.48it/s, train_loss=3.44, val_loss=0.261\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.48it/s, train_loss=1.69, val_loss=0.656\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.48it/s, train_loss=1.69, val_loss=0.656\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.48it/s, train_loss=3.23, val_loss=0.671\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.48it/s, train_loss=3.23, val_loss=0.671\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.48it/s, train_loss=3.15, val_loss=0.249\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.48it/s, train_loss=3.15, val_loss=0.249\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.48it/s, train_loss=1.26, val_loss=0.253\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.54it/s, train_loss=1.26, val_loss=0.253\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.54it/s, train_loss=1.26, val_loss=0.253\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.54it/s, train_loss=1.6, val_loss=0.375,\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.54it/s, train_loss=1.6, val_loss=0.375,\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.54it/s, train_loss=2.28, val_loss=0.177\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.54it/s, train_loss=2.28, val_loss=0.177\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.54it/s, train_loss=1.09, val_loss=0.061\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.54it/s, train_loss=1.09, val_loss=0.061\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.54it/s, train_loss=0.48, val_loss=0.28,\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.67it/s, train_loss=0.48, val_loss=0.28,\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.67it/s, train_loss=0.48, val_loss=0.28,\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.67it/s, train_loss=1.65, val_loss=0.323\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.67it/s, train_loss=1.65, val_loss=0.323\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.67it/s, train_loss=1.89, val_loss=0.124\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.67it/s, train_loss=1.89, val_loss=0.124\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.67it/s, train_loss=0.854, val_loss=0.08\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.67it/s, train_loss=0.854, val_loss=0.08\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.67it/s, train_loss=0.659, val_loss=0.17\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.76it/s, train_loss=0.659, val_loss=0.17\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.76it/s, train_loss=0.659, val_loss=0.17\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.76it/s, train_loss=1.18, val_loss=0.126\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.76it/s, train_loss=1.18, val_loss=0.126\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.76it/s, train_loss=0.907, val_loss=0.08\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.76it/s, train_loss=0.907, val_loss=0.08\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.76it/s, train_loss=0.604, val_loss=0.09\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.76it/s, train_loss=0.604, val_loss=0.09\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.76it/s, train_loss=0.56, val_loss=0.13,\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.77it/s, train_loss=0.56, val_loss=0.13,\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.77it/s, train_loss=0.56, val_loss=0.13,\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.77it/s, train_loss=0.671, val_loss=0.12\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.77it/s, train_loss=0.671, val_loss=0.12\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.77it/s, train_loss=0.625, val_loss=0.07\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.77it/s, train_loss=0.625, val_loss=0.07\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.77it/s, train_loss=0.396, val_loss=0.03\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.77it/s, train_loss=0.396, val_loss=0.03\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    29: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.77it/s, train_loss=0.253, val_loss=0.04\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.74it/s, train_loss=0.253, val_loss=0.04\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.74it/s, train_loss=0.253, val_loss=0.04\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.74it/s, train_loss=0.345, val_loss=0.07\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.74it/s, train_loss=0.345, val_loss=0.07\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.74it/s, train_loss=0.495, val_loss=0.07\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.74it/s, train_loss=0.495, val_loss=0.07\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.74it/s, train_loss=0.479, val_loss=0.04\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.74it/s, train_loss=0.479, val_loss=0.04\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.74it/s, train_loss=0.326, val_loss=0.03\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.78it/s, train_loss=0.326, val_loss=0.03\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.78it/s, train_loss=0.326, val_loss=0.03\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.78it/s, train_loss=0.231, val_loss=0.04\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.78it/s, train_loss=0.231, val_loss=0.04\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.78it/s, train_loss=0.257, val_loss=0.05\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.78it/s, train_loss=0.257, val_loss=0.05\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.78it/s, train_loss=0.284, val_loss=0.04\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.78it/s, train_loss=0.284, val_loss=0.04\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.78it/s, train_loss=0.229, val_loss=0.03\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.75it/s, train_loss=0.229, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.75it/s, train_loss=0.229, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.75it/s, train_loss=0.173, val_loss=0.03\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.75it/s, train_loss=0.173, val_loss=0.03\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.75it/s, train_loss=0.199, val_loss=0.04\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.75it/s, train_loss=0.199, val_loss=0.04\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.75it/s, train_loss=0.25, val_loss=0.043\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.75it/s, train_loss=0.25, val_loss=0.043\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.75it/s, train_loss=0.237, val_loss=0.03\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.76it/s, train_loss=0.237, val_loss=0.03\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.76it/s, train_loss=0.237, val_loss=0.03\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.76it/s, train_loss=0.191, val_loss=0.02\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.76it/s, train_loss=0.191, val_loss=0.02\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.64it/s, train_loss=0.143, val_loss=0.02\u001b[A\n",
      " 46%|███████████████████▎                      | 46/100 [01:10<01:18,  1.46s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=8.91, val_loss=58.7, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=8.91, val_loss=58.7, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=313, val_loss=19.5, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=313, val_loss=19.5, train_cind\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    46: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Curent best Test AUC: 0.23844708829054478\n",
      "Its' val AUC : 0.28408318608128313\n",
      "Its external  AUC: 0.39089333543406335\n",
      "Curent best Test AUC: 0.5327489041953664\n",
      "Its' val AUC : 0.5767296452358298\n",
      "Its external  AUC: 0.6075311170631794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=123, val_loss=6.1, train_cinde\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=123, val_loss=6.1, train_cinde\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=30.8, val_loss=9.73, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.47it/s, train_loss=30.8, val_loss=9.73, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.47it/s, train_loss=30.8, val_loss=9.73, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.47it/s, train_loss=66.2, val_loss=1.66, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.47it/s, train_loss=66.2, val_loss=1.66, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.47it/s, train_loss=10.3, val_loss=0.836, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.47it/s, train_loss=10.3, val_loss=0.836, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.47it/s, train_loss=4.75, val_loss=0.946, \u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.47it/s, train_loss=4.75, val_loss=0.946, \u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.47it/s, train_loss=4.3, val_loss=1.02, tr\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 33.79it/s, train_loss=4.3, val_loss=1.02, tr\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.79it/s, train_loss=4.3, val_loss=1.02, tr\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.79it/s, train_loss=5.94, val_loss=0.907, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.79it/s, train_loss=5.94, val_loss=0.907, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.79it/s, train_loss=5.65, val_loss=0.738, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.79it/s, train_loss=5.65, val_loss=0.738,\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.5356293049467752\n",
      "Its' val AUC : 0.46608672013048796\n",
      "Its external  AUC: 0.3973530801953679\n",
      "Curent best Test AUC: 0.6008766437069505\n",
      "Its' val AUC : 0.5350006796248471\n",
      "Its external  AUC: 0.4972427918701749\n",
      "Curent best Test AUC: 0.6916718847839699\n",
      "Its' val AUC : 0.582302568981922\n",
      "Its external  AUC: 0.5927209705372617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.79it/s, train_loss=3.77, val_loss=0.394,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.79it/s, train_loss=3.77, val_loss=0.394,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.79it/s, train_loss=2.38, val_loss=1.19, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.06it/s, train_loss=2.38, val_loss=1.19,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.06it/s, train_loss=2.38, val_loss=1.19,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.06it/s, train_loss=5.45, val_loss=0.596\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.06it/s, train_loss=5.45, val_loss=0.596\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.06it/s, train_loss=3.58, val_loss=0.399\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.06it/s, train_loss=3.58, val_loss=0.399\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.06it/s, train_loss=2.32, val_loss=0.465\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.06it/s, train_loss=2.32, val_loss=0.465\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.06it/s, train_loss=2.32, val_loss=0.21,\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.40it/s, train_loss=2.32, val_loss=0.21,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.40it/s, train_loss=2.32, val_loss=0.21,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.40it/s, train_loss=1.07, val_loss=0.454\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.40it/s, train_loss=1.07, val_loss=0.454\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.40it/s, train_loss=2.33, val_loss=0.84,\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.40it/s, train_loss=2.33, val_loss=0.84,\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7070757670632436\n",
      "Its' val AUC : 0.6245752344705723\n",
      "Its external  AUC: 0.6391996218685994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.40it/s, train_loss=4.69, val_loss=1.72,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.40it/s, train_loss=4.69, val_loss=1.72,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.40it/s, train_loss=8.05, val_loss=0.235\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.59it/s, train_loss=8.05, val_loss=0.235\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.59it/s, train_loss=8.05, val_loss=0.235\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.59it/s, train_loss=1.12, val_loss=1.15,\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.59it/s, train_loss=1.12, val_loss=1.15,\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.59it/s, train_loss=6.07, val_loss=0.537\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.59it/s, train_loss=6.07, val_loss=0.537\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.59it/s, train_loss=2.66, val_loss=0.626\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.59it/s, train_loss=2.66, val_loss=0.626\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.59it/s, train_loss=3.18, val_loss=0.186\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.48it/s, train_loss=3.18, val_loss=0.186\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.48it/s, train_loss=3.18, val_loss=0.186\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.48it/s, train_loss=0.851, val_loss=0.50\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.48it/s, train_loss=0.851, val_loss=0.50\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.48it/s, train_loss=2.37, val_loss=0.378\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.48it/s, train_loss=2.37, val_loss=0.378\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.48it/s, train_loss=1.73, val_loss=0.154\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.48it/s, train_loss=1.73, val_loss=0.154\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7199749530369443\n",
      "Its' val AUC : 0.6764985727878211\n",
      "Its external  AUC: 0.641090278871908\n",
      "Epoch    22: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.48it/s, train_loss=0.714, val_loss=0.39\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.64it/s, train_loss=0.714, val_loss=0.39\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.64it/s, train_loss=0.714, val_loss=0.39\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.64it/s, train_loss=2.07, val_loss=0.18,\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.64it/s, train_loss=2.07, val_loss=0.18,\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.64it/s, train_loss=0.934, val_loss=0.20\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.64it/s, train_loss=0.934, val_loss=0.20\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.64it/s, train_loss=0.945, val_loss=0.35\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.64it/s, train_loss=0.945, val_loss=0.35\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.64it/s, train_loss=1.6, val_loss=0.115,\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.67it/s, train_loss=1.6, val_loss=0.115,\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.67it/s, train_loss=1.6, val_loss=0.115,\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.67it/s, train_loss=0.506, val_loss=0.20\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.67it/s, train_loss=0.506, val_loss=0.20\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.67it/s, train_loss=1.11, val_loss=0.191\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.67it/s, train_loss=1.11, val_loss=0.191\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.67it/s, train_loss=1.05, val_loss=0.106\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.67it/s, train_loss=1.05, val_loss=0.106\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.67it/s, train_loss=0.45, val_loss=0.245\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.45, val_loss=0.245\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.45, val_loss=0.245\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=1.07, val_loss=0.122\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=1.07, val_loss=0.122\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.52, val_loss=0.11,\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.52, val_loss=0.11,\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.548, val_loss=0.16\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.548, val_loss=0.16\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.815, val_loss=0.07\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.67it/s, train_loss=0.815, val_loss=0.07\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.67it/s, train_loss=0.815, val_loss=0.07\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.67it/s, train_loss=0.34, val_loss=0.143\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.67it/s, train_loss=0.34, val_loss=0.143\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.67it/s, train_loss=0.693, val_loss=0.09\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.67it/s, train_loss=0.693, val_loss=0.09\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7228553537883532\n",
      "Its' val AUC : 0.6656245752344706\n",
      "Its external  AUC: 0.5791712620135497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.67it/s, train_loss=0.449, val_loss=0.08\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.67it/s, train_loss=0.449, val_loss=0.08\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.67it/s, train_loss=0.417, val_loss=0.11\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.67it/s, train_loss=0.417, val_loss=0.11\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.67it/s, train_loss=0.417, val_loss=0.11\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.67it/s, train_loss=0.553, val_loss=0.05\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.67it/s, train_loss=0.553, val_loss=0.05\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.67it/s, train_loss=0.252, val_loss=0.11\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.67it/s, train_loss=0.252, val_loss=0.11\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.67it/s, train_loss=0.512, val_loss=0.05\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.67it/s, train_loss=0.512, val_loss=0.05\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.67it/s, train_loss=0.246, val_loss=0.07\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.67it/s, train_loss=0.246, val_loss=0.07\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.67it/s, train_loss=0.246, val_loss=0.07\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.67it/s, train_loss=0.378, val_loss=0.06\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.67it/s, train_loss=0.378, val_loss=0.06\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.53it/s, train_loss=0.326, val_loss=0.05\u001b[A\n",
      " 47%|███████████████████▋                      | 47/100 [01:12<01:17,  1.46s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=16.6, val_loss=232, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=16.6, val_loss=232, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=1.33e+3, val_loss=22.9, train_\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=1.33e+3, val_loss=22.9, train_\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=119, val_loss=0.955, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=119, val_loss=0.955, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=5.39, val_loss=0.0485, train_c\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.80it/s, train_loss=5.39, val_loss=0.0485,\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.80it/s, train_loss=5.39, val_loss=0.0485,\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.80it/s, train_loss=0.26, val_loss=0.0566,\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.80it/s, train_loss=0.26, val_loss=0.0566,\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.80it/s, train_loss=0.287, val_loss=0.0819\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.80it/s, train_loss=0.287, val_loss=0.0819\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.80it/s, train_loss=0.411, val_loss=0.11, \u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.80it/s, train_loss=0.411, val_loss=0.11, \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.6798998121477771\n",
      "Its' val AUC : 0.6830229713198315\n",
      "Its external  AUC: 0.606113124310698\n",
      "Curent best Test AUC: 0.6886662492172824\n",
      "Its' val AUC : 0.6898192197906755\n",
      "Its external  AUC: 0.6280132345990231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.80it/s, train_loss=0.553, val_loss=0.134,\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 34.45it/s, train_loss=0.553, val_loss=0.134,\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.45it/s, train_loss=0.553, val_loss=0.134,\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.45it/s, train_loss=0.672, val_loss=0.149,\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.45it/s, train_loss=0.672, val_loss=0.149,\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.45it/s, train_loss=0.747, val_loss=0.157,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.45it/s, train_loss=0.747, val_loss=0.157\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.45it/s, train_loss=0.791, val_loss=0.158\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.45it/s, train_loss=0.791, val_loss=0.158\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.45it/s, train_loss=0.794, val_loss=0.154\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.69it/s, train_loss=0.794, val_loss=0.15\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.69it/s, train_loss=0.794, val_loss=0.15\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.69it/s, train_loss=0.775, val_loss=0.14\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.69it/s, train_loss=0.775, val_loss=0.14\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.69it/s, train_loss=0.736, val_loss=0.13\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.69it/s, train_loss=0.736, val_loss=0.13\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.69it/s, train_loss=0.684, val_loss=0.12\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.69it/s, train_loss=0.684, val_loss=0.12\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.69it/s, train_loss=0.626, val_loss=0.11\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.66it/s, train_loss=0.626, val_loss=0.11\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.66it/s, train_loss=0.626, val_loss=0.11\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    10: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch    16: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.66it/s, train_loss=0.565, val_loss=0.10\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.66it/s, train_loss=0.565, val_loss=0.10\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.66it/s, train_loss=0.534, val_loss=0.09\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.66it/s, train_loss=0.534, val_loss=0.09\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.66it/s, train_loss=0.501, val_loss=0.09\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.66it/s, train_loss=0.501, val_loss=0.09\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.66it/s, train_loss=0.468, val_loss=0.08\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.83it/s, train_loss=0.468, val_loss=0.08\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.83it/s, train_loss=0.468, val_loss=0.08\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.83it/s, train_loss=0.436, val_loss=0.07\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.83it/s, train_loss=0.436, val_loss=0.07\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.83it/s, train_loss=0.402, val_loss=0.06\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.83it/s, train_loss=0.402, val_loss=0.06\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.83it/s, train_loss=0.348, val_loss=0.06\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.83it/s, train_loss=0.348, val_loss=0.06\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.83it/s, train_loss=0.319, val_loss=0.06\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.48it/s, train_loss=0.319, val_loss=0.06\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.48it/s, train_loss=0.319, val_loss=0.06\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.48it/s, train_loss=0.324, val_loss=0.06\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.48it/s, train_loss=0.324, val_loss=0.06\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.48it/s, train_loss=0.316, val_loss=0.05\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.48it/s, train_loss=0.316, val_loss=0.05\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.48it/s, train_loss=0.289, val_loss=0.05\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.48it/s, train_loss=0.289, val_loss=0.05\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.48it/s, train_loss=0.277, val_loss=0.05\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.64it/s, train_loss=0.277, val_loss=0.05\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.64it/s, train_loss=0.277, val_loss=0.05\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    22: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.64it/s, train_loss=0.273, val_loss=0.05\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.64it/s, train_loss=0.273, val_loss=0.05\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.64it/s, train_loss=0.269, val_loss=0.05\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.64it/s, train_loss=0.269, val_loss=0.05\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.64it/s, train_loss=0.263, val_loss=0.05\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.64it/s, train_loss=0.263, val_loss=0.05\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.64it/s, train_loss=0.254, val_loss=0.04\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.71it/s, train_loss=0.254, val_loss=0.04\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.71it/s, train_loss=0.254, val_loss=0.04\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.71it/s, train_loss=0.243, val_loss=0.04\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.71it/s, train_loss=0.243, val_loss=0.04\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.71it/s, train_loss=0.233, val_loss=0.04\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.71it/s, train_loss=0.233, val_loss=0.04\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.71it/s, train_loss=0.226, val_loss=0.04\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.71it/s, train_loss=0.226, val_loss=0.04\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.71it/s, train_loss=0.221, val_loss=0.04\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.82it/s, train_loss=0.221, val_loss=0.04\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.82it/s, train_loss=0.221, val_loss=0.04\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.82it/s, train_loss=0.218, val_loss=0.04\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.82it/s, train_loss=0.218, val_loss=0.04\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.82it/s, train_loss=0.21, val_loss=0.040\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.82it/s, train_loss=0.21, val_loss=0.040\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.82it/s, train_loss=0.202, val_loss=0.03\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.82it/s, train_loss=0.202, val_loss=0.03\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.82it/s, train_loss=0.195, val_loss=0.03\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.87it/s, train_loss=0.195, val_loss=0.03\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.87it/s, train_loss=0.195, val_loss=0.03\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.87it/s, train_loss=0.19, val_loss=0.037\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.87it/s, train_loss=0.19, val_loss=0.037\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.87it/s, train_loss=0.185, val_loss=0.03\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.87it/s, train_loss=0.185, val_loss=0.03\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.87it/s, train_loss=0.181, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.87it/s, train_loss=0.181, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.87it/s, train_loss=0.176, val_loss=0.03\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.95it/s, train_loss=0.176, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.95it/s, train_loss=0.176, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.95it/s, train_loss=0.17, val_loss=0.033\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.95it/s, train_loss=0.17, val_loss=0.033\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.95it/s, train_loss=0.164, val_loss=0.03\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.95it/s, train_loss=0.164, val_loss=0.03\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.95it/s, train_loss=0.16, val_loss=0.031\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.95it/s, train_loss=0.16, val_loss=0.031\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.95it/s, train_loss=0.156, val_loss=0.03\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.97it/s, train_loss=0.156, val_loss=0.03\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.97it/s, train_loss=0.156, val_loss=0.03\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.97it/s, train_loss=0.152, val_loss=0.03\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.97it/s, train_loss=0.152, val_loss=0.03\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.77it/s, train_loss=0.148, val_loss=0.02\u001b[A\n",
      " 48%|████████████████████▏                     | 48/100 [01:13<01:15,  1.46s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=21.3, val_loss=17.9, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=21.3, val_loss=17.9, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=98.9, val_loss=17.3, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=98.9, val_loss=17.3, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=91.5, val_loss=39.2, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=91.5, val_loss=39.2, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=207, val_loss=8.59, train_cind\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 34.20it/s, train_loss=207, val_loss=8.59, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 34.20it/s, train_loss=207, val_loss=8.59, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 34.20it/s, train_loss=43.1, val_loss=2.94, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 34.20it/s, train_loss=43.1, val_loss=2.94, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 34.20it/s, train_loss=15.4, val_loss=8.15, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 34.20it/s, train_loss=15.4, val_loss=8.15, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 34.20it/s, train_loss=41.1, val_loss=0.739, \u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 34.20it/s, train_loss=41.1, val_loss=0.739, \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.297683155917345\n",
      "Its' val AUC : 0.30868560554573876\n",
      "Its external  AUC: 0.4299669135024421\n",
      "Curent best Test AUC: 0.687413901064496\n",
      "Its' val AUC : 0.6936251189343482\n",
      "Its external  AUC: 0.5810619190168583\n",
      "Curent best Test AUC: 0.7312460864120225\n",
      "Its' val AUC : 0.700829142313443\n",
      "Its external  AUC: 0.6265952418465417\n",
      "Curent best Test AUC: 0.7537883531621791\n",
      "Its' val AUC : 0.7136060894386299\n",
      "Its external  AUC: 0.6251772490940601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 34.20it/s, train_loss=4.71, val_loss=4.25, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 33.97it/s, train_loss=4.71, val_loss=4.25, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.97it/s, train_loss=4.71, val_loss=4.25, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.97it/s, train_loss=21.3, val_loss=2.61, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.97it/s, train_loss=21.3, val_loss=2.61, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.97it/s, train_loss=12.3, val_loss=0.467, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.97it/s, train_loss=12.3, val_loss=0.467,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.97it/s, train_loss=3.08, val_loss=2.62, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.97it/s, train_loss=3.08, val_loss=2.62, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.97it/s, train_loss=15.4, val_loss=0.468,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.41it/s, train_loss=15.4, val_loss=0.468\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.41it/s, train_loss=15.4, val_loss=0.468\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.41it/s, train_loss=3.11, val_loss=1.66,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.41it/s, train_loss=3.11, val_loss=1.66,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.41it/s, train_loss=8.3, val_loss=1.35, \u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.41it/s, train_loss=8.3, val_loss=1.35, \u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.41it/s, train_loss=7.25, val_loss=0.552\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.41it/s, train_loss=7.25, val_loss=0.552\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.41it/s, train_loss=2.91, val_loss=1.75,\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.39it/s, train_loss=2.91, val_loss=1.75,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.39it/s, train_loss=2.91, val_loss=1.75,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.39it/s, train_loss=8.33, val_loss=0.662\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.39it/s, train_loss=8.33, val_loss=0.662\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.39it/s, train_loss=3.07, val_loss=0.19,\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.39it/s, train_loss=3.07, val_loss=0.19,\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.39it/s, train_loss=1.06, val_loss=0.882\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.39it/s, train_loss=1.06, val_loss=0.882\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.39it/s, train_loss=4.73, val_loss=0.635\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.60it/s, train_loss=4.73, val_loss=0.635\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.60it/s, train_loss=4.73, val_loss=0.635\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.60it/s, train_loss=3.42, val_loss=0.125\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.60it/s, train_loss=3.42, val_loss=0.125\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.60it/s, train_loss=0.735, val_loss=0.29\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.60it/s, train_loss=0.735, val_loss=0.29\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    16: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Curent best Test AUC: 0.7615529117094553\n",
      "Its' val AUC : 0.6909066195460106\n",
      "Its external  AUC: 0.5971325035449819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.60it/s, train_loss=1.6, val_loss=0.518,\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.60it/s, train_loss=1.6, val_loss=0.518,\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.60it/s, train_loss=2.68, val_loss=0.211\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.70it/s, train_loss=2.68, val_loss=0.211\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.70it/s, train_loss=2.68, val_loss=0.211\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.70it/s, train_loss=1.11, val_loss=0.086\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.70it/s, train_loss=1.11, val_loss=0.086\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.70it/s, train_loss=0.551, val_loss=0.33\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.70it/s, train_loss=0.551, val_loss=0.33\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.70it/s, train_loss=1.89, val_loss=0.31,\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.70it/s, train_loss=1.89, val_loss=0.31,\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.70it/s, train_loss=1.7, val_loss=0.0827\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.78it/s, train_loss=1.7, val_loss=0.0827\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.78it/s, train_loss=1.7, val_loss=0.0827\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.78it/s, train_loss=0.508, val_loss=0.15\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.78it/s, train_loss=0.508, val_loss=0.15\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.78it/s, train_loss=1.14, val_loss=0.213\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.78it/s, train_loss=1.14, val_loss=0.213\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.78it/s, train_loss=1.28, val_loss=0.083\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.78it/s, train_loss=1.28, val_loss=0.083\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.78it/s, train_loss=0.505, val_loss=0.08\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.79it/s, train_loss=0.505, val_loss=0.08\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.79it/s, train_loss=0.505, val_loss=0.08\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.79it/s, train_loss=0.486, val_loss=0.17\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.79it/s, train_loss=0.486, val_loss=0.17\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.79it/s, train_loss=0.941, val_loss=0.10\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.79it/s, train_loss=0.941, val_loss=0.10\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.79it/s, train_loss=0.606, val_loss=0.05\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.79it/s, train_loss=0.606, val_loss=0.05\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.79it/s, train_loss=0.337, val_loss=0.04\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.78it/s, train_loss=0.337, val_loss=0.04\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.78it/s, train_loss=0.337, val_loss=0.04\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.78it/s, train_loss=0.285, val_loss=0.07\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.78it/s, train_loss=0.285, val_loss=0.07\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.78it/s, train_loss=0.439, val_loss=0.09\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.78it/s, train_loss=0.439, val_loss=0.09\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.78it/s, train_loss=0.507, val_loss=0.06\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.78it/s, train_loss=0.507, val_loss=0.06\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.78it/s, train_loss=0.376, val_loss=0.03\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.78it/s, train_loss=0.376, val_loss=0.03\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.78it/s, train_loss=0.376, val_loss=0.03\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    34: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.78it/s, train_loss=0.243, val_loss=0.04\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.78it/s, train_loss=0.243, val_loss=0.04\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.78it/s, train_loss=0.278, val_loss=0.06\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.78it/s, train_loss=0.278, val_loss=0.06\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.78it/s, train_loss=0.378, val_loss=0.05\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.78it/s, train_loss=0.378, val_loss=0.05\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.78it/s, train_loss=0.362, val_loss=0.03\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.82it/s, train_loss=0.362, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.82it/s, train_loss=0.362, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.82it/s, train_loss=0.245, val_loss=0.03\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.82it/s, train_loss=0.245, val_loss=0.03\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.82it/s, train_loss=0.191, val_loss=0.04\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.82it/s, train_loss=0.191, val_loss=0.04\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.82it/s, train_loss=0.249, val_loss=0.05\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.82it/s, train_loss=0.249, val_loss=0.05\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.82it/s, train_loss=0.295, val_loss=0.04\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.85it/s, train_loss=0.295, val_loss=0.04\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.85it/s, train_loss=0.295, val_loss=0.04\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.85it/s, train_loss=0.248, val_loss=0.03\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.85it/s, train_loss=0.248, val_loss=0.03\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.68it/s, train_loss=0.186, val_loss=0.03\u001b[A\n",
      " 49%|████████████████████▌                     | 49/100 [01:14<01:14,  1.46s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=6.51, val_loss=10.6, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=6.51, val_loss=10.6, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=49.3, val_loss=0.233, train_ci\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=49.3, val_loss=0.233, train_ci\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=1.31, val_loss=0.505, train_ci\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=1.31, val_loss=0.505, train_ci\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=2.78, val_loss=0.0707, train_c\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 34.53it/s, train_loss=2.78, val_loss=0.0707,\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 34.53it/s, train_loss=2.78, val_loss=0.0707,\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 34.53it/s, train_loss=0.461, val_loss=0.592,\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 34.53it/s, train_loss=0.461, val_loss=0.592,\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 34.53it/s, train_loss=3.34, val_loss=0.0904,\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 34.53it/s, train_loss=3.34, val_loss=0.0904,\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 34.53it/s, train_loss=0.452, val_loss=0.339,\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 34.53it/s, train_loss=0.452, val_loss=0.339,\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7437695679398872\n",
      "Its' val AUC : 0.7004213674051923\n",
      "Its external  AUC: 0.5884669922798172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 34.53it/s, train_loss=1.94, val_loss=0.705, \u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 34.77it/s, train_loss=1.94, val_loss=0.705, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.77it/s, train_loss=1.94, val_loss=0.705, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.77it/s, train_loss=3.77, val_loss=0.984, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.77it/s, train_loss=3.77, val_loss=0.984, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.77it/s, train_loss=5.13, val_loss=0.142, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.77it/s, train_loss=5.13, val_loss=0.142,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.77it/s, train_loss=0.771, val_loss=0.313\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.77it/s, train_loss=0.771, val_loss=0.313\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.77it/s, train_loss=1.61, val_loss=0.124,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.86it/s, train_loss=1.61, val_loss=0.124\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.86it/s, train_loss=1.61, val_loss=0.124\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.86it/s, train_loss=0.68, val_loss=0.21,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.86it/s, train_loss=0.68, val_loss=0.21,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.86it/s, train_loss=1.19, val_loss=0.188\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.86it/s, train_loss=1.19, val_loss=0.188\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.86it/s, train_loss=1.03, val_loss=0.154\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.86it/s, train_loss=1.03, val_loss=0.154\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.86it/s, train_loss=0.645, val_loss=0.26\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.65it/s, train_loss=0.645, val_loss=0.26\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.65it/s, train_loss=0.645, val_loss=0.26\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    10: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Curent best Test AUC: 0.7511584220413274\n",
      "Its' val AUC : 0.6419736305559332\n",
      "Its external  AUC: 0.5032298723806523\n",
      "Epoch    16: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.65it/s, train_loss=1.09, val_loss=0.169\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.65it/s, train_loss=1.09, val_loss=0.169\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.65it/s, train_loss=0.683, val_loss=0.08\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.65it/s, train_loss=0.683, val_loss=0.08\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.65it/s, train_loss=0.368, val_loss=0.11\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.65it/s, train_loss=0.368, val_loss=0.11\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.65it/s, train_loss=0.57, val_loss=0.115\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.77it/s, train_loss=0.57, val_loss=0.115\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.77it/s, train_loss=0.57, val_loss=0.115\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.77it/s, train_loss=0.592, val_loss=0.05\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.77it/s, train_loss=0.592, val_loss=0.05\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.77it/s, train_loss=0.268, val_loss=0.04\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.77it/s, train_loss=0.268, val_loss=0.04\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.77it/s, train_loss=0.217, val_loss=0.08\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.77it/s, train_loss=0.217, val_loss=0.08\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.77it/s, train_loss=0.412, val_loss=0.06\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.79it/s, train_loss=0.412, val_loss=0.06\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.79it/s, train_loss=0.412, val_loss=0.06\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.79it/s, train_loss=0.338, val_loss=0.02\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.79it/s, train_loss=0.338, val_loss=0.02\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.79it/s, train_loss=0.149, val_loss=0.04\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.79it/s, train_loss=0.149, val_loss=0.04\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.79it/s, train_loss=0.222, val_loss=0.06\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.79it/s, train_loss=0.222, val_loss=0.06\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.79it/s, train_loss=0.313, val_loss=0.03\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.76it/s, train_loss=0.313, val_loss=0.03\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.76it/s, train_loss=0.313, val_loss=0.03\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.76it/s, train_loss=0.183, val_loss=0.02\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.76it/s, train_loss=0.183, val_loss=0.02\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.76it/s, train_loss=0.12, val_loss=0.042\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.76it/s, train_loss=0.12, val_loss=0.042\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.76it/s, train_loss=0.215, val_loss=0.03\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.76it/s, train_loss=0.215, val_loss=0.03\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.76it/s, train_loss=0.196, val_loss=0.01\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.77it/s, train_loss=0.196, val_loss=0.01\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.77it/s, train_loss=0.196, val_loss=0.01\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.77it/s, train_loss=0.0938, val_loss=0.0\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.77it/s, train_loss=0.0938, val_loss=0.0\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.77it/s, train_loss=0.128, val_loss=0.03\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.77it/s, train_loss=0.128, val_loss=0.03\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.77it/s, train_loss=0.186, val_loss=0.02\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.77it/s, train_loss=0.186, val_loss=0.02\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.77it/s, train_loss=0.129, val_loss=0.02\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.70it/s, train_loss=0.129, val_loss=0.02\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.70it/s, train_loss=0.129, val_loss=0.02\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.70it/s, train_loss=0.11, val_loss=0.039\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.70it/s, train_loss=0.11, val_loss=0.039\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.70it/s, train_loss=0.166, val_loss=0.03\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.70it/s, train_loss=0.166, val_loss=0.03\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.70it/s, train_loss=0.145, val_loss=0.02\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.70it/s, train_loss=0.145, val_loss=0.02\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.70it/s, train_loss=0.109, val_loss=0.02\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.77it/s, train_loss=0.109, val_loss=0.02\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.77it/s, train_loss=0.109, val_loss=0.02\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.77it/s, train_loss=0.0929, val_loss=0.0\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.77it/s, train_loss=0.0929, val_loss=0.0\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.77it/s, train_loss=0.106, val_loss=0.02\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.77it/s, train_loss=0.106, val_loss=0.02\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.77it/s, train_loss=0.118, val_loss=0.02\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.77it/s, train_loss=0.118, val_loss=0.02\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.77it/s, train_loss=0.104, val_loss=0.01\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.76it/s, train_loss=0.104, val_loss=0.01\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.76it/s, train_loss=0.104, val_loss=0.01\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    38: reducing learning rate of group 0 to 1.2500e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.76it/s, train_loss=0.0803, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.76it/s, train_loss=0.0803, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.76it/s, train_loss=0.0752, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.76it/s, train_loss=0.0752, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.76it/s, train_loss=0.0857, val_loss=0.0\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.76it/s, train_loss=0.0857, val_loss=0.0\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.76it/s, train_loss=0.0887, val_loss=0.0\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.81it/s, train_loss=0.0887, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.81it/s, train_loss=0.0887, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.81it/s, train_loss=0.0763, val_loss=0.0\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.81it/s, train_loss=0.0763, val_loss=0.0\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.75it/s, train_loss=0.0648, val_loss=0.0\u001b[A\n",
      " 50%|█████████████████████                     | 50/100 [01:16<01:12,  1.46s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=19.2, val_loss=19.7, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=19.2, val_loss=19.7, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=94.2, val_loss=4, train_cindex\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=94.2, val_loss=4, train_cindex\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=21.8, val_loss=4.67, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=21.8, val_loss=4.67, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=32.7, val_loss=30.4, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 34.11it/s, train_loss=32.7, val_loss=30.4, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 34.11it/s, train_loss=32.7, val_loss=30.4, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 34.11it/s, train_loss=150, val_loss=56.7, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 34.11it/s, train_loss=150, val_loss=56.7, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 34.11it/s, train_loss=324, val_loss=19.7, tr\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 34.11it/s, train_loss=324, val_loss=19.7, tr\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 34.11it/s, train_loss=117, val_loss=35.6, tr\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 34.11it/s, train_loss=117, val_loss=35.6, tr\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7522855353788354\n",
      "Its' val AUC : 0.7371211091477504\n",
      "Its external  AUC: 0.6215534898377186\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 34.11it/s, train_loss=169, val_loss=20.5, tr\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 34.52it/s, train_loss=169, val_loss=20.5, tr\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.52it/s, train_loss=169, val_loss=20.5, tr\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.52it/s, train_loss=103, val_loss=5.32, tr\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.52it/s, train_loss=103, val_loss=5.32, tr\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.52it/s, train_loss=31.1, val_loss=10.5, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.52it/s, train_loss=31.1, val_loss=10.5, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.52it/s, train_loss=55.8, val_loss=13.2, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.52it/s, train_loss=55.8, val_loss=13.2, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.52it/s, train_loss=68.2, val_loss=7.62, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.64it/s, train_loss=68.2, val_loss=7.62,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.64it/s, train_loss=68.2, val_loss=7.62,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.64it/s, train_loss=40, val_loss=2.81, t\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.64it/s, train_loss=40, val_loss=2.81, t\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.64it/s, train_loss=16.5, val_loss=5.26,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.64it/s, train_loss=16.5, val_loss=5.26,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.64it/s, train_loss=29.2, val_loss=5.55,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.64it/s, train_loss=29.2, val_loss=5.55,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.64it/s, train_loss=28.4, val_loss=1.65,\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.67it/s, train_loss=28.4, val_loss=1.65,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.67it/s, train_loss=28.4, val_loss=1.65,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.67it/s, train_loss=7.78, val_loss=1.15,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.67it/s, train_loss=7.78, val_loss=1.15,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.67it/s, train_loss=7.53, val_loss=2.97,\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.67it/s, train_loss=7.53, val_loss=2.97,\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.67it/s, train_loss=18.9, val_loss=2.92,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.67it/s, train_loss=18.9, val_loss=2.92,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.67it/s, train_loss=18.9, val_loss=1.08,\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.73it/s, train_loss=18.9, val_loss=1.08,\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.73it/s, train_loss=18.9, val_loss=1.08,\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.73it/s, train_loss=7.75, val_loss=0.726\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.73it/s, train_loss=7.75, val_loss=0.726\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.73it/s, train_loss=4.01, val_loss=2.09,\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.73it/s, train_loss=4.01, val_loss=2.09,\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.73it/s, train_loss=10.6, val_loss=1.94,\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.73it/s, train_loss=10.6, val_loss=1.94,\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.73it/s, train_loss=10.6, val_loss=0.863\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.69it/s, train_loss=10.6, val_loss=0.863\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.69it/s, train_loss=10.6, val_loss=0.863\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.69it/s, train_loss=5.28, val_loss=1.34,\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.69it/s, train_loss=5.28, val_loss=1.34,\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.69it/s, train_loss=7.09, val_loss=1.99,\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.69it/s, train_loss=7.09, val_loss=1.99,\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.69it/s, train_loss=10.1, val_loss=1.18,\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.69it/s, train_loss=10.1, val_loss=1.18,\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.69it/s, train_loss=6.48, val_loss=0.656\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.70it/s, train_loss=6.48, val_loss=0.656\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.70it/s, train_loss=6.48, val_loss=0.656\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.70it/s, train_loss=4.15, val_loss=0.45,\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.70it/s, train_loss=4.15, val_loss=0.45,\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.70it/s, train_loss=3.36, val_loss=0.618\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.70it/s, train_loss=3.36, val_loss=0.618\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.70it/s, train_loss=4.33, val_loss=0.828\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.70it/s, train_loss=4.33, val_loss=0.828\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.70it/s, train_loss=5.34, val_loss=0.768\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.65it/s, train_loss=5.34, val_loss=0.768\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.65it/s, train_loss=5.34, val_loss=0.768\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.65it/s, train_loss=4.9, val_loss=0.513,\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.65it/s, train_loss=4.9, val_loss=0.513,\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    27: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.65it/s, train_loss=3.45, val_loss=0.366\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.65it/s, train_loss=3.45, val_loss=0.366\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.65it/s, train_loss=2.55, val_loss=0.439\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.65it/s, train_loss=2.55, val_loss=0.439\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.65it/s, train_loss=2.75, val_loss=0.554\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.73it/s, train_loss=2.75, val_loss=0.554\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.73it/s, train_loss=2.75, val_loss=0.554\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.73it/s, train_loss=3.18, val_loss=0.531\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.73it/s, train_loss=3.18, val_loss=0.531\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.73it/s, train_loss=2.91, val_loss=0.403\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.73it/s, train_loss=2.91, val_loss=0.403\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.73it/s, train_loss=2.12, val_loss=0.337\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.73it/s, train_loss=2.12, val_loss=0.337\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.73it/s, train_loss=1.65, val_loss=0.396\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.75it/s, train_loss=1.65, val_loss=0.396\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.75it/s, train_loss=1.65, val_loss=0.396\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.75it/s, train_loss=1.82, val_loss=0.462\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.75it/s, train_loss=1.82, val_loss=0.462\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.75it/s, train_loss=2.06, val_loss=0.408\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.75it/s, train_loss=2.06, val_loss=0.408\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.75it/s, train_loss=1.77, val_loss=0.274\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.75it/s, train_loss=1.77, val_loss=0.274\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.75it/s, train_loss=1.15, val_loss=0.189\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.77it/s, train_loss=1.15, val_loss=0.189\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.77it/s, train_loss=1.15, val_loss=0.189\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.77it/s, train_loss=0.844, val_loss=0.19\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.77it/s, train_loss=0.844, val_loss=0.19\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.77it/s, train_loss=0.991, val_loss=0.21\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.77it/s, train_loss=0.991, val_loss=0.21\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.77it/s, train_loss=1.16, val_loss=0.18,\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.77it/s, train_loss=1.16, val_loss=0.18,\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.77it/s, train_loss=1.01, val_loss=0.136\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.76it/s, train_loss=1.01, val_loss=0.136\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.76it/s, train_loss=1.01, val_loss=0.136\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.76it/s, train_loss=0.718, val_loss=0.14\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.76it/s, train_loss=0.718, val_loss=0.14\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.69it/s, train_loss=0.638, val_loss=0.18\u001b[A\n",
      " 51%|█████████████████████▍                    | 51/100 [01:17<01:11,  1.46s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=40.2, val_loss=64.2, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=40.2, val_loss=64.2, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=335, val_loss=16.4, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=335, val_loss=16.4, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=113, val_loss=6.2, train_cinde\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=113, val_loss=6.2, train_cinde\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=37.7, val_loss=4.3, train_cind\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.41it/s, train_loss=37.7, val_loss=4.3, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.41it/s, train_loss=37.7, val_loss=4.3, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.41it/s, train_loss=23, val_loss=2.08, tra\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.41it/s, train_loss=23, val_loss=2.08, tra\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.41it/s, train_loss=14.3, val_loss=6.78, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.41it/s, train_loss=14.3, val_loss=6.78, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.41it/s, train_loss=35.1, val_loss=1.36, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.41it/s, train_loss=35.1, val_loss=1.36, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.2388227927363807\n",
      "Its' val AUC : 0.2752480630691858\n",
      "Its external  AUC: 0.3853789191744131\n",
      "Curent best Test AUC: 0.5187226048841578\n",
      "Its' val AUC : 0.5345929047165965\n",
      "Its external  AUC: 0.6106822120686939\n",
      "Curent best Test AUC: 0.6629931120851597\n",
      "Its' val AUC : 0.653663177925785\n",
      "Its external  AUC: 0.6251772490940601\n",
      "Curent best Test AUC: 0.7546649968691296\n",
      "Its' val AUC : 0.7016446921299443\n",
      "Its external  AUC: 0.6240743658421302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.41it/s, train_loss=9.32, val_loss=2.52, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 33.52it/s, train_loss=9.32, val_loss=2.52, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.52it/s, train_loss=9.32, val_loss=2.52, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.52it/s, train_loss=13.5, val_loss=0.84, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.52it/s, train_loss=13.5, val_loss=0.84, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.52it/s, train_loss=5.56, val_loss=2.65, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.52it/s, train_loss=5.56, val_loss=2.65, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.52it/s, train_loss=17, val_loss=0.874, t\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.52it/s, train_loss=17, val_loss=0.874, t\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.52it/s, train_loss=5.26, val_loss=1.18, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.15it/s, train_loss=5.26, val_loss=1.18,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.15it/s, train_loss=5.26, val_loss=1.18,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.15it/s, train_loss=6.25, val_loss=0.414\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.15it/s, train_loss=6.25, val_loss=0.414\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.15it/s, train_loss=2.24, val_loss=1.99,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.15it/s, train_loss=2.24, val_loss=1.99,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.15it/s, train_loss=9.72, val_loss=0.697\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.15it/s, train_loss=9.72, val_loss=0.697\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.15it/s, train_loss=4.68, val_loss=0.902\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.35it/s, train_loss=4.68, val_loss=0.902\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.35it/s, train_loss=4.68, val_loss=0.902\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.35it/s, train_loss=5.34, val_loss=0.227\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.35it/s, train_loss=5.34, val_loss=0.227\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.35it/s, train_loss=1.44, val_loss=0.951\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.35it/s, train_loss=1.44, val_loss=0.951\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.35it/s, train_loss=5.94, val_loss=0.749\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.35it/s, train_loss=5.94, val_loss=0.749\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.35it/s, train_loss=4.32, val_loss=0.668\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.57it/s, train_loss=4.32, val_loss=0.668\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.57it/s, train_loss=4.32, val_loss=0.668\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.57it/s, train_loss=3.67, val_loss=0.46,\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.57it/s, train_loss=3.67, val_loss=0.46,\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.57it/s, train_loss=2.26, val_loss=0.46,\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.57it/s, train_loss=2.26, val_loss=0.46,\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.57it/s, train_loss=2.19, val_loss=0.613\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.57it/s, train_loss=2.19, val_loss=0.613\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.57it/s, train_loss=3.84, val_loss=0.355\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.63it/s, train_loss=3.84, val_loss=0.355\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.63it/s, train_loss=3.84, val_loss=0.355\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.63it/s, train_loss=2.25, val_loss=0.194\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.63it/s, train_loss=2.25, val_loss=0.194\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.63it/s, train_loss=0.886, val_loss=0.43\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.63it/s, train_loss=0.886, val_loss=0.43\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.63it/s, train_loss=2.03, val_loss=0.123\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.63it/s, train_loss=2.03, val_loss=0.123\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.63it/s, train_loss=0.714, val_loss=0.21\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.69it/s, train_loss=0.714, val_loss=0.21\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.69it/s, train_loss=0.714, val_loss=0.21\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.69it/s, train_loss=1.25, val_loss=0.385\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.69it/s, train_loss=1.25, val_loss=0.385\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    23: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.69it/s, train_loss=2.12, val_loss=0.116\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.69it/s, train_loss=2.12, val_loss=0.116\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.69it/s, train_loss=0.786, val_loss=0.20\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.69it/s, train_loss=0.786, val_loss=0.20\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.69it/s, train_loss=1.17, val_loss=0.169\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.71it/s, train_loss=1.17, val_loss=0.169\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.71it/s, train_loss=1.17, val_loss=0.169\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.71it/s, train_loss=0.877, val_loss=0.05\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.71it/s, train_loss=0.877, val_loss=0.05\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.71it/s, train_loss=0.308, val_loss=0.21\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.71it/s, train_loss=0.308, val_loss=0.21\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.71it/s, train_loss=1.23, val_loss=0.132\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.71it/s, train_loss=1.23, val_loss=0.132\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.71it/s, train_loss=0.793, val_loss=0.13\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.65it/s, train_loss=0.793, val_loss=0.13\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.65it/s, train_loss=0.793, val_loss=0.13\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.65it/s, train_loss=0.677, val_loss=0.17\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.65it/s, train_loss=0.677, val_loss=0.17\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.65it/s, train_loss=0.828, val_loss=0.03\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.65it/s, train_loss=0.828, val_loss=0.03\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.65it/s, train_loss=0.182, val_loss=0.11\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.65it/s, train_loss=0.182, val_loss=0.11\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.65it/s, train_loss=0.622, val_loss=0.10\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.63it/s, train_loss=0.622, val_loss=0.10\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.63it/s, train_loss=0.622, val_loss=0.10\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.63it/s, train_loss=0.578, val_loss=0.06\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.63it/s, train_loss=0.578, val_loss=0.06\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.63it/s, train_loss=0.428, val_loss=0.11\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.63it/s, train_loss=0.428, val_loss=0.11\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.63it/s, train_loss=0.624, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.63it/s, train_loss=0.624, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.63it/s, train_loss=0.171, val_loss=0.06\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.69it/s, train_loss=0.171, val_loss=0.06\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.69it/s, train_loss=0.171, val_loss=0.06\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.69it/s, train_loss=0.353, val_loss=0.05\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.69it/s, train_loss=0.353, val_loss=0.05\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.69it/s, train_loss=0.339, val_loss=0.03\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.69it/s, train_loss=0.339, val_loss=0.03\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.69it/s, train_loss=0.19, val_loss=0.060\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.69it/s, train_loss=0.19, val_loss=0.060\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.69it/s, train_loss=0.25, val_loss=0.083\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.74it/s, train_loss=0.25, val_loss=0.083\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.74it/s, train_loss=0.25, val_loss=0.083\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.74it/s, train_loss=0.341, val_loss=0.05\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.74it/s, train_loss=0.341, val_loss=0.05\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.55it/s, train_loss=0.218, val_loss=0.02\u001b[A\n",
      " 52%|█████████████████████▊                    | 52/100 [01:19<01:10,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    44: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=11.9, val_loss=31.4, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=11.9, val_loss=31.4, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=159, val_loss=39.8, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=159, val_loss=39.8, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=208, val_loss=0.73, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=208, val_loss=0.73, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=5.31, val_loss=15.9, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.31it/s, train_loss=5.31, val_loss=15.9, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.31it/s, train_loss=5.31, val_loss=15.9, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.31it/s, train_loss=82.1, val_loss=28, tra\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.31it/s, train_loss=82.1, val_loss=28, tra\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.31it/s, train_loss=141, val_loss=3.62, tr\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.31it/s, train_loss=141, val_loss=3.62, tr\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.31it/s, train_loss=22.3, val_loss=14.9, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.31it/s, train_loss=22.3, val_loss=14.9, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.25948653725735754\n",
      "Its' val AUC : 0.28082098681527795\n",
      "Its external  AUC: 0.4000315109500551\n",
      "Curent best Test AUC: 0.7180964308077645\n",
      "Its' val AUC : 0.7107516650808754\n",
      "Its external  AUC: 0.6224988183393729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.31it/s, train_loss=87.9, val_loss=4.42, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 33.74it/s, train_loss=87.9, val_loss=4.42, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.74it/s, train_loss=87.9, val_loss=4.42, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.74it/s, train_loss=30.9, val_loss=7.56, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.74it/s, train_loss=30.9, val_loss=7.56, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.74it/s, train_loss=42.4, val_loss=7.34, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.74it/s, train_loss=42.4, val_loss=7.34, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.74it/s, train_loss=41.2, val_loss=3.51, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.74it/s, train_loss=41.2, val_loss=3.51, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.74it/s, train_loss=22.2, val_loss=2.65, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.13it/s, train_loss=22.2, val_loss=2.65,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.13it/s, train_loss=22.2, val_loss=2.65,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.13it/s, train_loss=18.3, val_loss=4.66,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.13it/s, train_loss=18.3, val_loss=4.66,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.13it/s, train_loss=28.5, val_loss=2.62,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.13it/s, train_loss=28.5, val_loss=2.62,\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.720100187852223\n",
      "Its' val AUC : 0.5803996194100857\n",
      "Its external  AUC: 0.6380967386166693\n",
      "Epoch     9: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Curent best Test AUC: 0.7211020663744521\n",
      "Its' val AUC : 0.5958950659236102\n",
      "Its external  AUC: 0.6417204978730109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.13it/s, train_loss=16.9, val_loss=1.5, \u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.13it/s, train_loss=16.9, val_loss=1.5, \u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.13it/s, train_loss=9.32, val_loss=1.73,\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.07it/s, train_loss=9.32, val_loss=1.73,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.07it/s, train_loss=9.32, val_loss=1.73,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.07it/s, train_loss=9.67, val_loss=1.95,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.07it/s, train_loss=9.67, val_loss=1.95,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.07it/s, train_loss=10.3, val_loss=1.72,\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.07it/s, train_loss=10.3, val_loss=1.72,\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.07it/s, train_loss=9.04, val_loss=1.15,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.07it/s, train_loss=9.04, val_loss=1.15,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.07it/s, train_loss=6.23, val_loss=0.693\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.29it/s, train_loss=6.23, val_loss=0.693\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.29it/s, train_loss=6.23, val_loss=0.693\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.29it/s, train_loss=3.96, val_loss=0.653\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.29it/s, train_loss=3.96, val_loss=0.653\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    15: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.29it/s, train_loss=3.69, val_loss=0.865\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.29it/s, train_loss=3.69, val_loss=0.865\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.29it/s, train_loss=4.6, val_loss=0.92, \u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.29it/s, train_loss=4.6, val_loss=0.92, \u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.29it/s, train_loss=4.75, val_loss=0.68,\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.23it/s, train_loss=4.75, val_loss=0.68,\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.23it/s, train_loss=4.75, val_loss=0.68,\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.23it/s, train_loss=3.53, val_loss=0.396\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.23it/s, train_loss=3.53, val_loss=0.396\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.23it/s, train_loss=2.15, val_loss=0.323\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.23it/s, train_loss=2.15, val_loss=0.323\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.23it/s, train_loss=1.81, val_loss=0.397\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.23it/s, train_loss=1.81, val_loss=0.397\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.23it/s, train_loss=2.14, val_loss=0.4, \u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.35it/s, train_loss=2.14, val_loss=0.4, \u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.35it/s, train_loss=2.14, val_loss=0.4, \u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.35it/s, train_loss=2.09, val_loss=0.276\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.35it/s, train_loss=2.09, val_loss=0.276\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7221039448966813\n",
      "Its' val AUC : 0.6123419872230529\n",
      "Its external  AUC: 0.6484953521348669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.35it/s, train_loss=1.43, val_loss=0.152\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.35it/s, train_loss=1.43, val_loss=0.152\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.35it/s, train_loss=0.84, val_loss=0.149\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.35it/s, train_loss=0.84, val_loss=0.149\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.35it/s, train_loss=0.932, val_loss=0.22\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.45it/s, train_loss=0.932, val_loss=0.22\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.45it/s, train_loss=0.932, val_loss=0.22\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.45it/s, train_loss=1.39, val_loss=0.237\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.45it/s, train_loss=1.39, val_loss=0.237\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.45it/s, train_loss=1.49, val_loss=0.172\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.45it/s, train_loss=1.49, val_loss=0.172\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.45it/s, train_loss=1.1, val_loss=0.126,\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.45it/s, train_loss=1.1, val_loss=0.126,\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.45it/s, train_loss=0.756, val_loss=0.16\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.53it/s, train_loss=0.756, val_loss=0.16\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.53it/s, train_loss=0.756, val_loss=0.16\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.53it/s, train_loss=0.865, val_loss=0.23\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.53it/s, train_loss=0.865, val_loss=0.23\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.53it/s, train_loss=1.17, val_loss=0.239\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.53it/s, train_loss=1.17, val_loss=0.239\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.53it/s, train_loss=1.24, val_loss=0.194\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.53it/s, train_loss=1.24, val_loss=0.194\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.53it/s, train_loss=1.04, val_loss=0.162\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.60it/s, train_loss=1.04, val_loss=0.162\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.60it/s, train_loss=1.04, val_loss=0.162\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.60it/s, train_loss=0.903, val_loss=0.17\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.60it/s, train_loss=0.903, val_loss=0.17\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.60it/s, train_loss=0.973, val_loss=0.17\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.60it/s, train_loss=0.973, val_loss=0.17\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.60it/s, train_loss=0.997, val_loss=0.16\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.60it/s, train_loss=0.997, val_loss=0.16\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.60it/s, train_loss=0.941, val_loss=0.14\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.64it/s, train_loss=0.941, val_loss=0.14\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.64it/s, train_loss=0.941, val_loss=0.14\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.64it/s, train_loss=0.833, val_loss=0.12\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.64it/s, train_loss=0.833, val_loss=0.12\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.64it/s, train_loss=0.74, val_loss=0.116\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.64it/s, train_loss=0.74, val_loss=0.116\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.64it/s, train_loss=0.71, val_loss=0.123\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.64it/s, train_loss=0.71, val_loss=0.123\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    41: reducing learning rate of group 0 to 1.2500e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.64it/s, train_loss=0.735, val_loss=0.13\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.62it/s, train_loss=0.735, val_loss=0.13\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.62it/s, train_loss=0.735, val_loss=0.13\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.62it/s, train_loss=0.767, val_loss=0.12\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.62it/s, train_loss=0.767, val_loss=0.12\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.27it/s, train_loss=0.761, val_loss=0.11\u001b[A\n",
      " 53%|██████████████████████▎                   | 53/100 [01:20<01:09,  1.47s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=12.5, val_loss=47.7, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=12.5, val_loss=47.7, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=241, val_loss=4.79, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=241, val_loss=4.79, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=29.2, val_loss=5.12, train_cin\u001b[A\n",
      "Epoch 2:   6%| | 3/50 [00:00<00:01, 27.53it/s, train_loss=29.2, val_loss=5.12, t\u001b[A\n",
      "Epoch 3:   6%| | 3/50 [00:00<00:01, 27.53it/s, train_loss=29.2, val_loss=5.12, t\u001b[A\n",
      "Epoch 3:   6%| | 3/50 [00:00<00:01, 27.53it/s, train_loss=34.5, val_loss=14.9, t\u001b[A\n",
      "Epoch 4:   6%| | 3/50 [00:00<00:01, 27.53it/s, train_loss=34.5, val_loss=14.9, t\u001b[A\n",
      "Epoch 4:   6%| | 3/50 [00:00<00:01, 27.53it/s, train_loss=71.8, val_loss=2.67, t\u001b[A\n",
      "Epoch 5:   6%| | 3/50 [00:00<00:01, 27.53it/s, train_loss=71.8, val_loss=2.67, t\u001b[A\n",
      "Epoch 5:   6%| | 3/50 [00:00<00:01, 27.53it/s, train_loss=15.5, val_loss=1.37, t\u001b[A\n",
      "Epoch 6:   6%| | 3/50 [00:00<00:01, 27.53it/s, train_loss=15.5, val_loss=1.37, t\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.3148403256105197\n",
      "Its' val AUC : 0.29917085768655705\n",
      "Its external  AUC: 0.3959350874428864\n",
      "Curent best Test AUC: 0.7437695679398872\n",
      "Its' val AUC : 0.674323773277151\n",
      "Its external  AUC: 0.6694501339215377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6:   6%| | 3/50 [00:00<00:01, 27.53it/s, train_loss=7.15, val_loss=3.17, t\u001b[A\n",
      "Epoch 6:  14%|▏| 7/50 [00:00<00:01, 30.81it/s, train_loss=7.15, val_loss=3.17, t\u001b[A\n",
      "Epoch 7:  14%|▏| 7/50 [00:00<00:01, 30.81it/s, train_loss=7.15, val_loss=3.17, t\u001b[A\n",
      "Epoch 7:  14%|▏| 7/50 [00:00<00:01, 30.81it/s, train_loss=20.4, val_loss=2.12, t\u001b[A\n",
      "Epoch 8:  14%|▏| 7/50 [00:00<00:01, 30.81it/s, train_loss=20.4, val_loss=2.12, t\u001b[A\n",
      "Epoch 8:  14%|▏| 7/50 [00:00<00:01, 30.81it/s, train_loss=13.2, val_loss=1.51, t\u001b[A\n",
      "Epoch 9:  14%|▏| 7/50 [00:00<00:01, 30.81it/s, train_loss=13.2, val_loss=1.51, t\u001b[A\n",
      "Epoch 9:  14%|▏| 7/50 [00:00<00:01, 30.81it/s, train_loss=7.92, val_loss=1.61, t\u001b[A\n",
      "Epoch 10:  14%|▏| 7/50 [00:00<00:01, 30.81it/s, train_loss=7.92, val_loss=1.61, \u001b[A\n",
      "Epoch 10:  14%|▏| 7/50 [00:00<00:01, 30.81it/s, train_loss=9.82, val_loss=1.49, \u001b[A\n",
      "Epoch 10:  22%|▏| 11/50 [00:00<00:01, 31.70it/s, train_loss=9.82, val_loss=1.49,\u001b[A\n",
      "Epoch 11:  22%|▏| 11/50 [00:00<00:01, 31.70it/s, train_loss=9.82, val_loss=1.49,\u001b[A\n",
      "Epoch 11:  22%|▏| 11/50 [00:00<00:01, 31.70it/s, train_loss=7.07, val_loss=1.02,\u001b[A\n",
      "Epoch 12:  22%|▏| 11/50 [00:00<00:01, 31.70it/s, train_loss=7.07, val_loss=1.02,\u001b[A\n",
      "Epoch 12:  22%|▏| 11/50 [00:00<00:01, 31.70it/s, train_loss=4.7, val_loss=1.36, \u001b[A\n",
      "Epoch 13:  22%|▏| 11/50 [00:00<00:01, 31.70it/s, train_loss=4.7, val_loss=1.36, \u001b[A\n",
      "Epoch 13:  22%|▏| 11/50 [00:00<00:01, 31.70it/s, train_loss=8.46, val_loss=0.713\u001b[A\n",
      "Epoch 14:  22%|▏| 11/50 [00:00<00:01, 31.70it/s, train_loss=8.46, val_loss=0.713\u001b[A\n",
      "Epoch 14:  22%|▏| 11/50 [00:00<00:01, 31.70it/s, train_loss=4.53, val_loss=0.495\u001b[A\n",
      "Epoch 14:  30%|▎| 15/50 [00:00<00:01, 31.92it/s, train_loss=4.53, val_loss=0.495\u001b[A\n",
      "Epoch 15:  30%|▎| 15/50 [00:00<00:01, 31.92it/s, train_loss=4.53, val_loss=0.495\u001b[A\n",
      "Epoch 15:  30%|▎| 15/50 [00:00<00:01, 31.92it/s, train_loss=2.75, val_loss=0.266\u001b[A\n",
      "Epoch 16:  30%|▎| 15/50 [00:00<00:01, 31.92it/s, train_loss=2.75, val_loss=0.266\u001b[A\n",
      "Epoch 16:  30%|▎| 15/50 [00:00<00:01, 31.92it/s, train_loss=1.81, val_loss=0.262\u001b[A\n",
      "Epoch 17:  30%|▎| 15/50 [00:00<00:01, 31.92it/s, train_loss=1.81, val_loss=0.262\u001b[A\n",
      "Epoch 17:  30%|▎| 15/50 [00:00<00:01, 31.92it/s, train_loss=1.6, val_loss=0.288,\u001b[A\n",
      "Epoch 18:  30%|▎| 15/50 [00:00<00:01, 31.92it/s, train_loss=1.6, val_loss=0.288,\u001b[A\n",
      "Epoch 18:  30%|▎| 15/50 [00:00<00:01, 31.92it/s, train_loss=1.51, val_loss=0.1, \u001b[A\n",
      "Epoch 18:  38%|▍| 19/50 [00:00<00:00, 32.21it/s, train_loss=1.51, val_loss=0.1, \u001b[A\n",
      "Epoch 19:  38%|▍| 19/50 [00:00<00:00, 32.21it/s, train_loss=1.51, val_loss=0.1, \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7561678146524734\n",
      "Its' val AUC : 0.7085768655702053\n",
      "Its external  AUC: 0.6458169213801797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19:  38%|▍| 19/50 [00:00<00:00, 32.21it/s, train_loss=0.498, val_loss=0.18\u001b[A\n",
      "Epoch 20:  38%|▍| 19/50 [00:00<00:00, 32.21it/s, train_loss=0.498, val_loss=0.18\u001b[A\n",
      "Epoch 20:  38%|▍| 19/50 [00:00<00:00, 32.21it/s, train_loss=0.782, val_loss=0.20\u001b[A\n",
      "Epoch 21:  38%|▍| 19/50 [00:00<00:00, 32.21it/s, train_loss=0.782, val_loss=0.20\u001b[A\n",
      "Epoch 21:  38%|▍| 19/50 [00:00<00:00, 32.21it/s, train_loss=1.18, val_loss=0.154\u001b[A\n",
      "Epoch 22:  38%|▍| 19/50 [00:00<00:00, 32.21it/s, train_loss=1.18, val_loss=0.154\u001b[A\n",
      "Epoch 22:  38%|▍| 19/50 [00:00<00:00, 32.21it/s, train_loss=0.808, val_loss=0.12\u001b[A\n",
      "Epoch 22:  46%|▍| 23/50 [00:00<00:00, 32.14it/s, train_loss=0.808, val_loss=0.12\u001b[A\n",
      "Epoch 23:  46%|▍| 23/50 [00:00<00:00, 32.14it/s, train_loss=0.808, val_loss=0.12\u001b[A\n",
      "Epoch 23:  46%|▍| 23/50 [00:00<00:00, 32.14it/s, train_loss=0.66, val_loss=0.108\u001b[A\n",
      "Epoch 24:  46%|▍| 23/50 [00:00<00:00, 32.14it/s, train_loss=0.66, val_loss=0.108\u001b[A\n",
      "Epoch 24:  46%|▍| 23/50 [00:00<00:00, 32.14it/s, train_loss=0.7, val_loss=0.13, \u001b[A\n",
      "Epoch 25:  46%|▍| 23/50 [00:00<00:00, 32.14it/s, train_loss=0.7, val_loss=0.13, \u001b[A\n",
      "Epoch 25:  46%|▍| 23/50 [00:00<00:00, 32.14it/s, train_loss=0.779, val_loss=0.09\u001b[A\n",
      "Epoch 26:  46%|▍| 23/50 [00:00<00:00, 32.14it/s, train_loss=0.779, val_loss=0.09\u001b[A\n",
      "Epoch 26:  46%|▍| 23/50 [00:00<00:00, 32.14it/s, train_loss=0.593, val_loss=0.07\u001b[A\n",
      "Epoch 26:  54%|▌| 27/50 [00:00<00:00, 32.51it/s, train_loss=0.593, val_loss=0.07\u001b[A\n",
      "Epoch 27:  54%|▌| 27/50 [00:00<00:00, 32.51it/s, train_loss=0.593, val_loss=0.07\u001b[A\n",
      "Epoch 27:  54%|▌| 27/50 [00:00<00:00, 32.51it/s, train_loss=0.444, val_loss=0.07\u001b[A\n",
      "Epoch 28:  54%|▌| 27/50 [00:00<00:00, 32.51it/s, train_loss=0.444, val_loss=0.07\u001b[A\n",
      "Epoch 28:  54%|▌| 27/50 [00:00<00:00, 32.51it/s, train_loss=0.353, val_loss=0.08\u001b[A\n",
      "Epoch 29:  54%|▌| 27/50 [00:00<00:00, 32.51it/s, train_loss=0.353, val_loss=0.08\u001b[A\n",
      "Epoch 29:  54%|▌| 27/50 [00:00<00:00, 32.51it/s, train_loss=0.444, val_loss=0.09\u001b[A\n",
      "Epoch 30:  54%|▌| 27/50 [00:00<00:00, 32.51it/s, train_loss=0.444, val_loss=0.09\u001b[A\n",
      "Epoch 30:  54%|▌| 27/50 [00:00<00:00, 32.51it/s, train_loss=0.489, val_loss=0.07\u001b[A\n",
      "Epoch 30:  62%|▌| 31/50 [00:00<00:00, 33.20it/s, train_loss=0.489, val_loss=0.07\u001b[A\n",
      "Epoch 31:  62%|▌| 31/50 [00:00<00:00, 33.20it/s, train_loss=0.489, val_loss=0.07\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    25: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31:  62%|▌| 31/50 [00:00<00:00, 33.20it/s, train_loss=0.412, val_loss=0.06\u001b[A\n",
      "Epoch 32:  62%|▌| 31/50 [00:00<00:00, 33.20it/s, train_loss=0.412, val_loss=0.06\u001b[A\n",
      "Epoch 32:  62%|▌| 31/50 [00:01<00:00, 33.20it/s, train_loss=0.332, val_loss=0.05\u001b[A\n",
      "Epoch 33:  62%|▌| 31/50 [00:01<00:00, 33.20it/s, train_loss=0.332, val_loss=0.05\u001b[A\n",
      "Epoch 33:  62%|▌| 31/50 [00:01<00:00, 33.20it/s, train_loss=0.28, val_loss=0.068\u001b[A\n",
      "Epoch 34:  62%|▌| 31/50 [00:01<00:00, 33.20it/s, train_loss=0.28, val_loss=0.068\u001b[A\n",
      "Epoch 34:  62%|▌| 31/50 [00:01<00:00, 33.20it/s, train_loss=0.34, val_loss=0.065\u001b[A\n",
      "Epoch 34:  70%|▋| 35/50 [00:01<00:00, 33.64it/s, train_loss=0.34, val_loss=0.065\u001b[A\n",
      "Epoch 35:  70%|▋| 35/50 [00:01<00:00, 33.64it/s, train_loss=0.34, val_loss=0.065\u001b[A\n",
      "Epoch 35:  70%|▋| 35/50 [00:01<00:00, 33.64it/s, train_loss=0.337, val_loss=0.05\u001b[A\n",
      "Epoch 36:  70%|▋| 35/50 [00:01<00:00, 33.64it/s, train_loss=0.337, val_loss=0.05\u001b[A\n",
      "Epoch 36:  70%|▋| 35/50 [00:01<00:00, 33.64it/s, train_loss=0.274, val_loss=0.04\u001b[A\n",
      "Epoch 37:  70%|▋| 35/50 [00:01<00:00, 33.64it/s, train_loss=0.274, val_loss=0.04\u001b[A\n",
      "Epoch 37:  70%|▋| 35/50 [00:01<00:00, 33.64it/s, train_loss=0.198, val_loss=0.04\u001b[A\n",
      "Epoch 38:  70%|▋| 35/50 [00:01<00:00, 33.64it/s, train_loss=0.198, val_loss=0.04\u001b[A\n",
      "Epoch 38:  70%|▋| 35/50 [00:01<00:00, 33.64it/s, train_loss=0.229, val_loss=0.04\u001b[A\n",
      "Epoch 38:  78%|▊| 39/50 [00:01<00:00, 33.97it/s, train_loss=0.229, val_loss=0.04\u001b[A\n",
      "Epoch 39:  78%|▊| 39/50 [00:01<00:00, 33.97it/s, train_loss=0.229, val_loss=0.04\u001b[A\n",
      "Epoch 39:  78%|▊| 39/50 [00:01<00:00, 33.97it/s, train_loss=0.243, val_loss=0.05\u001b[A\n",
      "Epoch 40:  78%|▊| 39/50 [00:01<00:00, 33.97it/s, train_loss=0.243, val_loss=0.05\u001b[A\n",
      "Epoch 40:  78%|▊| 39/50 [00:01<00:00, 33.97it/s, train_loss=0.25, val_loss=0.040\u001b[A\n",
      "Epoch 41:  78%|▊| 39/50 [00:01<00:00, 33.97it/s, train_loss=0.25, val_loss=0.040\u001b[A\n",
      "Epoch 41:  78%|▊| 39/50 [00:01<00:00, 33.97it/s, train_loss=0.21, val_loss=0.041\u001b[A\n",
      "Epoch 42:  78%|▊| 39/50 [00:01<00:00, 33.97it/s, train_loss=0.21, val_loss=0.041\u001b[A\n",
      "Epoch 42:  78%|▊| 39/50 [00:01<00:00, 33.97it/s, train_loss=0.211, val_loss=0.04\u001b[A\n",
      "Epoch 42:  86%|▊| 43/50 [00:01<00:00, 34.20it/s, train_loss=0.211, val_loss=0.04\u001b[A\n",
      "Epoch 43:  86%|▊| 43/50 [00:01<00:00, 34.20it/s, train_loss=0.211, val_loss=0.04\u001b[A\n",
      "Epoch 43:  86%|▊| 43/50 [00:01<00:00, 34.20it/s, train_loss=0.201, val_loss=0.04\u001b[A\n",
      "Epoch 44:  86%|▊| 43/50 [00:01<00:00, 34.20it/s, train_loss=0.201, val_loss=0.04\u001b[A\n",
      "Epoch 44:  86%|▊| 43/50 [00:01<00:00, 34.20it/s, train_loss=0.2, val_loss=0.039,\u001b[A\n",
      "Epoch 45:  86%|▊| 43/50 [00:01<00:00, 34.20it/s, train_loss=0.2, val_loss=0.039,\u001b[A\n",
      "Epoch 45:  86%|▊| 43/50 [00:01<00:00, 34.20it/s, train_loss=0.19, val_loss=0.036\u001b[A\n",
      "Epoch 46:  86%|▊| 43/50 [00:01<00:00, 34.20it/s, train_loss=0.19, val_loss=0.036\u001b[A\n",
      "Epoch 46:  86%|▊| 43/50 [00:01<00:00, 34.20it/s, train_loss=0.171, val_loss=0.03\u001b[A\n",
      "Epoch 46:  94%|▉| 47/50 [00:01<00:00, 34.27it/s, train_loss=0.171, val_loss=0.03\u001b[A\n",
      "Epoch 47:  94%|▉| 47/50 [00:01<00:00, 34.27it/s, train_loss=0.171, val_loss=0.03\u001b[A\n",
      "Epoch 47:  94%|▉| 47/50 [00:01<00:00, 34.27it/s, train_loss=0.157, val_loss=0.03\u001b[A\n",
      "Epoch 48:  94%|▉| 47/50 [00:01<00:00, 34.27it/s, train_loss=0.157, val_loss=0.03\u001b[A\n",
      "Epoch 48:  94%|▉| 47/50 [00:01<00:00, 34.27it/s, train_loss=0.152, val_loss=0.03\u001b[A\n",
      "Epoch 49:  94%|▉| 47/50 [00:01<00:00, 34.27it/s, train_loss=0.152, val_loss=0.03\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    43: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 33.12it/s, train_loss=0.151, val_loss=0.03\u001b[A\n",
      " 54%|██████████████████████▋                   | 54/100 [01:22<01:08,  1.49s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=20.6, val_loss=56.8, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=20.6, val_loss=56.8, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=302, val_loss=41.8, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=302, val_loss=41.8, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=207, val_loss=5.74, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=207, val_loss=5.74, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=26.6, val_loss=7.65, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 32.42it/s, train_loss=26.6, val_loss=7.65, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.42it/s, train_loss=26.6, val_loss=7.65, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.42it/s, train_loss=44.2, val_loss=3.92, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.42it/s, train_loss=44.2, val_loss=3.92, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.42it/s, train_loss=21.9, val_loss=5.06, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.42it/s, train_loss=21.9, val_loss=5.06, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.42it/s, train_loss=26.6, val_loss=3.26, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.42it/s, train_loss=26.6, val_loss=3.26, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.2627426424546024\n",
      "Its' val AUC : 0.2757917629468533\n",
      "Its external  AUC: 0.3817551599180715\n",
      "Curent best Test AUC: 0.7097056981840951\n",
      "Its' val AUC : 0.7057224412124508\n",
      "Its external  AUC: 0.599338270048842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.42it/s, train_loss=21.6, val_loss=3.05, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 33.74it/s, train_loss=21.6, val_loss=3.05, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.74it/s, train_loss=21.6, val_loss=3.05, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.74it/s, train_loss=20.3, val_loss=3.29, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.74it/s, train_loss=20.3, val_loss=3.29, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.74it/s, train_loss=19, val_loss=2.09, tra\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.74it/s, train_loss=19, val_loss=2.09, tr\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.74it/s, train_loss=13.1, val_loss=2.61, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.74it/s, train_loss=13.1, val_loss=2.61, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.74it/s, train_loss=15.8, val_loss=2.27, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.24it/s, train_loss=15.8, val_loss=2.27,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.24it/s, train_loss=15.8, val_loss=2.27,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.24it/s, train_loss=12.9, val_loss=0.704\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.24it/s, train_loss=12.9, val_loss=0.704\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.24it/s, train_loss=4.32, val_loss=1.1, \u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.24it/s, train_loss=4.32, val_loss=1.1, \u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.24it/s, train_loss=5.49, val_loss=0.485\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.24it/s, train_loss=5.49, val_loss=0.485\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.24it/s, train_loss=3.39, val_loss=0.858\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.43it/s, train_loss=3.39, val_loss=0.858\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.43it/s, train_loss=3.39, val_loss=0.858\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.43it/s, train_loss=6.1, val_loss=0.659,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.43it/s, train_loss=6.1, val_loss=0.659,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.43it/s, train_loss=3.84, val_loss=0.424\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.43it/s, train_loss=3.84, val_loss=0.424\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.43it/s, train_loss=2.8, val_loss=0.984,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.43it/s, train_loss=2.8, val_loss=0.984,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.43it/s, train_loss=5.23, val_loss=0.549\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.34it/s, train_loss=5.23, val_loss=0.549\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.34it/s, train_loss=5.23, val_loss=0.549\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.34it/s, train_loss=3.57, val_loss=0.645\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.34it/s, train_loss=3.57, val_loss=0.645\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.34it/s, train_loss=4.23, val_loss=0.426\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.34it/s, train_loss=4.23, val_loss=0.426\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.34it/s, train_loss=2.58, val_loss=0.445\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.34it/s, train_loss=2.58, val_loss=0.445\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.34it/s, train_loss=3, val_loss=0.787, t\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.39it/s, train_loss=3, val_loss=0.787, t\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.39it/s, train_loss=3, val_loss=0.787, t\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.39it/s, train_loss=4.4, val_loss=0.444,\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.39it/s, train_loss=4.4, val_loss=0.444,\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.715090795241077\n",
      "Its' val AUC : 0.6203615604186489\n",
      "Its external  AUC: 0.5826374665196156\n",
      "Epoch    24: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.39it/s, train_loss=2.77, val_loss=0.376\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.39it/s, train_loss=2.77, val_loss=0.376\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.39it/s, train_loss=2.63, val_loss=0.368\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.39it/s, train_loss=2.63, val_loss=0.368\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.39it/s, train_loss=2.46, val_loss=0.21,\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.29it/s, train_loss=2.46, val_loss=0.21,\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.29it/s, train_loss=2.46, val_loss=0.21,\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.29it/s, train_loss=1.49, val_loss=0.4, \u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.29it/s, train_loss=1.49, val_loss=0.4, \u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.29it/s, train_loss=2.45, val_loss=0.219\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.29it/s, train_loss=2.45, val_loss=0.219\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.29it/s, train_loss=1.49, val_loss=0.241\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.29it/s, train_loss=1.49, val_loss=0.241\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.29it/s, train_loss=1.48, val_loss=0.189\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.25it/s, train_loss=1.48, val_loss=0.189\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.25it/s, train_loss=1.48, val_loss=0.189\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.25it/s, train_loss=1.15, val_loss=0.089\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.25it/s, train_loss=1.15, val_loss=0.089\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.25it/s, train_loss=0.584, val_loss=0.20\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.25it/s, train_loss=0.584, val_loss=0.20\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.25it/s, train_loss=1.18, val_loss=0.103\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.25it/s, train_loss=1.18, val_loss=0.103\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.25it/s, train_loss=0.691, val_loss=0.19\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.08it/s, train_loss=0.691, val_loss=0.19\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.08it/s, train_loss=0.691, val_loss=0.19\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.08it/s, train_loss=1.07, val_loss=0.136\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.08it/s, train_loss=1.07, val_loss=0.136\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.08it/s, train_loss=0.66, val_loss=0.117\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.08it/s, train_loss=0.66, val_loss=0.117\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.08it/s, train_loss=0.583, val_loss=0.15\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.08it/s, train_loss=0.583, val_loss=0.15\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.08it/s, train_loss=0.718, val_loss=0.13\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.15it/s, train_loss=0.718, val_loss=0.13\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.15it/s, train_loss=0.718, val_loss=0.13\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.15it/s, train_loss=0.502, val_loss=0.17\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.15it/s, train_loss=0.502, val_loss=0.17\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.15it/s, train_loss=0.556, val_loss=0.21\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.15it/s, train_loss=0.556, val_loss=0.21\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.15it/s, train_loss=0.69, val_loss=0.159\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.15it/s, train_loss=0.69, val_loss=0.159\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.15it/s, train_loss=0.494, val_loss=0.10\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.10it/s, train_loss=0.494, val_loss=0.10\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.10it/s, train_loss=0.494, val_loss=0.10\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.10it/s, train_loss=0.367, val_loss=0.10\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.10it/s, train_loss=0.367, val_loss=0.10\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    39: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch    45: reducing learning rate of group 0 to 1.2500e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.10it/s, train_loss=0.468, val_loss=0.09\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.10it/s, train_loss=0.468, val_loss=0.09\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.10it/s, train_loss=0.437, val_loss=0.08\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.10it/s, train_loss=0.437, val_loss=0.08\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.10it/s, train_loss=0.338, val_loss=0.07\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.14it/s, train_loss=0.338, val_loss=0.07\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.14it/s, train_loss=0.338, val_loss=0.07\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.14it/s, train_loss=0.302, val_loss=0.08\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.14it/s, train_loss=0.302, val_loss=0.08\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.15it/s, train_loss=0.353, val_loss=0.09\u001b[A\n",
      " 55%|███████████████████████                   | 55/100 [01:23<01:06,  1.49s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=41.6, val_loss=39.6, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=41.6, val_loss=39.6, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=195, val_loss=7.61, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=195, val_loss=7.61, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=41.1, val_loss=1.31, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=41.1, val_loss=1.31, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=6.89, val_loss=2.63, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.94it/s, train_loss=6.89, val_loss=2.63, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.94it/s, train_loss=6.89, val_loss=2.63, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.94it/s, train_loss=13.6, val_loss=4.97, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.94it/s, train_loss=13.6, val_loss=4.97, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.94it/s, train_loss=28, val_loss=2.42, tra\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.94it/s, train_loss=28, val_loss=2.42, tra\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.94it/s, train_loss=14, val_loss=4.08, tra\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.94it/s, train_loss=14, val_loss=4.08, tra\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7365059486537258\n",
      "Its' val AUC : 0.7232567622672285\n",
      "Its external  AUC: 0.6188750590830313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.94it/s, train_loss=21.9, val_loss=1.71, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 34.14it/s, train_loss=21.9, val_loss=1.71, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.14it/s, train_loss=21.9, val_loss=1.71, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.14it/s, train_loss=12.2, val_loss=3.06, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.14it/s, train_loss=12.2, val_loss=3.06, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.14it/s, train_loss=19.9, val_loss=1.71, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.14it/s, train_loss=19.9, val_loss=1.71, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.14it/s, train_loss=12, val_loss=1.77, tr\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.14it/s, train_loss=12, val_loss=1.77, tr\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.14it/s, train_loss=10.9, val_loss=2.56, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 33.68it/s, train_loss=10.9, val_loss=2.56,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.68it/s, train_loss=10.9, val_loss=2.56,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.68it/s, train_loss=14, val_loss=1.63, t\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.68it/s, train_loss=14, val_loss=1.63, t\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.68it/s, train_loss=9.15, val_loss=0.861\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.68it/s, train_loss=9.15, val_loss=0.861\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.68it/s, train_loss=5.63, val_loss=1.39,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.68it/s, train_loss=5.63, val_loss=1.39,\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     9: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Curent best Test AUC: 0.741014402003757\n",
      "Its' val AUC : 0.5971183906483621\n",
      "Its external  AUC: 0.6366787458641878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.68it/s, train_loss=8.3, val_loss=0.939,\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 33.73it/s, train_loss=8.3, val_loss=0.939,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.73it/s, train_loss=8.3, val_loss=0.939,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.73it/s, train_loss=5.38, val_loss=0.382\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.73it/s, train_loss=5.38, val_loss=0.382\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.73it/s, train_loss=2.1, val_loss=0.712,\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.73it/s, train_loss=2.1, val_loss=0.712,\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.73it/s, train_loss=3.79, val_loss=0.643\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.73it/s, train_loss=3.79, val_loss=0.643\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.73it/s, train_loss=3.5, val_loss=0.231,\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 33.85it/s, train_loss=3.5, val_loss=0.231,\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.85it/s, train_loss=3.5, val_loss=0.231,\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.85it/s, train_loss=1.19, val_loss=1.83,\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.85it/s, train_loss=1.19, val_loss=1.83,\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.85it/s, train_loss=9.32, val_loss=0.6, \u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.85it/s, train_loss=9.32, val_loss=0.6, \u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.85it/s, train_loss=2.99, val_loss=1.89,\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.85it/s, train_loss=2.99, val_loss=1.89,\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.85it/s, train_loss=9.12, val_loss=1.51,\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.21it/s, train_loss=9.12, val_loss=1.51,\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.21it/s, train_loss=9.12, val_loss=1.51,\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.21it/s, train_loss=7.08, val_loss=0.696\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.21it/s, train_loss=7.08, val_loss=0.696\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.21it/s, train_loss=3.75, val_loss=1.2, \u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.21it/s, train_loss=3.75, val_loss=1.2, \u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.21it/s, train_loss=7.37, val_loss=1.15,\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.21it/s, train_loss=7.37, val_loss=1.15,\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.21it/s, train_loss=7.11, val_loss=0.702\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.37it/s, train_loss=7.11, val_loss=0.702\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.37it/s, train_loss=7.11, val_loss=0.702\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.37it/s, train_loss=4.23, val_loss=0.611\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.37it/s, train_loss=4.23, val_loss=0.611\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.37it/s, train_loss=3.21, val_loss=0.878\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.37it/s, train_loss=3.21, val_loss=0.878\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.37it/s, train_loss=4.26, val_loss=1.06,\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.37it/s, train_loss=4.26, val_loss=1.06,\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.37it/s, train_loss=5.18, val_loss=0.907\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.53it/s, train_loss=5.18, val_loss=0.907\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.53it/s, train_loss=5.18, val_loss=0.907\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    26: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch    32: reducing learning rate of group 0 to 1.2500e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.53it/s, train_loss=4.54, val_loss=0.726\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.53it/s, train_loss=4.54, val_loss=0.726\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.53it/s, train_loss=3.73, val_loss=0.545\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.53it/s, train_loss=3.73, val_loss=0.545\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.53it/s, train_loss=2.92, val_loss=0.452\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.53it/s, train_loss=2.92, val_loss=0.452\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.53it/s, train_loss=2.57, val_loss=0.468\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.60it/s, train_loss=2.57, val_loss=0.468\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.60it/s, train_loss=2.57, val_loss=0.468\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.60it/s, train_loss=2.74, val_loss=0.532\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.60it/s, train_loss=2.74, val_loss=0.532\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.60it/s, train_loss=3.12, val_loss=0.561\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.60it/s, train_loss=3.12, val_loss=0.561\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.60it/s, train_loss=3.27, val_loss=0.525\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.60it/s, train_loss=3.27, val_loss=0.525\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.60it/s, train_loss=3.05, val_loss=0.45,\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.34it/s, train_loss=3.05, val_loss=0.45,\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.34it/s, train_loss=3.05, val_loss=0.45,\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.34it/s, train_loss=2.6, val_loss=0.379,\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.34it/s, train_loss=2.6, val_loss=0.379,\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.34it/s, train_loss=2.15, val_loss=0.348\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.34it/s, train_loss=2.15, val_loss=0.348\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.34it/s, train_loss=1.91, val_loss=0.363\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.34it/s, train_loss=1.91, val_loss=0.363\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.34it/s, train_loss=1.91, val_loss=0.4, \u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.50it/s, train_loss=1.91, val_loss=0.4, \u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.50it/s, train_loss=1.91, val_loss=0.4, \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.50it/s, train_loss=2.04, val_loss=0.427\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.50it/s, train_loss=2.04, val_loss=0.427\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.50it/s, train_loss=2.15, val_loss=0.422\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.50it/s, train_loss=2.15, val_loss=0.422\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.50it/s, train_loss=2.11, val_loss=0.383\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.50it/s, train_loss=2.11, val_loss=0.383\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.50it/s, train_loss=1.92, val_loss=0.329\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.67it/s, train_loss=1.92, val_loss=0.329\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.67it/s, train_loss=1.92, val_loss=0.329\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.67it/s, train_loss=1.67, val_loss=0.285\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.67it/s, train_loss=1.67, val_loss=0.285\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.34it/s, train_loss=1.46, val_loss=0.263\u001b[A\n",
      " 56%|███████████████████████▌                  | 56/100 [01:25<01:05,  1.48s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=31.4, val_loss=123, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=31.4, val_loss=123, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=656, val_loss=11.9, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=656, val_loss=11.9, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=60, val_loss=0.358, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=60, val_loss=0.358, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=2.38, val_loss=1.26, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.46it/s, train_loss=2.38, val_loss=1.26, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.46it/s, train_loss=2.38, val_loss=1.26, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.46it/s, train_loss=6.9, val_loss=5.88, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.46it/s, train_loss=6.9, val_loss=5.88, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.46it/s, train_loss=29.6, val_loss=0.971, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.46it/s, train_loss=29.6, val_loss=0.971, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.46it/s, train_loss=4.71, val_loss=1.02, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.46it/s, train_loss=4.71, val_loss=1.02, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.2505948653725736\n",
      "Its' val AUC : 0.26722848987358977\n",
      "Its external  AUC: 0.38663935717661885\n",
      "Curent best Test AUC: 0.5416405760801503\n",
      "Its' val AUC : 0.44393095011553624\n",
      "Its external  AUC: 0.6109973215692454\n",
      "Curent best Test AUC: 0.7090795241077019\n",
      "Its' val AUC : 0.6357210819627566\n",
      "Its external  AUC: 0.6464471403812825\n",
      "Curent best Test AUC: 0.7383844708829055\n",
      "Its' val AUC : 0.7144216392551311\n",
      "Its external  AUC: 0.6158815188277926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.46it/s, train_loss=5.54, val_loss=2.67, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 34.01it/s, train_loss=5.54, val_loss=2.67, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.01it/s, train_loss=5.54, val_loss=2.67, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.01it/s, train_loss=13.7, val_loss=0.967, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.01it/s, train_loss=13.7, val_loss=0.967, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.01it/s, train_loss=4.97, val_loss=0.16, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.01it/s, train_loss=4.97, val_loss=0.16, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.01it/s, train_loss=0.84, val_loss=0.175,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.01it/s, train_loss=0.84, val_loss=0.175,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.01it/s, train_loss=0.974, val_loss=0.73,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 33.68it/s, train_loss=0.974, val_loss=0.73\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.68it/s, train_loss=0.974, val_loss=0.73\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.68it/s, train_loss=3.87, val_loss=0.852\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.68it/s, train_loss=3.87, val_loss=0.852\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.68it/s, train_loss=4.45, val_loss=0.456\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.68it/s, train_loss=4.45, val_loss=0.456\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.68it/s, train_loss=2.35, val_loss=0.081\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.68it/s, train_loss=2.35, val_loss=0.081\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     9: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.68it/s, train_loss=0.389, val_loss=0.16\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 33.89it/s, train_loss=0.389, val_loss=0.16\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.89it/s, train_loss=0.389, val_loss=0.16\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.89it/s, train_loss=0.846, val_loss=0.48\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.89it/s, train_loss=0.846, val_loss=0.48\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.89it/s, train_loss=2.49, val_loss=0.493\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.89it/s, train_loss=2.49, val_loss=0.493\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.89it/s, train_loss=2.58, val_loss=0.207\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.89it/s, train_loss=2.58, val_loss=0.207\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.89it/s, train_loss=1.09, val_loss=0.049\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 33.61it/s, train_loss=1.09, val_loss=0.049\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.61it/s, train_loss=1.09, val_loss=0.049\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.61it/s, train_loss=0.215, val_loss=0.19\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.61it/s, train_loss=0.215, val_loss=0.19\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.61it/s, train_loss=0.895, val_loss=0.36\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.61it/s, train_loss=0.895, val_loss=0.36\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.61it/s, train_loss=1.74, val_loss=0.289\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.61it/s, train_loss=1.74, val_loss=0.289\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.61it/s, train_loss=1.39, val_loss=0.084\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.05it/s, train_loss=1.39, val_loss=0.084\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.05it/s, train_loss=1.39, val_loss=0.084\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.05it/s, train_loss=0.384, val_loss=0.04\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.05it/s, train_loss=0.384, val_loss=0.04\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.05it/s, train_loss=0.204, val_loss=0.17\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.05it/s, train_loss=0.204, val_loss=0.17\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.05it/s, train_loss=0.885, val_loss=0.22\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.05it/s, train_loss=0.885, val_loss=0.22\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.05it/s, train_loss=1.15, val_loss=0.114\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.31it/s, train_loss=1.15, val_loss=0.114\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.31it/s, train_loss=1.15, val_loss=0.114\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.31it/s, train_loss=0.572, val_loss=0.02\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.31it/s, train_loss=0.572, val_loss=0.02\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.31it/s, train_loss=0.126, val_loss=0.08\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.31it/s, train_loss=0.126, val_loss=0.08\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.31it/s, train_loss=0.422, val_loss=0.16\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.31it/s, train_loss=0.422, val_loss=0.16\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.31it/s, train_loss=0.801, val_loss=0.12\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.38it/s, train_loss=0.801, val_loss=0.12\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.38it/s, train_loss=0.801, val_loss=0.12\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.38it/s, train_loss=0.587, val_loss=0.03\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.38it/s, train_loss=0.587, val_loss=0.03\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.38it/s, train_loss=0.163, val_loss=0.03\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.38it/s, train_loss=0.163, val_loss=0.03\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.38it/s, train_loss=0.198, val_loss=0.09\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.38it/s, train_loss=0.198, val_loss=0.09\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.38it/s, train_loss=0.506, val_loss=0.09\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.51it/s, train_loss=0.506, val_loss=0.09\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.51it/s, train_loss=0.506, val_loss=0.09\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.51it/s, train_loss=0.488, val_loss=0.05\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.51it/s, train_loss=0.488, val_loss=0.05\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.51it/s, train_loss=0.297, val_loss=0.02\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.51it/s, train_loss=0.297, val_loss=0.02\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.51it/s, train_loss=0.12, val_loss=0.026\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.51it/s, train_loss=0.12, val_loss=0.026\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.51it/s, train_loss=0.102, val_loss=0.05\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.21it/s, train_loss=0.102, val_loss=0.05\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.21it/s, train_loss=0.102, val_loss=0.05\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.21it/s, train_loss=0.214, val_loss=0.07\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.21it/s, train_loss=0.214, val_loss=0.07\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    35: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.21it/s, train_loss=0.312, val_loss=0.06\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.21it/s, train_loss=0.312, val_loss=0.06\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.21it/s, train_loss=0.292, val_loss=0.04\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.21it/s, train_loss=0.292, val_loss=0.04\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.21it/s, train_loss=0.179, val_loss=0.02\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.08it/s, train_loss=0.179, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.08it/s, train_loss=0.179, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.08it/s, train_loss=0.0871, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.08it/s, train_loss=0.0871, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.08it/s, train_loss=0.092, val_loss=0.03\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.08it/s, train_loss=0.092, val_loss=0.03\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.08it/s, train_loss=0.163, val_loss=0.04\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.08it/s, train_loss=0.163, val_loss=0.04\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.08it/s, train_loss=0.208, val_loss=0.03\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.17it/s, train_loss=0.208, val_loss=0.03\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.17it/s, train_loss=0.208, val_loss=0.03\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.17it/s, train_loss=0.177, val_loss=0.02\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.17it/s, train_loss=0.177, val_loss=0.02\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.08it/s, train_loss=0.107, val_loss=0.01\u001b[A\n",
      " 57%|███████████████████████▉                  | 57/100 [01:26<01:03,  1.48s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=5.05, val_loss=1.09, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=5.05, val_loss=1.09, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=6.13, val_loss=16.7, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=6.13, val_loss=16.7, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=104, val_loss=43.3, train_cind\u001b[A\n",
      "Epoch 2:   6%| | 3/50 [00:00<00:01, 27.74it/s, train_loss=104, val_loss=43.3, tr\u001b[A\n",
      "Epoch 3:   6%| | 3/50 [00:00<00:01, 27.74it/s, train_loss=104, val_loss=43.3, tr\u001b[A\n",
      "Epoch 3:   6%| | 3/50 [00:00<00:01, 27.74it/s, train_loss=237, val_loss=4.42, tr\u001b[A\n",
      "Epoch 4:   6%| | 3/50 [00:00<00:01, 27.74it/s, train_loss=237, val_loss=4.42, tr\u001b[A\n",
      "Epoch 4:   6%| | 3/50 [00:00<00:01, 27.74it/s, train_loss=24.9, val_loss=26.3, t\u001b[A\n",
      "Epoch 5:   6%| | 3/50 [00:00<00:01, 27.74it/s, train_loss=24.9, val_loss=26.3, t\u001b[A\n",
      "Epoch 5:   6%| | 3/50 [00:00<00:01, 27.74it/s, train_loss=137, val_loss=7.77, tr\u001b[A\n",
      "Epoch 5:  12%| | 6/50 [00:00<00:01, 27.42it/s, train_loss=137, val_loss=7.77, tr\u001b[A\n",
      "Epoch 6:  12%| | 6/50 [00:00<00:01, 27.42it/s, train_loss=137, val_loss=7.77, tr\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.5180964308077646\n",
      "Its' val AUC : 0.5855647682479271\n",
      "Its external  AUC: 0.5752323932566566\n",
      "Curent best Test AUC: 0.7102066374452097\n",
      "Its' val AUC : 0.6907706945765937\n",
      "Its external  AUC: 0.6155664093272413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6:  12%| | 6/50 [00:00<00:01, 27.42it/s, train_loss=39.7, val_loss=0.37, t\u001b[A\n",
      "Epoch 7:  12%| | 6/50 [00:00<00:01, 27.42it/s, train_loss=39.7, val_loss=0.37, t\u001b[A\n",
      "Epoch 7:  12%| | 6/50 [00:00<00:01, 27.42it/s, train_loss=1.81, val_loss=8.25, t\u001b[A\n",
      "Epoch 8:  12%| | 6/50 [00:00<00:01, 27.42it/s, train_loss=1.81, val_loss=8.25, t\u001b[A\n",
      "Epoch 8:  12%| | 6/50 [00:00<00:01, 27.42it/s, train_loss=38.6, val_loss=7.64, t\u001b[A\n",
      "Epoch 8:  18%|▏| 9/50 [00:00<00:01, 28.30it/s, train_loss=38.6, val_loss=7.64, t\u001b[A\n",
      "Epoch 9:  18%|▏| 9/50 [00:00<00:01, 28.30it/s, train_loss=38.6, val_loss=7.64, t\u001b[A\n",
      "Epoch 9:  18%|▏| 9/50 [00:00<00:01, 28.30it/s, train_loss=35.6, val_loss=1.12, t\u001b[A\n",
      "Epoch 10:  18%|▏| 9/50 [00:00<00:01, 28.30it/s, train_loss=35.6, val_loss=1.12, \u001b[A\n",
      "Epoch 10:  18%|▏| 9/50 [00:00<00:01, 28.30it/s, train_loss=5.4, val_loss=1.34, t\u001b[A\n",
      "Epoch 11:  18%|▏| 9/50 [00:00<00:01, 28.30it/s, train_loss=5.4, val_loss=1.34, t\u001b[A\n",
      "Epoch 11:  18%|▏| 9/50 [00:00<00:01, 28.30it/s, train_loss=7.77, val_loss=3.6, t\u001b[A\n",
      "Epoch 12:  18%|▏| 9/50 [00:00<00:01, 28.30it/s, train_loss=7.77, val_loss=3.6, t\u001b[A\n",
      "Epoch 12:  18%|▏| 9/50 [00:00<00:01, 28.30it/s, train_loss=19.2, val_loss=2.78, \u001b[A\n",
      "Epoch 12:  26%|▎| 13/50 [00:00<00:01, 29.49it/s, train_loss=19.2, val_loss=2.78,\u001b[A\n",
      "Epoch 13:  26%|▎| 13/50 [00:00<00:01, 29.49it/s, train_loss=19.2, val_loss=2.78,\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7484032561051972\n",
      "Its' val AUC : 0.6198178605409814\n",
      "Its external  AUC: 0.6322672128564676\n",
      "Curent best Test AUC: 0.7608015028177834\n",
      "Its' val AUC : 0.7161886638575506\n",
      "Its external  AUC: 0.6226563730896486\n",
      "Curent best Test AUC: 0.7614276768941766\n",
      "Its' val AUC : 0.7171401386434688\n",
      "Its external  AUC: 0.6177721758311013\n",
      "Epoch    13: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13:  26%|▎| 13/50 [00:00<00:01, 29.49it/s, train_loss=14.8, val_loss=1.1, \u001b[A\n",
      "Epoch 14:  26%|▎| 13/50 [00:00<00:01, 29.49it/s, train_loss=14.8, val_loss=1.1, \u001b[A\n",
      "Epoch 14:  26%|▎| 13/50 [00:00<00:01, 29.49it/s, train_loss=6.16, val_loss=0.165\u001b[A\n",
      "Epoch 15:  26%|▎| 13/50 [00:00<00:01, 29.49it/s, train_loss=6.16, val_loss=0.165\u001b[A\n",
      "Epoch 15:  26%|▎| 13/50 [00:00<00:01, 29.49it/s, train_loss=1.18, val_loss=0.997\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 29.16it/s, train_loss=1.18, val_loss=0.997\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 29.16it/s, train_loss=1.18, val_loss=0.997\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 29.16it/s, train_loss=5.1, val_loss=1.54, \u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 29.16it/s, train_loss=5.1, val_loss=1.54, \u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 29.16it/s, train_loss=7.74, val_loss=0.955\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 29.16it/s, train_loss=7.74, val_loss=0.955\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 29.16it/s, train_loss=4.89, val_loss=0.231\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 29.16it/s, train_loss=4.89, val_loss=0.231\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 29.16it/s, train_loss=1.42, val_loss=0.223\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 30.39it/s, train_loss=1.42, val_loss=0.223\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 30.39it/s, train_loss=1.42, val_loss=0.223\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 30.39it/s, train_loss=1.6, val_loss=0.653,\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 30.39it/s, train_loss=1.6, val_loss=0.653,\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 30.39it/s, train_loss=3.92, val_loss=0.691\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 30.39it/s, train_loss=3.92, val_loss=0.691\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 30.39it/s, train_loss=4.09, val_loss=0.498\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 30.39it/s, train_loss=4.09, val_loss=0.498\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 30.39it/s, train_loss=3.01, val_loss=0.234\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 31.67it/s, train_loss=3.01, val_loss=0.234\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 31.67it/s, train_loss=3.01, val_loss=0.234\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 31.67it/s, train_loss=1.55, val_loss=0.096\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 31.67it/s, train_loss=1.55, val_loss=0.096\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 31.67it/s, train_loss=0.724, val_loss=0.16\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 31.67it/s, train_loss=0.724, val_loss=0.16\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 31.67it/s, train_loss=0.948, val_loss=0.33\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 31.67it/s, train_loss=0.948, val_loss=0.33\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    21: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 31.67it/s, train_loss=1.75, val_loss=0.435\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 32.64it/s, train_loss=1.75, val_loss=0.435\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 32.64it/s, train_loss=1.75, val_loss=0.435\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 32.64it/s, train_loss=2.25, val_loss=0.376\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 32.64it/s, train_loss=2.25, val_loss=0.376\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 32.64it/s, train_loss=1.96, val_loss=0.213\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 32.64it/s, train_loss=1.96, val_loss=0.213\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 32.64it/s, train_loss=1.15, val_loss=0.081\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 32.64it/s, train_loss=1.15, val_loss=0.081\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:01<00:00, 32.64it/s, train_loss=0.5, val_loss=0.068,\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:01<00:00, 33.24it/s, train_loss=0.5, val_loss=0.068,\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:01<00:00, 33.24it/s, train_loss=0.5, val_loss=0.068,\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:01<00:00, 33.24it/s, train_loss=0.436, val_loss=0.14\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 33.24it/s, train_loss=0.436, val_loss=0.14\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 33.24it/s, train_loss=0.825, val_loss=0.21\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 33.24it/s, train_loss=0.825, val_loss=0.21\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 33.24it/s, train_loss=1.18, val_loss=0.209\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 33.24it/s, train_loss=1.18, val_loss=0.209\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 33.24it/s, train_loss=1.13, val_loss=0.132\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 33.80it/s, train_loss=1.13, val_loss=0.132\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.80it/s, train_loss=1.13, val_loss=0.132\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.80it/s, train_loss=0.73, val_loss=0.054\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.80it/s, train_loss=0.73, val_loss=0.054\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.80it/s, train_loss=0.318, val_loss=0.03\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.80it/s, train_loss=0.318, val_loss=0.03\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.80it/s, train_loss=0.212, val_loss=0.07\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.80it/s, train_loss=0.212, val_loss=0.07\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.80it/s, train_loss=0.41, val_loss=0.125\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.17it/s, train_loss=0.41, val_loss=0.125\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.17it/s, train_loss=0.41, val_loss=0.125\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.17it/s, train_loss=0.637, val_loss=0.12\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.17it/s, train_loss=0.637, val_loss=0.12\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.17it/s, train_loss=0.637, val_loss=0.08\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.17it/s, train_loss=0.637, val_loss=0.08\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.17it/s, train_loss=0.415, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.17it/s, train_loss=0.415, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.17it/s, train_loss=0.188, val_loss=0.02\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.33it/s, train_loss=0.188, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.33it/s, train_loss=0.188, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.33it/s, train_loss=0.146, val_loss=0.04\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.33it/s, train_loss=0.146, val_loss=0.04\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.33it/s, train_loss=0.27, val_loss=0.070\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.33it/s, train_loss=0.27, val_loss=0.070\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.33it/s, train_loss=0.391, val_loss=0.06\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.33it/s, train_loss=0.391, val_loss=0.06\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.33it/s, train_loss=0.372, val_loss=0.04\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.27it/s, train_loss=0.372, val_loss=0.04\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.27it/s, train_loss=0.372, val_loss=0.04\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.27it/s, train_loss=0.238, val_loss=0.02\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.27it/s, train_loss=0.238, val_loss=0.02\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 32.39it/s, train_loss=0.124, val_loss=0.02\u001b[A\n",
      " 58%|████████████████████████▎                 | 58/100 [01:28<01:03,  1.51s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=44.5, val_loss=55.8, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=44.5, val_loss=55.8, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=292, val_loss=0.911, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=292, val_loss=0.911, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=5.26, val_loss=1.01, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=5.26, val_loss=1.01, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=4.06, val_loss=0.736, train_ci\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 32.20it/s, train_loss=4.06, val_loss=0.736, \u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.20it/s, train_loss=4.06, val_loss=0.736, \u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.20it/s, train_loss=3.48, val_loss=0.289, \u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.20it/s, train_loss=3.48, val_loss=0.289, \u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.20it/s, train_loss=1.34, val_loss=0.297, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.20it/s, train_loss=1.34, val_loss=0.297, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.20it/s, train_loss=1.63, val_loss=0.206, \u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.20it/s, train_loss=1.63, val_loss=0.206, \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.27075767063243583\n",
      "Its' val AUC : 0.2768791627021884\n",
      "Its external  AUC: 0.3962501969434378\n",
      "Curent best Test AUC: 0.29355040701314966\n",
      "Its' val AUC : 0.3150740791083322\n",
      "Its external  AUC: 0.515204033401607\n",
      "Curent best Test AUC: 0.5719474013775829\n",
      "Its' val AUC : 0.6009242897920348\n",
      "Its external  AUC: 0.5854734520245786\n",
      "Curent best Test AUC: 0.673888541014402\n",
      "Its' val AUC : 0.6649449503873862\n",
      "Its external  AUC: 0.6228139278399244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.20it/s, train_loss=1.17, val_loss=0.043, \u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 33.68it/s, train_loss=1.17, val_loss=0.043, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.68it/s, train_loss=1.17, val_loss=0.043, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.68it/s, train_loss=0.263, val_loss=0.13, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.68it/s, train_loss=0.263, val_loss=0.13, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.68it/s, train_loss=0.734, val_loss=0.055,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.68it/s, train_loss=0.734, val_loss=0.055\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.68it/s, train_loss=0.346, val_loss=0.096\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.68it/s, train_loss=0.346, val_loss=0.096\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.68it/s, train_loss=0.408, val_loss=0.181\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 33.99it/s, train_loss=0.408, val_loss=0.18\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.99it/s, train_loss=0.408, val_loss=0.18\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.99it/s, train_loss=0.733, val_loss=0.29\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.99it/s, train_loss=0.733, val_loss=0.29\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.99it/s, train_loss=1.76, val_loss=0.177\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.99it/s, train_loss=1.76, val_loss=0.177\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.99it/s, train_loss=0.713, val_loss=0.14\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.99it/s, train_loss=0.713, val_loss=0.14\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7212273011897308\n",
      "Its' val AUC : 0.7000135924969417\n",
      "Its external  AUC: 0.6240743658421302\n",
      "Epoch    14: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.99it/s, train_loss=0.6, val_loss=0.0982\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.38it/s, train_loss=0.6, val_loss=0.0982\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.38it/s, train_loss=0.6, val_loss=0.0982\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.38it/s, train_loss=0.447, val_loss=0.09\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.38it/s, train_loss=0.447, val_loss=0.09\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.38it/s, train_loss=0.433, val_loss=0.07\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.38it/s, train_loss=0.433, val_loss=0.07\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.38it/s, train_loss=0.315, val_loss=0.04\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.38it/s, train_loss=0.315, val_loss=0.04\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.38it/s, train_loss=0.179, val_loss=0.04\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.60it/s, train_loss=0.179, val_loss=0.04\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.60it/s, train_loss=0.179, val_loss=0.04\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.60it/s, train_loss=0.181, val_loss=0.04\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.60it/s, train_loss=0.181, val_loss=0.04\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.60it/s, train_loss=0.185, val_loss=0.03\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.60it/s, train_loss=0.185, val_loss=0.03\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.60it/s, train_loss=0.15, val_loss=0.034\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.60it/s, train_loss=0.15, val_loss=0.034\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.60it/s, train_loss=0.186, val_loss=0.04\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.71it/s, train_loss=0.186, val_loss=0.04\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.71it/s, train_loss=0.186, val_loss=0.04\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.71it/s, train_loss=0.231, val_loss=0.03\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.71it/s, train_loss=0.231, val_loss=0.03\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.71it/s, train_loss=0.213, val_loss=0.03\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.71it/s, train_loss=0.213, val_loss=0.03\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.71it/s, train_loss=0.198, val_loss=0.03\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.71it/s, train_loss=0.198, val_loss=0.03\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.71it/s, train_loss=0.203, val_loss=0.02\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.73it/s, train_loss=0.203, val_loss=0.02\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.73it/s, train_loss=0.203, val_loss=0.02\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.73it/s, train_loss=0.147, val_loss=0.02\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.73it/s, train_loss=0.147, val_loss=0.02\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.73it/s, train_loss=0.125, val_loss=0.01\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.73it/s, train_loss=0.125, val_loss=0.01\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.73it/s, train_loss=0.116, val_loss=0.01\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.73it/s, train_loss=0.116, val_loss=0.01\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.73it/s, train_loss=0.0951, val_loss=0.0\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.81it/s, train_loss=0.0951, val_loss=0.0\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.81it/s, train_loss=0.0951, val_loss=0.0\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.81it/s, train_loss=0.0832, val_loss=0.0\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.81it/s, train_loss=0.0832, val_loss=0.0\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.81it/s, train_loss=0.0834, val_loss=0.0\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.81it/s, train_loss=0.0834, val_loss=0.0\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.81it/s, train_loss=0.0776, val_loss=0.0\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.81it/s, train_loss=0.0776, val_loss=0.0\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.81it/s, train_loss=0.0797, val_loss=0.0\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.83it/s, train_loss=0.0797, val_loss=0.0\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.83it/s, train_loss=0.0797, val_loss=0.0\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.83it/s, train_loss=0.089, val_loss=0.01\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.83it/s, train_loss=0.089, val_loss=0.01\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.83it/s, train_loss=0.088, val_loss=0.01\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.83it/s, train_loss=0.088, val_loss=0.01\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.83it/s, train_loss=0.0886, val_loss=0.0\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.83it/s, train_loss=0.0886, val_loss=0.0\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.83it/s, train_loss=0.0882, val_loss=0.0\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.90it/s, train_loss=0.0882, val_loss=0.0\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.90it/s, train_loss=0.0882, val_loss=0.0\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.90it/s, train_loss=0.0841, val_loss=0.0\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.90it/s, train_loss=0.0841, val_loss=0.0\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.90it/s, train_loss=0.0791, val_loss=0.0\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.90it/s, train_loss=0.0791, val_loss=0.0\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.90it/s, train_loss=0.0762, val_loss=0.0\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.90it/s, train_loss=0.0762, val_loss=0.0\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.90it/s, train_loss=0.0732, val_loss=0.0\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.96it/s, train_loss=0.0732, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.96it/s, train_loss=0.0732, val_loss=0.0\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    38: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.96it/s, train_loss=0.0684, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.96it/s, train_loss=0.0684, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.96it/s, train_loss=0.0644, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.96it/s, train_loss=0.0644, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.96it/s, train_loss=0.0624, val_loss=0.0\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.96it/s, train_loss=0.0624, val_loss=0.0\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.96it/s, train_loss=0.061, val_loss=0.01\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.79it/s, train_loss=0.061, val_loss=0.01\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.79it/s, train_loss=0.061, val_loss=0.01\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.79it/s, train_loss=0.0587, val_loss=0.0\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.79it/s, train_loss=0.0587, val_loss=0.0\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.60it/s, train_loss=0.0569, val_loss=0.0\u001b[A\n",
      " 59%|████████████████████████▊                 | 59/100 [01:29<01:01,  1.50s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=12, val_loss=45.9, train_cinde\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=12, val_loss=45.9, train_cinde\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=244, val_loss=0.611, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=244, val_loss=0.611, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=3.39, val_loss=2.9, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=3.39, val_loss=2.9, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=13.9, val_loss=7.29, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.48it/s, train_loss=13.9, val_loss=7.29, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.48it/s, train_loss=13.9, val_loss=7.29, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.48it/s, train_loss=39.2, val_loss=1.11, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.48it/s, train_loss=39.2, val_loss=1.11, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.48it/s, train_loss=5.1, val_loss=1.2, tra\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.48it/s, train_loss=5.1, val_loss=1.2, tra\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.48it/s, train_loss=7.38, val_loss=0.903, \u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.48it/s, train_loss=7.38, val_loss=0.903, \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.28065122103944895\n",
      "Its' val AUC : 0.29291830909338046\n",
      "Its external  AUC: 0.39640775169371356\n",
      "Curent best Test AUC: 0.7287413901064496\n",
      "Its' val AUC : 0.6828870463504145\n",
      "Its external  AUC: 0.6218685993382701\n",
      "Curent best Test AUC: 0.7540388227927364\n",
      "Its' val AUC : 0.7127905396221286\n",
      "Its external  AUC: 0.611154876319521\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.48it/s, train_loss=5.55, val_loss=0.645, \u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 34.24it/s, train_loss=5.55, val_loss=0.645, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.24it/s, train_loss=5.55, val_loss=0.645, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.24it/s, train_loss=3.2, val_loss=0.735, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.24it/s, train_loss=3.2, val_loss=0.735, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.24it/s, train_loss=3.81, val_loss=0.264, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.24it/s, train_loss=3.81, val_loss=0.264,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.24it/s, train_loss=1.56, val_loss=0.343,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.24it/s, train_loss=1.56, val_loss=0.343,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.24it/s, train_loss=1.83, val_loss=0.701,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.44it/s, train_loss=1.83, val_loss=0.701\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.44it/s, train_loss=1.83, val_loss=0.701\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.44it/s, train_loss=3.47, val_loss=0.426\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.44it/s, train_loss=3.47, val_loss=0.426\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.44it/s, train_loss=2.15, val_loss=0.223\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.44it/s, train_loss=2.15, val_loss=0.223\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.44it/s, train_loss=1.33, val_loss=0.385\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.44it/s, train_loss=1.33, val_loss=0.385\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.44it/s, train_loss=2.2, val_loss=0.219,\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.51it/s, train_loss=2.2, val_loss=0.219,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.51it/s, train_loss=2.2, val_loss=0.219,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.51it/s, train_loss=1.33, val_loss=0.112\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.51it/s, train_loss=1.33, val_loss=0.112\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.51it/s, train_loss=0.778, val_loss=0.22\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.51it/s, train_loss=0.778, val_loss=0.22\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.51it/s, train_loss=1.36, val_loss=0.178\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.51it/s, train_loss=1.36, val_loss=0.178\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.51it/s, train_loss=1.11, val_loss=0.095\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.57it/s, train_loss=1.11, val_loss=0.095\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.57it/s, train_loss=1.11, val_loss=0.095\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.57it/s, train_loss=0.609, val_loss=0.16\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.57it/s, train_loss=0.609, val_loss=0.16\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.57it/s, train_loss=0.92, val_loss=0.16,\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.57it/s, train_loss=0.92, val_loss=0.16,\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.57it/s, train_loss=0.899, val_loss=0.07\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.57it/s, train_loss=0.899, val_loss=0.07\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.57it/s, train_loss=0.484, val_loss=0.11\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.62it/s, train_loss=0.484, val_loss=0.11\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.62it/s, train_loss=0.484, val_loss=0.11\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.62it/s, train_loss=0.642, val_loss=0.13\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.62it/s, train_loss=0.642, val_loss=0.13\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.62it/s, train_loss=0.684, val_loss=0.04\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.62it/s, train_loss=0.684, val_loss=0.04\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.62it/s, train_loss=0.279, val_loss=0.04\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.62it/s, train_loss=0.279, val_loss=0.04\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.62it/s, train_loss=0.316, val_loss=0.06\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.61it/s, train_loss=0.316, val_loss=0.06\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.61it/s, train_loss=0.316, val_loss=0.06\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.61it/s, train_loss=0.441, val_loss=0.03\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.61it/s, train_loss=0.441, val_loss=0.03\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.61it/s, train_loss=0.239, val_loss=0.06\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.61it/s, train_loss=0.239, val_loss=0.06\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.61it/s, train_loss=0.349, val_loss=0.08\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.61it/s, train_loss=0.349, val_loss=0.08\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.61it/s, train_loss=0.47, val_loss=0.042\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.71it/s, train_loss=0.47, val_loss=0.042\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.71it/s, train_loss=0.47, val_loss=0.042\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.71it/s, train_loss=0.272, val_loss=0.04\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.71it/s, train_loss=0.272, val_loss=0.04\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.71it/s, train_loss=0.313, val_loss=0.05\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.71it/s, train_loss=0.313, val_loss=0.05\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.71it/s, train_loss=0.355, val_loss=0.02\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.71it/s, train_loss=0.355, val_loss=0.02\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.71it/s, train_loss=0.21, val_loss=0.045\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.72it/s, train_loss=0.21, val_loss=0.045\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.72it/s, train_loss=0.21, val_loss=0.045\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.72it/s, train_loss=0.27, val_loss=0.049\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.72it/s, train_loss=0.27, val_loss=0.049\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.72it/s, train_loss=0.284, val_loss=0.02\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.72it/s, train_loss=0.284, val_loss=0.02\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.72it/s, train_loss=0.173, val_loss=0.04\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.72it/s, train_loss=0.173, val_loss=0.04\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.72it/s, train_loss=0.233, val_loss=0.04\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.73it/s, train_loss=0.233, val_loss=0.04\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.73it/s, train_loss=0.233, val_loss=0.04\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.73it/s, train_loss=0.222, val_loss=0.02\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.73it/s, train_loss=0.222, val_loss=0.02\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.73it/s, train_loss=0.16, val_loss=0.031\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.73it/s, train_loss=0.16, val_loss=0.031\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.73it/s, train_loss=0.174, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.73it/s, train_loss=0.174, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.73it/s, train_loss=0.169, val_loss=0.02\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.71it/s, train_loss=0.169, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.71it/s, train_loss=0.169, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.71it/s, train_loss=0.136, val_loss=0.02\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.71it/s, train_loss=0.136, val_loss=0.02\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.71it/s, train_loss=0.114, val_loss=0.02\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.71it/s, train_loss=0.114, val_loss=0.02\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.71it/s, train_loss=0.117, val_loss=0.02\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.71it/s, train_loss=0.117, val_loss=0.02\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    41: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.71it/s, train_loss=0.114, val_loss=0.01\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.71it/s, train_loss=0.114, val_loss=0.01\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.71it/s, train_loss=0.114, val_loss=0.01\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.71it/s, train_loss=0.0982, val_loss=0.0\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.71it/s, train_loss=0.0982, val_loss=0.0\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.60it/s, train_loss=0.0936, val_loss=0.0\u001b[A\n",
      " 60%|█████████████████████████▏                | 60/100 [01:31<00:59,  1.49s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=11, val_loss=52.3, train_cinde\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=11, val_loss=52.3, train_cinde\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=260, val_loss=20.7, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=260, val_loss=20.7, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=108, val_loss=0.767, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=108, val_loss=0.767, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=5, val_loss=0.251, train_cinde\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.66it/s, train_loss=5, val_loss=0.251, tra\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.66it/s, train_loss=5, val_loss=0.251, tra\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.66it/s, train_loss=1.89, val_loss=0.293, \u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.66it/s, train_loss=1.89, val_loss=0.293, \u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.66it/s, train_loss=2.07, val_loss=1.04, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.66it/s, train_loss=2.07, val_loss=1.04, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.66it/s, train_loss=5.84, val_loss=5.61, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.66it/s, train_loss=5.84, val_loss=5.61, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7443957420162806\n",
      "Its' val AUC : 0.7133342394997961\n",
      "Its external  AUC: 0.6204506065857885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.66it/s, train_loss=27.7, val_loss=7.53, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 34.19it/s, train_loss=27.7, val_loss=7.53, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.19it/s, train_loss=27.7, val_loss=7.53, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.19it/s, train_loss=40, val_loss=0.421, tr\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.19it/s, train_loss=40, val_loss=0.421, tr\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.19it/s, train_loss=2.6, val_loss=11.6, tr\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.19it/s, train_loss=2.6, val_loss=11.6, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.19it/s, train_loss=54.9, val_loss=0.758,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.19it/s, train_loss=54.9, val_loss=0.758,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.19it/s, train_loss=3.85, val_loss=2.25, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.43it/s, train_loss=3.85, val_loss=2.25,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.43it/s, train_loss=3.85, val_loss=2.25,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.43it/s, train_loss=11.3, val_loss=3.91,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.43it/s, train_loss=11.3, val_loss=3.91,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.43it/s, train_loss=19.8, val_loss=0.787\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.43it/s, train_loss=19.8, val_loss=0.787\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.43it/s, train_loss=4, val_loss=0.843, t\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.43it/s, train_loss=4, val_loss=0.843, t\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.43it/s, train_loss=4.79, val_loss=2.44,\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.43it/s, train_loss=4.79, val_loss=2.44,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.43it/s, train_loss=4.79, val_loss=2.44,\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    10: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Curent best Test AUC: 0.7467752035065748\n",
      "Its' val AUC : 0.6798966970232432\n",
      "Its external  AUC: 0.6098944383173153\n",
      "Epoch    16: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.43it/s, train_loss=13.1, val_loss=1.34,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.43it/s, train_loss=13.1, val_loss=1.34,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.43it/s, train_loss=7.39, val_loss=0.212\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.43it/s, train_loss=7.39, val_loss=0.212\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.43it/s, train_loss=1.49, val_loss=0.634\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.43it/s, train_loss=1.49, val_loss=0.634\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.43it/s, train_loss=3.44, val_loss=1.56,\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.59it/s, train_loss=3.44, val_loss=1.56,\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.59it/s, train_loss=3.44, val_loss=1.56,\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.59it/s, train_loss=8.06, val_loss=1.44,\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.59it/s, train_loss=8.06, val_loss=1.44,\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.59it/s, train_loss=7.57, val_loss=0.532\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.59it/s, train_loss=7.57, val_loss=0.532\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.59it/s, train_loss=3.06, val_loss=0.166\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.59it/s, train_loss=3.06, val_loss=0.166\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.59it/s, train_loss=1.19, val_loss=0.731\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.51it/s, train_loss=1.19, val_loss=0.731\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.51it/s, train_loss=1.19, val_loss=0.731\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.51it/s, train_loss=3.91, val_loss=1.1, \u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.51it/s, train_loss=3.91, val_loss=1.1, \u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.51it/s, train_loss=5.68, val_loss=0.626\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.51it/s, train_loss=5.68, val_loss=0.626\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.51it/s, train_loss=3.3, val_loss=0.121,\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.51it/s, train_loss=3.3, val_loss=0.121,\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.51it/s, train_loss=0.843, val_loss=0.28\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.29it/s, train_loss=0.843, val_loss=0.28\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.29it/s, train_loss=0.843, val_loss=0.28\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.29it/s, train_loss=1.72, val_loss=0.629\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.29it/s, train_loss=1.72, val_loss=0.629\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.29it/s, train_loss=3.51, val_loss=0.515\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.29it/s, train_loss=3.51, val_loss=0.515\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7500313087038196\n",
      "Its' val AUC : 0.6581487019165421\n",
      "Its external  AUC: 0.5963447297936033\n",
      "Curent best Test AUC: 0.7501565435190983\n",
      "Its' val AUC : 0.673100448552399\n",
      "Its external  AUC: 0.6155664093272413\n",
      "Curent best Test AUC: 0.7507827175954915\n",
      "Its' val AUC : 0.6533913279869512\n",
      "Its external  AUC: 0.5755475027572081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.29it/s, train_loss=2.91, val_loss=0.147\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.29it/s, train_loss=2.91, val_loss=0.147\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.29it/s, train_loss=0.984, val_loss=0.13\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.50it/s, train_loss=0.984, val_loss=0.13\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.50it/s, train_loss=0.984, val_loss=0.13\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.50it/s, train_loss=0.796, val_loss=0.38\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.50it/s, train_loss=0.796, val_loss=0.38\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.50it/s, train_loss=1.95, val_loss=0.375\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.50it/s, train_loss=1.95, val_loss=0.375\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.50it/s, train_loss=1.87, val_loss=0.223\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.50it/s, train_loss=1.87, val_loss=0.223\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.50it/s, train_loss=1.14, val_loss=0.080\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=1.14, val_loss=0.080\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=1.14, val_loss=0.080\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.485, val_loss=0.04\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.485, val_loss=0.04\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.388, val_loss=0.10\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.388, val_loss=0.10\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.722, val_loss=0.16\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.722, val_loss=0.16\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    33: reducing learning rate of group 0 to 1.2500e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=1.02, val_loss=0.154\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.67it/s, train_loss=1.02, val_loss=0.154\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.67it/s, train_loss=1.02, val_loss=0.154\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.67it/s, train_loss=0.955, val_loss=0.09\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.67it/s, train_loss=0.955, val_loss=0.09\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.67it/s, train_loss=0.607, val_loss=0.04\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.67it/s, train_loss=0.607, val_loss=0.04\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.67it/s, train_loss=0.315, val_loss=0.05\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.67it/s, train_loss=0.315, val_loss=0.05\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.67it/s, train_loss=0.315, val_loss=0.09\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.77it/s, train_loss=0.315, val_loss=0.09\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.77it/s, train_loss=0.315, val_loss=0.09\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.77it/s, train_loss=0.526, val_loss=0.12\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.77it/s, train_loss=0.526, val_loss=0.12\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.77it/s, train_loss=0.668, val_loss=0.10\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.77it/s, train_loss=0.668, val_loss=0.10\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.77it/s, train_loss=0.578, val_loss=0.06\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.77it/s, train_loss=0.578, val_loss=0.06\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.77it/s, train_loss=0.364, val_loss=0.03\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.74it/s, train_loss=0.364, val_loss=0.03\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.74it/s, train_loss=0.364, val_loss=0.03\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.74it/s, train_loss=0.245, val_loss=0.04\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.74it/s, train_loss=0.245, val_loss=0.04\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.56it/s, train_loss=0.308, val_loss=0.07\u001b[A\n",
      " 61%|█████████████████████████▌                | 61/100 [01:32<00:57,  1.48s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=14.2, val_loss=77.8, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=14.2, val_loss=77.8, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=369, val_loss=40.1, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=369, val_loss=40.1, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=220, val_loss=39, train_cindex\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=220, val_loss=39, train_cindex\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=205, val_loss=4.04, train_cind\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.77it/s, train_loss=205, val_loss=4.04, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.77it/s, train_loss=205, val_loss=4.04, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.77it/s, train_loss=21, val_loss=13.6, tra\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.77it/s, train_loss=21, val_loss=13.6, tra\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.77it/s, train_loss=66.8, val_loss=5.58, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.77it/s, train_loss=66.8, val_loss=5.58, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.77it/s, train_loss=26.8, val_loss=0.43, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.77it/s, train_loss=26.8, val_loss=0.43, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7560425798371947\n",
      "Its' val AUC : 0.7197227130623896\n",
      "Its external  AUC: 0.6102095478178667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.77it/s, train_loss=2.97, val_loss=2.94, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 34.10it/s, train_loss=2.97, val_loss=2.94, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.10it/s, train_loss=2.97, val_loss=2.94, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.10it/s, train_loss=15.3, val_loss=2.47, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.10it/s, train_loss=15.3, val_loss=2.47, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.10it/s, train_loss=12.1, val_loss=0.224, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.10it/s, train_loss=12.1, val_loss=0.224,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.10it/s, train_loss=1.54, val_loss=2.34, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.10it/s, train_loss=1.54, val_loss=2.34, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.10it/s, train_loss=14.3, val_loss=0.69, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.50it/s, train_loss=14.3, val_loss=0.69,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.50it/s, train_loss=14.3, val_loss=0.69,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.50it/s, train_loss=4.57, val_loss=1.06,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.50it/s, train_loss=4.57, val_loss=1.06,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.50it/s, train_loss=4.95, val_loss=1.52,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.50it/s, train_loss=4.95, val_loss=1.52,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.50it/s, train_loss=7.47, val_loss=0.16,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.50it/s, train_loss=7.47, val_loss=0.16,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.50it/s, train_loss=1.02, val_loss=1.07,\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.64it/s, train_loss=1.02, val_loss=1.07,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.64it/s, train_loss=1.02, val_loss=1.07,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.64it/s, train_loss=5.28, val_loss=0.852\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.64it/s, train_loss=5.28, val_loss=0.852\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.64it/s, train_loss=4, val_loss=0.151, t\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.64it/s, train_loss=4, val_loss=0.151, t\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.64it/s, train_loss=0.952, val_loss=0.84\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.64it/s, train_loss=0.952, val_loss=0.84\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.64it/s, train_loss=4.61, val_loss=0.195\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.68it/s, train_loss=4.61, val_loss=0.195\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.68it/s, train_loss=4.61, val_loss=0.195\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.68it/s, train_loss=1.11, val_loss=0.756\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.68it/s, train_loss=1.11, val_loss=0.756\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.68it/s, train_loss=3.92, val_loss=0.392\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.68it/s, train_loss=3.92, val_loss=0.392\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.68it/s, train_loss=2.02, val_loss=0.196\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.68it/s, train_loss=2.02, val_loss=0.196\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.68it/s, train_loss=1.14, val_loss=0.381\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.73it/s, train_loss=1.14, val_loss=0.381\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.73it/s, train_loss=1.14, val_loss=0.381\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.73it/s, train_loss=2.25, val_loss=0.112\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.73it/s, train_loss=2.25, val_loss=0.112\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.73it/s, train_loss=0.743, val_loss=0.11\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.73it/s, train_loss=0.743, val_loss=0.11\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.73it/s, train_loss=0.616, val_loss=0.32\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.73it/s, train_loss=0.616, val_loss=0.32\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.73it/s, train_loss=1.63, val_loss=0.174\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.71it/s, train_loss=1.63, val_loss=0.174\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.71it/s, train_loss=1.63, val_loss=0.174\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.71it/s, train_loss=0.921, val_loss=0.02\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.71it/s, train_loss=0.921, val_loss=0.02\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.71it/s, train_loss=0.178, val_loss=0.18\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.71it/s, train_loss=0.178, val_loss=0.18\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    24: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.71it/s, train_loss=0.965, val_loss=0.20\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.71it/s, train_loss=0.965, val_loss=0.20\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.71it/s, train_loss=1.07, val_loss=0.049\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.75it/s, train_loss=1.07, val_loss=0.049\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.75it/s, train_loss=1.07, val_loss=0.049\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.75it/s, train_loss=0.305, val_loss=0.08\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.75it/s, train_loss=0.305, val_loss=0.08\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.75it/s, train_loss=0.537, val_loss=0.15\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.75it/s, train_loss=0.537, val_loss=0.15\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.75it/s, train_loss=0.857, val_loss=0.04\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.75it/s, train_loss=0.857, val_loss=0.04\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.75it/s, train_loss=0.264, val_loss=0.01\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.75it/s, train_loss=0.264, val_loss=0.01\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.75it/s, train_loss=0.264, val_loss=0.01\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.75it/s, train_loss=0.115, val_loss=0.03\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.75it/s, train_loss=0.115, val_loss=0.03\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.75it/s, train_loss=0.221, val_loss=0.05\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.75it/s, train_loss=0.221, val_loss=0.05\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.75it/s, train_loss=0.368, val_loss=0.04\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.75it/s, train_loss=0.368, val_loss=0.04\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.75it/s, train_loss=0.325, val_loss=0.02\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.27it/s, train_loss=0.325, val_loss=0.02\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.27it/s, train_loss=0.325, val_loss=0.02\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.27it/s, train_loss=0.173, val_loss=0.02\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.27it/s, train_loss=0.173, val_loss=0.02\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    35: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.27it/s, train_loss=0.136, val_loss=0.04\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.27it/s, train_loss=0.136, val_loss=0.04\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.27it/s, train_loss=0.228, val_loss=0.05\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.27it/s, train_loss=0.228, val_loss=0.05\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.27it/s, train_loss=0.246, val_loss=0.04\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 33.74it/s, train_loss=0.246, val_loss=0.04\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 33.74it/s, train_loss=0.246, val_loss=0.04\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 33.74it/s, train_loss=0.206, val_loss=0.02\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 33.74it/s, train_loss=0.206, val_loss=0.02\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 33.74it/s, train_loss=0.142, val_loss=0.01\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 33.74it/s, train_loss=0.142, val_loss=0.01\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 33.74it/s, train_loss=0.105, val_loss=0.01\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 33.74it/s, train_loss=0.105, val_loss=0.01\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 33.74it/s, train_loss=0.114, val_loss=0.02\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 33.35it/s, train_loss=0.114, val_loss=0.02\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 33.35it/s, train_loss=0.114, val_loss=0.02\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    42: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 33.35it/s, train_loss=0.149, val_loss=0.02\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 33.35it/s, train_loss=0.149, val_loss=0.02\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.02it/s, train_loss=0.167, val_loss=0.02\u001b[A\n",
      " 62%|██████████████████████████                | 62/100 [01:34<00:56,  1.48s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=271, val_loss=22, train_cindex\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=271, val_loss=22, train_cindex\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=114, val_loss=2.88, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=114, val_loss=2.88, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=16.1, val_loss=2.64, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=16.1, val_loss=2.64, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=15.2, val_loss=6.97, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.13it/s, train_loss=15.2, val_loss=6.97, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.13it/s, train_loss=15.2, val_loss=6.97, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.13it/s, train_loss=38.4, val_loss=2.74, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.13it/s, train_loss=38.4, val_loss=2.74, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.13it/s, train_loss=13.2, val_loss=1.94, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.13it/s, train_loss=13.2, val_loss=1.94, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.13it/s, train_loss=8.6, val_loss=4.28, tr\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.13it/s, train_loss=8.6, val_loss=4.28, tr\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.30745147150907953\n",
      "Its' val AUC : 0.3077341307598206\n",
      "Its external  AUC: 0.3875846856782732\n",
      "Curent best Test AUC: 0.6810269254852849\n",
      "Its' val AUC : 0.7043631915182819\n",
      "Its external  AUC: 0.599338270048842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.13it/s, train_loss=24.8, val_loss=0.422, \u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 33.85it/s, train_loss=24.8, val_loss=0.422, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.85it/s, train_loss=24.8, val_loss=0.422, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.85it/s, train_loss=2.06, val_loss=4.38, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.85it/s, train_loss=2.06, val_loss=4.38, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.85it/s, train_loss=20.6, val_loss=1.59, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.85it/s, train_loss=20.6, val_loss=1.59, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.85it/s, train_loss=8.43, val_loss=1.62, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.85it/s, train_loss=8.43, val_loss=1.62, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.85it/s, train_loss=8.43, val_loss=2, tra\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 33.16it/s, train_loss=8.43, val_loss=2, tr\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.16it/s, train_loss=8.43, val_loss=2, tr\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.16it/s, train_loss=10.7, val_loss=0.402\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.16it/s, train_loss=10.7, val_loss=0.402\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.16it/s, train_loss=2.46, val_loss=2.23,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.16it/s, train_loss=2.46, val_loss=2.23,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.16it/s, train_loss=11.5, val_loss=0.191\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.16it/s, train_loss=11.5, val_loss=0.191\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7167188478396994\n",
      "Its' val AUC : 0.7182275383988038\n",
      "Its external  AUC: 0.6125728690720025\n",
      "Curent best Test AUC: 0.7393863494051346\n",
      "Its' val AUC : 0.708848715509039\n",
      "Its external  AUC: 0.6163541830786198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.16it/s, train_loss=1.32, val_loss=2.14,\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 33.85it/s, train_loss=1.32, val_loss=2.14,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.85it/s, train_loss=1.32, val_loss=2.14,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.85it/s, train_loss=11.3, val_loss=0.599\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.85it/s, train_loss=11.3, val_loss=0.599\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.85it/s, train_loss=3.3, val_loss=1.17, \u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.85it/s, train_loss=3.3, val_loss=1.17, \u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.85it/s, train_loss=6.22, val_loss=1.03,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.85it/s, train_loss=6.22, val_loss=1.03,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.85it/s, train_loss=5.29, val_loss=0.298\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.22it/s, train_loss=5.29, val_loss=0.298\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.22it/s, train_loss=5.29, val_loss=0.298\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.22it/s, train_loss=1.6, val_loss=1.19, \u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.22it/s, train_loss=1.6, val_loss=1.19, \u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.22it/s, train_loss=6.21, val_loss=0.351\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.22it/s, train_loss=6.21, val_loss=0.351\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.22it/s, train_loss=1.85, val_loss=0.404\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.22it/s, train_loss=1.85, val_loss=0.404\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.22it/s, train_loss=2.15, val_loss=0.834\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.41it/s, train_loss=2.15, val_loss=0.834\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.41it/s, train_loss=2.15, val_loss=0.834\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.41it/s, train_loss=4.35, val_loss=0.049\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.41it/s, train_loss=4.35, val_loss=0.049\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.41it/s, train_loss=0.317, val_loss=0.64\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.41it/s, train_loss=0.317, val_loss=0.64\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.41it/s, train_loss=3.32, val_loss=0.381\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.41it/s, train_loss=3.32, val_loss=0.381\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    21: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.41it/s, train_loss=2.02, val_loss=0.149\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.31it/s, train_loss=2.02, val_loss=0.149\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.31it/s, train_loss=2.02, val_loss=0.149\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.31it/s, train_loss=0.829, val_loss=0.6,\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.31it/s, train_loss=0.829, val_loss=0.6,\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.31it/s, train_loss=3.07, val_loss=0.076\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.31it/s, train_loss=3.07, val_loss=0.076\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.31it/s, train_loss=0.464, val_loss=0.36\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.31it/s, train_loss=0.464, val_loss=0.36\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.31it/s, train_loss=1.91, val_loss=0.33,\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.51it/s, train_loss=1.91, val_loss=0.33,\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.51it/s, train_loss=1.91, val_loss=0.33,\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.51it/s, train_loss=1.73, val_loss=0.075\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.51it/s, train_loss=1.73, val_loss=0.075\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.51it/s, train_loss=0.43, val_loss=0.126\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.51it/s, train_loss=0.43, val_loss=0.126\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.51it/s, train_loss=0.701, val_loss=0.29\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.51it/s, train_loss=0.701, val_loss=0.29\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.51it/s, train_loss=1.55, val_loss=0.165\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 33.98it/s, train_loss=1.55, val_loss=0.165\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.98it/s, train_loss=1.55, val_loss=0.165\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.98it/s, train_loss=0.897, val_loss=0.04\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.98it/s, train_loss=0.897, val_loss=0.04\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    31: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.98it/s, train_loss=0.271, val_loss=0.16\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.98it/s, train_loss=0.271, val_loss=0.16\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.98it/s, train_loss=0.871, val_loss=0.20\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.98it/s, train_loss=0.871, val_loss=0.20\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.98it/s, train_loss=1.06, val_loss=0.069\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.33it/s, train_loss=1.06, val_loss=0.069\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.33it/s, train_loss=1.06, val_loss=0.069\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.33it/s, train_loss=0.382, val_loss=0.06\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.33it/s, train_loss=0.382, val_loss=0.06\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.33it/s, train_loss=0.356, val_loss=0.15\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.33it/s, train_loss=0.356, val_loss=0.15\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.33it/s, train_loss=0.83, val_loss=0.098\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.33it/s, train_loss=0.83, val_loss=0.098\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.33it/s, train_loss=0.56, val_loss=0.046\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.40it/s, train_loss=0.56, val_loss=0.046\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.40it/s, train_loss=0.56, val_loss=0.046\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.40it/s, train_loss=0.291, val_loss=0.03\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.40it/s, train_loss=0.291, val_loss=0.03\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.40it/s, train_loss=0.219, val_loss=0.06\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.40it/s, train_loss=0.219, val_loss=0.06\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.40it/s, train_loss=0.375, val_loss=0.09\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.40it/s, train_loss=0.375, val_loss=0.09\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.40it/s, train_loss=0.493, val_loss=0.07\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.62it/s, train_loss=0.493, val_loss=0.07\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.62it/s, train_loss=0.493, val_loss=0.07\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.62it/s, train_loss=0.406, val_loss=0.04\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.62it/s, train_loss=0.406, val_loss=0.04\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    43: reducing learning rate of group 0 to 1.2500e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.27it/s, train_loss=0.242, val_loss=0.03\u001b[A\n",
      " 63%|██████████████████████████▍               | 63/100 [01:35<00:54,  1.48s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=18.5, val_loss=15.1, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=18.5, val_loss=15.1, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=81.1, val_loss=10.1, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=81.1, val_loss=10.1, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=46.4, val_loss=6.17, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=46.4, val_loss=6.17, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=28, val_loss=0.429, train_cind\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.45it/s, train_loss=28, val_loss=0.429, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.45it/s, train_loss=28, val_loss=0.429, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.45it/s, train_loss=3.1, val_loss=2.7, tra\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.45it/s, train_loss=3.1, val_loss=2.7, tra\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.45it/s, train_loss=15.6, val_loss=2.57, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.45it/s, train_loss=15.6, val_loss=2.57, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.45it/s, train_loss=13.9, val_loss=0.999, \u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.45it/s, train_loss=13.9, val_loss=0.999, \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.24821540388227928\n",
      "Its' val AUC : 0.2775587875492728\n",
      "Its external  AUC: 0.3751378604064913\n",
      "Curent best Test AUC: 0.7365059486537258\n",
      "Its' val AUC : 0.7104798151420416\n",
      "Its external  AUC: 0.5966598392941547\n",
      "Curent best Test AUC: 0.7402629931120852\n",
      "Its' val AUC : 0.7099361152643741\n",
      "Its external  AUC: 0.5942965180400189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.45it/s, train_loss=5.57, val_loss=2.1, tr\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 34.04it/s, train_loss=5.57, val_loss=2.1, tr\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.04it/s, train_loss=5.57, val_loss=2.1, tr\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.04it/s, train_loss=10.9, val_loss=1.05, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.04it/s, train_loss=10.9, val_loss=1.05, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.04it/s, train_loss=5.05, val_loss=0.519, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.04it/s, train_loss=5.05, val_loss=0.519,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.04it/s, train_loss=3.25, val_loss=0.519,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.04it/s, train_loss=3.25, val_loss=0.519,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.04it/s, train_loss=3.38, val_loss=0.399,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 32.65it/s, train_loss=3.38, val_loss=0.399\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.65it/s, train_loss=3.38, val_loss=0.399\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.65it/s, train_loss=2.51, val_loss=0.191\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.65it/s, train_loss=2.51, val_loss=0.191\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.65it/s, train_loss=1.23, val_loss=0.242\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.65it/s, train_loss=1.23, val_loss=0.242\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.65it/s, train_loss=1.63, val_loss=0.355\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.65it/s, train_loss=1.63, val_loss=0.355\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.65it/s, train_loss=2.37, val_loss=0.298\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 33.25it/s, train_loss=2.37, val_loss=0.298\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.25it/s, train_loss=2.37, val_loss=0.298\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    10: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.25it/s, train_loss=1.88, val_loss=0.359\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.25it/s, train_loss=1.88, val_loss=0.359\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.25it/s, train_loss=1.9, val_loss=0.301,\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.25it/s, train_loss=1.9, val_loss=0.301,\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.25it/s, train_loss=1.57, val_loss=0.13,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.25it/s, train_loss=1.57, val_loss=0.13,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.25it/s, train_loss=0.767, val_loss=0.16\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 33.40it/s, train_loss=0.767, val_loss=0.16\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.40it/s, train_loss=0.767, val_loss=0.16\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.40it/s, train_loss=0.97, val_loss=0.203\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.40it/s, train_loss=0.97, val_loss=0.203\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.40it/s, train_loss=1.18, val_loss=0.167\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.40it/s, train_loss=1.18, val_loss=0.167\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.40it/s, train_loss=1.1, val_loss=0.137,\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.40it/s, train_loss=1.1, val_loss=0.137,\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.40it/s, train_loss=0.88, val_loss=0.074\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 33.65it/s, train_loss=0.88, val_loss=0.074\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.65it/s, train_loss=0.88, val_loss=0.074\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.65it/s, train_loss=0.412, val_loss=0.06\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.65it/s, train_loss=0.412, val_loss=0.06\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.65it/s, train_loss=0.301, val_loss=0.09\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.65it/s, train_loss=0.301, val_loss=0.09\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.65it/s, train_loss=0.429, val_loss=0.08\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.65it/s, train_loss=0.429, val_loss=0.08\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.65it/s, train_loss=0.398, val_loss=0.08\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 33.87it/s, train_loss=0.398, val_loss=0.08\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.87it/s, train_loss=0.398, val_loss=0.08\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.87it/s, train_loss=0.379, val_loss=0.06\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.87it/s, train_loss=0.379, val_loss=0.06\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.87it/s, train_loss=0.288, val_loss=0.04\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.87it/s, train_loss=0.288, val_loss=0.04\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.87it/s, train_loss=0.145, val_loss=0.05\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.87it/s, train_loss=0.145, val_loss=0.05\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.87it/s, train_loss=0.206, val_loss=0.05\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.04it/s, train_loss=0.206, val_loss=0.05\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.04it/s, train_loss=0.206, val_loss=0.05\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.04it/s, train_loss=0.251, val_loss=0.05\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.04it/s, train_loss=0.251, val_loss=0.05\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 34.04it/s, train_loss=0.274, val_loss=0.04\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.04it/s, train_loss=0.274, val_loss=0.04\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.04it/s, train_loss=0.273, val_loss=0.03\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.04it/s, train_loss=0.273, val_loss=0.03\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.04it/s, train_loss=0.171, val_loss=0.03\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.02it/s, train_loss=0.171, val_loss=0.03\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.02it/s, train_loss=0.171, val_loss=0.03\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.02it/s, train_loss=0.157, val_loss=0.03\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.02it/s, train_loss=0.157, val_loss=0.03\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.02it/s, train_loss=0.175, val_loss=0.03\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.02it/s, train_loss=0.175, val_loss=0.03\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.02it/s, train_loss=0.202, val_loss=0.04\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.02it/s, train_loss=0.202, val_loss=0.04\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.02it/s, train_loss=0.234, val_loss=0.03\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.15it/s, train_loss=0.234, val_loss=0.03\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.15it/s, train_loss=0.234, val_loss=0.03\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.15it/s, train_loss=0.186, val_loss=0.02\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.15it/s, train_loss=0.186, val_loss=0.02\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.15it/s, train_loss=0.152, val_loss=0.02\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.15it/s, train_loss=0.152, val_loss=0.02\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.15it/s, train_loss=0.137, val_loss=0.02\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.15it/s, train_loss=0.137, val_loss=0.02\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.15it/s, train_loss=0.127, val_loss=0.02\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.03it/s, train_loss=0.127, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.03it/s, train_loss=0.127, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.03it/s, train_loss=0.151, val_loss=0.02\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.03it/s, train_loss=0.151, val_loss=0.02\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.03it/s, train_loss=0.136, val_loss=0.02\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.03it/s, train_loss=0.136, val_loss=0.02\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.03it/s, train_loss=0.11, val_loss=0.019\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.03it/s, train_loss=0.11, val_loss=0.019\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.03it/s, train_loss=0.093, val_loss=0.01\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 33.99it/s, train_loss=0.093, val_loss=0.01\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 33.99it/s, train_loss=0.093, val_loss=0.01\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 33.99it/s, train_loss=0.0775, val_loss=0.0\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 33.99it/s, train_loss=0.0775, val_loss=0.0\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 33.79it/s, train_loss=0.0922, val_loss=0.0\u001b[A\n",
      " 64%|██████████████████████████▉               | 64/100 [01:37<00:53,  1.49s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=5.67, val_loss=196, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=5.67, val_loss=196, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=1.07e+3, val_loss=14.3, train_\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=1.07e+3, val_loss=14.3, train_\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=65.1, val_loss=81.8, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=65.1, val_loss=81.8, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=454, val_loss=4.55, train_cind\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 31.32it/s, train_loss=454, val_loss=4.55, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.32it/s, train_loss=454, val_loss=4.55, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.32it/s, train_loss=27, val_loss=38.5, tra\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.32it/s, train_loss=27, val_loss=38.5, tra\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.32it/s, train_loss=197, val_loss=37.9, tr\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.32it/s, train_loss=197, val_loss=37.9, tr\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.32it/s, train_loss=193, val_loss=3.92, tr\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.32it/s, train_loss=193, val_loss=3.92, tr\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.24383218534752663\n",
      "Its' val AUC : 0.2786461873046079\n",
      "Its external  AUC: 0.3951473136915078\n",
      "Curent best Test AUC: 0.2825297432686287\n",
      "Its' val AUC : 0.2887046350414571\n",
      "Its external  AUC: 0.41436899322514575\n",
      "Curent best Test AUC: 0.7481527864746399\n",
      "Its' val AUC : 0.7106157401114584\n",
      "Its external  AUC: 0.6220261540885458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.32it/s, train_loss=18.2, val_loss=15.3, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 32.22it/s, train_loss=18.2, val_loss=15.3, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.22it/s, train_loss=18.2, val_loss=15.3, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.22it/s, train_loss=81.9, val_loss=17.9, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.22it/s, train_loss=81.9, val_loss=17.9, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.22it/s, train_loss=93, val_loss=1.47, tra\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.22it/s, train_loss=93, val_loss=1.47, tr\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.22it/s, train_loss=8.22, val_loss=8.29, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.22it/s, train_loss=8.22, val_loss=8.29, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.22it/s, train_loss=43.3, val_loss=14.5, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 32.97it/s, train_loss=43.3, val_loss=14.5,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.97it/s, train_loss=43.3, val_loss=14.5,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.97it/s, train_loss=75.6, val_loss=5.26,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.97it/s, train_loss=75.6, val_loss=5.26,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.97it/s, train_loss=28.6, val_loss=1.56,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.97it/s, train_loss=28.6, val_loss=1.56,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.97it/s, train_loss=6.83, val_loss=7, tr\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.97it/s, train_loss=6.83, val_loss=7, tr\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.97it/s, train_loss=30.6, val_loss=1.51,\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 33.39it/s, train_loss=30.6, val_loss=1.51,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.39it/s, train_loss=30.6, val_loss=1.51,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.39it/s, train_loss=6.8, val_loss=0.428,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.39it/s, train_loss=6.8, val_loss=0.428,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.39it/s, train_loss=2.16, val_loss=0.493\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.39it/s, train_loss=2.16, val_loss=0.493\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.39it/s, train_loss=2.87, val_loss=1.01,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.39it/s, train_loss=2.87, val_loss=1.01,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.39it/s, train_loss=5.43, val_loss=0.982\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 33.18it/s, train_loss=5.43, val_loss=0.982\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.18it/s, train_loss=5.43, val_loss=0.982\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.18it/s, train_loss=5.09, val_loss=0.445\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.18it/s, train_loss=5.09, val_loss=0.445\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.18it/s, train_loss=2.28, val_loss=0.166\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.18it/s, train_loss=2.28, val_loss=0.166\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    16: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.18it/s, train_loss=1.05, val_loss=0.441\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.18it/s, train_loss=1.05, val_loss=0.441\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.18it/s, train_loss=2.76, val_loss=0.692\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 33.55it/s, train_loss=2.76, val_loss=0.692\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.55it/s, train_loss=2.76, val_loss=0.692\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.55it/s, train_loss=4.13, val_loss=0.48,\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.55it/s, train_loss=4.13, val_loss=0.48,\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.55it/s, train_loss=2.84, val_loss=0.216\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.55it/s, train_loss=2.84, val_loss=0.216\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.55it/s, train_loss=1.08, val_loss=0.39,\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.55it/s, train_loss=1.08, val_loss=0.39,\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.55it/s, train_loss=1.55, val_loss=0.679\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 33.68it/s, train_loss=1.55, val_loss=0.679\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.68it/s, train_loss=1.55, val_loss=0.679\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.68it/s, train_loss=2.84, val_loss=0.644\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.68it/s, train_loss=2.84, val_loss=0.644\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.68it/s, train_loss=2.72, val_loss=0.456\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.68it/s, train_loss=2.72, val_loss=0.456\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.68it/s, train_loss=1.9, val_loss=0.266,\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.68it/s, train_loss=1.9, val_loss=0.266,\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.68it/s, train_loss=1.12, val_loss=0.199\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 33.80it/s, train_loss=1.12, val_loss=0.199\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 33.80it/s, train_loss=1.12, val_loss=0.199\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 33.80it/s, train_loss=0.95, val_loss=0.253\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 33.80it/s, train_loss=0.95, val_loss=0.253\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 33.80it/s, train_loss=1.35, val_loss=0.296\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 33.80it/s, train_loss=1.35, val_loss=0.296\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    28: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch    34: reducing learning rate of group 0 to 1.2500e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 33.80it/s, train_loss=1.65, val_loss=0.262\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 33.80it/s, train_loss=1.65, val_loss=0.262\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 33.80it/s, train_loss=1.45, val_loss=0.184\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 33.80it/s, train_loss=1.45, val_loss=0.184\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.80it/s, train_loss=1.45, val_loss=0.184\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.80it/s, train_loss=0.999, val_loss=0.13\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.80it/s, train_loss=0.999, val_loss=0.13\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.80it/s, train_loss=0.718, val_loss=0.15\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.80it/s, train_loss=0.718, val_loss=0.15\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.80it/s, train_loss=0.787, val_loss=0.18\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.80it/s, train_loss=0.787, val_loss=0.18\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.80it/s, train_loss=0.933, val_loss=0.19\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 33.98it/s, train_loss=0.933, val_loss=0.19\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 33.98it/s, train_loss=0.933, val_loss=0.19\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 33.98it/s, train_loss=0.963, val_loss=0.16\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 33.98it/s, train_loss=0.963, val_loss=0.16\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 33.98it/s, train_loss=0.826, val_loss=0.11\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 33.98it/s, train_loss=0.826, val_loss=0.11\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 33.98it/s, train_loss=0.596, val_loss=0.07\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 33.98it/s, train_loss=0.596, val_loss=0.07\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 33.98it/s, train_loss=0.439, val_loss=0.07\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.13it/s, train_loss=0.439, val_loss=0.07\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.13it/s, train_loss=0.439, val_loss=0.07\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.13it/s, train_loss=0.492, val_loss=0.09\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.13it/s, train_loss=0.492, val_loss=0.09\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.13it/s, train_loss=0.626, val_loss=0.09\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.13it/s, train_loss=0.626, val_loss=0.09\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.13it/s, train_loss=0.648, val_loss=0.08\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.13it/s, train_loss=0.648, val_loss=0.08\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.13it/s, train_loss=0.548, val_loss=0.06\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.15it/s, train_loss=0.548, val_loss=0.06\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.15it/s, train_loss=0.548, val_loss=0.06\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.15it/s, train_loss=0.433, val_loss=0.06\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.15it/s, train_loss=0.433, val_loss=0.06\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 33.66it/s, train_loss=0.423, val_loss=0.07\u001b[A\n",
      " 65%|███████████████████████████▎              | 65/100 [01:38<00:52,  1.49s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=11.6, val_loss=42.9, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=11.6, val_loss=42.9, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=229, val_loss=2.25, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=229, val_loss=2.25, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=10.4, val_loss=3.59, train_cin\u001b[A\n",
      "Epoch 2:   6%| | 3/50 [00:00<00:01, 29.20it/s, train_loss=10.4, val_loss=3.59, t\u001b[A\n",
      "Epoch 3:   6%| | 3/50 [00:00<00:01, 29.20it/s, train_loss=10.4, val_loss=3.59, t\u001b[A\n",
      "Epoch 3:   6%| | 3/50 [00:00<00:01, 29.20it/s, train_loss=19.3, val_loss=0.51, t\u001b[A\n",
      "Epoch 4:   6%| | 3/50 [00:00<00:01, 29.20it/s, train_loss=19.3, val_loss=0.51, t\u001b[A\n",
      "Epoch 4:   6%| | 3/50 [00:00<00:01, 29.20it/s, train_loss=2.67, val_loss=0.696, \u001b[A\n",
      "Epoch 5:   6%| | 3/50 [00:00<00:01, 29.20it/s, train_loss=2.67, val_loss=0.696, \u001b[A\n",
      "Epoch 5:   6%| | 3/50 [00:00<00:01, 29.20it/s, train_loss=3.84, val_loss=0.431, \u001b[A\n",
      "Epoch 6:   6%| | 3/50 [00:00<00:01, 29.20it/s, train_loss=3.84, val_loss=0.431, \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.3082028804007514\n",
      "Its' val AUC : 0.3048797064020661\n",
      "Its external  AUC: 0.39152355443516623\n",
      "Curent best Test AUC: 0.319724483406387\n",
      "Its' val AUC : 0.35055049612613837\n",
      "Its external  AUC: 0.4064912557113597\n",
      "Curent best Test AUC: 0.674264245460238\n",
      "Its' val AUC : 0.7032757917629469\n",
      "Its external  AUC: 0.606113124310698\n",
      "Curent best Test AUC: 0.7420162805259862\n",
      "Its' val AUC : 0.7053146663042\n",
      "Its external  AUC: 0.5475027572081298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6:   6%| | 3/50 [00:00<00:01, 29.20it/s, train_loss=2.08, val_loss=0.32, t\u001b[A\n",
      "Epoch 6:  14%|▏| 7/50 [00:00<00:01, 30.28it/s, train_loss=2.08, val_loss=0.32, t\u001b[A\n",
      "Epoch 7:  14%|▏| 7/50 [00:00<00:01, 30.28it/s, train_loss=2.08, val_loss=0.32, t\u001b[A\n",
      "Epoch 7:  14%|▏| 7/50 [00:00<00:01, 30.28it/s, train_loss=1.52, val_loss=0.37, t\u001b[A\n",
      "Epoch 8:  14%|▏| 7/50 [00:00<00:01, 30.28it/s, train_loss=1.52, val_loss=0.37, t\u001b[A\n",
      "Epoch 8:  14%|▏| 7/50 [00:00<00:01, 30.28it/s, train_loss=1.96, val_loss=0.164, \u001b[A\n",
      "Epoch 9:  14%|▏| 7/50 [00:00<00:01, 30.28it/s, train_loss=1.96, val_loss=0.164, \u001b[A\n",
      "Epoch 9:  14%|▏| 7/50 [00:00<00:01, 30.28it/s, train_loss=0.874, val_loss=0.372,\u001b[A\n",
      "Epoch 10:  14%|▏| 7/50 [00:00<00:01, 30.28it/s, train_loss=0.874, val_loss=0.372\u001b[A\n",
      "Epoch 10:  14%|▏| 7/50 [00:00<00:01, 30.28it/s, train_loss=2.05, val_loss=0.0759\u001b[A\n",
      "Epoch 10:  22%|▏| 11/50 [00:00<00:01, 31.06it/s, train_loss=2.05, val_loss=0.075\u001b[A\n",
      "Epoch 11:  22%|▏| 11/50 [00:00<00:01, 31.06it/s, train_loss=2.05, val_loss=0.075\u001b[A\n",
      "Epoch 11:  22%|▏| 11/50 [00:00<00:01, 31.06it/s, train_loss=0.434, val_loss=0.23\u001b[A\n",
      "Epoch 12:  22%|▏| 11/50 [00:00<00:01, 31.06it/s, train_loss=0.434, val_loss=0.23\u001b[A\n",
      "Epoch 12:  22%|▏| 11/50 [00:00<00:01, 31.06it/s, train_loss=1.34, val_loss=0.182\u001b[A\n",
      "Epoch 13:  22%|▏| 11/50 [00:00<00:01, 31.06it/s, train_loss=1.34, val_loss=0.182\u001b[A\n",
      "Epoch 13:  22%|▏| 11/50 [00:00<00:01, 31.06it/s, train_loss=1.03, val_loss=0.103\u001b[A\n",
      "Epoch 14:  22%|▏| 11/50 [00:00<00:01, 31.06it/s, train_loss=1.03, val_loss=0.103\u001b[A\n",
      "Epoch 14:  22%|▏| 11/50 [00:00<00:01, 31.06it/s, train_loss=0.578, val_loss=0.12\u001b[A\n",
      "Epoch 14:  30%|▎| 15/50 [00:00<00:01, 31.01it/s, train_loss=0.578, val_loss=0.12\u001b[A\n",
      "Epoch 15:  30%|▎| 15/50 [00:00<00:01, 31.01it/s, train_loss=0.578, val_loss=0.12\u001b[A\n",
      "Epoch 15:  30%|▎| 15/50 [00:00<00:01, 31.01it/s, train_loss=0.576, val_loss=0.07\u001b[A\n",
      "Epoch 16:  30%|▎| 15/50 [00:00<00:01, 31.01it/s, train_loss=0.576, val_loss=0.07\u001b[A\n",
      "Epoch 16:  30%|▎| 15/50 [00:00<00:01, 31.01it/s, train_loss=0.387, val_loss=0.09\u001b[A\n",
      "Epoch 17:  30%|▎| 15/50 [00:00<00:01, 31.01it/s, train_loss=0.387, val_loss=0.09\u001b[A\n",
      "Epoch 17:  30%|▎| 15/50 [00:00<00:01, 31.01it/s, train_loss=0.586, val_loss=0.03\u001b[A\n",
      "Epoch 18:  30%|▎| 15/50 [00:00<00:01, 31.01it/s, train_loss=0.586, val_loss=0.03\u001b[A\n",
      "Epoch 18:  30%|▎| 15/50 [00:00<00:01, 31.01it/s, train_loss=0.196, val_loss=0.04\u001b[A\n",
      "Epoch 18:  38%|▍| 19/50 [00:00<00:00, 31.97it/s, train_loss=0.196, val_loss=0.04\u001b[A\n",
      "Epoch 19:  38%|▍| 19/50 [00:00<00:00, 31.97it/s, train_loss=0.196, val_loss=0.04\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7433938634940513\n",
      "Its' val AUC : 0.5803996194100857\n",
      "Its external  AUC: 0.5054356388845124\n",
      "Curent best Test AUC: 0.7546649968691296\n",
      "Its' val AUC : 0.5952154410765258\n",
      "Its external  AUC: 0.5356861509374508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19:  38%|▍| 19/50 [00:00<00:00, 31.97it/s, train_loss=0.296, val_loss=0.10\u001b[A\n",
      "Epoch 20:  38%|▍| 19/50 [00:00<00:00, 31.97it/s, train_loss=0.296, val_loss=0.10\u001b[A\n",
      "Epoch 20:  38%|▍| 19/50 [00:00<00:00, 31.97it/s, train_loss=0.533, val_loss=0.02\u001b[A\n",
      "Epoch 21:  38%|▍| 19/50 [00:00<00:00, 31.97it/s, train_loss=0.533, val_loss=0.02\u001b[A\n",
      "Epoch 21:  38%|▍| 19/50 [00:00<00:00, 31.97it/s, train_loss=0.206, val_loss=0.03\u001b[A\n",
      "Epoch 22:  38%|▍| 19/50 [00:00<00:00, 31.97it/s, train_loss=0.206, val_loss=0.03\u001b[A\n",
      "Epoch 22:  38%|▍| 19/50 [00:00<00:00, 31.97it/s, train_loss=0.237, val_loss=0.07\u001b[A\n",
      "Epoch 22:  46%|▍| 23/50 [00:00<00:00, 32.77it/s, train_loss=0.237, val_loss=0.07\u001b[A\n",
      "Epoch 23:  46%|▍| 23/50 [00:00<00:00, 32.77it/s, train_loss=0.237, val_loss=0.07\u001b[A\n",
      "Epoch 23:  46%|▍| 23/50 [00:00<00:00, 32.77it/s, train_loss=0.47, val_loss=0.039\u001b[A\n",
      "Epoch 24:  46%|▍| 23/50 [00:00<00:00, 32.77it/s, train_loss=0.47, val_loss=0.039\u001b[A\n",
      "Epoch 24:  46%|▍| 23/50 [00:00<00:00, 32.77it/s, train_loss=0.275, val_loss=0.02\u001b[A\n",
      "Epoch 25:  46%|▍| 23/50 [00:00<00:00, 32.77it/s, train_loss=0.275, val_loss=0.02\u001b[A\n",
      "Epoch 25:  46%|▍| 23/50 [00:00<00:00, 32.77it/s, train_loss=0.185, val_loss=0.06\u001b[A\n",
      "Epoch 26:  46%|▍| 23/50 [00:00<00:00, 32.77it/s, train_loss=0.185, val_loss=0.06\u001b[A\n",
      "Epoch 26:  46%|▍| 23/50 [00:00<00:00, 32.77it/s, train_loss=0.366, val_loss=0.04\u001b[A\n",
      "Epoch 26:  54%|▌| 27/50 [00:00<00:00, 32.84it/s, train_loss=0.366, val_loss=0.04\u001b[A\n",
      "Epoch 27:  54%|▌| 27/50 [00:00<00:00, 32.84it/s, train_loss=0.366, val_loss=0.04\u001b[A\n",
      "Epoch 27:  54%|▌| 27/50 [00:00<00:00, 32.84it/s, train_loss=0.315, val_loss=0.01\u001b[A\n",
      "Epoch 28:  54%|▌| 27/50 [00:00<00:00, 32.84it/s, train_loss=0.315, val_loss=0.01\u001b[A\n",
      "Epoch 28:  54%|▌| 27/50 [00:00<00:00, 32.84it/s, train_loss=0.116, val_loss=0.02\u001b[A\n",
      "Epoch 29:  54%|▌| 27/50 [00:00<00:00, 32.84it/s, train_loss=0.116, val_loss=0.02\u001b[A\n",
      "Epoch 29:  54%|▌| 27/50 [00:00<00:00, 32.84it/s, train_loss=0.171, val_loss=0.05\u001b[A\n",
      "Epoch 30:  54%|▌| 27/50 [00:00<00:00, 32.84it/s, train_loss=0.171, val_loss=0.05\u001b[A\n",
      "Epoch 30:  54%|▌| 27/50 [00:00<00:00, 32.84it/s, train_loss=0.3, val_loss=0.0357\u001b[A\n",
      "Epoch 30:  62%|▌| 31/50 [00:00<00:00, 32.57it/s, train_loss=0.3, val_loss=0.0357\u001b[A\n",
      "Epoch 31:  62%|▌| 31/50 [00:00<00:00, 32.57it/s, train_loss=0.3, val_loss=0.0357\u001b[A\n",
      "Epoch 31:  62%|▌| 31/50 [00:00<00:00, 32.57it/s, train_loss=0.209, val_loss=0.01\u001b[A\n",
      "Epoch 32:  62%|▌| 31/50 [00:00<00:00, 32.57it/s, train_loss=0.209, val_loss=0.01\u001b[A\n",
      "Epoch 32:  62%|▌| 31/50 [00:01<00:00, 32.57it/s, train_loss=0.0883, val_loss=0.0\u001b[A\n",
      "Epoch 33:  62%|▌| 31/50 [00:01<00:00, 32.57it/s, train_loss=0.0883, val_loss=0.0\u001b[A\n",
      "Epoch 33:  62%|▌| 31/50 [00:01<00:00, 32.57it/s, train_loss=0.119, val_loss=0.03\u001b[A\n",
      "Epoch 34:  62%|▌| 31/50 [00:01<00:00, 32.57it/s, train_loss=0.119, val_loss=0.03\u001b[A\n",
      "Epoch 34:  62%|▌| 31/50 [00:01<00:00, 32.57it/s, train_loss=0.194, val_loss=0.01\u001b[A\n",
      "Epoch 34:  70%|▋| 35/50 [00:01<00:00, 32.24it/s, train_loss=0.194, val_loss=0.01\u001b[A\n",
      "Epoch 35:  70%|▋| 35/50 [00:01<00:00, 32.24it/s, train_loss=0.194, val_loss=0.01\u001b[A\n",
      "Epoch 35:  70%|▋| 35/50 [00:01<00:00, 32.24it/s, train_loss=0.0825, val_loss=0.0\u001b[A\n",
      "Epoch 36:  70%|▋| 35/50 [00:01<00:00, 32.24it/s, train_loss=0.0825, val_loss=0.0\u001b[A\n",
      "Epoch 36:  70%|▋| 35/50 [00:01<00:00, 32.24it/s, train_loss=0.159, val_loss=0.01\u001b[A\n",
      "Epoch 37:  70%|▋| 35/50 [00:01<00:00, 32.24it/s, train_loss=0.159, val_loss=0.01\u001b[A\n",
      "Epoch 37:  70%|▋| 35/50 [00:01<00:00, 32.24it/s, train_loss=0.0834, val_loss=0.0\u001b[A\n",
      "Epoch 38:  70%|▋| 35/50 [00:01<00:00, 32.24it/s, train_loss=0.0834, val_loss=0.0\u001b[A\n",
      "Epoch 38:  70%|▋| 35/50 [00:01<00:00, 32.24it/s, train_loss=0.159, val_loss=0.01\u001b[A\n",
      "Epoch 38:  78%|▊| 39/50 [00:01<00:00, 32.20it/s, train_loss=0.159, val_loss=0.01\u001b[A\n",
      "Epoch 39:  78%|▊| 39/50 [00:01<00:00, 32.20it/s, train_loss=0.159, val_loss=0.01\u001b[A\n",
      "Epoch 39:  78%|▊| 39/50 [00:01<00:00, 32.20it/s, train_loss=0.0787, val_loss=0.0\u001b[A\n",
      "Epoch 40:  78%|▊| 39/50 [00:01<00:00, 32.20it/s, train_loss=0.0787, val_loss=0.0\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    34: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Curent best Test AUC: 0.7655604257983719\n",
      "Its' val AUC : 0.667527524806307\n",
      "Its external  AUC: 0.42602804474554906\n",
      "Epoch    40: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40:  78%|▊| 39/50 [00:01<00:00, 32.20it/s, train_loss=0.122, val_loss=0.01\u001b[A\n",
      "Epoch 41:  78%|▊| 39/50 [00:01<00:00, 32.20it/s, train_loss=0.122, val_loss=0.01\u001b[A\n",
      "Epoch 41:  78%|▊| 39/50 [00:01<00:00, 32.20it/s, train_loss=0.0781, val_loss=0.0\u001b[A\n",
      "Epoch 42:  78%|▊| 39/50 [00:01<00:00, 32.20it/s, train_loss=0.0781, val_loss=0.0\u001b[A\n",
      "Epoch 42:  78%|▊| 39/50 [00:01<00:00, 32.20it/s, train_loss=0.0691, val_loss=0.0\u001b[A\n",
      "Epoch 42:  86%|▊| 43/50 [00:01<00:00, 32.22it/s, train_loss=0.0691, val_loss=0.0\u001b[A\n",
      "Epoch 43:  86%|▊| 43/50 [00:01<00:00, 32.22it/s, train_loss=0.0691, val_loss=0.0\u001b[A\n",
      "Epoch 43:  86%|▊| 43/50 [00:01<00:00, 32.22it/s, train_loss=0.104, val_loss=0.01\u001b[A\n",
      "Epoch 44:  86%|▊| 43/50 [00:01<00:00, 32.22it/s, train_loss=0.104, val_loss=0.01\u001b[A\n",
      "Epoch 44:  86%|▊| 43/50 [00:01<00:00, 32.22it/s, train_loss=0.0717, val_loss=0.0\u001b[A\n",
      "Epoch 45:  86%|▊| 43/50 [00:01<00:00, 32.22it/s, train_loss=0.0717, val_loss=0.0\u001b[A\n",
      "Epoch 45:  86%|▊| 43/50 [00:01<00:00, 32.22it/s, train_loss=0.0646, val_loss=0.0\u001b[A\n",
      "Epoch 46:  86%|▊| 43/50 [00:01<00:00, 32.22it/s, train_loss=0.0646, val_loss=0.0\u001b[A\n",
      "Epoch 46:  86%|▊| 43/50 [00:01<00:00, 32.22it/s, train_loss=0.0857, val_loss=0.0\u001b[A\n",
      "Epoch 46:  94%|▉| 47/50 [00:01<00:00, 31.80it/s, train_loss=0.0857, val_loss=0.0\u001b[A\n",
      "Epoch 47:  94%|▉| 47/50 [00:01<00:00, 31.80it/s, train_loss=0.0857, val_loss=0.0\u001b[A\n",
      "Epoch 47:  94%|▉| 47/50 [00:01<00:00, 31.80it/s, train_loss=0.0577, val_loss=0.0\u001b[A\n",
      "Epoch 48:  94%|▉| 47/50 [00:01<00:00, 31.80it/s, train_loss=0.0577, val_loss=0.0\u001b[A\n",
      "Epoch 48:  94%|▉| 47/50 [00:01<00:00, 31.80it/s, train_loss=0.0569, val_loss=0.0\u001b[A\n",
      "Epoch 49:  94%|▉| 47/50 [00:01<00:00, 31.80it/s, train_loss=0.0569, val_loss=0.0\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 31.83it/s, train_loss=0.0731, val_loss=0.0\u001b[A\n",
      " 66%|███████████████████████████▋              | 66/100 [01:40<00:51,  1.52s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=34.6, val_loss=14.1, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=34.6, val_loss=14.1, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=77.3, val_loss=0.177, train_ci\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=77.3, val_loss=0.177, train_ci\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=0.857, val_loss=2.04, train_ci\u001b[A\n",
      "Epoch 2:   6%| | 3/50 [00:00<00:01, 29.15it/s, train_loss=0.857, val_loss=2.04, \u001b[A\n",
      "Epoch 3:   6%| | 3/50 [00:00<00:01, 29.15it/s, train_loss=0.857, val_loss=2.04, \u001b[A\n",
      "Epoch 3:   6%| | 3/50 [00:00<00:01, 29.15it/s, train_loss=10.1, val_loss=0.602, \u001b[A\n",
      "Epoch 4:   6%| | 3/50 [00:00<00:01, 29.15it/s, train_loss=10.1, val_loss=0.602, \u001b[A\n",
      "Epoch 4:   6%| | 3/50 [00:00<00:01, 29.15it/s, train_loss=2.95, val_loss=0.131, \u001b[A\n",
      "Epoch 5:   6%| | 3/50 [00:00<00:01, 29.15it/s, train_loss=2.95, val_loss=0.131, \u001b[A\n",
      "Epoch 5:   6%| | 3/50 [00:00<00:01, 29.15it/s, train_loss=0.93, val_loss=0.413, \u001b[A\n",
      "Epoch 6:   6%| | 3/50 [00:00<00:01, 29.15it/s, train_loss=0.93, val_loss=0.413, \u001b[A\n",
      "Epoch 6:   6%| | 3/50 [00:00<00:01, 29.15it/s, train_loss=2.51, val_loss=0.294, \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.24596117720726363\n",
      "Its' val AUC : 0.2732091885279326\n",
      "Its external  AUC: 0.37592563415786984\n",
      "Curent best Test AUC: 0.6725109580463369\n",
      "Its' val AUC : 0.7059942911512845\n",
      "Its external  AUC: 0.5591618087285332\n",
      "Curent best Test AUC: 0.7045710707576707\n",
      "Its' val AUC : 0.7138779393774637\n",
      "Its external  AUC: 0.5930360800378132\n",
      "Curent best Test AUC: 0.720100187852223\n",
      "Its' val AUC : 0.7235286122060622\n",
      "Its external  AUC: 0.5716086340003151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6:  14%|▏| 7/50 [00:00<00:01, 30.32it/s, train_loss=2.51, val_loss=0.294, \u001b[A\n",
      "Epoch 7:  14%|▏| 7/50 [00:00<00:01, 30.32it/s, train_loss=2.51, val_loss=0.294, \u001b[A\n",
      "Epoch 7:  14%|▏| 7/50 [00:00<00:01, 30.32it/s, train_loss=1.71, val_loss=0.0425,\u001b[A\n",
      "Epoch 8:  14%|▏| 7/50 [00:00<00:01, 30.32it/s, train_loss=1.71, val_loss=0.0425,\u001b[A\n",
      "Epoch 8:  14%|▏| 7/50 [00:00<00:01, 30.32it/s, train_loss=0.297, val_loss=0.138,\u001b[A\n",
      "Epoch 9:  14%|▏| 7/50 [00:00<00:01, 30.32it/s, train_loss=0.297, val_loss=0.138,\u001b[A\n",
      "Epoch 9:  14%|▏| 7/50 [00:00<00:01, 30.32it/s, train_loss=0.803, val_loss=0.215,\u001b[A\n",
      "Epoch 10:  14%|▏| 7/50 [00:00<00:01, 30.32it/s, train_loss=0.803, val_loss=0.215\u001b[A\n",
      "Epoch 10:  14%|▏| 7/50 [00:00<00:01, 30.32it/s, train_loss=1.27, val_loss=0.0691\u001b[A\n",
      "Epoch 10:  22%|▏| 11/50 [00:00<00:01, 31.68it/s, train_loss=1.27, val_loss=0.069\u001b[A\n",
      "Epoch 11:  22%|▏| 11/50 [00:00<00:01, 31.68it/s, train_loss=1.27, val_loss=0.069\u001b[A\n",
      "Epoch 11:  22%|▏| 11/50 [00:00<00:01, 31.68it/s, train_loss=0.464, val_loss=0.06\u001b[A\n",
      "Epoch 12:  22%|▏| 11/50 [00:00<00:01, 31.68it/s, train_loss=0.464, val_loss=0.06\u001b[A\n",
      "Epoch 12:  22%|▏| 11/50 [00:00<00:01, 31.68it/s, train_loss=0.33, val_loss=0.043\u001b[A\n",
      "Epoch 13:  22%|▏| 11/50 [00:00<00:01, 31.68it/s, train_loss=0.33, val_loss=0.043\u001b[A\n",
      "Epoch 13:  22%|▏| 11/50 [00:00<00:01, 31.68it/s, train_loss=0.239, val_loss=0.06\u001b[A\n",
      "Epoch 14:  22%|▏| 11/50 [00:00<00:01, 31.68it/s, train_loss=0.239, val_loss=0.06\u001b[A\n",
      "Epoch 14:  22%|▏| 11/50 [00:00<00:01, 31.68it/s, train_loss=0.327, val_loss=0.04\u001b[A\n",
      "Epoch 14:  30%|▎| 15/50 [00:00<00:01, 32.61it/s, train_loss=0.327, val_loss=0.04\u001b[A\n",
      "Epoch 15:  30%|▎| 15/50 [00:00<00:01, 32.61it/s, train_loss=0.327, val_loss=0.04\u001b[A\n",
      "Epoch 15:  30%|▎| 15/50 [00:00<00:01, 32.61it/s, train_loss=0.236, val_loss=0.03\u001b[A\n",
      "Epoch 16:  30%|▎| 15/50 [00:00<00:01, 32.61it/s, train_loss=0.236, val_loss=0.03\u001b[A\n",
      "Epoch 16:  30%|▎| 15/50 [00:00<00:01, 32.61it/s, train_loss=0.179, val_loss=0.02\u001b[A\n",
      "Epoch 17:  30%|▎| 15/50 [00:00<00:01, 32.61it/s, train_loss=0.179, val_loss=0.02\u001b[A\n",
      "Epoch 17:  30%|▎| 15/50 [00:00<00:01, 32.61it/s, train_loss=0.168, val_loss=0.02\u001b[A\n",
      "Epoch 18:  30%|▎| 15/50 [00:00<00:01, 32.61it/s, train_loss=0.168, val_loss=0.02\u001b[A\n",
      "Epoch 18:  30%|▎| 15/50 [00:00<00:01, 32.61it/s, train_loss=0.173, val_loss=0.02\u001b[A\n",
      "Epoch 18:  38%|▍| 19/50 [00:00<00:00, 33.15it/s, train_loss=0.173, val_loss=0.02\u001b[A\n",
      "Epoch 19:  38%|▍| 19/50 [00:00<00:00, 33.15it/s, train_loss=0.173, val_loss=0.02\u001b[A\n",
      "Epoch 19:  38%|▍| 19/50 [00:00<00:00, 33.15it/s, train_loss=0.168, val_loss=0.01\u001b[A\n",
      "Epoch 20:  38%|▍| 19/50 [00:00<00:00, 33.15it/s, train_loss=0.168, val_loss=0.01\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    14: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20:  38%|▍| 19/50 [00:00<00:00, 33.15it/s, train_loss=0.13, val_loss=0.014\u001b[A\n",
      "Epoch 21:  38%|▍| 19/50 [00:00<00:00, 33.15it/s, train_loss=0.13, val_loss=0.014\u001b[A\n",
      "Epoch 21:  38%|▍| 19/50 [00:00<00:00, 33.15it/s, train_loss=0.0981, val_loss=0.0\u001b[A\n",
      "Epoch 22:  38%|▍| 19/50 [00:00<00:00, 33.15it/s, train_loss=0.0981, val_loss=0.0\u001b[A\n",
      "Epoch 22:  38%|▍| 19/50 [00:00<00:00, 33.15it/s, train_loss=0.0995, val_loss=0.0\u001b[A\n",
      "Epoch 22:  46%|▍| 23/50 [00:00<00:00, 33.58it/s, train_loss=0.0995, val_loss=0.0\u001b[A\n",
      "Epoch 23:  46%|▍| 23/50 [00:00<00:00, 33.58it/s, train_loss=0.0995, val_loss=0.0\u001b[A\n",
      "Epoch 23:  46%|▍| 23/50 [00:00<00:00, 33.58it/s, train_loss=0.111, val_loss=0.01\u001b[A\n",
      "Epoch 24:  46%|▍| 23/50 [00:00<00:00, 33.58it/s, train_loss=0.111, val_loss=0.01\u001b[A\n",
      "Epoch 24:  46%|▍| 23/50 [00:00<00:00, 33.58it/s, train_loss=0.0978, val_loss=0.0\u001b[A\n",
      "Epoch 25:  46%|▍| 23/50 [00:00<00:00, 33.58it/s, train_loss=0.0978, val_loss=0.0\u001b[A\n",
      "Epoch 25:  46%|▍| 23/50 [00:00<00:00, 33.58it/s, train_loss=0.0699, val_loss=0.0\u001b[A\n",
      "Epoch 26:  46%|▍| 23/50 [00:00<00:00, 33.58it/s, train_loss=0.0699, val_loss=0.0\u001b[A\n",
      "Epoch 26:  46%|▍| 23/50 [00:00<00:00, 33.58it/s, train_loss=0.0599, val_loss=0.0\u001b[A\n",
      "Epoch 26:  54%|▌| 27/50 [00:00<00:00, 33.86it/s, train_loss=0.0599, val_loss=0.0\u001b[A\n",
      "Epoch 27:  54%|▌| 27/50 [00:00<00:00, 33.86it/s, train_loss=0.0599, val_loss=0.0\u001b[A\n",
      "Epoch 27:  54%|▌| 27/50 [00:00<00:00, 33.86it/s, train_loss=0.0692, val_loss=0.0\u001b[A\n",
      "Epoch 28:  54%|▌| 27/50 [00:00<00:00, 33.86it/s, train_loss=0.0692, val_loss=0.0\u001b[A\n",
      "Epoch 28:  54%|▌| 27/50 [00:00<00:00, 33.86it/s, train_loss=0.0702, val_loss=0.0\u001b[A\n",
      "Epoch 29:  54%|▌| 27/50 [00:00<00:00, 33.86it/s, train_loss=0.0702, val_loss=0.0\u001b[A\n",
      "Epoch 29:  54%|▌| 27/50 [00:00<00:00, 33.86it/s, train_loss=0.0538, val_loss=0.0\u001b[A\n",
      "Epoch 30:  54%|▌| 27/50 [00:00<00:00, 33.86it/s, train_loss=0.0538, val_loss=0.0\u001b[A\n",
      "Epoch 30:  54%|▌| 27/50 [00:00<00:00, 33.86it/s, train_loss=0.0429, val_loss=0.0\u001b[A\n",
      "Epoch 30:  62%|▌| 31/50 [00:00<00:00, 34.03it/s, train_loss=0.0429, val_loss=0.0\u001b[A\n",
      "Epoch 31:  62%|▌| 31/50 [00:00<00:00, 34.03it/s, train_loss=0.0429, val_loss=0.0\u001b[A\n",
      "Epoch 31:  62%|▌| 31/50 [00:00<00:00, 34.03it/s, train_loss=0.0496, val_loss=0.0\u001b[A\n",
      "Epoch 32:  62%|▌| 31/50 [00:00<00:00, 34.03it/s, train_loss=0.0496, val_loss=0.0\u001b[A\n",
      "Epoch 32:  62%|▌| 31/50 [00:00<00:00, 34.03it/s, train_loss=0.0545, val_loss=0.0\u001b[A\n",
      "Epoch 33:  62%|▌| 31/50 [00:00<00:00, 34.03it/s, train_loss=0.0545, val_loss=0.0\u001b[A\n",
      "Epoch 33:  62%|▌| 31/50 [00:01<00:00, 34.03it/s, train_loss=0.0452, val_loss=0.0\u001b[A\n",
      "Epoch 34:  62%|▌| 31/50 [00:01<00:00, 34.03it/s, train_loss=0.0452, val_loss=0.0\u001b[A\n",
      "Epoch 34:  62%|▌| 31/50 [00:01<00:00, 34.03it/s, train_loss=0.0376, val_loss=0.0\u001b[A\n",
      "Epoch 34:  70%|▋| 35/50 [00:01<00:00, 34.07it/s, train_loss=0.0376, val_loss=0.0\u001b[A\n",
      "Epoch 35:  70%|▋| 35/50 [00:01<00:00, 34.07it/s, train_loss=0.0376, val_loss=0.0\u001b[A\n",
      "Epoch 35:  70%|▋| 35/50 [00:01<00:00, 34.07it/s, train_loss=0.0426, val_loss=0.0\u001b[A\n",
      "Epoch 36:  70%|▋| 35/50 [00:01<00:00, 34.07it/s, train_loss=0.0426, val_loss=0.0\u001b[A\n",
      "Epoch 36:  70%|▋| 35/50 [00:01<00:00, 34.07it/s, train_loss=0.0456, val_loss=0.0\u001b[A\n",
      "Epoch 37:  70%|▋| 35/50 [00:01<00:00, 34.07it/s, train_loss=0.0456, val_loss=0.0\u001b[A\n",
      "Epoch 37:  70%|▋| 35/50 [00:01<00:00, 34.07it/s, train_loss=0.0382, val_loss=0.0\u001b[A\n",
      "Epoch 38:  70%|▋| 35/50 [00:01<00:00, 34.07it/s, train_loss=0.0382, val_loss=0.0\u001b[A\n",
      "Epoch 38:  70%|▋| 35/50 [00:01<00:00, 34.07it/s, train_loss=0.0335, val_loss=0.0\u001b[A\n",
      "Epoch 38:  78%|▊| 39/50 [00:01<00:00, 33.49it/s, train_loss=0.0335, val_loss=0.0\u001b[A\n",
      "Epoch 39:  78%|▊| 39/50 [00:01<00:00, 33.49it/s, train_loss=0.0335, val_loss=0.0\u001b[A\n",
      "Epoch 39:  78%|▊| 39/50 [00:01<00:00, 33.49it/s, train_loss=0.0371, val_loss=0.0\u001b[A\n",
      "Epoch 40:  78%|▊| 39/50 [00:01<00:00, 33.49it/s, train_loss=0.0371, val_loss=0.0\u001b[A\n",
      "Epoch 40:  78%|▊| 39/50 [00:01<00:00, 33.49it/s, train_loss=0.0375, val_loss=0.0\u001b[A\n",
      "Epoch 41:  78%|▊| 39/50 [00:01<00:00, 33.49it/s, train_loss=0.0375, val_loss=0.0\u001b[A\n",
      "Epoch 41:  78%|▊| 39/50 [00:01<00:00, 33.49it/s, train_loss=0.0326, val_loss=0.0\u001b[A\n",
      "Epoch 42:  78%|▊| 39/50 [00:01<00:00, 33.49it/s, train_loss=0.0326, val_loss=0.0\u001b[A\n",
      "Epoch 42:  78%|▊| 39/50 [00:01<00:00, 33.49it/s, train_loss=0.031, val_loss=0.00\u001b[A\n",
      "Epoch 42:  86%|▊| 43/50 [00:01<00:00, 33.14it/s, train_loss=0.031, val_loss=0.00\u001b[A\n",
      "Epoch 43:  86%|▊| 43/50 [00:01<00:00, 33.14it/s, train_loss=0.031, val_loss=0.00\u001b[A\n",
      "Epoch 43:  86%|▊| 43/50 [00:01<00:00, 33.14it/s, train_loss=0.0334, val_loss=0.0\u001b[A\n",
      "Epoch 44:  86%|▊| 43/50 [00:01<00:00, 33.14it/s, train_loss=0.0334, val_loss=0.0\u001b[A\n",
      "Epoch 44:  86%|▊| 43/50 [00:01<00:00, 33.14it/s, train_loss=0.0326, val_loss=0.0\u001b[A\n",
      "Epoch 45:  86%|▊| 43/50 [00:01<00:00, 33.14it/s, train_loss=0.0326, val_loss=0.0\u001b[A\n",
      "Epoch 45:  86%|▊| 43/50 [00:01<00:00, 33.14it/s, train_loss=0.0288, val_loss=0.0\u001b[A\n",
      "Epoch 46:  86%|▊| 43/50 [00:01<00:00, 33.14it/s, train_loss=0.0288, val_loss=0.0\u001b[A\n",
      "Epoch 46:  86%|▊| 43/50 [00:01<00:00, 33.14it/s, train_loss=0.0282, val_loss=0.0\u001b[A\n",
      "Epoch 46:  94%|▉| 47/50 [00:01<00:00, 32.83it/s, train_loss=0.0282, val_loss=0.0\u001b[A\n",
      "Epoch 47:  94%|▉| 47/50 [00:01<00:00, 32.83it/s, train_loss=0.0282, val_loss=0.0\u001b[A\n",
      "Epoch 47:  94%|▉| 47/50 [00:01<00:00, 32.83it/s, train_loss=0.0293, val_loss=0.0\u001b[A\n",
      "Epoch 48:  94%|▉| 47/50 [00:01<00:00, 32.83it/s, train_loss=0.0293, val_loss=0.0\u001b[A\n",
      "Epoch 48:  94%|▉| 47/50 [00:01<00:00, 32.83it/s, train_loss=0.028, val_loss=0.00\u001b[A\n",
      "Epoch 49:  94%|▉| 47/50 [00:01<00:00, 32.83it/s, train_loss=0.028, val_loss=0.00\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 32.93it/s, train_loss=0.0255, val_loss=0.0\u001b[A\n",
      " 67%|████████████████████████████▏             | 67/100 [01:41<00:50,  1.53s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=20.5, val_loss=13.5, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=20.5, val_loss=13.5, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=69.7, val_loss=34.5, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=69.7, val_loss=34.5, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=184, val_loss=22.5, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=184, val_loss=22.5, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=116, val_loss=20.4, train_cind\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 31.96it/s, train_loss=116, val_loss=20.4, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.96it/s, train_loss=116, val_loss=20.4, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.96it/s, train_loss=108, val_loss=2.17, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.96it/s, train_loss=108, val_loss=2.17, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.96it/s, train_loss=15, val_loss=11.2, tra\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.96it/s, train_loss=15, val_loss=11.2, tra\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.96it/s, train_loss=61, val_loss=8.55, tra\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.96it/s, train_loss=61, val_loss=8.55, tra\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7060738885410144\n",
      "Its' val AUC : 0.7182275383988038\n",
      "Its external  AUC: 0.6201354970852371\n",
      "Curent best Test AUC: 0.7457733249843457\n",
      "Its' val AUC : 0.6981106429251054\n",
      "Its external  AUC: 0.6127304238222783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.96it/s, train_loss=51.1, val_loss=3.56, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 32.08it/s, train_loss=51.1, val_loss=3.56, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.08it/s, train_loss=51.1, val_loss=3.56, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.08it/s, train_loss=22.7, val_loss=1.02, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.08it/s, train_loss=22.7, val_loss=1.02, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.08it/s, train_loss=6.69, val_loss=3.26, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.08it/s, train_loss=6.69, val_loss=3.26, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.08it/s, train_loss=15.4, val_loss=4.99, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.08it/s, train_loss=15.4, val_loss=4.99, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.08it/s, train_loss=24.1, val_loss=3.36, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 32.17it/s, train_loss=24.1, val_loss=3.36,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.17it/s, train_loss=24.1, val_loss=3.36,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.17it/s, train_loss=16.3, val_loss=1.12,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.17it/s, train_loss=16.3, val_loss=1.12,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.17it/s, train_loss=5.38, val_loss=0.090\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.17it/s, train_loss=5.38, val_loss=0.090\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.17it/s, train_loss=0.635, val_loss=0.84\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.17it/s, train_loss=0.635, val_loss=0.84\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.17it/s, train_loss=4.67, val_loss=1.52,\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 32.09it/s, train_loss=4.67, val_loss=1.52,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.09it/s, train_loss=4.67, val_loss=1.52,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.09it/s, train_loss=7.94, val_loss=1.14,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.09it/s, train_loss=7.94, val_loss=1.14,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.09it/s, train_loss=5.97, val_loss=0.324\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.09it/s, train_loss=5.97, val_loss=0.324\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.09it/s, train_loss=2.04, val_loss=0.326\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.09it/s, train_loss=2.04, val_loss=0.326\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.09it/s, train_loss=2.26, val_loss=0.716\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 32.06it/s, train_loss=2.26, val_loss=0.716\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.06it/s, train_loss=2.26, val_loss=0.716\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.06it/s, train_loss=4.06, val_loss=0.669\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.06it/s, train_loss=4.06, val_loss=0.669\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7683155917345021\n",
      "Its' val AUC : 0.7070816909066195\n",
      "Its external  AUC: 0.606113124310698\n",
      "Epoch    20: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.06it/s, train_loss=3.66, val_loss=0.381\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.06it/s, train_loss=3.66, val_loss=0.381\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.06it/s, train_loss=2.03, val_loss=0.099\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.06it/s, train_loss=2.03, val_loss=0.099\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.06it/s, train_loss=0.521, val_loss=0.03\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 32.16it/s, train_loss=0.521, val_loss=0.03\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 32.16it/s, train_loss=0.521, val_loss=0.03\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 32.16it/s, train_loss=0.246, val_loss=0.19\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 32.16it/s, train_loss=0.246, val_loss=0.19\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 32.16it/s, train_loss=1.16, val_loss=0.372\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 32.16it/s, train_loss=1.16, val_loss=0.372\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 32.16it/s, train_loss=2.15, val_loss=0.378\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 32.16it/s, train_loss=2.15, val_loss=0.378\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 32.16it/s, train_loss=2.21, val_loss=0.236\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 32.13it/s, train_loss=2.21, val_loss=0.236\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 32.13it/s, train_loss=2.21, val_loss=0.236\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 32.13it/s, train_loss=1.42, val_loss=0.105\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 32.13it/s, train_loss=1.42, val_loss=0.105\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 32.13it/s, train_loss=0.626, val_loss=0.10\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 32.13it/s, train_loss=0.626, val_loss=0.10\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 32.13it/s, train_loss=0.49, val_loss=0.142\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 32.13it/s, train_loss=0.49, val_loss=0.142\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 32.13it/s, train_loss=0.635, val_loss=0.17\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 32.14it/s, train_loss=0.635, val_loss=0.17\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 32.14it/s, train_loss=0.635, val_loss=0.17\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:01<00:00, 32.14it/s, train_loss=0.765, val_loss=0.17\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 32.14it/s, train_loss=0.765, val_loss=0.17\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 32.14it/s, train_loss=0.783, val_loss=0.14\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 32.14it/s, train_loss=0.783, val_loss=0.14\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 32.14it/s, train_loss=0.659, val_loss=0.1,\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 32.14it/s, train_loss=0.659, val_loss=0.1,\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 32.14it/s, train_loss=0.446, val_loss=0.05\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 32.19it/s, train_loss=0.446, val_loss=0.05\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 32.19it/s, train_loss=0.446, val_loss=0.05\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    30: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch    36: reducing learning rate of group 0 to 1.2500e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 32.19it/s, train_loss=0.236, val_loss=0.03\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 32.19it/s, train_loss=0.236, val_loss=0.03\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 32.19it/s, train_loss=0.158, val_loss=0.02\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 32.19it/s, train_loss=0.158, val_loss=0.02\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 32.19it/s, train_loss=0.11, val_loss=0.016\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 32.19it/s, train_loss=0.11, val_loss=0.016\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 32.19it/s, train_loss=0.0925, val_loss=0.0\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 32.18it/s, train_loss=0.0925, val_loss=0.0\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 32.18it/s, train_loss=0.0925, val_loss=0.0\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 32.18it/s, train_loss=0.104, val_loss=0.02\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 32.18it/s, train_loss=0.104, val_loss=0.02\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 32.18it/s, train_loss=0.132, val_loss=0.02\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 32.18it/s, train_loss=0.132, val_loss=0.02\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 32.18it/s, train_loss=0.163, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 32.18it/s, train_loss=0.163, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 32.18it/s, train_loss=0.187, val_loss=0.03\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 31.49it/s, train_loss=0.187, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 31.49it/s, train_loss=0.187, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 31.49it/s, train_loss=0.196, val_loss=0.03\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 31.49it/s, train_loss=0.196, val_loss=0.03\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 31.49it/s, train_loss=0.189, val_loss=0.02\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 31.49it/s, train_loss=0.189, val_loss=0.02\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 31.49it/s, train_loss=0.173, val_loss=0.02\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 31.49it/s, train_loss=0.173, val_loss=0.02\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 31.49it/s, train_loss=0.152, val_loss=0.01\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 31.54it/s, train_loss=0.152, val_loss=0.01\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 31.54it/s, train_loss=0.152, val_loss=0.01\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 31.54it/s, train_loss=0.13, val_loss=0.015\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 31.54it/s, train_loss=0.13, val_loss=0.015\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 31.90it/s, train_loss=0.112, val_loss=0.01\u001b[A\n",
      " 68%|████████████████████████████▌             | 68/100 [01:43<00:49,  1.54s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=19.3, val_loss=67.8, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=19.3, val_loss=67.8, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=366, val_loss=9.92, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=366, val_loss=9.92, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=50.9, val_loss=1.07, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=50.9, val_loss=1.07, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=6.94, val_loss=1.47, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 30.16it/s, train_loss=6.94, val_loss=1.47, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 30.16it/s, train_loss=6.94, val_loss=1.47, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 30.16it/s, train_loss=7.93, val_loss=3.11, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 30.16it/s, train_loss=7.93, val_loss=3.11, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 30.16it/s, train_loss=16.4, val_loss=0.0569,\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 30.16it/s, train_loss=16.4, val_loss=0.0569,\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 30.16it/s, train_loss=0.397, val_loss=0.498,\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 30.16it/s, train_loss=0.397, val_loss=0.498,\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.29718221665623046\n",
      "Its' val AUC : 0.29359793394046485\n",
      "Its external  AUC: 0.38711202142744605\n",
      "Curent best Test AUC: 0.3564182842830307\n",
      "Its' val AUC : 0.33423949979611256\n",
      "Its external  AUC: 0.3981408539467465\n",
      "Curent best Test AUC: 0.4722604884157796\n",
      "Its' val AUC : 0.4191926056816637\n",
      "Its external  AUC: 0.461635418307862\n",
      "Curent best Test AUC: 0.7470256731371321\n",
      "Its' val AUC : 0.6978387929862716\n",
      "Its external  AUC: 0.6029620293051835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 30.16it/s, train_loss=2.58, val_loss=0.745, \u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 31.28it/s, train_loss=2.58, val_loss=0.745, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 31.28it/s, train_loss=2.58, val_loss=0.745, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 31.28it/s, train_loss=3.75, val_loss=0.223, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 31.28it/s, train_loss=3.75, val_loss=0.223, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 31.28it/s, train_loss=1.29, val_loss=0.275, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 31.28it/s, train_loss=1.29, val_loss=0.275,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 31.28it/s, train_loss=1.66, val_loss=0.308,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 31.28it/s, train_loss=1.66, val_loss=0.308,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 31.28it/s, train_loss=1.71, val_loss=0.0781\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 31.64it/s, train_loss=1.71, val_loss=0.078\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 31.64it/s, train_loss=1.71, val_loss=0.078\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 31.64it/s, train_loss=0.449, val_loss=0.09\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 31.64it/s, train_loss=0.449, val_loss=0.09\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 31.64it/s, train_loss=0.588, val_loss=0.09\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 31.64it/s, train_loss=0.588, val_loss=0.09\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 31.64it/s, train_loss=0.625, val_loss=0.07\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 31.64it/s, train_loss=0.625, val_loss=0.07\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 31.64it/s, train_loss=0.48, val_loss=0.079\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 31.88it/s, train_loss=0.48, val_loss=0.079\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 31.88it/s, train_loss=0.48, val_loss=0.079\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 31.88it/s, train_loss=0.496, val_loss=0.10\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 31.88it/s, train_loss=0.496, val_loss=0.10\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 31.88it/s, train_loss=0.576, val_loss=0.07\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 31.88it/s, train_loss=0.576, val_loss=0.07\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    12: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch    18: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 31.88it/s, train_loss=0.442, val_loss=0.05\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 31.88it/s, train_loss=0.442, val_loss=0.05\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 31.88it/s, train_loss=0.32, val_loss=0.036\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 31.93it/s, train_loss=0.32, val_loss=0.036\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 31.93it/s, train_loss=0.32, val_loss=0.036\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 31.93it/s, train_loss=0.229, val_loss=0.03\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 31.93it/s, train_loss=0.229, val_loss=0.03\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 31.93it/s, train_loss=0.21, val_loss=0.036\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 31.93it/s, train_loss=0.21, val_loss=0.036\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 31.93it/s, train_loss=0.222, val_loss=0.03\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 31.93it/s, train_loss=0.222, val_loss=0.03\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 31.93it/s, train_loss=0.201, val_loss=0.02\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 31.96it/s, train_loss=0.201, val_loss=0.02\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 31.96it/s, train_loss=0.201, val_loss=0.02\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 31.96it/s, train_loss=0.147, val_loss=0.01\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 31.96it/s, train_loss=0.147, val_loss=0.01\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 31.96it/s, train_loss=0.114, val_loss=0.01\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 31.96it/s, train_loss=0.114, val_loss=0.01\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 31.96it/s, train_loss=0.134, val_loss=0.02\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 31.96it/s, train_loss=0.134, val_loss=0.02\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 31.96it/s, train_loss=0.179, val_loss=0.02\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 32.04it/s, train_loss=0.179, val_loss=0.02\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 32.04it/s, train_loss=0.179, val_loss=0.02\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 32.04it/s, train_loss=0.198, val_loss=0.02\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 32.04it/s, train_loss=0.198, val_loss=0.02\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 32.04it/s, train_loss=0.18, val_loss=0.020\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 32.04it/s, train_loss=0.18, val_loss=0.020\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 32.04it/s, train_loss=0.155, val_loss=0.02\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 32.04it/s, train_loss=0.155, val_loss=0.02\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:01<00:00, 32.04it/s, train_loss=0.148, val_loss=0.02\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:01<00:00, 31.98it/s, train_loss=0.148, val_loss=0.02\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:01<00:00, 31.98it/s, train_loss=0.148, val_loss=0.02\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:01<00:00, 31.98it/s, train_loss=0.15, val_loss=0.021\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 31.98it/s, train_loss=0.15, val_loss=0.021\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 31.98it/s, train_loss=0.149, val_loss=0.02\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 31.98it/s, train_loss=0.149, val_loss=0.02\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 31.98it/s, train_loss=0.142, val_loss=0.01\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 31.98it/s, train_loss=0.142, val_loss=0.01\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 31.98it/s, train_loss=0.13, val_loss=0.016\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 32.11it/s, train_loss=0.13, val_loss=0.016\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 32.11it/s, train_loss=0.13, val_loss=0.016\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 32.11it/s, train_loss=0.117, val_loss=0.01\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 32.11it/s, train_loss=0.117, val_loss=0.01\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    31: reducing learning rate of group 0 to 1.2500e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 32.11it/s, train_loss=0.107, val_loss=0.01\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 32.11it/s, train_loss=0.107, val_loss=0.01\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 32.11it/s, train_loss=0.103, val_loss=0.01\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 32.11it/s, train_loss=0.103, val_loss=0.01\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 32.11it/s, train_loss=0.103, val_loss=0.01\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 31.96it/s, train_loss=0.103, val_loss=0.01\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 31.96it/s, train_loss=0.103, val_loss=0.01\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 31.96it/s, train_loss=0.104, val_loss=0.01\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 31.96it/s, train_loss=0.104, val_loss=0.01\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 31.96it/s, train_loss=0.103, val_loss=0.01\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 31.96it/s, train_loss=0.103, val_loss=0.01\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 31.96it/s, train_loss=0.101, val_loss=0.01\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 31.96it/s, train_loss=0.101, val_loss=0.01\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 31.96it/s, train_loss=0.0969, val_loss=0.0\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 31.63it/s, train_loss=0.0969, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 31.63it/s, train_loss=0.0969, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 31.63it/s, train_loss=0.0938, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 31.63it/s, train_loss=0.0938, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 31.63it/s, train_loss=0.0927, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 31.63it/s, train_loss=0.0927, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 31.63it/s, train_loss=0.0923, val_loss=0.0\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 31.63it/s, train_loss=0.0923, val_loss=0.0\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 31.63it/s, train_loss=0.092, val_loss=0.01\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 31.55it/s, train_loss=0.092, val_loss=0.01\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 31.55it/s, train_loss=0.092, val_loss=0.01\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 31.55it/s, train_loss=0.0908, val_loss=0.0\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 31.55it/s, train_loss=0.0908, val_loss=0.0\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 31.74it/s, train_loss=0.0886, val_loss=0.0\u001b[A\n",
      " 69%|████████████████████████████▉             | 69/100 [01:45<00:48,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=46.2, val_loss=78.1, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=46.2, val_loss=78.1, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=397, val_loss=0.317, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=397, val_loss=0.317, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=2.01, val_loss=46.1, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=2.01, val_loss=46.1, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=244, val_loss=1.7, train_cinde\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 31.06it/s, train_loss=244, val_loss=1.7, tra\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.06it/s, train_loss=244, val_loss=1.7, tra\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.06it/s, train_loss=9.75, val_loss=1.21, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.06it/s, train_loss=9.75, val_loss=1.21, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.06it/s, train_loss=7.92, val_loss=2.65, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.06it/s, train_loss=7.92, val_loss=2.65, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.06it/s, train_loss=16.1, val_loss=2.08, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.06it/s, train_loss=16.1, val_loss=2.08, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7045710707576707\n",
      "Its' val AUC : 0.7069457659372027\n",
      "Its external  AUC: 0.6116275405703482\n",
      "Curent best Test AUC: 0.7281152160300564\n",
      "Its' val AUC : 0.5784966698382493\n",
      "Its external  AUC: 0.5671971009925949\n",
      "Curent best Test AUC: 0.7433938634940513\n",
      "Its' val AUC : 0.6565176022835395\n",
      "Its external  AUC: 0.6213959350874428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.06it/s, train_loss=13.1, val_loss=0.931, \u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 31.38it/s, train_loss=13.1, val_loss=0.931, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 31.38it/s, train_loss=13.1, val_loss=0.931, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 31.38it/s, train_loss=4.95, val_loss=1.02, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 31.38it/s, train_loss=4.95, val_loss=1.02, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 31.38it/s, train_loss=5.09, val_loss=0.543, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 31.38it/s, train_loss=5.09, val_loss=0.543,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 31.38it/s, train_loss=3.29, val_loss=0.467,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 31.38it/s, train_loss=3.29, val_loss=0.467,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 31.38it/s, train_loss=3.05, val_loss=0.287,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 31.80it/s, train_loss=3.05, val_loss=0.287\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 31.80it/s, train_loss=3.05, val_loss=0.287\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 31.80it/s, train_loss=1.67, val_loss=0.404\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 31.80it/s, train_loss=1.67, val_loss=0.404\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 31.80it/s, train_loss=2.16, val_loss=0.167\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 31.80it/s, train_loss=2.16, val_loss=0.167\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 31.80it/s, train_loss=0.982, val_loss=0.25\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 31.80it/s, train_loss=0.982, val_loss=0.25\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 31.80it/s, train_loss=1.4, val_loss=0.161,\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 31.92it/s, train_loss=1.4, val_loss=0.161,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 31.92it/s, train_loss=1.4, val_loss=0.161,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 31.92it/s, train_loss=0.917, val_loss=0.14\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 31.92it/s, train_loss=0.917, val_loss=0.14\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 31.92it/s, train_loss=0.977, val_loss=0.08\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 31.92it/s, train_loss=0.977, val_loss=0.08\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 31.92it/s, train_loss=0.703, val_loss=0.12\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 31.92it/s, train_loss=0.703, val_loss=0.12\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 31.92it/s, train_loss=0.693, val_loss=0.13\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 32.03it/s, train_loss=0.693, val_loss=0.13\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.03it/s, train_loss=0.693, val_loss=0.13\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.03it/s, train_loss=0.716, val_loss=0.07\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.03it/s, train_loss=0.716, val_loss=0.07\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.03it/s, train_loss=0.526, val_loss=0.11\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.03it/s, train_loss=0.526, val_loss=0.11\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.03it/s, train_loss=0.696, val_loss=0.09\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.03it/s, train_loss=0.696, val_loss=0.09\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.03it/s, train_loss=0.533, val_loss=0.14\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 32.12it/s, train_loss=0.533, val_loss=0.14\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 32.12it/s, train_loss=0.533, val_loss=0.14\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 32.12it/s, train_loss=0.701, val_loss=0.11\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 32.12it/s, train_loss=0.701, val_loss=0.11\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 32.12it/s, train_loss=0.635, val_loss=0.13\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 32.12it/s, train_loss=0.635, val_loss=0.13\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 32.12it/s, train_loss=0.746, val_loss=0.10\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 32.12it/s, train_loss=0.746, val_loss=0.10\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 32.12it/s, train_loss=0.575, val_loss=0.11\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 32.07it/s, train_loss=0.575, val_loss=0.11\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 32.07it/s, train_loss=0.575, val_loss=0.11\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 32.07it/s, train_loss=0.585, val_loss=0.09\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 32.07it/s, train_loss=0.585, val_loss=0.09\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 32.07it/s, train_loss=0.53, val_loss=0.084\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 32.07it/s, train_loss=0.53, val_loss=0.084\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 32.07it/s, train_loss=0.455, val_loss=0.07\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 32.07it/s, train_loss=0.455, val_loss=0.07\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:01<00:00, 32.07it/s, train_loss=0.423, val_loss=0.06\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:01<00:00, 32.14it/s, train_loss=0.423, val_loss=0.06\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:01<00:00, 32.14it/s, train_loss=0.423, val_loss=0.06\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:01<00:00, 32.14it/s, train_loss=0.367, val_loss=0.06\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 32.14it/s, train_loss=0.367, val_loss=0.06\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    27: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 32.14it/s, train_loss=0.326, val_loss=0.06\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 32.14it/s, train_loss=0.326, val_loss=0.06\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 32.14it/s, train_loss=0.317, val_loss=0.05\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 32.14it/s, train_loss=0.317, val_loss=0.05\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 32.14it/s, train_loss=0.28, val_loss=0.041\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 32.12it/s, train_loss=0.28, val_loss=0.041\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 32.12it/s, train_loss=0.28, val_loss=0.041\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 32.12it/s, train_loss=0.262, val_loss=0.04\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 32.12it/s, train_loss=0.262, val_loss=0.04\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 32.12it/s, train_loss=0.268, val_loss=0.03\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 32.12it/s, train_loss=0.268, val_loss=0.03\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 32.12it/s, train_loss=0.253, val_loss=0.04\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 32.12it/s, train_loss=0.253, val_loss=0.04\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 32.12it/s, train_loss=0.249, val_loss=0.04\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 31.99it/s, train_loss=0.249, val_loss=0.04\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 31.99it/s, train_loss=0.249, val_loss=0.04\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 31.99it/s, train_loss=0.246, val_loss=0.03\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 31.99it/s, train_loss=0.246, val_loss=0.03\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 31.99it/s, train_loss=0.225, val_loss=0.03\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 31.99it/s, train_loss=0.225, val_loss=0.03\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 31.99it/s, train_loss=0.223, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 31.99it/s, train_loss=0.223, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 31.99it/s, train_loss=0.219, val_loss=0.03\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 31.87it/s, train_loss=0.219, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 31.87it/s, train_loss=0.219, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 31.87it/s, train_loss=0.205, val_loss=0.03\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 31.87it/s, train_loss=0.205, val_loss=0.03\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 31.87it/s, train_loss=0.204, val_loss=0.03\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 31.87it/s, train_loss=0.204, val_loss=0.03\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 31.87it/s, train_loss=0.198, val_loss=0.03\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 31.87it/s, train_loss=0.198, val_loss=0.03\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 31.87it/s, train_loss=0.194, val_loss=0.03\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 32.05it/s, train_loss=0.194, val_loss=0.03\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 32.05it/s, train_loss=0.194, val_loss=0.03\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 32.05it/s, train_loss=0.198, val_loss=0.03\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 32.05it/s, train_loss=0.198, val_loss=0.03\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 31.96it/s, train_loss=0.192, val_loss=0.03\u001b[A\n",
      " 70%|█████████████████████████████▍            | 70/100 [01:46<00:46,  1.57s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=4.01, val_loss=44, train_cinde\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=4.01, val_loss=44, train_cinde\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=247, val_loss=4.14, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=247, val_loss=4.14, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=20.2, val_loss=1.38, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=20.2, val_loss=1.38, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=8.51, val_loss=7.38, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 31.75it/s, train_loss=8.51, val_loss=7.38, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.75it/s, train_loss=8.51, val_loss=7.38, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    49: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Curent best Test AUC: 0.6276768941765811\n",
      "Its' val AUC : 0.6718771238276472\n",
      "Its external  AUC: 0.6199779423349614\n",
      "Curent best Test AUC: 0.7577958672510958\n",
      "Its' val AUC : 0.7161886638575506\n",
      "Its external  AUC: 0.6034346935560108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.75it/s, train_loss=36.6, val_loss=3.32, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.75it/s, train_loss=36.6, val_loss=3.32, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.75it/s, train_loss=18.3, val_loss=1.16, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.75it/s, train_loss=18.3, val_loss=1.16, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.75it/s, train_loss=6.52, val_loss=3.75, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.75it/s, train_loss=6.52, val_loss=3.75, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.75it/s, train_loss=22, val_loss=0.723, tr\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 31.37it/s, train_loss=22, val_loss=0.723, tr\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 31.37it/s, train_loss=22, val_loss=0.723, tr\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 31.37it/s, train_loss=4.68, val_loss=1.78, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 31.37it/s, train_loss=4.68, val_loss=1.78, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 31.37it/s, train_loss=10.2, val_loss=0.989, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 31.37it/s, train_loss=10.2, val_loss=0.989,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 31.37it/s, train_loss=4.74, val_loss=1.83, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 31.37it/s, train_loss=4.74, val_loss=1.83, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 31.37it/s, train_loss=8.62, val_loss=0.813,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 31.43it/s, train_loss=8.62, val_loss=0.813\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 31.43it/s, train_loss=8.62, val_loss=0.813\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 31.43it/s, train_loss=4.79, val_loss=0.787\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 31.43it/s, train_loss=4.79, val_loss=0.787\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 31.43it/s, train_loss=3.87, val_loss=1.19,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 31.43it/s, train_loss=3.87, val_loss=1.19,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 31.43it/s, train_loss=7.43, val_loss=0.606\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 31.43it/s, train_loss=7.43, val_loss=0.606\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 31.43it/s, train_loss=3.97, val_loss=0.508\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 31.39it/s, train_loss=3.97, val_loss=0.508\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 31.39it/s, train_loss=3.97, val_loss=0.508\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 31.39it/s, train_loss=2.25, val_loss=0.971\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 31.39it/s, train_loss=2.25, val_loss=0.971\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 31.39it/s, train_loss=4.45, val_loss=0.249\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 31.39it/s, train_loss=4.45, val_loss=0.249\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 31.39it/s, train_loss=1.26, val_loss=0.494\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 31.39it/s, train_loss=1.26, val_loss=0.494\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 31.39it/s, train_loss=2.71, val_loss=0.757\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    14: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Curent best Test AUC: 0.7805886036318096\n",
      "Its' val AUC : 0.6824792714421639\n",
      "Its external  AUC: 0.6456593666299039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 31.22it/s, train_loss=2.71, val_loss=0.757\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 31.22it/s, train_loss=2.71, val_loss=0.757\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 31.22it/s, train_loss=3.97, val_loss=0.155\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 31.22it/s, train_loss=3.97, val_loss=0.155\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 31.22it/s, train_loss=0.966, val_loss=0.41\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 31.22it/s, train_loss=0.966, val_loss=0.41\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 31.22it/s, train_loss=2.33, val_loss=0.302\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 31.22it/s, train_loss=2.33, val_loss=0.302\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 31.22it/s, train_loss=1.68, val_loss=0.135\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 31.22it/s, train_loss=1.68, val_loss=0.135\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 31.22it/s, train_loss=1.68, val_loss=0.135\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 31.22it/s, train_loss=0.838, val_loss=0.49\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 31.22it/s, train_loss=0.838, val_loss=0.49\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 31.22it/s, train_loss=2.76, val_loss=0.185\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 31.22it/s, train_loss=2.76, val_loss=0.185\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 31.22it/s, train_loss=1.07, val_loss=0.202\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 31.22it/s, train_loss=1.07, val_loss=0.202\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 31.22it/s, train_loss=1.18, val_loss=0.234\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 31.56it/s, train_loss=1.18, val_loss=0.234\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 31.56it/s, train_loss=1.18, val_loss=0.234\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 31.56it/s, train_loss=1.4, val_loss=0.063,\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 31.56it/s, train_loss=1.4, val_loss=0.063,\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 31.56it/s, train_loss=0.348, val_loss=0.32\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 31.56it/s, train_loss=0.348, val_loss=0.32\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 31.56it/s, train_loss=1.52, val_loss=0.16,\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 31.56it/s, train_loss=1.52, val_loss=0.16,\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:01<00:00, 31.56it/s, train_loss=0.798, val_loss=0.09\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:01<00:00, 32.09it/s, train_loss=0.798, val_loss=0.09\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:01<00:00, 32.09it/s, train_loss=0.798, val_loss=0.09\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:01<00:00, 32.09it/s, train_loss=0.563, val_loss=0.18\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 32.09it/s, train_loss=0.563, val_loss=0.18\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 32.09it/s, train_loss=0.937, val_loss=0.05\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 32.09it/s, train_loss=0.937, val_loss=0.05\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 32.09it/s, train_loss=0.319, val_loss=0.17\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 32.09it/s, train_loss=0.319, val_loss=0.17\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 32.09it/s, train_loss=1.06, val_loss=0.070\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 32.88it/s, train_loss=1.06, val_loss=0.070\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 32.88it/s, train_loss=1.06, val_loss=0.070\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 32.88it/s, train_loss=0.468, val_loss=0.08\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 32.88it/s, train_loss=0.468, val_loss=0.08\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 32.88it/s, train_loss=0.397, val_loss=0.07\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 32.88it/s, train_loss=0.397, val_loss=0.07\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 32.88it/s, train_loss=0.362, val_loss=0.03\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 32.88it/s, train_loss=0.362, val_loss=0.03\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 32.88it/s, train_loss=0.17, val_loss=0.107\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 33.44it/s, train_loss=0.17, val_loss=0.107\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 33.44it/s, train_loss=0.17, val_loss=0.107\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 33.44it/s, train_loss=0.483, val_loss=0.05\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 33.44it/s, train_loss=0.483, val_loss=0.05\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 33.44it/s, train_loss=0.239, val_loss=0.05\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 33.44it/s, train_loss=0.239, val_loss=0.05\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 33.44it/s, train_loss=0.316, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 33.44it/s, train_loss=0.316, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 33.44it/s, train_loss=0.209, val_loss=0.03\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 33.82it/s, train_loss=0.209, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 33.82it/s, train_loss=0.209, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 33.82it/s, train_loss=0.201, val_loss=0.05\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 33.82it/s, train_loss=0.201, val_loss=0.05\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 33.82it/s, train_loss=0.318, val_loss=0.02\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 33.82it/s, train_loss=0.318, val_loss=0.02\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 33.82it/s, train_loss=0.19, val_loss=0.025\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 33.82it/s, train_loss=0.19, val_loss=0.025\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 33.82it/s, train_loss=0.16, val_loss=0.036\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.12it/s, train_loss=0.16, val_loss=0.036\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.12it/s, train_loss=0.16, val_loss=0.036\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.12it/s, train_loss=0.205, val_loss=0.02\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.12it/s, train_loss=0.205, val_loss=0.02\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 32.70it/s, train_loss=0.136, val_loss=0.01\u001b[A\n",
      " 71%|█████████████████████████████▊            | 71/100 [01:48<00:45,  1.56s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=13.4, val_loss=17.7, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=13.4, val_loss=17.7, train_cin\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    45: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Curent best Test AUC: 0.26900438321853476\n",
      "Its' val AUC : 0.2641022155770015\n",
      "Its external  AUC: 0.37986450291476287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=94.8, val_loss=85.3, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=94.8, val_loss=85.3, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=439, val_loss=3.18, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=439, val_loss=3.18, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=18.3, val_loss=8.62, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.77it/s, train_loss=18.3, val_loss=8.62, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.77it/s, train_loss=18.3, val_loss=8.62, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.77it/s, train_loss=46, val_loss=0.864, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.77it/s, train_loss=46, val_loss=0.864, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.77it/s, train_loss=5.3, val_loss=3.79, tr\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.77it/s, train_loss=5.3, val_loss=3.79, tr\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.77it/s, train_loss=22.9, val_loss=19.6, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.77it/s, train_loss=22.9, val_loss=19.6, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.77it/s, train_loss=101, val_loss=2.43, tr\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 34.09it/s, train_loss=101, val_loss=2.43, tr\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.09it/s, train_loss=101, val_loss=2.43, tr\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7289918597370069\n",
      "Its' val AUC : 0.7095283403561234\n",
      "Its external  AUC: 0.6130455333228297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.09it/s, train_loss=15.8, val_loss=8.19, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.09it/s, train_loss=15.8, val_loss=8.19, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.09it/s, train_loss=42.9, val_loss=4.57, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.09it/s, train_loss=42.9, val_loss=4.57, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.09it/s, train_loss=27.2, val_loss=2.02, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.09it/s, train_loss=27.2, val_loss=2.02, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.09it/s, train_loss=12.8, val_loss=0.578,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.27it/s, train_loss=12.8, val_loss=0.578\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.27it/s, train_loss=12.8, val_loss=0.578\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.27it/s, train_loss=3.65, val_loss=1.51,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.27it/s, train_loss=3.65, val_loss=1.51,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.27it/s, train_loss=7.76, val_loss=0.672\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.27it/s, train_loss=7.76, val_loss=0.672\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.27it/s, train_loss=3.65, val_loss=0.659\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.27it/s, train_loss=3.65, val_loss=0.659\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.27it/s, train_loss=3.26, val_loss=1.68,\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 33.82it/s, train_loss=3.26, val_loss=1.68,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.82it/s, train_loss=3.26, val_loss=1.68,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.82it/s, train_loss=8.25, val_loss=0.948\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.82it/s, train_loss=8.25, val_loss=0.948\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    11: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Curent best Test AUC: 0.7544145272385723\n",
      "Its' val AUC : 0.68003262199266\n",
      "Its external  AUC: 0.6080037813140066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.82it/s, train_loss=4.82, val_loss=0.761\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.82it/s, train_loss=4.82, val_loss=0.761\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.82it/s, train_loss=4.6, val_loss=0.788,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.82it/s, train_loss=4.6, val_loss=0.788,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.82it/s, train_loss=4.71, val_loss=0.436\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 33.30it/s, train_loss=4.71, val_loss=0.436\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.30it/s, train_loss=4.71, val_loss=0.436\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.30it/s, train_loss=2.55, val_loss=0.284\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.30it/s, train_loss=2.55, val_loss=0.284\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.30it/s, train_loss=1.57, val_loss=0.454\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.30it/s, train_loss=1.57, val_loss=0.454\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.30it/s, train_loss=2.56, val_loss=0.456\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.30it/s, train_loss=2.56, val_loss=0.456\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.30it/s, train_loss=2.66, val_loss=0.256\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 33.47it/s, train_loss=2.66, val_loss=0.256\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.47it/s, train_loss=2.66, val_loss=0.256\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    18: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.47it/s, train_loss=1.56, val_loss=0.255\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.47it/s, train_loss=1.56, val_loss=0.255\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.47it/s, train_loss=1.41, val_loss=0.414\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.47it/s, train_loss=1.41, val_loss=0.414\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.47it/s, train_loss=2.11, val_loss=0.337\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.47it/s, train_loss=2.11, val_loss=0.337\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.47it/s, train_loss=1.69, val_loss=0.15,\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 33.75it/s, train_loss=1.69, val_loss=0.15,\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.75it/s, train_loss=1.69, val_loss=0.15,\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.75it/s, train_loss=0.782, val_loss=0.15\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.75it/s, train_loss=0.782, val_loss=0.15\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.75it/s, train_loss=0.854, val_loss=0.20\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.75it/s, train_loss=0.854, val_loss=0.20\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.75it/s, train_loss=1.14, val_loss=0.149\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.75it/s, train_loss=1.14, val_loss=0.149\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.75it/s, train_loss=0.801, val_loss=0.09\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.05it/s, train_loss=0.801, val_loss=0.09\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.05it/s, train_loss=0.801, val_loss=0.09\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.05it/s, train_loss=0.513, val_loss=0.14\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.05it/s, train_loss=0.513, val_loss=0.14\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 34.05it/s, train_loss=0.8, val_loss=0.156,\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.05it/s, train_loss=0.8, val_loss=0.156,\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.05it/s, train_loss=0.889, val_loss=0.08\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.05it/s, train_loss=0.889, val_loss=0.08\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.05it/s, train_loss=0.494, val_loss=0.06\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 33.84it/s, train_loss=0.494, val_loss=0.06\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.84it/s, train_loss=0.494, val_loss=0.06\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.84it/s, train_loss=0.37, val_loss=0.109\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.84it/s, train_loss=0.37, val_loss=0.109\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.84it/s, train_loss=0.573, val_loss=0.09\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.84it/s, train_loss=0.573, val_loss=0.09\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.84it/s, train_loss=0.523, val_loss=0.06\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.84it/s, train_loss=0.523, val_loss=0.06\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.84it/s, train_loss=0.353, val_loss=0.09\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.01it/s, train_loss=0.353, val_loss=0.09\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.01it/s, train_loss=0.353, val_loss=0.09\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.01it/s, train_loss=0.483, val_loss=0.12\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.01it/s, train_loss=0.483, val_loss=0.12\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.01it/s, train_loss=0.614, val_loss=0.08\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.01it/s, train_loss=0.614, val_loss=0.08\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.01it/s, train_loss=0.451, val_loss=0.05\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.01it/s, train_loss=0.451, val_loss=0.05\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.01it/s, train_loss=0.356, val_loss=0.06\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 33.76it/s, train_loss=0.356, val_loss=0.06\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 33.76it/s, train_loss=0.356, val_loss=0.06\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 33.76it/s, train_loss=0.439, val_loss=0.05\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 33.76it/s, train_loss=0.439, val_loss=0.05\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 33.76it/s, train_loss=0.395, val_loss=0.04\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 33.76it/s, train_loss=0.395, val_loss=0.04\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 33.76it/s, train_loss=0.29, val_loss=0.061\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 33.76it/s, train_loss=0.29, val_loss=0.061\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 33.76it/s, train_loss=0.355, val_loss=0.07\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 33.81it/s, train_loss=0.355, val_loss=0.07\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 33.81it/s, train_loss=0.355, val_loss=0.07\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 33.81it/s, train_loss=0.41, val_loss=0.052\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 33.81it/s, train_loss=0.41, val_loss=0.052\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 33.84it/s, train_loss=0.322, val_loss=0.04\u001b[A\n",
      " 72%|██████████████████████████████▏           | 72/100 [01:49<00:43,  1.54s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=68.4, val_loss=2.55, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=68.4, val_loss=2.55, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=14.3, val_loss=1.4, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=14.3, val_loss=1.4, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=7.01, val_loss=0.66, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=7.01, val_loss=0.66, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=3.34, val_loss=0.328, train_ci\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.13it/s, train_loss=3.34, val_loss=0.328, \u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.13it/s, train_loss=3.34, val_loss=0.328, \u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.13it/s, train_loss=1.76, val_loss=0.26, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.13it/s, train_loss=1.76, val_loss=0.26, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.13it/s, train_loss=1.65, val_loss=0.619, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.13it/s, train_loss=1.65, val_loss=0.619, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.13it/s, train_loss=3.76, val_loss=1.72, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.13it/s, train_loss=3.76, val_loss=1.72, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.2422041327489042\n",
      "Its' val AUC : 0.2982193829006389\n",
      "Its external  AUC: 0.3965653064439893\n",
      "Curent best Test AUC: 0.2850344395742016\n",
      "Its' val AUC : 0.308413755606905\n",
      "Its external  AUC: 0.41815030723176305\n",
      "Curent best Test AUC: 0.6142767689417659\n",
      "Its' val AUC : 0.5919532418105206\n",
      "Its external  AUC: 0.6335276508586734\n",
      "Curent best Test AUC: 0.747964934251722\n",
      "Its' val AUC : 0.585700693217344\n",
      "Its external  AUC: 0.6240743658421302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.13it/s, train_loss=8.73, val_loss=1.08, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 33.59it/s, train_loss=8.73, val_loss=1.08, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.59it/s, train_loss=8.73, val_loss=1.08, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.59it/s, train_loss=6.01, val_loss=0.108, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.59it/s, train_loss=6.01, val_loss=0.108, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.59it/s, train_loss=0.659, val_loss=0.506,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.59it/s, train_loss=0.659, val_loss=0.506\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.59it/s, train_loss=2.71, val_loss=0.564,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.59it/s, train_loss=2.71, val_loss=0.564,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.59it/s, train_loss=2.49, val_loss=0.0866\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.15it/s, train_loss=2.49, val_loss=0.086\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.15it/s, train_loss=2.49, val_loss=0.086\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.15it/s, train_loss=0.362, val_loss=0.31\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.15it/s, train_loss=0.362, val_loss=0.31\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.15it/s, train_loss=1.8, val_loss=0.243,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.15it/s, train_loss=1.8, val_loss=0.243,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.15it/s, train_loss=1.11, val_loss=0.055\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.15it/s, train_loss=1.11, val_loss=0.055\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.15it/s, train_loss=0.328, val_loss=0.29\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.04it/s, train_loss=0.328, val_loss=0.29\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.04it/s, train_loss=0.328, val_loss=0.29\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.04it/s, train_loss=1.61, val_loss=0.136\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.04it/s, train_loss=1.61, val_loss=0.136\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.04it/s, train_loss=0.785, val_loss=0.11\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.04it/s, train_loss=0.785, val_loss=0.11\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.04it/s, train_loss=0.68, val_loss=0.262\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.04it/s, train_loss=0.68, val_loss=0.262\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.04it/s, train_loss=1.43, val_loss=0.083\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.07it/s, train_loss=1.43, val_loss=0.083\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.07it/s, train_loss=1.43, val_loss=0.083\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.07it/s, train_loss=0.467, val_loss=0.08\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.07it/s, train_loss=0.467, val_loss=0.08\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.07it/s, train_loss=0.417, val_loss=0.04\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.07it/s, train_loss=0.417, val_loss=0.04\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.07it/s, train_loss=0.27, val_loss=0.06,\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.07it/s, train_loss=0.27, val_loss=0.06,\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.07it/s, train_loss=0.365, val_loss=0.02\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.26it/s, train_loss=0.365, val_loss=0.02\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.26it/s, train_loss=0.365, val_loss=0.02\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.26it/s, train_loss=0.136, val_loss=0.06\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.26it/s, train_loss=0.136, val_loss=0.06\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.26it/s, train_loss=0.292, val_loss=0.02\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.26it/s, train_loss=0.292, val_loss=0.02\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.26it/s, train_loss=0.0962, val_loss=0.0\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.26it/s, train_loss=0.0962, val_loss=0.0\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    21: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.26it/s, train_loss=0.173, val_loss=0.02\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.21it/s, train_loss=0.173, val_loss=0.02\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.21it/s, train_loss=0.173, val_loss=0.02\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.21it/s, train_loss=0.112, val_loss=0.02\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.21it/s, train_loss=0.112, val_loss=0.02\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.21it/s, train_loss=0.12, val_loss=0.028\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.21it/s, train_loss=0.12, val_loss=0.028\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.21it/s, train_loss=0.154, val_loss=0.02\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.21it/s, train_loss=0.154, val_loss=0.02\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.21it/s, train_loss=0.0963, val_loss=0.0\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.22it/s, train_loss=0.0963, val_loss=0.0\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.22it/s, train_loss=0.0963, val_loss=0.0\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.22it/s, train_loss=0.163, val_loss=0.01\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.22it/s, train_loss=0.163, val_loss=0.01\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.22it/s, train_loss=0.0803, val_loss=0.0\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.22it/s, train_loss=0.0803, val_loss=0.0\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.22it/s, train_loss=0.153, val_loss=0.01\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.22it/s, train_loss=0.153, val_loss=0.01\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.22it/s, train_loss=0.0715, val_loss=0.0\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.11it/s, train_loss=0.0715, val_loss=0.0\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.11it/s, train_loss=0.0715, val_loss=0.0\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.11it/s, train_loss=0.134, val_loss=0.01\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.11it/s, train_loss=0.134, val_loss=0.01\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.11it/s, train_loss=0.0705, val_loss=0.0\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.11it/s, train_loss=0.0705, val_loss=0.0\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.11it/s, train_loss=0.118, val_loss=0.01\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.11it/s, train_loss=0.118, val_loss=0.01\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.11it/s, train_loss=0.0615, val_loss=0.0\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 33.75it/s, train_loss=0.0615, val_loss=0.0\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 33.75it/s, train_loss=0.0615, val_loss=0.0\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 33.75it/s, train_loss=0.0888, val_loss=0.0\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 33.75it/s, train_loss=0.0888, val_loss=0.0\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 33.75it/s, train_loss=0.0455, val_loss=0.0\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 33.75it/s, train_loss=0.0455, val_loss=0.0\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 33.75it/s, train_loss=0.0708, val_loss=0.0\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 33.75it/s, train_loss=0.0708, val_loss=0.0\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 33.75it/s, train_loss=0.0423, val_loss=0.0\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 33.25it/s, train_loss=0.0423, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 33.25it/s, train_loss=0.0423, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 33.25it/s, train_loss=0.065, val_loss=0.01\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 33.25it/s, train_loss=0.065, val_loss=0.01\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 33.25it/s, train_loss=0.0422, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 33.25it/s, train_loss=0.0422, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 33.25it/s, train_loss=0.062, val_loss=0.00\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 33.25it/s, train_loss=0.062, val_loss=0.00\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 33.25it/s, train_loss=0.0402, val_loss=0.0\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 33.46it/s, train_loss=0.0402, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 33.46it/s, train_loss=0.0402, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 33.46it/s, train_loss=0.0572, val_loss=0.0\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 33.46it/s, train_loss=0.0572, val_loss=0.0\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 33.75it/s, train_loss=0.0377, val_loss=0.0\u001b[A\n",
      " 73%|██████████████████████████████▋           | 73/100 [01:51<00:41,  1.53s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=62.2, val_loss=2.61, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=62.2, val_loss=2.61, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=15.6, val_loss=66.2, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=15.6, val_loss=66.2, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=304, val_loss=12.8, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=304, val_loss=12.8, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=74.9, val_loss=17.3, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 31.79it/s, train_loss=74.9, val_loss=17.3, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.79it/s, train_loss=74.9, val_loss=17.3, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.79it/s, train_loss=95.9, val_loss=3.95, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.79it/s, train_loss=95.9, val_loss=3.95, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.79it/s, train_loss=20.4, val_loss=8.39, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.79it/s, train_loss=20.4, val_loss=8.39, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.79it/s, train_loss=45.8, val_loss=4.17, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.79it/s, train_loss=45.8, val_loss=4.17, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.23807138384470883\n",
      "Its' val AUC : 0.3882017126546147\n",
      "Its external  AUC: 0.3680478966440838\n",
      "Curent best Test AUC: 0.728616155291171\n",
      "Its' val AUC : 0.7102079652032078\n",
      "Its external  AUC: 0.6026469198046321\n",
      "Curent best Test AUC: 0.7579211020663744\n",
      "Its' val AUC : 0.6872366453717548\n",
      "Its external  AUC: 0.5961871750433275\n",
      "Epoch     7: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.79it/s, train_loss=19.7, val_loss=5.48, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 33.08it/s, train_loss=19.7, val_loss=5.48, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.08it/s, train_loss=19.7, val_loss=5.48, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.08it/s, train_loss=25.8, val_loss=1.32, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.08it/s, train_loss=25.8, val_loss=1.32, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.08it/s, train_loss=5.87, val_loss=1.69, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.08it/s, train_loss=5.87, val_loss=1.69, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.08it/s, train_loss=9.84, val_loss=3.69, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.08it/s, train_loss=9.84, val_loss=3.69, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.08it/s, train_loss=20, val_loss=0.791, t\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 33.82it/s, train_loss=20, val_loss=0.791, \u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.82it/s, train_loss=20, val_loss=0.791, \u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.82it/s, train_loss=4.3, val_loss=0.869,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.82it/s, train_loss=4.3, val_loss=0.869,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.82it/s, train_loss=5.03, val_loss=2.23,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.82it/s, train_loss=5.03, val_loss=2.23,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.82it/s, train_loss=12.7, val_loss=0.867\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.82it/s, train_loss=12.7, val_loss=0.867\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.82it/s, train_loss=5.76, val_loss=0.47,\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 33.62it/s, train_loss=5.76, val_loss=0.47,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.62it/s, train_loss=5.76, val_loss=0.47,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.62it/s, train_loss=2.56, val_loss=1.84,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.62it/s, train_loss=2.56, val_loss=1.84,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.62it/s, train_loss=8.46, val_loss=0.935\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.62it/s, train_loss=8.46, val_loss=0.935\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.62it/s, train_loss=4.32, val_loss=0.227\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.62it/s, train_loss=4.32, val_loss=0.227\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.62it/s, train_loss=1.46, val_loss=1.05,\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 33.25it/s, train_loss=1.46, val_loss=1.05,\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.25it/s, train_loss=1.46, val_loss=1.05,\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.25it/s, train_loss=5.9, val_loss=0.727,\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.25it/s, train_loss=5.9, val_loss=0.727,\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.25it/s, train_loss=4.17, val_loss=0.147\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.25it/s, train_loss=4.17, val_loss=0.147\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.25it/s, train_loss=0.944, val_loss=0.77\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.25it/s, train_loss=0.944, val_loss=0.77\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.25it/s, train_loss=3.78, val_loss=0.674\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 33.76it/s, train_loss=3.78, val_loss=0.674\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.76it/s, train_loss=3.78, val_loss=0.674\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.76it/s, train_loss=3.3, val_loss=0.142,\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.76it/s, train_loss=3.3, val_loss=0.142,\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.76it/s, train_loss=0.907, val_loss=0.49\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.76it/s, train_loss=0.907, val_loss=0.49\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.76it/s, train_loss=2.79, val_loss=0.498\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.76it/s, train_loss=2.79, val_loss=0.498\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.76it/s, train_loss=2.67, val_loss=0.097\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 33.87it/s, train_loss=2.67, val_loss=0.097\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.87it/s, train_loss=2.67, val_loss=0.097\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.87it/s, train_loss=0.528, val_loss=0.34\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.87it/s, train_loss=0.528, val_loss=0.34\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.87it/s, train_loss=1.86, val_loss=0.373\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.87it/s, train_loss=1.86, val_loss=0.373\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.87it/s, train_loss=2.05, val_loss=0.083\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.87it/s, train_loss=2.05, val_loss=0.083\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.87it/s, train_loss=0.487, val_loss=0.30\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.04it/s, train_loss=0.487, val_loss=0.30\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.04it/s, train_loss=0.487, val_loss=0.30\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.04it/s, train_loss=1.5, val_loss=0.288,\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.04it/s, train_loss=1.5, val_loss=0.288,\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 34.04it/s, train_loss=1.48, val_loss=0.056\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.04it/s, train_loss=1.48, val_loss=0.056\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.04it/s, train_loss=0.366, val_loss=0.24\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.04it/s, train_loss=0.366, val_loss=0.24\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.04it/s, train_loss=1.28, val_loss=0.217\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.26it/s, train_loss=1.28, val_loss=0.217\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.26it/s, train_loss=1.28, val_loss=0.217\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.26it/s, train_loss=1.13, val_loss=0.056\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.26it/s, train_loss=1.13, val_loss=0.056\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.26it/s, train_loss=0.359, val_loss=0.19\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.26it/s, train_loss=0.359, val_loss=0.19\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.26it/s, train_loss=1.06, val_loss=0.131\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.26it/s, train_loss=1.06, val_loss=0.131\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.26it/s, train_loss=0.712, val_loss=0.06\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.39it/s, train_loss=0.712, val_loss=0.06\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.39it/s, train_loss=0.712, val_loss=0.06\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.39it/s, train_loss=0.353, val_loss=0.10\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.39it/s, train_loss=0.353, val_loss=0.10\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.39it/s, train_loss=0.566, val_loss=0.09\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.39it/s, train_loss=0.566, val_loss=0.09\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.39it/s, train_loss=0.521, val_loss=0.05\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.39it/s, train_loss=0.521, val_loss=0.05\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.39it/s, train_loss=0.3, val_loss=0.0516\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.50it/s, train_loss=0.3, val_loss=0.0516\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.50it/s, train_loss=0.3, val_loss=0.0516\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.50it/s, train_loss=0.285, val_loss=0.07\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.50it/s, train_loss=0.285, val_loss=0.07\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.50it/s, train_loss=0.421, val_loss=0.07\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.50it/s, train_loss=0.421, val_loss=0.07\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    40: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.50it/s, train_loss=0.396, val_loss=0.04\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.50it/s, train_loss=0.396, val_loss=0.04\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.50it/s, train_loss=0.258, val_loss=0.05\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.50it/s, train_loss=0.258, val_loss=0.05\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.50it/s, train_loss=0.258, val_loss=0.05\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.50it/s, train_loss=0.258, val_loss=0.06\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.50it/s, train_loss=0.258, val_loss=0.06\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.02it/s, train_loss=0.343, val_loss=0.06\u001b[A\n",
      " 74%|███████████████████████████████           | 74/100 [01:52<00:39,  1.52s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=6.01, val_loss=228, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=6.01, val_loss=228, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=1.18e+3, val_loss=6.52, train_\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=1.18e+3, val_loss=6.52, train_\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=35.7, val_loss=7.02, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=35.7, val_loss=7.02, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=37.5, val_loss=3.68, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 34.13it/s, train_loss=37.5, val_loss=3.68, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 34.13it/s, train_loss=37.5, val_loss=3.68, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 34.13it/s, train_loss=19.8, val_loss=0.339, \u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 34.13it/s, train_loss=19.8, val_loss=0.339, \u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 34.13it/s, train_loss=1.85, val_loss=0.319, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 34.13it/s, train_loss=1.85, val_loss=0.319, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 34.13it/s, train_loss=1.71, val_loss=0.134, \u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 34.13it/s, train_loss=1.71, val_loss=0.134, \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.6758922980588603\n",
      "Its' val AUC : 0.6970232431697703\n",
      "Its external  AUC: 0.6117850953206239\n",
      "Curent best Test AUC: 0.6830306825297433\n",
      "Its' val AUC : 0.6060894386298763\n",
      "Its external  AUC: 0.6403025051205293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 34.13it/s, train_loss=0.961, val_loss=0.112,\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 34.12it/s, train_loss=0.961, val_loss=0.112,\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.12it/s, train_loss=0.961, val_loss=0.112,\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.12it/s, train_loss=0.788, val_loss=0.147,\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.12it/s, train_loss=0.788, val_loss=0.147,\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.12it/s, train_loss=0.937, val_loss=0.194,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.12it/s, train_loss=0.937, val_loss=0.194\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.12it/s, train_loss=1.16, val_loss=0.154,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.12it/s, train_loss=1.16, val_loss=0.154,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.12it/s, train_loss=0.977, val_loss=0.134\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.30it/s, train_loss=0.977, val_loss=0.13\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.30it/s, train_loss=0.977, val_loss=0.13\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.30it/s, train_loss=0.911, val_loss=0.13\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.30it/s, train_loss=0.911, val_loss=0.13\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.30it/s, train_loss=0.96, val_loss=0.146\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.30it/s, train_loss=0.96, val_loss=0.146\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.30it/s, train_loss=0.952, val_loss=0.13\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.30it/s, train_loss=0.952, val_loss=0.13\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.30it/s, train_loss=0.843, val_loss=0.13\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.05it/s, train_loss=0.843, val_loss=0.13\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.05it/s, train_loss=0.843, val_loss=0.13\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.05it/s, train_loss=0.827, val_loss=0.13\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.05it/s, train_loss=0.827, val_loss=0.13\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.05it/s, train_loss=0.78, val_loss=0.111\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.05it/s, train_loss=0.78, val_loss=0.111\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.05it/s, train_loss=0.683, val_loss=0.09\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.05it/s, train_loss=0.683, val_loss=0.09\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.05it/s, train_loss=0.631, val_loss=0.09\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 33.93it/s, train_loss=0.631, val_loss=0.09\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.93it/s, train_loss=0.631, val_loss=0.09\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    14: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.93it/s, train_loss=0.588, val_loss=0.08\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.93it/s, train_loss=0.588, val_loss=0.08\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.93it/s, train_loss=0.502, val_loss=0.07\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.93it/s, train_loss=0.502, val_loss=0.07\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.93it/s, train_loss=0.432, val_loss=0.06\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.93it/s, train_loss=0.432, val_loss=0.06\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.93it/s, train_loss=0.395, val_loss=0.06\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.24it/s, train_loss=0.395, val_loss=0.06\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.24it/s, train_loss=0.395, val_loss=0.06\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.24it/s, train_loss=0.343, val_loss=0.05\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.24it/s, train_loss=0.343, val_loss=0.05\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.24it/s, train_loss=0.289, val_loss=0.04\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.24it/s, train_loss=0.289, val_loss=0.04\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.24it/s, train_loss=0.261, val_loss=0.04\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.24it/s, train_loss=0.261, val_loss=0.04\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.24it/s, train_loss=0.226, val_loss=0.03\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.41it/s, train_loss=0.226, val_loss=0.03\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.41it/s, train_loss=0.226, val_loss=0.03\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.41it/s, train_loss=0.186, val_loss=0.03\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.41it/s, train_loss=0.186, val_loss=0.03\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.41it/s, train_loss=0.165, val_loss=0.02\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.41it/s, train_loss=0.165, val_loss=0.02\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.41it/s, train_loss=0.148, val_loss=0.02\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.41it/s, train_loss=0.148, val_loss=0.02\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.41it/s, train_loss=0.127, val_loss=0.02\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.09it/s, train_loss=0.127, val_loss=0.02\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.09it/s, train_loss=0.127, val_loss=0.02\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.09it/s, train_loss=0.116, val_loss=0.02\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.09it/s, train_loss=0.116, val_loss=0.02\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.09it/s, train_loss=0.11, val_loss=0.022\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.09it/s, train_loss=0.11, val_loss=0.022\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.09it/s, train_loss=0.0986, val_loss=0.0\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.09it/s, train_loss=0.0986, val_loss=0.0\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.09it/s, train_loss=0.0894, val_loss=0.0\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.20it/s, train_loss=0.0894, val_loss=0.0\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.20it/s, train_loss=0.0894, val_loss=0.0\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.20it/s, train_loss=0.0857, val_loss=0.0\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.20it/s, train_loss=0.0857, val_loss=0.0\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.20it/s, train_loss=0.0794, val_loss=0.0\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.20it/s, train_loss=0.0794, val_loss=0.0\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.20it/s, train_loss=0.0735, val_loss=0.0\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.20it/s, train_loss=0.0735, val_loss=0.0\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.20it/s, train_loss=0.0742, val_loss=0.0\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.41it/s, train_loss=0.0742, val_loss=0.0\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.41it/s, train_loss=0.0742, val_loss=0.0\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.41it/s, train_loss=0.0693, val_loss=0.0\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.41it/s, train_loss=0.0693, val_loss=0.0\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.41it/s, train_loss=0.0692, val_loss=0.0\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.41it/s, train_loss=0.0692, val_loss=0.0\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.41it/s, train_loss=0.068, val_loss=0.01\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.41it/s, train_loss=0.068, val_loss=0.01\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.41it/s, train_loss=0.066, val_loss=0.01\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.53it/s, train_loss=0.066, val_loss=0.01\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.53it/s, train_loss=0.066, val_loss=0.01\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.53it/s, train_loss=0.0675, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.53it/s, train_loss=0.0675, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.53it/s, train_loss=0.0654, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.53it/s, train_loss=0.0654, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.53it/s, train_loss=0.0652, val_loss=0.0\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.53it/s, train_loss=0.0652, val_loss=0.0\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.53it/s, train_loss=0.0641, val_loss=0.0\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.63it/s, train_loss=0.0641, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.63it/s, train_loss=0.0641, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.63it/s, train_loss=0.0627, val_loss=0.0\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.63it/s, train_loss=0.0627, val_loss=0.0\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.35it/s, train_loss=0.0623, val_loss=0.0\u001b[A\n",
      " 75%|███████████████████████████████▌          | 75/100 [01:54<00:37,  1.51s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=10.2, val_loss=11.8, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=10.2, val_loss=11.8, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=76.5, val_loss=39.5, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=76.5, val_loss=39.5, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=238, val_loss=41.3, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=238, val_loss=41.3, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=216, val_loss=20.2, train_cind\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.93it/s, train_loss=216, val_loss=20.2, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.93it/s, train_loss=216, val_loss=20.2, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.93it/s, train_loss=107, val_loss=5.43, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.93it/s, train_loss=107, val_loss=5.43, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.93it/s, train_loss=24.9, val_loss=13.9, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.93it/s, train_loss=24.9, val_loss=13.9, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.93it/s, train_loss=64.3, val_loss=2.38, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.93it/s, train_loss=64.3, val_loss=2.38, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.4900438321853475\n",
      "Its' val AUC : 0.4643196955280685\n",
      "Its external  AUC: 0.42870647550023633\n",
      "Curent best Test AUC: 0.6303068252974326\n",
      "Its' val AUC : 0.6683430746228082\n",
      "Its external  AUC: 0.6109973215692454\n",
      "Curent best Test AUC: 0.7420162805259862\n",
      "Its' val AUC : 0.7157808889493\n",
      "Its external  AUC: 0.5501811879628171\n",
      "Curent best Test AUC: 0.7447714464621165\n",
      "Its' val AUC : 0.7212178877259753\n",
      "Its external  AUC: 0.5985504962974634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.93it/s, train_loss=14.7, val_loss=11, tra\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 33.66it/s, train_loss=14.7, val_loss=11, tra\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.66it/s, train_loss=14.7, val_loss=11, tra\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.66it/s, train_loss=57, val_loss=0.781, tr\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.66it/s, train_loss=57, val_loss=0.781, tr\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.66it/s, train_loss=4.17, val_loss=4.48, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.66it/s, train_loss=4.17, val_loss=4.48, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.66it/s, train_loss=23.2, val_loss=0.949,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.66it/s, train_loss=23.2, val_loss=0.949,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.66it/s, train_loss=5.12, val_loss=3.06, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 33.88it/s, train_loss=5.12, val_loss=3.06,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.88it/s, train_loss=5.12, val_loss=3.06,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.88it/s, train_loss=16.2, val_loss=0.501\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.88it/s, train_loss=16.2, val_loss=0.501\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.88it/s, train_loss=2.58, val_loss=2.19,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.88it/s, train_loss=2.58, val_loss=2.19,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.88it/s, train_loss=9.98, val_loss=0.561\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.88it/s, train_loss=9.98, val_loss=0.561\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7509079524107702\n",
      "Its' val AUC : 0.6929454940872638\n",
      "Its external  AUC: 0.6291161178509532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.88it/s, train_loss=4.21, val_loss=1.61,\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.02it/s, train_loss=4.21, val_loss=1.61,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.02it/s, train_loss=4.21, val_loss=1.61,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.02it/s, train_loss=10.3, val_loss=0.201\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.02it/s, train_loss=10.3, val_loss=0.201\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.02it/s, train_loss=1.15, val_loss=1.8, \u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.02it/s, train_loss=1.15, val_loss=1.8, \u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.02it/s, train_loss=9.72, val_loss=0.452\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.02it/s, train_loss=9.72, val_loss=0.452\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.02it/s, train_loss=2.49, val_loss=1.18,\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.35it/s, train_loss=2.49, val_loss=1.18,\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.35it/s, train_loss=2.49, val_loss=1.18,\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.35it/s, train_loss=6.23, val_loss=0.269\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.35it/s, train_loss=6.23, val_loss=0.269\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.35it/s, train_loss=1.47, val_loss=0.842\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.35it/s, train_loss=1.47, val_loss=0.842\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.35it/s, train_loss=4.11, val_loss=0.28,\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.35it/s, train_loss=4.11, val_loss=0.28,\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.35it/s, train_loss=2.12, val_loss=0.291\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.42it/s, train_loss=2.12, val_loss=0.291\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.42it/s, train_loss=2.12, val_loss=0.291\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.42it/s, train_loss=2.13, val_loss=0.107\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.42it/s, train_loss=2.13, val_loss=0.107\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.42it/s, train_loss=0.753, val_loss=0.33\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.42it/s, train_loss=0.753, val_loss=0.33\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.42it/s, train_loss=1.59, val_loss=0.317\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.42it/s, train_loss=1.59, val_loss=0.317\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.42it/s, train_loss=1.56, val_loss=0.122\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.51it/s, train_loss=1.56, val_loss=0.122\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.51it/s, train_loss=1.56, val_loss=0.122\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.51it/s, train_loss=0.814, val_loss=0.19\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.51it/s, train_loss=0.814, val_loss=0.19\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    23: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.51it/s, train_loss=1.3, val_loss=0.0925\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.51it/s, train_loss=1.3, val_loss=0.0925\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.51it/s, train_loss=0.69, val_loss=0.137\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.51it/s, train_loss=0.69, val_loss=0.137\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.51it/s, train_loss=0.652, val_loss=0.27\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.61it/s, train_loss=0.652, val_loss=0.27\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.61it/s, train_loss=0.652, val_loss=0.27\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.61it/s, train_loss=1.25, val_loss=0.117\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.61it/s, train_loss=1.25, val_loss=0.117\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.61it/s, train_loss=0.626, val_loss=0.11\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.61it/s, train_loss=0.626, val_loss=0.11\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.61it/s, train_loss=0.799, val_loss=0.09\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.61it/s, train_loss=0.799, val_loss=0.09\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.61it/s, train_loss=0.646, val_loss=0.06\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.70it/s, train_loss=0.646, val_loss=0.06\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.70it/s, train_loss=0.646, val_loss=0.06\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.70it/s, train_loss=0.396, val_loss=0.15\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.70it/s, train_loss=0.396, val_loss=0.15\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.70it/s, train_loss=0.81, val_loss=0.073\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.70it/s, train_loss=0.81, val_loss=0.073\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.70it/s, train_loss=0.416, val_loss=0.08\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.70it/s, train_loss=0.416, val_loss=0.08\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.70it/s, train_loss=0.513, val_loss=0.07\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.72it/s, train_loss=0.513, val_loss=0.07\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.72it/s, train_loss=0.513, val_loss=0.07\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.72it/s, train_loss=0.483, val_loss=0.06\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.72it/s, train_loss=0.483, val_loss=0.06\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.72it/s, train_loss=0.374, val_loss=0.11\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.72it/s, train_loss=0.374, val_loss=0.11\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.72it/s, train_loss=0.598, val_loss=0.04\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.72it/s, train_loss=0.598, val_loss=0.04\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.72it/s, train_loss=0.3, val_loss=0.0623\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.33it/s, train_loss=0.3, val_loss=0.0623\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.33it/s, train_loss=0.3, val_loss=0.0623\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.33it/s, train_loss=0.403, val_loss=0.04\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.33it/s, train_loss=0.403, val_loss=0.04\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.33it/s, train_loss=0.317, val_loss=0.05\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.33it/s, train_loss=0.317, val_loss=0.05\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.33it/s, train_loss=0.334, val_loss=0.06\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.33it/s, train_loss=0.334, val_loss=0.06\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.33it/s, train_loss=0.394, val_loss=0.03\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.12it/s, train_loss=0.394, val_loss=0.03\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.12it/s, train_loss=0.394, val_loss=0.03\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.760551033187226\n",
      "Its' val AUC : 0.7220334375424765\n",
      "Its external  AUC: 0.4444619505278084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.12it/s, train_loss=0.238, val_loss=0.05\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.12it/s, train_loss=0.238, val_loss=0.05\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.24it/s, train_loss=0.34, val_loss=0.033\u001b[A\n",
      " 76%|███████████████████████████████▉          | 76/100 [01:55<00:35,  1.50s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=14.6, val_loss=4.4, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=14.6, val_loss=4.4, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=22.3, val_loss=3.28, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=22.3, val_loss=3.28, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=16.1, val_loss=0.436, train_ci\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=16.1, val_loss=0.436, train_ci\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=2.02, val_loss=1.4, train_cind\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 32.42it/s, train_loss=2.02, val_loss=1.4, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.42it/s, train_loss=2.02, val_loss=1.4, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.42it/s, train_loss=7.96, val_loss=0.248, \u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.42it/s, train_loss=7.96, val_loss=0.248, \u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.42it/s, train_loss=1.76, val_loss=1.45, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.42it/s, train_loss=1.76, val_loss=1.45, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.42it/s, train_loss=7.24, val_loss=1.54, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.42it/s, train_loss=7.24, val_loss=1.54, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.28991859737006886\n",
      "Its' val AUC : 0.2881609351637896\n",
      "Its external  AUC: 0.38853001417992755\n",
      "Curent best Test AUC: 0.7601753287413902\n",
      "Its' val AUC : 0.7151012641022155\n",
      "Its external  AUC: 0.610051993067591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.42it/s, train_loss=9.21, val_loss=1.27, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 33.67it/s, train_loss=9.21, val_loss=1.27, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.67it/s, train_loss=9.21, val_loss=1.27, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.67it/s, train_loss=5.85, val_loss=0.154, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.67it/s, train_loss=5.85, val_loss=0.154, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.67it/s, train_loss=0.823, val_loss=0.805,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.67it/s, train_loss=0.823, val_loss=0.805\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.67it/s, train_loss=4.39, val_loss=0.199,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.67it/s, train_loss=4.39, val_loss=0.199,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.67it/s, train_loss=1.11, val_loss=0.669,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.15it/s, train_loss=1.11, val_loss=0.669\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.15it/s, train_loss=1.11, val_loss=0.669\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.15it/s, train_loss=3.36, val_loss=0.128\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.15it/s, train_loss=3.36, val_loss=0.128\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.15it/s, train_loss=0.632, val_loss=0.50\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.15it/s, train_loss=0.632, val_loss=0.50\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.15it/s, train_loss=2.62, val_loss=0.113\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.15it/s, train_loss=2.62, val_loss=0.113\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.15it/s, train_loss=0.623, val_loss=0.47\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.40it/s, train_loss=0.623, val_loss=0.47\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.40it/s, train_loss=0.623, val_loss=0.47\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.40it/s, train_loss=2.31, val_loss=0.067\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.40it/s, train_loss=2.31, val_loss=0.067\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.40it/s, train_loss=0.307, val_loss=0.31\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.40it/s, train_loss=0.307, val_loss=0.31\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.40it/s, train_loss=1.77, val_loss=0.051\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.40it/s, train_loss=1.77, val_loss=0.051\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.40it/s, train_loss=0.309, val_loss=0.35\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.50it/s, train_loss=0.309, val_loss=0.35\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.50it/s, train_loss=0.309, val_loss=0.35\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.50it/s, train_loss=1.54, val_loss=0.061\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.50it/s, train_loss=1.54, val_loss=0.061\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.50it/s, train_loss=0.265, val_loss=0.23\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.50it/s, train_loss=0.265, val_loss=0.23\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.50it/s, train_loss=1.31, val_loss=0.030\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.50it/s, train_loss=1.31, val_loss=0.030\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.50it/s, train_loss=0.181, val_loss=0.22\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 32.63it/s, train_loss=0.181, val_loss=0.22\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 32.63it/s, train_loss=0.181, val_loss=0.22\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 32.63it/s, train_loss=1.11, val_loss=0.020\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 32.63it/s, train_loss=1.11, val_loss=0.020\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 32.63it/s, train_loss=0.121, val_loss=0.17\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 32.63it/s, train_loss=0.121, val_loss=0.17\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 32.63it/s, train_loss=0.927, val_loss=0.01\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 32.63it/s, train_loss=0.927, val_loss=0.01\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 32.63it/s, train_loss=0.103, val_loss=0.15\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 32.16it/s, train_loss=0.103, val_loss=0.15\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 32.16it/s, train_loss=0.103, val_loss=0.15\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 32.16it/s, train_loss=0.723, val_loss=0.01\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 32.16it/s, train_loss=0.723, val_loss=0.01\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 32.16it/s, train_loss=0.125, val_loss=0.08\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 32.16it/s, train_loss=0.125, val_loss=0.08\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 32.16it/s, train_loss=0.493, val_loss=0.06\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 32.16it/s, train_loss=0.493, val_loss=0.06\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 32.16it/s, train_loss=0.238, val_loss=0.07\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 32.21it/s, train_loss=0.238, val_loss=0.07\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 32.21it/s, train_loss=0.238, val_loss=0.07\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:01<00:00, 32.21it/s, train_loss=0.3, val_loss=0.0587\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 32.21it/s, train_loss=0.3, val_loss=0.0587\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 32.21it/s, train_loss=0.372, val_loss=0.03\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 32.21it/s, train_loss=0.372, val_loss=0.03\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 32.21it/s, train_loss=0.228, val_loss=0.02\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 32.21it/s, train_loss=0.228, val_loss=0.02\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 32.21it/s, train_loss=0.0908, val_loss=0.0\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 31.72it/s, train_loss=0.0908, val_loss=0.0\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 31.72it/s, train_loss=0.0908, val_loss=0.0\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 31.72it/s, train_loss=0.312, val_loss=0.01\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 31.72it/s, train_loss=0.312, val_loss=0.01\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 31.72it/s, train_loss=0.0897, val_loss=0.0\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 31.72it/s, train_loss=0.0897, val_loss=0.0\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    33: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch    39: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 31.72it/s, train_loss=0.145, val_loss=0.03\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 31.72it/s, train_loss=0.145, val_loss=0.03\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 31.72it/s, train_loss=0.22, val_loss=0.015\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 30.78it/s, train_loss=0.22, val_loss=0.015\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 30.78it/s, train_loss=0.22, val_loss=0.015\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 30.78it/s, train_loss=0.0927, val_loss=0.0\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 30.78it/s, train_loss=0.0927, val_loss=0.0\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 30.78it/s, train_loss=0.0539, val_loss=0.0\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 30.78it/s, train_loss=0.0539, val_loss=0.0\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 30.78it/s, train_loss=0.146, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 30.78it/s, train_loss=0.146, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 30.78it/s, train_loss=0.144, val_loss=0.01\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 30.19it/s, train_loss=0.144, val_loss=0.01\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 30.19it/s, train_loss=0.144, val_loss=0.01\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 30.19it/s, train_loss=0.0494, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 30.19it/s, train_loss=0.0494, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 30.19it/s, train_loss=0.0497, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 30.19it/s, train_loss=0.0497, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 30.19it/s, train_loss=0.111, val_loss=0.01\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 30.19it/s, train_loss=0.111, val_loss=0.01\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 30.19it/s, train_loss=0.0847, val_loss=0.0\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 30.93it/s, train_loss=0.0847, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 30.93it/s, train_loss=0.0847, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 30.93it/s, train_loss=0.0281, val_loss=0.0\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 30.93it/s, train_loss=0.0281, val_loss=0.0\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 32.00it/s, train_loss=0.0541, val_loss=0.0\u001b[A\n",
      " 77%|████████████████████████████████▎         | 77/100 [01:57<00:35,  1.52s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=4.09, val_loss=6.68, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=4.09, val_loss=6.68, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=33, val_loss=23.6, train_cinde\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=33, val_loss=23.6, train_cinde\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=123, val_loss=2.59, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=123, val_loss=2.59, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=11.9, val_loss=12.9, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 31.52it/s, train_loss=11.9, val_loss=12.9, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.52it/s, train_loss=11.9, val_loss=12.9, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.52it/s, train_loss=60.4, val_loss=1.02, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.52it/s, train_loss=60.4, val_loss=1.02, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.52it/s, train_loss=6.54, val_loss=8.72, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.52it/s, train_loss=6.54, val_loss=8.72, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.52it/s, train_loss=46.1, val_loss=1.94, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.52it/s, train_loss=46.1, val_loss=1.94, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.6363180964308077\n",
      "Its' val AUC : 0.674323773277151\n",
      "Its external  AUC: 0.5487631952103356\n",
      "Curent best Test AUC: 0.7274890419536632\n",
      "Its' val AUC : 0.7114312899279598\n",
      "Its external  AUC: 0.6051677958090437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.52it/s, train_loss=9.31, val_loss=5.57, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 33.17it/s, train_loss=9.31, val_loss=5.57, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.17it/s, train_loss=9.31, val_loss=5.57, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.17it/s, train_loss=30.6, val_loss=2.51, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.17it/s, train_loss=30.6, val_loss=2.51, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.17it/s, train_loss=14.1, val_loss=2.28, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.17it/s, train_loss=14.1, val_loss=2.28, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.17it/s, train_loss=10.3, val_loss=3.96, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.17it/s, train_loss=10.3, val_loss=3.96, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.17it/s, train_loss=19.4, val_loss=1.07, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 33.31it/s, train_loss=19.4, val_loss=1.07,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.31it/s, train_loss=19.4, val_loss=1.07,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.31it/s, train_loss=5.19, val_loss=0.266\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.31it/s, train_loss=5.19, val_loss=0.266\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.31it/s, train_loss=1.36, val_loss=1.9, \u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.31it/s, train_loss=1.36, val_loss=1.9, \u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.31it/s, train_loss=9.21, val_loss=1.62,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.31it/s, train_loss=9.21, val_loss=1.62,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.31it/s, train_loss=7.61, val_loss=0.339\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 32.88it/s, train_loss=7.61, val_loss=0.339\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.88it/s, train_loss=7.61, val_loss=0.339\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.88it/s, train_loss=1.55, val_loss=0.44,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.88it/s, train_loss=1.55, val_loss=0.44,\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    11: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Curent best Test AUC: 0.7352536005009392\n",
      "Its' val AUC : 0.7254315617778986\n",
      "Its external  AUC: 0.5985504962974634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.88it/s, train_loss=2.66, val_loss=1, tr\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.88it/s, train_loss=2.66, val_loss=1, tr\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.88it/s, train_loss=5.73, val_loss=0.808\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.88it/s, train_loss=5.73, val_loss=0.808\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.88it/s, train_loss=4.62, val_loss=0.361\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 32.49it/s, train_loss=4.62, val_loss=0.361\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.49it/s, train_loss=4.62, val_loss=0.361\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.49it/s, train_loss=2.15, val_loss=0.093\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.49it/s, train_loss=2.15, val_loss=0.093\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.49it/s, train_loss=0.54, val_loss=0.24,\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.49it/s, train_loss=0.54, val_loss=0.24,\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.49it/s, train_loss=1.06, val_loss=0.534\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.49it/s, train_loss=1.06, val_loss=0.534\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.49it/s, train_loss=2.43, val_loss=0.583\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 32.60it/s, train_loss=2.43, val_loss=0.583\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 32.60it/s, train_loss=2.43, val_loss=0.583\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 32.60it/s, train_loss=2.7, val_loss=0.329,\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 32.60it/s, train_loss=2.7, val_loss=0.329,\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    19: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 32.60it/s, train_loss=1.52, val_loss=0.067\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 32.60it/s, train_loss=1.52, val_loss=0.067\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 32.60it/s, train_loss=0.303, val_loss=0.05\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 32.60it/s, train_loss=0.303, val_loss=0.05\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 32.60it/s, train_loss=0.289, val_loss=0.21\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 32.85it/s, train_loss=0.289, val_loss=0.21\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 32.85it/s, train_loss=0.289, val_loss=0.21\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 32.85it/s, train_loss=1.15, val_loss=0.321\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 32.85it/s, train_loss=1.15, val_loss=0.321\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 32.85it/s, train_loss=1.65, val_loss=0.238\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 32.85it/s, train_loss=1.65, val_loss=0.238\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 32.85it/s, train_loss=1.19, val_loss=0.089\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 32.85it/s, train_loss=1.19, val_loss=0.089\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 32.85it/s, train_loss=0.415, val_loss=0.05\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 32.72it/s, train_loss=0.415, val_loss=0.05\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 32.72it/s, train_loss=0.415, val_loss=0.05\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:01<00:00, 32.72it/s, train_loss=0.231, val_loss=0.14\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 32.72it/s, train_loss=0.231, val_loss=0.14\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 32.72it/s, train_loss=0.707, val_loss=0.18\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 32.72it/s, train_loss=0.707, val_loss=0.18\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 32.72it/s, train_loss=0.883, val_loss=0.16\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 32.72it/s, train_loss=0.883, val_loss=0.16\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 32.72it/s, train_loss=0.804, val_loss=0.11\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 32.37it/s, train_loss=0.804, val_loss=0.11\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 32.37it/s, train_loss=0.804, val_loss=0.11\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 32.37it/s, train_loss=0.538, val_loss=0.05\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 32.37it/s, train_loss=0.538, val_loss=0.05\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 32.37it/s, train_loss=0.266, val_loss=0.03\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 32.37it/s, train_loss=0.266, val_loss=0.03\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 32.37it/s, train_loss=0.143, val_loss=0.04\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 32.37it/s, train_loss=0.143, val_loss=0.04\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    33: reducing learning rate of group 0 to 1.2500e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 32.37it/s, train_loss=0.198, val_loss=0.07\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 32.56it/s, train_loss=0.198, val_loss=0.07\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 32.56it/s, train_loss=0.198, val_loss=0.07\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 32.56it/s, train_loss=0.339, val_loss=0.09\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 32.56it/s, train_loss=0.339, val_loss=0.09\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 32.56it/s, train_loss=0.432, val_loss=0.08\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 32.56it/s, train_loss=0.432, val_loss=0.08\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 32.56it/s, train_loss=0.4, val_loss=0.0554\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 32.56it/s, train_loss=0.4, val_loss=0.0554\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 32.56it/s, train_loss=0.269, val_loss=0.02\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 32.94it/s, train_loss=0.269, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 32.94it/s, train_loss=0.269, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 32.94it/s, train_loss=0.13, val_loss=0.016\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 32.94it/s, train_loss=0.13, val_loss=0.016\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 32.94it/s, train_loss=0.0703, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 32.94it/s, train_loss=0.0703, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 32.94it/s, train_loss=0.107, val_loss=0.04\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 32.94it/s, train_loss=0.107, val_loss=0.04\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 32.94it/s, train_loss=0.188, val_loss=0.05\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 32.99it/s, train_loss=0.188, val_loss=0.05\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 32.99it/s, train_loss=0.188, val_loss=0.05\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 32.99it/s, train_loss=0.239, val_loss=0.04\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 32.99it/s, train_loss=0.239, val_loss=0.04\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 32.81it/s, train_loss=0.22, val_loss=0.035\u001b[A\n",
      " 78%|████████████████████████████████▊         | 78/100 [01:58<00:33,  1.53s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=8.21, val_loss=66.8, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=8.21, val_loss=66.8, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=382, val_loss=2.21, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=382, val_loss=2.21, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=14.9, val_loss=10.9, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=14.9, val_loss=10.9, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=65.2, val_loss=3.82, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 34.26it/s, train_loss=65.2, val_loss=3.82, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 34.26it/s, train_loss=65.2, val_loss=3.82, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 34.26it/s, train_loss=24.1, val_loss=14.7, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 34.26it/s, train_loss=24.1, val_loss=14.7, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 34.26it/s, train_loss=70.2, val_loss=1.79, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 34.26it/s, train_loss=70.2, val_loss=1.79, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 34.26it/s, train_loss=11.6, val_loss=4.26, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 34.26it/s, train_loss=11.6, val_loss=4.26, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.6523481527864746\n",
      "Its' val AUC : 0.6752752480630692\n",
      "Its external  AUC: 0.6086340003151095\n",
      "Curent best Test AUC: 0.7541640576080151\n",
      "Its' val AUC : 0.7127905396221286\n",
      "Its external  AUC: 0.5734992910036237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 34.26it/s, train_loss=24.6, val_loss=1.51, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 33.89it/s, train_loss=24.6, val_loss=1.51, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.89it/s, train_loss=24.6, val_loss=1.51, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.89it/s, train_loss=10.1, val_loss=4.64, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.89it/s, train_loss=10.1, val_loss=4.64, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.89it/s, train_loss=27, val_loss=1.27, tra\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.89it/s, train_loss=27, val_loss=1.27, tr\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.89it/s, train_loss=8, val_loss=2.35, tra\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.89it/s, train_loss=8, val_loss=2.35, tra\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.89it/s, train_loss=14.1, val_loss=1.1, t\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 33.38it/s, train_loss=14.1, val_loss=1.1, \u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.38it/s, train_loss=14.1, val_loss=1.1, \u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.38it/s, train_loss=7.93, val_loss=1.76,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.38it/s, train_loss=7.93, val_loss=1.76,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.38it/s, train_loss=9.33, val_loss=1.42,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.38it/s, train_loss=9.33, val_loss=1.42,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.38it/s, train_loss=7.42, val_loss=0.413\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.38it/s, train_loss=7.42, val_loss=0.413\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.38it/s, train_loss=2.31, val_loss=1.46,\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=2.31, val_loss=1.46,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=2.31, val_loss=1.46,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=6.72, val_loss=0.498\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=6.72, val_loss=0.498\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=2.62, val_loss=1.18,\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=2.62, val_loss=1.18,\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=7.35, val_loss=0.261\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=7.35, val_loss=0.261\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=1.34, val_loss=0.693\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 33.54it/s, train_loss=1.34, val_loss=0.693\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.54it/s, train_loss=1.34, val_loss=0.693\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.54it/s, train_loss=3.43, val_loss=1.95,\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.54it/s, train_loss=3.43, val_loss=1.95,\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.54it/s, train_loss=9.69, val_loss=0.689\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.54it/s, train_loss=9.69, val_loss=0.689\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.54it/s, train_loss=4.85, val_loss=1.23,\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.54it/s, train_loss=4.85, val_loss=1.23,\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.54it/s, train_loss=7.48, val_loss=0.342\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 33.88it/s, train_loss=7.48, val_loss=0.342\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.88it/s, train_loss=7.48, val_loss=0.342\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.88it/s, train_loss=1.97, val_loss=0.873\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.88it/s, train_loss=1.97, val_loss=0.873\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.88it/s, train_loss=5.02, val_loss=0.22,\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.88it/s, train_loss=5.02, val_loss=0.22,\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.88it/s, train_loss=1.24, val_loss=0.787\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.88it/s, train_loss=1.24, val_loss=0.787\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.88it/s, train_loss=3.61, val_loss=0.857\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.09it/s, train_loss=3.61, val_loss=0.857\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.09it/s, train_loss=3.61, val_loss=0.857\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.09it/s, train_loss=3.94, val_loss=0.156\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.09it/s, train_loss=3.94, val_loss=0.156\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.09it/s, train_loss=0.717, val_loss=0.42\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.09it/s, train_loss=0.717, val_loss=0.42\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.09it/s, train_loss=2.41, val_loss=0.456\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.09it/s, train_loss=2.41, val_loss=0.456\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    25: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.09it/s, train_loss=2.55, val_loss=0.094\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.25it/s, train_loss=2.55, val_loss=0.094\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.25it/s, train_loss=2.55, val_loss=0.094\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.25it/s, train_loss=0.632, val_loss=0.39\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.25it/s, train_loss=0.632, val_loss=0.39\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 34.25it/s, train_loss=2.22, val_loss=0.316\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.25it/s, train_loss=2.22, val_loss=0.316\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.25it/s, train_loss=1.75, val_loss=0.038\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.25it/s, train_loss=1.75, val_loss=0.038\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.25it/s, train_loss=0.293, val_loss=0.29\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 33.92it/s, train_loss=0.293, val_loss=0.29\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.92it/s, train_loss=0.293, val_loss=0.29\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.92it/s, train_loss=1.66, val_loss=0.195\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.92it/s, train_loss=1.66, val_loss=0.195\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.92it/s, train_loss=1.14, val_loss=0.124\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.92it/s, train_loss=1.14, val_loss=0.124\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.92it/s, train_loss=0.606, val_loss=0.34\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.92it/s, train_loss=0.606, val_loss=0.34\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.92it/s, train_loss=1.62, val_loss=0.147\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.11it/s, train_loss=1.62, val_loss=0.147\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.11it/s, train_loss=1.62, val_loss=0.147\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.11it/s, train_loss=0.718, val_loss=0.08\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.11it/s, train_loss=0.718, val_loss=0.08\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.11it/s, train_loss=0.524, val_loss=0.12\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.11it/s, train_loss=0.524, val_loss=0.12\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.11it/s, train_loss=0.768, val_loss=0.07\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.11it/s, train_loss=0.768, val_loss=0.07\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.11it/s, train_loss=0.512, val_loss=0.03\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.18it/s, train_loss=0.512, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.18it/s, train_loss=0.512, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.18it/s, train_loss=0.253, val_loss=0.07\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.18it/s, train_loss=0.253, val_loss=0.07\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.18it/s, train_loss=0.416, val_loss=0.10\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.18it/s, train_loss=0.416, val_loss=0.10\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.18it/s, train_loss=0.573, val_loss=0.07\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.18it/s, train_loss=0.573, val_loss=0.07\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    41: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.18it/s, train_loss=0.376, val_loss=0.03\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 33.80it/s, train_loss=0.376, val_loss=0.03\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 33.80it/s, train_loss=0.376, val_loss=0.03\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 33.80it/s, train_loss=0.191, val_loss=0.05\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 33.80it/s, train_loss=0.191, val_loss=0.05\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 33.85it/s, train_loss=0.321, val_loss=0.06\u001b[A\n",
      " 79%|█████████████████████████████████▏        | 79/100 [02:00<00:31,  1.52s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=17.9, val_loss=33.9, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=17.9, val_loss=33.9, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=209, val_loss=5.45, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=209, val_loss=5.45, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=28.4, val_loss=4.54, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=28.4, val_loss=4.54, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=26.2, val_loss=2.57, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.26it/s, train_loss=26.2, val_loss=2.57, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.26it/s, train_loss=26.2, val_loss=2.57, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.26it/s, train_loss=13.5, val_loss=1.45, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.26it/s, train_loss=13.5, val_loss=1.45, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.26it/s, train_loss=6.91, val_loss=0.437, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.26it/s, train_loss=6.91, val_loss=0.437, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.26it/s, train_loss=2.86, val_loss=1.22, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.26it/s, train_loss=2.86, val_loss=1.22, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.5968691296180338\n",
      "Its' val AUC : 0.6551583525893707\n",
      "Its external  AUC: 0.5853158972743028\n",
      "Curent best Test AUC: 0.7128365685660614\n",
      "Its' val AUC : 0.6971591681391872\n",
      "Its external  AUC: 0.4120056719710099\n",
      "Curent best Test AUC: 0.7144646211646838\n",
      "Its' val AUC : 0.7130623895609624\n",
      "Its external  AUC: 0.46502284543878997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.26it/s, train_loss=7.32, val_loss=0.783, \u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 32.88it/s, train_loss=7.32, val_loss=0.783, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.88it/s, train_loss=7.32, val_loss=0.783, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.88it/s, train_loss=5.41, val_loss=1.03, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.88it/s, train_loss=5.41, val_loss=1.03, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.88it/s, train_loss=6.05, val_loss=0.219, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.88it/s, train_loss=6.05, val_loss=0.219,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.88it/s, train_loss=1.63, val_loss=0.509,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.88it/s, train_loss=1.63, val_loss=0.509,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.88it/s, train_loss=3.57, val_loss=0.571,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 32.90it/s, train_loss=3.57, val_loss=0.571\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.90it/s, train_loss=3.57, val_loss=0.571\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.90it/s, train_loss=2.9, val_loss=0.598,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.90it/s, train_loss=2.9, val_loss=0.598,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.90it/s, train_loss=2.95, val_loss=0.261\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.90it/s, train_loss=2.95, val_loss=0.261\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.90it/s, train_loss=1.64, val_loss=0.241\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.90it/s, train_loss=1.64, val_loss=0.241\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.90it/s, train_loss=1.23, val_loss=0.381\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 33.16it/s, train_loss=1.23, val_loss=0.381\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.16it/s, train_loss=1.23, val_loss=0.381\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.16it/s, train_loss=2.48, val_loss=0.22,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.16it/s, train_loss=2.48, val_loss=0.22,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.16it/s, train_loss=1.5, val_loss=0.198,\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.16it/s, train_loss=1.5, val_loss=0.198,\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.16it/s, train_loss=0.989, val_loss=0.28\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.16it/s, train_loss=0.989, val_loss=0.28\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7382592360676268\n",
      "Its' val AUC : 0.7320918852793258\n",
      "Its external  AUC: 0.5323775011816606\n",
      "Epoch    16: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.16it/s, train_loss=1.35, val_loss=0.063\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 33.69it/s, train_loss=1.35, val_loss=0.063\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.69it/s, train_loss=1.35, val_loss=0.063\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.69it/s, train_loss=0.337, val_loss=0.13\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.69it/s, train_loss=0.337, val_loss=0.13\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.69it/s, train_loss=0.747, val_loss=0.22\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.69it/s, train_loss=0.747, val_loss=0.22\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.69it/s, train_loss=1.17, val_loss=0.096\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.69it/s, train_loss=1.17, val_loss=0.096\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.69it/s, train_loss=0.613, val_loss=0.15\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 33.52it/s, train_loss=0.613, val_loss=0.15\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.52it/s, train_loss=0.613, val_loss=0.15\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.52it/s, train_loss=1.02, val_loss=0.090\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.52it/s, train_loss=1.02, val_loss=0.090\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.52it/s, train_loss=0.588, val_loss=0.04\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.52it/s, train_loss=0.588, val_loss=0.04\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.52it/s, train_loss=0.273, val_loss=0.12\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.52it/s, train_loss=0.273, val_loss=0.12\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7728240450845335\n",
      "Its' val AUC : 0.6697023243169771\n",
      "Its external  AUC: 0.5766503860091382\n",
      "Curent best Test AUC: 0.7772072636192862\n",
      "Its' val AUC : 0.6837025961669159\n",
      "Its external  AUC: 0.6043800220576651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.52it/s, train_loss=0.698, val_loss=0.05\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 33.81it/s, train_loss=0.698, val_loss=0.05\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.81it/s, train_loss=0.698, val_loss=0.05\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.81it/s, train_loss=0.331, val_loss=0.11\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.81it/s, train_loss=0.331, val_loss=0.11\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.81it/s, train_loss=0.616, val_loss=0.09\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.81it/s, train_loss=0.616, val_loss=0.09\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.81it/s, train_loss=0.544, val_loss=0.03\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.81it/s, train_loss=0.544, val_loss=0.03\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.81it/s, train_loss=0.224, val_loss=0.07\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.09it/s, train_loss=0.224, val_loss=0.07\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.09it/s, train_loss=0.224, val_loss=0.07\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.09it/s, train_loss=0.414, val_loss=0.02\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.09it/s, train_loss=0.414, val_loss=0.02\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 34.09it/s, train_loss=0.131, val_loss=0.05\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.09it/s, train_loss=0.131, val_loss=0.05\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.09it/s, train_loss=0.314, val_loss=0.05\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.09it/s, train_loss=0.314, val_loss=0.05\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.09it/s, train_loss=0.326, val_loss=0.03\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.19it/s, train_loss=0.326, val_loss=0.03\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.19it/s, train_loss=0.326, val_loss=0.03\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.19it/s, train_loss=0.189, val_loss=0.05\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.19it/s, train_loss=0.189, val_loss=0.05\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.19it/s, train_loss=0.296, val_loss=0.01\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.19it/s, train_loss=0.296, val_loss=0.01\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.19it/s, train_loss=0.0882, val_loss=0.0\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.19it/s, train_loss=0.0882, val_loss=0.0\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.19it/s, train_loss=0.219, val_loss=0.04\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.35it/s, train_loss=0.219, val_loss=0.04\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.35it/s, train_loss=0.219, val_loss=0.04\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.35it/s, train_loss=0.189, val_loss=0.03\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.35it/s, train_loss=0.189, val_loss=0.03\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.35it/s, train_loss=0.186, val_loss=0.03\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.35it/s, train_loss=0.186, val_loss=0.03\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.35it/s, train_loss=0.221, val_loss=0.01\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.35it/s, train_loss=0.221, val_loss=0.01\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.35it/s, train_loss=0.0924, val_loss=0.0\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.40it/s, train_loss=0.0924, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.40it/s, train_loss=0.0924, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.40it/s, train_loss=0.168, val_loss=0.02\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.40it/s, train_loss=0.168, val_loss=0.02\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.40it/s, train_loss=0.115, val_loss=0.01\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.40it/s, train_loss=0.115, val_loss=0.01\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.40it/s, train_loss=0.0864, val_loss=0.0\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.40it/s, train_loss=0.0864, val_loss=0.0\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.40it/s, train_loss=0.143, val_loss=0.02\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.07it/s, train_loss=0.143, val_loss=0.02\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.07it/s, train_loss=0.143, val_loss=0.02\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.07it/s, train_loss=0.134, val_loss=0.01\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.07it/s, train_loss=0.134, val_loss=0.01\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 33.86it/s, train_loss=0.0812, val_loss=0.0\u001b[A\n",
      " 80%|█████████████████████████████████▌        | 80/100 [02:01<00:30,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    44: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=1.55, val_loss=33, train_cinde\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=1.55, val_loss=33, train_cinde\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=173, val_loss=3.41, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=173, val_loss=3.41, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=22.7, val_loss=8.87, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=22.7, val_loss=8.87, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=45.5, val_loss=1.19, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 32.31it/s, train_loss=45.5, val_loss=1.19, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.31it/s, train_loss=45.5, val_loss=1.19, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.31it/s, train_loss=6.7, val_loss=2.21, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.31it/s, train_loss=6.7, val_loss=2.21, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.31it/s, train_loss=10.7, val_loss=1.2, tr\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.31it/s, train_loss=10.7, val_loss=1.2, tr\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.31it/s, train_loss=5.96, val_loss=1.03, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.31it/s, train_loss=5.96, val_loss=1.03, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.28027551659361305\n",
      "Its' val AUC : 0.3047437814326492\n",
      "Its external  AUC: 0.4003466204506066\n",
      "Curent best Test AUC: 0.4681277395115842\n",
      "Its' val AUC : 0.4845725159711839\n",
      "Its external  AUC: 0.44225618402394834\n",
      "Curent best Test AUC: 0.7217282404508454\n",
      "Its' val AUC : 0.7053146663042\n",
      "Its external  AUC: 0.6176146210808255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.31it/s, train_loss=5.22, val_loss=0.116, \u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 33.91it/s, train_loss=5.22, val_loss=0.116, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.91it/s, train_loss=5.22, val_loss=0.116, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.91it/s, train_loss=0.671, val_loss=0.563,\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.91it/s, train_loss=0.671, val_loss=0.563,\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.91it/s, train_loss=3.12, val_loss=0.735, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.91it/s, train_loss=3.12, val_loss=0.735,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.91it/s, train_loss=3.74, val_loss=0.394,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.91it/s, train_loss=3.74, val_loss=0.394,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.91it/s, train_loss=1.92, val_loss=0.077,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.28it/s, train_loss=1.92, val_loss=0.077\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.28it/s, train_loss=1.92, val_loss=0.077\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.28it/s, train_loss=0.371, val_loss=0.12\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.28it/s, train_loss=0.371, val_loss=0.12\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.28it/s, train_loss=0.778, val_loss=0.04\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.28it/s, train_loss=0.778, val_loss=0.04\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.28it/s, train_loss=0.327, val_loss=0.03\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.28it/s, train_loss=0.327, val_loss=0.03\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.28it/s, train_loss=0.168, val_loss=0.03\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.02it/s, train_loss=0.168, val_loss=0.03\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.02it/s, train_loss=0.168, val_loss=0.03\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.02it/s, train_loss=0.149, val_loss=0.02\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.02it/s, train_loss=0.149, val_loss=0.02\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.02it/s, train_loss=0.126, val_loss=0.02\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.02it/s, train_loss=0.126, val_loss=0.02\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.02it/s, train_loss=0.1, val_loss=0.016,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.02it/s, train_loss=0.1, val_loss=0.016,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.02it/s, train_loss=0.0754, val_loss=0.0\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 32.90it/s, train_loss=0.0754, val_loss=0.0\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.90it/s, train_loss=0.0754, val_loss=0.0\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.90it/s, train_loss=0.0533, val_loss=0.0\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.90it/s, train_loss=0.0533, val_loss=0.0\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7380087664370695\n",
      "Its' val AUC : 0.7190430882153052\n",
      "Its external  AUC: 0.6005987080510478\n",
      "Curent best Test AUC: 0.739261114589856\n",
      "Its' val AUC : 0.7183634633682208\n",
      "Its external  AUC: 0.6001260438002206\n",
      "Curent best Test AUC: 0.7442705072010019\n",
      "Its' val AUC : 0.7176838385211364\n",
      "Its external  AUC: 0.5991807152985662\n",
      "Curent best Test AUC: 0.7465247338760176\n",
      "Its' val AUC : 0.7175479135517194\n",
      "Its external  AUC: 0.600441153300772\n",
      "Curent best Test AUC: 0.7495303694427051\n",
      "Its' val AUC : 0.7172760636128857\n",
      "Its external  AUC: 0.6001260438002206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.90it/s, train_loss=0.0353, val_loss=0.0\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.90it/s, train_loss=0.0353, val_loss=0.0\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.90it/s, train_loss=0.0221, val_loss=0.0\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.90it/s, train_loss=0.0221, val_loss=0.0\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.90it/s, train_loss=0.0135, val_loss=0.0\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 32.88it/s, train_loss=0.0135, val_loss=0.0\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 32.88it/s, train_loss=0.0135, val_loss=0.0\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 32.88it/s, train_loss=0.00914, val_loss=0.\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 32.88it/s, train_loss=0.00914, val_loss=0.\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 32.88it/s, train_loss=0.00859, val_loss=0.\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 32.88it/s, train_loss=0.00859, val_loss=0.\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 32.88it/s, train_loss=0.0113, val_loss=0.0\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 32.88it/s, train_loss=0.0113, val_loss=0.0\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 32.88it/s, train_loss=0.0157, val_loss=0.0\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 33.32it/s, train_loss=0.0157, val_loss=0.0\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.32it/s, train_loss=0.0157, val_loss=0.0\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.32it/s, train_loss=0.0211, val_loss=0.0\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.32it/s, train_loss=0.0211, val_loss=0.0\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.32it/s, train_loss=0.0257, val_loss=0.0\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.32it/s, train_loss=0.0257, val_loss=0.0\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.32it/s, train_loss=0.0289, val_loss=0.0\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.32it/s, train_loss=0.0289, val_loss=0.0\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.32it/s, train_loss=0.0311, val_loss=0.0\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 33.20it/s, train_loss=0.0311, val_loss=0.0\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 33.20it/s, train_loss=0.0311, val_loss=0.0\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 33.20it/s, train_loss=0.0314, val_loss=0.0\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 33.20it/s, train_loss=0.0314, val_loss=0.0\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 33.20it/s, train_loss=0.0305, val_loss=0.0\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 33.20it/s, train_loss=0.0305, val_loss=0.0\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 33.20it/s, train_loss=0.029, val_loss=0.00\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 33.20it/s, train_loss=0.029, val_loss=0.00\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 33.20it/s, train_loss=0.027, val_loss=0.00\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 33.70it/s, train_loss=0.027, val_loss=0.00\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.70it/s, train_loss=0.027, val_loss=0.00\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.70it/s, train_loss=0.0242, val_loss=0.0\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.70it/s, train_loss=0.0242, val_loss=0.0\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    31: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch    37: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.70it/s, train_loss=0.0212, val_loss=0.0\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.70it/s, train_loss=0.0212, val_loss=0.0\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.70it/s, train_loss=0.0198, val_loss=0.0\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.70it/s, train_loss=0.0198, val_loss=0.0\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.70it/s, train_loss=0.0181, val_loss=0.0\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.08it/s, train_loss=0.0181, val_loss=0.0\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.08it/s, train_loss=0.0181, val_loss=0.0\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.08it/s, train_loss=0.0163, val_loss=0.0\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.08it/s, train_loss=0.0163, val_loss=0.0\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.08it/s, train_loss=0.0143, val_loss=0.0\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.08it/s, train_loss=0.0143, val_loss=0.0\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.08it/s, train_loss=0.0121, val_loss=0.0\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.08it/s, train_loss=0.0121, val_loss=0.0\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.08it/s, train_loss=0.0103, val_loss=0.0\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.31it/s, train_loss=0.0103, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.31it/s, train_loss=0.0103, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.31it/s, train_loss=0.00997, val_loss=0.\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.31it/s, train_loss=0.00997, val_loss=0.\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.31it/s, train_loss=0.00986, val_loss=0.\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.31it/s, train_loss=0.00986, val_loss=0.\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.31it/s, train_loss=0.00922, val_loss=0.\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.31it/s, train_loss=0.00922, val_loss=0.\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.31it/s, train_loss=0.00863, val_loss=0.\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.42it/s, train_loss=0.00863, val_loss=0.\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.42it/s, train_loss=0.00863, val_loss=0.\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.42it/s, train_loss=0.00798, val_loss=0.\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.42it/s, train_loss=0.00798, val_loss=0.\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    43: reducing learning rate of group 0 to 1.2500e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 33.84it/s, train_loss=0.00771, val_loss=0.\u001b[A\n",
      " 81%|██████████████████████████████████        | 81/100 [02:03<00:28,  1.51s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=60.2, val_loss=117, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=60.2, val_loss=117, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=645, val_loss=2.77, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=645, val_loss=2.77, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=14.2, val_loss=1.36, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=14.2, val_loss=1.36, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=6.54, val_loss=3.4, train_cind\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 32.85it/s, train_loss=6.54, val_loss=3.4, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.85it/s, train_loss=6.54, val_loss=3.4, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.85it/s, train_loss=18.5, val_loss=2.19, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.85it/s, train_loss=18.5, val_loss=2.19, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.85it/s, train_loss=10.3, val_loss=1.55, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.85it/s, train_loss=10.3, val_loss=1.55, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.85it/s, train_loss=9.22, val_loss=0.163, \u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.85it/s, train_loss=9.22, val_loss=0.163, \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.28202880400751407\n",
      "Its' val AUC : 0.2847628109283675\n",
      "Its external  AUC: 0.3893177879313061\n",
      "Curent best Test AUC: 0.2968065122103945\n",
      "Its' val AUC : 0.30623895609623486\n",
      "Its external  AUC: 0.4389475342681582\n",
      "Curent best Test AUC: 0.5693174702567314\n",
      "Its' val AUC : 0.609895337773549\n",
      "Its external  AUC: 0.6466046951315583\n",
      "Curent best Test AUC: 0.7277395115842205\n",
      "Its' val AUC : 0.6872366453717548\n",
      "Its external  AUC: 0.624704584843233\n",
      "Curent best Test AUC: 0.7400125234815279\n",
      "Its' val AUC : 0.7039554166100312\n",
      "Its external  AUC: 0.6306916653537105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.85it/s, train_loss=1.12, val_loss=3.18, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 33.69it/s, train_loss=1.12, val_loss=3.18, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.69it/s, train_loss=1.12, val_loss=3.18, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.69it/s, train_loss=13.2, val_loss=1.36, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.69it/s, train_loss=13.2, val_loss=1.36, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.69it/s, train_loss=8.17, val_loss=0.996, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.69it/s, train_loss=8.17, val_loss=0.996,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.69it/s, train_loss=5.92, val_loss=1.17, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.69it/s, train_loss=5.92, val_loss=1.17, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.69it/s, train_loss=5.51, val_loss=0.827,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.07it/s, train_loss=5.51, val_loss=0.827\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.07it/s, train_loss=5.51, val_loss=0.827\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.07it/s, train_loss=4.05, val_loss=0.566\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.07it/s, train_loss=4.05, val_loss=0.566\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.07it/s, train_loss=3.22, val_loss=0.453\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.07it/s, train_loss=3.22, val_loss=0.453\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.07it/s, train_loss=2.52, val_loss=0.117\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.07it/s, train_loss=2.52, val_loss=0.117\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.07it/s, train_loss=0.775, val_loss=0.41\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.20it/s, train_loss=0.775, val_loss=0.41\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.20it/s, train_loss=0.775, val_loss=0.41\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.20it/s, train_loss=2.25, val_loss=0.316\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.20it/s, train_loss=2.25, val_loss=0.316\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.20it/s, train_loss=1.75, val_loss=0.095\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.20it/s, train_loss=1.75, val_loss=0.095\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.20it/s, train_loss=0.68, val_loss=0.311\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.20it/s, train_loss=0.68, val_loss=0.311\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    13: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.20it/s, train_loss=1.82, val_loss=0.192\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.04it/s, train_loss=1.82, val_loss=0.192\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.04it/s, train_loss=1.82, val_loss=0.192\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.04it/s, train_loss=1.22, val_loss=0.084\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.04it/s, train_loss=1.22, val_loss=0.084\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.04it/s, train_loss=0.549, val_loss=0.27\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.04it/s, train_loss=0.549, val_loss=0.27\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.04it/s, train_loss=1.42, val_loss=0.111\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.04it/s, train_loss=1.42, val_loss=0.111\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.04it/s, train_loss=0.626, val_loss=0.07\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 33.79it/s, train_loss=0.626, val_loss=0.07\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.79it/s, train_loss=0.626, val_loss=0.07\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.79it/s, train_loss=0.427, val_loss=0.20\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.79it/s, train_loss=0.427, val_loss=0.20\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.79it/s, train_loss=1.03, val_loss=0.077\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.79it/s, train_loss=1.03, val_loss=0.077\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.79it/s, train_loss=0.376, val_loss=0.11\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.79it/s, train_loss=0.376, val_loss=0.11\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.79it/s, train_loss=0.662, val_loss=0.13\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.07it/s, train_loss=0.662, val_loss=0.13\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.07it/s, train_loss=0.662, val_loss=0.13\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.07it/s, train_loss=0.761, val_loss=0.06\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.07it/s, train_loss=0.761, val_loss=0.06\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.07it/s, train_loss=0.322, val_loss=0.13\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.07it/s, train_loss=0.322, val_loss=0.13\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.07it/s, train_loss=0.707, val_loss=0.07\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.07it/s, train_loss=0.707, val_loss=0.07\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.07it/s, train_loss=0.422, val_loss=0.06\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.29it/s, train_loss=0.422, val_loss=0.06\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.29it/s, train_loss=0.422, val_loss=0.06\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.29it/s, train_loss=0.348, val_loss=0.09\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.29it/s, train_loss=0.348, val_loss=0.09\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.29it/s, train_loss=0.481, val_loss=0.03\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.29it/s, train_loss=0.481, val_loss=0.03\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.29it/s, train_loss=0.194, val_loss=0.06\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.29it/s, train_loss=0.194, val_loss=0.06\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.29it/s, train_loss=0.382, val_loss=0.05\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.12it/s, train_loss=0.382, val_loss=0.05\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.12it/s, train_loss=0.382, val_loss=0.05\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.12it/s, train_loss=0.284, val_loss=0.04\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.12it/s, train_loss=0.284, val_loss=0.04\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.12it/s, train_loss=0.211, val_loss=0.06\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.12it/s, train_loss=0.211, val_loss=0.06\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.12it/s, train_loss=0.318, val_loss=0.02\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.12it/s, train_loss=0.318, val_loss=0.02\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.12it/s, train_loss=0.146, val_loss=0.04\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.04it/s, train_loss=0.146, val_loss=0.04\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.04it/s, train_loss=0.146, val_loss=0.04\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.04it/s, train_loss=0.242, val_loss=0.02\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.04it/s, train_loss=0.242, val_loss=0.02\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.04it/s, train_loss=0.173, val_loss=0.02\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.04it/s, train_loss=0.173, val_loss=0.02\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.04it/s, train_loss=0.151, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.04it/s, train_loss=0.151, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.04it/s, train_loss=0.223, val_loss=0.02\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.09it/s, train_loss=0.223, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.09it/s, train_loss=0.223, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.09it/s, train_loss=0.135, val_loss=0.04\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.09it/s, train_loss=0.135, val_loss=0.04\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.09it/s, train_loss=0.211, val_loss=0.02\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.09it/s, train_loss=0.211, val_loss=0.02\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.09it/s, train_loss=0.14, val_loss=0.028\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.09it/s, train_loss=0.14, val_loss=0.028\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.09it/s, train_loss=0.153, val_loss=0.02\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.17it/s, train_loss=0.153, val_loss=0.02\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.17it/s, train_loss=0.153, val_loss=0.02\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.17it/s, train_loss=0.154, val_loss=0.01\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.17it/s, train_loss=0.154, val_loss=0.01\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.05it/s, train_loss=0.114, val_loss=0.02\u001b[A\n",
      " 82%|██████████████████████████████████▍       | 82/100 [02:04<00:27,  1.50s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=10.7, val_loss=90.5, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=10.7, val_loss=90.5, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=473, val_loss=31.8, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=473, val_loss=31.8, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=165, val_loss=6.43, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=165, val_loss=6.43, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=32.3, val_loss=11.2, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.28it/s, train_loss=32.3, val_loss=11.2, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.28it/s, train_loss=32.3, val_loss=11.2, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.28it/s, train_loss=64.2, val_loss=7.93, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.28it/s, train_loss=64.2, val_loss=7.93, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.28it/s, train_loss=46.9, val_loss=6.34, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.28it/s, train_loss=46.9, val_loss=6.34, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.28it/s, train_loss=31.4, val_loss=6.49, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.28it/s, train_loss=31.4, val_loss=6.49, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.2498434564809017\n",
      "Its' val AUC : 0.28992795976620905\n",
      "Its external  AUC: 0.3830155979202773\n",
      "Curent best Test AUC: 0.6925485284909205\n",
      "Its' val AUC : 0.7259752616555661\n",
      "Its external  AUC: 0.6007562628013234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.28it/s, train_loss=33.2, val_loss=2.53, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 33.40it/s, train_loss=33.2, val_loss=2.53, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.40it/s, train_loss=33.2, val_loss=2.53, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.40it/s, train_loss=16.5, val_loss=2.7, tr\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.40it/s, train_loss=16.5, val_loss=2.7, tr\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.40it/s, train_loss=15.9, val_loss=2.48, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.40it/s, train_loss=15.9, val_loss=2.48, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.40it/s, train_loss=14.1, val_loss=2.81, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.40it/s, train_loss=14.1, val_loss=2.81, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.40it/s, train_loss=16.9, val_loss=0.685,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 33.78it/s, train_loss=16.9, val_loss=0.685\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.78it/s, train_loss=16.9, val_loss=0.685\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.78it/s, train_loss=3.35, val_loss=0.615\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.78it/s, train_loss=3.35, val_loss=0.615\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.78it/s, train_loss=3.52, val_loss=0.581\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.78it/s, train_loss=3.52, val_loss=0.581\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.78it/s, train_loss=3.4, val_loss=0.636,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.78it/s, train_loss=3.4, val_loss=0.636,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.78it/s, train_loss=3.51, val_loss=0.295\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 33.35it/s, train_loss=3.51, val_loss=0.295\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.35it/s, train_loss=3.51, val_loss=0.295\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.35it/s, train_loss=2.39, val_loss=0.368\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.35it/s, train_loss=2.39, val_loss=0.368\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.35it/s, train_loss=2.96, val_loss=0.512\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.35it/s, train_loss=2.96, val_loss=0.512\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.6966812773951159\n",
      "Its' val AUC : 0.6952562185673509\n",
      "Its external  AUC: 0.6113124310697967\n",
      "Curent best Test AUC: 0.7345021916092673\n",
      "Its' val AUC : 0.6963436183226859\n",
      "Its external  AUC: 0.6195052780841342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.35it/s, train_loss=2.55, val_loss=0.609\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.35it/s, train_loss=2.55, val_loss=0.609\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.35it/s, train_loss=2.87, val_loss=0.257\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 33.03it/s, train_loss=2.87, val_loss=0.257\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.03it/s, train_loss=2.87, val_loss=0.257\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.03it/s, train_loss=1.89, val_loss=0.332\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.03it/s, train_loss=1.89, val_loss=0.332\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.03it/s, train_loss=2.35, val_loss=0.227\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.03it/s, train_loss=2.35, val_loss=0.227\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.03it/s, train_loss=1.51, val_loss=0.275\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.03it/s, train_loss=1.51, val_loss=0.275\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.03it/s, train_loss=1.76, val_loss=0.196\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 32.48it/s, train_loss=1.76, val_loss=0.196\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 32.48it/s, train_loss=1.76, val_loss=0.196\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 32.48it/s, train_loss=1.08, val_loss=0.277\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 32.48it/s, train_loss=1.08, val_loss=0.277\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 32.48it/s, train_loss=1.3, val_loss=0.222,\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 32.48it/s, train_loss=1.3, val_loss=0.222,\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 32.48it/s, train_loss=1.2, val_loss=0.192,\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 32.48it/s, train_loss=1.2, val_loss=0.192,\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 32.48it/s, train_loss=1.05, val_loss=0.206\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 32.49it/s, train_loss=1.05, val_loss=0.206\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 32.49it/s, train_loss=1.05, val_loss=0.206\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 32.49it/s, train_loss=0.947, val_loss=0.10\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 32.49it/s, train_loss=0.947, val_loss=0.10\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 32.49it/s, train_loss=0.595, val_loss=0.13\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 32.49it/s, train_loss=0.595, val_loss=0.13\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 32.49it/s, train_loss=0.935, val_loss=0.08\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 32.49it/s, train_loss=0.935, val_loss=0.08\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 32.49it/s, train_loss=0.633, val_loss=0.15\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 32.82it/s, train_loss=0.633, val_loss=0.15\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 32.82it/s, train_loss=0.633, val_loss=0.15\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:01<00:00, 32.82it/s, train_loss=1.01, val_loss=0.109\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 32.82it/s, train_loss=1.01, val_loss=0.109\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 32.82it/s, train_loss=0.674, val_loss=0.17\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 32.82it/s, train_loss=0.674, val_loss=0.17\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 32.82it/s, train_loss=0.909, val_loss=0.09\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 32.82it/s, train_loss=0.909, val_loss=0.09\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 32.82it/s, train_loss=0.649, val_loss=0.06\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 32.36it/s, train_loss=0.649, val_loss=0.06\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 32.36it/s, train_loss=0.649, val_loss=0.06\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 32.36it/s, train_loss=0.445, val_loss=0.07\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 32.36it/s, train_loss=0.445, val_loss=0.07\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 32.36it/s, train_loss=0.466, val_loss=0.03\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 32.36it/s, train_loss=0.466, val_loss=0.03\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 32.36it/s, train_loss=0.265, val_loss=0.07\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 32.36it/s, train_loss=0.265, val_loss=0.07\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 32.36it/s, train_loss=0.405, val_loss=0.05\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 32.40it/s, train_loss=0.405, val_loss=0.05\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 32.40it/s, train_loss=0.405, val_loss=0.05\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 32.40it/s, train_loss=0.306, val_loss=0.06\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 32.40it/s, train_loss=0.306, val_loss=0.06\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 32.40it/s, train_loss=0.354, val_loss=0.06\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 32.40it/s, train_loss=0.354, val_loss=0.06\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 32.40it/s, train_loss=0.355, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 32.40it/s, train_loss=0.355, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 32.40it/s, train_loss=0.246, val_loss=0.05\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 32.48it/s, train_loss=0.246, val_loss=0.05\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 32.48it/s, train_loss=0.246, val_loss=0.05\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 32.48it/s, train_loss=0.327, val_loss=0.04\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 32.48it/s, train_loss=0.327, val_loss=0.04\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 32.48it/s, train_loss=0.252, val_loss=0.04\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 32.48it/s, train_loss=0.252, val_loss=0.04\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 32.48it/s, train_loss=0.289, val_loss=0.03\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 32.48it/s, train_loss=0.289, val_loss=0.03\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 32.48it/s, train_loss=0.233, val_loss=0.03\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 32.78it/s, train_loss=0.233, val_loss=0.03\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 32.78it/s, train_loss=0.233, val_loss=0.03\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 32.78it/s, train_loss=0.22, val_loss=0.042\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 32.78it/s, train_loss=0.22, val_loss=0.042\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 32.86it/s, train_loss=0.226, val_loss=0.02\u001b[A\n",
      " 83%|██████████████████████████████████▊       | 83/100 [02:06<00:25,  1.51s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    44: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=11.4, val_loss=60.9, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=11.4, val_loss=60.9, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=338, val_loss=105, train_cinde\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=338, val_loss=105, train_cinde\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=563, val_loss=7.6, train_cinde\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=563, val_loss=7.6, train_cinde\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=37.3, val_loss=7.32, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.49it/s, train_loss=37.3, val_loss=7.32, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.49it/s, train_loss=37.3, val_loss=7.32, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.49it/s, train_loss=36.4, val_loss=2.53, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.49it/s, train_loss=36.4, val_loss=2.53, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.49it/s, train_loss=14.8, val_loss=8.43, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.49it/s, train_loss=14.8, val_loss=8.43, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.49it/s, train_loss=52.5, val_loss=4.01, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.49it/s, train_loss=52.5, val_loss=4.01, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.3709455228553538\n",
      "Its' val AUC : 0.36658964251733045\n",
      "Its external  AUC: 0.42650070899637627\n",
      "Curent best Test AUC: 0.720100187852223\n",
      "Its' val AUC : 0.7201304879706402\n",
      "Its external  AUC: 0.6147786355758627\n",
      "Curent best Test AUC: 0.7521603005635566\n",
      "Its' val AUC : 0.6445562049748539\n",
      "Its external  AUC: 0.648968016385694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.49it/s, train_loss=19.5, val_loss=0.923, \u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 33.80it/s, train_loss=19.5, val_loss=0.923, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.80it/s, train_loss=19.5, val_loss=0.923, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.80it/s, train_loss=4.88, val_loss=0.807, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.80it/s, train_loss=4.88, val_loss=0.807, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.80it/s, train_loss=4.77, val_loss=1.18, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.80it/s, train_loss=4.77, val_loss=1.18, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.80it/s, train_loss=7.27, val_loss=1.32, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.80it/s, train_loss=7.27, val_loss=1.32, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.80it/s, train_loss=7.23, val_loss=1.01, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.28it/s, train_loss=7.23, val_loss=1.01,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.28it/s, train_loss=7.23, val_loss=1.01,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.28it/s, train_loss=4.75, val_loss=1.07,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.28it/s, train_loss=4.75, val_loss=1.07,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.28it/s, train_loss=5.04, val_loss=1.04,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.28it/s, train_loss=5.04, val_loss=1.04,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.28it/s, train_loss=4.55, val_loss=0.473\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.28it/s, train_loss=4.55, val_loss=0.473\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.28it/s, train_loss=1.8, val_loss=0.387,\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.53it/s, train_loss=1.8, val_loss=0.387,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.53it/s, train_loss=1.8, val_loss=0.387,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.53it/s, train_loss=1.54, val_loss=0.469\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.53it/s, train_loss=1.54, val_loss=0.469\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.53it/s, train_loss=2.79, val_loss=0.623\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.53it/s, train_loss=2.79, val_loss=0.623\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.53it/s, train_loss=3.01, val_loss=0.353\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.53it/s, train_loss=3.01, val_loss=0.353\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.53it/s, train_loss=1.86, val_loss=0.289\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.58it/s, train_loss=1.86, val_loss=0.289\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.58it/s, train_loss=1.86, val_loss=0.289\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.58it/s, train_loss=1.33, val_loss=0.344\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.58it/s, train_loss=1.33, val_loss=0.344\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.58it/s, train_loss=1.66, val_loss=0.429\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.58it/s, train_loss=1.66, val_loss=0.429\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.58it/s, train_loss=1.97, val_loss=0.354\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.58it/s, train_loss=1.97, val_loss=0.354\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.58it/s, train_loss=1.98, val_loss=0.361\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.64it/s, train_loss=1.98, val_loss=0.361\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.64it/s, train_loss=1.98, val_loss=0.361\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.64it/s, train_loss=2, val_loss=0.318, t\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.64it/s, train_loss=2, val_loss=0.318, t\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.64it/s, train_loss=1.91, val_loss=0.222\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.64it/s, train_loss=1.91, val_loss=0.222\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.64it/s, train_loss=1.2, val_loss=0.172,\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.64it/s, train_loss=1.2, val_loss=0.172,\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.64it/s, train_loss=0.86, val_loss=0.199\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.70it/s, train_loss=0.86, val_loss=0.199\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.70it/s, train_loss=0.86, val_loss=0.199\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.70it/s, train_loss=0.864, val_loss=0.16\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.70it/s, train_loss=0.864, val_loss=0.16\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.70it/s, train_loss=0.725, val_loss=0.12\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.70it/s, train_loss=0.725, val_loss=0.12\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.70it/s, train_loss=0.561, val_loss=0.12\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.70it/s, train_loss=0.561, val_loss=0.12\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.70it/s, train_loss=0.641, val_loss=0.12\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.76it/s, train_loss=0.641, val_loss=0.12\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.76it/s, train_loss=0.641, val_loss=0.12\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.76it/s, train_loss=0.692, val_loss=0.11\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.76it/s, train_loss=0.692, val_loss=0.11\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.76it/s, train_loss=0.587, val_loss=0.12\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.76it/s, train_loss=0.587, val_loss=0.12\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.76it/s, train_loss=0.588, val_loss=0.14\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.76it/s, train_loss=0.588, val_loss=0.14\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.76it/s, train_loss=0.694, val_loss=0.15\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.74it/s, train_loss=0.694, val_loss=0.15\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.74it/s, train_loss=0.694, val_loss=0.15\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.74it/s, train_loss=0.766, val_loss=0.26\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.74it/s, train_loss=0.766, val_loss=0.26\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.74it/s, train_loss=1.34, val_loss=0.959\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.74it/s, train_loss=1.34, val_loss=0.959\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.74it/s, train_loss=4.81, val_loss=3.34,\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.74it/s, train_loss=4.81, val_loss=3.34,\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.74it/s, train_loss=17.4, val_loss=0.533\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.40it/s, train_loss=17.4, val_loss=0.533\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.40it/s, train_loss=17.4, val_loss=0.533\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.40it/s, train_loss=2.63, val_loss=2.75,\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.40it/s, train_loss=2.63, val_loss=2.75,\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.40it/s, train_loss=14.2, val_loss=0.113\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.40it/s, train_loss=14.2, val_loss=0.113\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.40it/s, train_loss=0.463, val_loss=2.68\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.40it/s, train_loss=0.463, val_loss=2.68\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.40it/s, train_loss=13.2, val_loss=0.105\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.04it/s, train_loss=13.2, val_loss=0.105\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.04it/s, train_loss=13.2, val_loss=0.105\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.04it/s, train_loss=0.431, val_loss=1.88\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.04it/s, train_loss=0.431, val_loss=1.88\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    39: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.04it/s, train_loss=9.85, val_loss=0.391\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.04it/s, train_loss=9.85, val_loss=0.391\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.04it/s, train_loss=2.01, val_loss=1.24,\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.04it/s, train_loss=2.01, val_loss=1.24,\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.04it/s, train_loss=6.19, val_loss=0.68,\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.14it/s, train_loss=6.19, val_loss=0.68,\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.14it/s, train_loss=6.19, val_loss=0.68,\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.14it/s, train_loss=3.41, val_loss=0.606\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.14it/s, train_loss=3.41, val_loss=0.606\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.36it/s, train_loss=3.09, val_loss=0.954\u001b[A\n",
      " 84%|███████████████████████████████████▎      | 84/100 [02:07<00:24,  1.50s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=10.5, val_loss=6.65, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=10.5, val_loss=6.65, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=32.7, val_loss=0.984, train_ci\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=32.7, val_loss=0.984, train_ci\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=5.6, val_loss=21.3, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=5.6, val_loss=21.3, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=111, val_loss=21.4, train_cind\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.14it/s, train_loss=111, val_loss=21.4, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.14it/s, train_loss=111, val_loss=21.4, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.14it/s, train_loss=116, val_loss=5.88, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.14it/s, train_loss=116, val_loss=5.88, tr\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    50: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Curent best Test AUC: 0.7544145272385723\n",
      "Its' val AUC : 0.6841103710751665\n",
      "Its external  AUC: 0.6190326138333071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.14it/s, train_loss=29.5, val_loss=8.35, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.14it/s, train_loss=29.5, val_loss=8.35, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.14it/s, train_loss=44.7, val_loss=2.25, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.14it/s, train_loss=44.7, val_loss=2.25, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.14it/s, train_loss=14.9, val_loss=6.67, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 33.07it/s, train_loss=14.9, val_loss=6.67, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.07it/s, train_loss=14.9, val_loss=6.67, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.07it/s, train_loss=37.9, val_loss=2.05, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.07it/s, train_loss=37.9, val_loss=2.05, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.07it/s, train_loss=14.1, val_loss=3.46, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.07it/s, train_loss=14.1, val_loss=3.46, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.07it/s, train_loss=19.3, val_loss=2.19, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.07it/s, train_loss=19.3, val_loss=2.19, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.07it/s, train_loss=12.3, val_loss=1.81, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 33.27it/s, train_loss=12.3, val_loss=1.81,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.27it/s, train_loss=12.3, val_loss=1.81,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.27it/s, train_loss=12.1, val_loss=1.67,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.27it/s, train_loss=12.1, val_loss=1.67,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.27it/s, train_loss=10.8, val_loss=1.02,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.27it/s, train_loss=10.8, val_loss=1.02,\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch    14: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.27it/s, train_loss=6.77, val_loss=0.775\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.27it/s, train_loss=6.77, val_loss=0.775\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.27it/s, train_loss=5.19, val_loss=0.752\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 33.46it/s, train_loss=5.19, val_loss=0.752\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.46it/s, train_loss=5.19, val_loss=0.752\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.46it/s, train_loss=5.03, val_loss=0.758\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.46it/s, train_loss=5.03, val_loss=0.758\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.46it/s, train_loss=5.04, val_loss=0.64,\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.46it/s, train_loss=5.04, val_loss=0.64,\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.46it/s, train_loss=4.3, val_loss=0.529,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.46it/s, train_loss=4.3, val_loss=0.529,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.46it/s, train_loss=3.51, val_loss=0.525\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 33.74it/s, train_loss=3.51, val_loss=0.525\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.74it/s, train_loss=3.51, val_loss=0.525\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.74it/s, train_loss=3.26, val_loss=0.483\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.74it/s, train_loss=3.26, val_loss=0.483\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.74it/s, train_loss=2.87, val_loss=0.358\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.74it/s, train_loss=2.87, val_loss=0.358\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.74it/s, train_loss=2.11, val_loss=0.288\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.74it/s, train_loss=2.11, val_loss=0.288\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.74it/s, train_loss=1.67, val_loss=0.321\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.01it/s, train_loss=1.67, val_loss=0.321\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.01it/s, train_loss=1.67, val_loss=0.321\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.01it/s, train_loss=1.79, val_loss=0.33,\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.01it/s, train_loss=1.79, val_loss=0.33,\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.01it/s, train_loss=1.82, val_loss=0.265\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.01it/s, train_loss=1.82, val_loss=0.265\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.01it/s, train_loss=1.47, val_loss=0.21,\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.01it/s, train_loss=1.47, val_loss=0.21,\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.01it/s, train_loss=1.18, val_loss=0.2, \u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.27it/s, train_loss=1.18, val_loss=0.2, \u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.27it/s, train_loss=1.18, val_loss=0.2, \u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.27it/s, train_loss=1.12, val_loss=0.18,\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.27it/s, train_loss=1.12, val_loss=0.18,\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.27it/s, train_loss=0.998, val_loss=0.14\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.27it/s, train_loss=0.998, val_loss=0.14\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.27it/s, train_loss=0.832, val_loss=0.15\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.27it/s, train_loss=0.832, val_loss=0.15\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.27it/s, train_loss=0.89, val_loss=0.179\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.44it/s, train_loss=0.89, val_loss=0.179\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.44it/s, train_loss=0.89, val_loss=0.179\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.44it/s, train_loss=1.05, val_loss=0.175\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.44it/s, train_loss=1.05, val_loss=0.175\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.44it/s, train_loss=1.01, val_loss=0.159\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.44it/s, train_loss=1.01, val_loss=0.159\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.44it/s, train_loss=0.867, val_loss=0.16\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.44it/s, train_loss=0.867, val_loss=0.16\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.44it/s, train_loss=0.871, val_loss=0.18\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.60it/s, train_loss=0.871, val_loss=0.18\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.60it/s, train_loss=0.871, val_loss=0.18\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.60it/s, train_loss=0.932, val_loss=0.17\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.60it/s, train_loss=0.932, val_loss=0.17\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.60it/s, train_loss=0.903, val_loss=0.16\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.60it/s, train_loss=0.903, val_loss=0.16\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.60it/s, train_loss=0.84, val_loss=0.156\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.60it/s, train_loss=0.84, val_loss=0.156\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.60it/s, train_loss=0.805, val_loss=0.15\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.30it/s, train_loss=0.805, val_loss=0.15\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.30it/s, train_loss=0.805, val_loss=0.15\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.30it/s, train_loss=0.809, val_loss=0.15\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.30it/s, train_loss=0.809, val_loss=0.15\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.30it/s, train_loss=0.808, val_loss=0.15\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.30it/s, train_loss=0.808, val_loss=0.15\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    36: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.30it/s, train_loss=0.769, val_loss=0.14\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.30it/s, train_loss=0.769, val_loss=0.14\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.30it/s, train_loss=0.718, val_loss=0.13\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.26it/s, train_loss=0.718, val_loss=0.13\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.26it/s, train_loss=0.718, val_loss=0.13\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.26it/s, train_loss=0.678, val_loss=0.13\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.26it/s, train_loss=0.678, val_loss=0.13\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.26it/s, train_loss=0.664, val_loss=0.13\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.26it/s, train_loss=0.664, val_loss=0.13\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.26it/s, train_loss=0.665, val_loss=0.13\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.26it/s, train_loss=0.665, val_loss=0.13\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.26it/s, train_loss=0.656, val_loss=0.13\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.25it/s, train_loss=0.656, val_loss=0.13\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.25it/s, train_loss=0.656, val_loss=0.13\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.25it/s, train_loss=0.627, val_loss=0.12\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.25it/s, train_loss=0.627, val_loss=0.12\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.07it/s, train_loss=0.591, val_loss=0.11\u001b[A\n",
      " 85%|███████████████████████████████████▋      | 85/100 [02:09<00:22,  1.50s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=36.6, val_loss=3.8, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=36.6, val_loss=3.8, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=26.7, val_loss=19.5, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=26.7, val_loss=19.5, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=122, val_loss=47.3, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=122, val_loss=47.3, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=222, val_loss=8.95, train_cind\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 32.66it/s, train_loss=222, val_loss=8.95, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.66it/s, train_loss=222, val_loss=8.95, tr\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.66it/s, train_loss=46.7, val_loss=16.8, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.66it/s, train_loss=46.7, val_loss=16.8, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.66it/s, train_loss=93.5, val_loss=2.43, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.66it/s, train_loss=93.5, val_loss=2.43, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.66it/s, train_loss=13.5, val_loss=2.67, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.66it/s, train_loss=13.5, val_loss=2.67, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.39862241703193485\n",
      "Its' val AUC : 0.4901454397172761\n",
      "Its external  AUC: 0.36064282338112497\n",
      "Curent best Test AUC: 0.41064495929868505\n",
      "Its' val AUC : 0.4148430066603235\n",
      "Its external  AUC: 0.361745706633055\n",
      "Curent best Test AUC: 0.7202254226675016\n",
      "Its' val AUC : 0.7195867880929727\n",
      "Its external  AUC: 0.5764928312588624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.66it/s, train_loss=13.9, val_loss=0.96, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 33.24it/s, train_loss=13.9, val_loss=0.96, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.24it/s, train_loss=13.9, val_loss=0.96, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.24it/s, train_loss=5.51, val_loss=1.79, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.24it/s, train_loss=5.51, val_loss=1.79, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.24it/s, train_loss=9.43, val_loss=0.931, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.24it/s, train_loss=9.43, val_loss=0.931,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.24it/s, train_loss=5.72, val_loss=1.6, t\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.24it/s, train_loss=5.72, val_loss=1.6, t\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.24it/s, train_loss=9.3, val_loss=0.481, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 33.58it/s, train_loss=9.3, val_loss=0.481,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.58it/s, train_loss=9.3, val_loss=0.481,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.58it/s, train_loss=3.19, val_loss=1.89,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.58it/s, train_loss=3.19, val_loss=1.89,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.58it/s, train_loss=10.2, val_loss=0.197\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.58it/s, train_loss=10.2, val_loss=0.197\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.58it/s, train_loss=1.5, val_loss=0.194,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.58it/s, train_loss=1.5, val_loss=0.194,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.58it/s, train_loss=1.43, val_loss=0.257\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 33.25it/s, train_loss=1.43, val_loss=0.257\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.25it/s, train_loss=1.43, val_loss=0.257\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.25it/s, train_loss=1.36, val_loss=0.292\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.25it/s, train_loss=1.36, val_loss=0.292\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.25it/s, train_loss=1.57, val_loss=0.22,\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.25it/s, train_loss=1.57, val_loss=0.22,\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.25it/s, train_loss=1.56, val_loss=0.195\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.25it/s, train_loss=1.56, val_loss=0.195\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.25it/s, train_loss=1.39, val_loss=0.214\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 33.25it/s, train_loss=1.39, val_loss=0.214\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.25it/s, train_loss=1.39, val_loss=0.214\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.25it/s, train_loss=1.37, val_loss=0.223\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.25it/s, train_loss=1.37, val_loss=0.223\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.25it/s, train_loss=1.48, val_loss=0.213\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.25it/s, train_loss=1.48, val_loss=0.213\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.25it/s, train_loss=1.41, val_loss=0.228\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.25it/s, train_loss=1.41, val_loss=0.228\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.25it/s, train_loss=1.44, val_loss=0.199\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 33.38it/s, train_loss=1.44, val_loss=0.199\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.38it/s, train_loss=1.44, val_loss=0.199\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.38it/s, train_loss=1.26, val_loss=0.175\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.38it/s, train_loss=1.26, val_loss=0.175\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.38it/s, train_loss=1.15, val_loss=0.162\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.38it/s, train_loss=1.15, val_loss=0.162\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.38it/s, train_loss=1.05, val_loss=0.135\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.38it/s, train_loss=1.05, val_loss=0.135\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    21: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.38it/s, train_loss=0.862, val_loss=0.11\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 33.82it/s, train_loss=0.862, val_loss=0.11\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.82it/s, train_loss=0.862, val_loss=0.11\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.82it/s, train_loss=0.723, val_loss=0.08\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.82it/s, train_loss=0.723, val_loss=0.08\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.82it/s, train_loss=0.528, val_loss=0.07\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.82it/s, train_loss=0.528, val_loss=0.07\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.82it/s, train_loss=0.438, val_loss=0.06\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.82it/s, train_loss=0.438, val_loss=0.06\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.82it/s, train_loss=0.357, val_loss=0.05\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.06it/s, train_loss=0.357, val_loss=0.05\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.06it/s, train_loss=0.357, val_loss=0.05\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.06it/s, train_loss=0.293, val_loss=0.04\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.06it/s, train_loss=0.293, val_loss=0.04\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 34.06it/s, train_loss=0.263, val_loss=0.04\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.06it/s, train_loss=0.263, val_loss=0.04\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.06it/s, train_loss=0.253, val_loss=0.04\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.06it/s, train_loss=0.253, val_loss=0.04\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.06it/s, train_loss=0.25, val_loss=0.046\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 33.81it/s, train_loss=0.25, val_loss=0.046\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.81it/s, train_loss=0.25, val_loss=0.046\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.81it/s, train_loss=0.251, val_loss=0.04\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.81it/s, train_loss=0.251, val_loss=0.04\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.81it/s, train_loss=0.268, val_loss=0.05\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.81it/s, train_loss=0.268, val_loss=0.05\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.81it/s, train_loss=0.291, val_loss=0.05\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.81it/s, train_loss=0.291, val_loss=0.05\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.81it/s, train_loss=0.309, val_loss=0.05\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.04it/s, train_loss=0.309, val_loss=0.05\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.04it/s, train_loss=0.309, val_loss=0.05\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.04it/s, train_loss=0.312, val_loss=0.05\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.04it/s, train_loss=0.312, val_loss=0.05\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.04it/s, train_loss=0.309, val_loss=0.05\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.04it/s, train_loss=0.309, val_loss=0.05\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.04it/s, train_loss=0.304, val_loss=0.05\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.04it/s, train_loss=0.304, val_loss=0.05\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.04it/s, train_loss=0.297, val_loss=0.05\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.21it/s, train_loss=0.297, val_loss=0.05\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.21it/s, train_loss=0.297, val_loss=0.05\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.21it/s, train_loss=0.289, val_loss=0.05\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.21it/s, train_loss=0.289, val_loss=0.05\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    39: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch    45: reducing learning rate of group 0 to 1.2500e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.21it/s, train_loss=0.278, val_loss=0.04\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.21it/s, train_loss=0.278, val_loss=0.04\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.21it/s, train_loss=0.27, val_loss=0.047\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.21it/s, train_loss=0.27, val_loss=0.047\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.21it/s, train_loss=0.261, val_loss=0.04\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.24it/s, train_loss=0.261, val_loss=0.04\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.24it/s, train_loss=0.261, val_loss=0.04\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.24it/s, train_loss=0.252, val_loss=0.04\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.24it/s, train_loss=0.252, val_loss=0.04\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 33.85it/s, train_loss=0.244, val_loss=0.04\u001b[A\n",
      " 86%|████████████████████████████████████      | 86/100 [02:10<00:20,  1.50s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=19, val_loss=24, train_cindex=\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=19, val_loss=24, train_cindex=\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=124, val_loss=50.4, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=124, val_loss=50.4, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=244, val_loss=13.3, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=244, val_loss=13.3, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=75.2, val_loss=1.96, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 31.66it/s, train_loss=75.2, val_loss=1.96, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.66it/s, train_loss=75.2, val_loss=1.96, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.66it/s, train_loss=13.4, val_loss=7.7, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.66it/s, train_loss=13.4, val_loss=7.7, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.66it/s, train_loss=36.8, val_loss=1.91, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.66it/s, train_loss=36.8, val_loss=1.91, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.66it/s, train_loss=10.6, val_loss=6, trai\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.66it/s, train_loss=10.6, val_loss=6, trai\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.26023794614902945\n",
      "Its' val AUC : 0.28775316025553893\n",
      "Its external  AUC: 0.38805734992910035\n",
      "Curent best Test AUC: 0.713337507827176\n",
      "Its' val AUC : 0.7087127905396221\n",
      "Its external  AUC: 0.6169844020797227\n",
      "Curent best Test AUC: 0.721853475266124\n",
      "Its' val AUC : 0.6930814190566807\n",
      "Its external  AUC: 0.6026469198046321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.66it/s, train_loss=30.9, val_loss=2.24, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 32.12it/s, train_loss=30.9, val_loss=2.24, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.12it/s, train_loss=30.9, val_loss=2.24, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.12it/s, train_loss=11.4, val_loss=3.12, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.12it/s, train_loss=11.4, val_loss=3.12, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.12it/s, train_loss=15.6, val_loss=1.97, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.12it/s, train_loss=15.6, val_loss=1.97, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.12it/s, train_loss=10, val_loss=1.64, tr\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.12it/s, train_loss=10, val_loss=1.64, tr\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.12it/s, train_loss=9.25, val_loss=1.88, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 32.42it/s, train_loss=9.25, val_loss=1.88,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.42it/s, train_loss=9.25, val_loss=1.88,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.42it/s, train_loss=9.82, val_loss=0.731\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.42it/s, train_loss=9.82, val_loss=0.731\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.42it/s, train_loss=4.1, val_loss=1.92, \u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.42it/s, train_loss=4.1, val_loss=1.92, \u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.42it/s, train_loss=10.4, val_loss=0.352\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.42it/s, train_loss=10.4, val_loss=0.352\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.42it/s, train_loss=1.87, val_loss=1.56,\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 33.16it/s, train_loss=1.87, val_loss=1.56,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.16it/s, train_loss=1.87, val_loss=1.56,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.16it/s, train_loss=8.08, val_loss=0.163\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.16it/s, train_loss=8.08, val_loss=0.163\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.16it/s, train_loss=1.24, val_loss=1.7, \u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.16it/s, train_loss=1.24, val_loss=1.7, \u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.16it/s, train_loss=8.78, val_loss=0.154\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.16it/s, train_loss=8.78, val_loss=0.154\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.16it/s, train_loss=1.18, val_loss=1.28,\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 33.50it/s, train_loss=1.18, val_loss=1.28,\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.50it/s, train_loss=1.18, val_loss=1.28,\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.50it/s, train_loss=6.62, val_loss=0.278\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.50it/s, train_loss=6.62, val_loss=0.278\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.50it/s, train_loss=1.74, val_loss=0.96,\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.50it/s, train_loss=1.74, val_loss=0.96,\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.50it/s, train_loss=5.36, val_loss=0.127\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.50it/s, train_loss=5.36, val_loss=0.127\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.50it/s, train_loss=0.807, val_loss=0.72\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 33.95it/s, train_loss=0.807, val_loss=0.72\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.95it/s, train_loss=0.807, val_loss=0.72\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7615529117094553\n",
      "Its' val AUC : 0.6873725703411717\n",
      "Its external  AUC: 0.6220261540885458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.95it/s, train_loss=4.05, val_loss=0.211\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.95it/s, train_loss=4.05, val_loss=0.211\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.95it/s, train_loss=1.41, val_loss=0.639\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.95it/s, train_loss=1.41, val_loss=0.639\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.95it/s, train_loss=3.32, val_loss=0.142\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.95it/s, train_loss=3.32, val_loss=0.142\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.95it/s, train_loss=0.931, val_loss=0.61\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.12it/s, train_loss=0.931, val_loss=0.61\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.12it/s, train_loss=0.931, val_loss=0.61\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.12it/s, train_loss=2.98, val_loss=0.175\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.12it/s, train_loss=2.98, val_loss=0.175\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.12it/s, train_loss=0.914, val_loss=0.07\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.12it/s, train_loss=0.914, val_loss=0.07\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.12it/s, train_loss=0.609, val_loss=0.18\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.12it/s, train_loss=0.609, val_loss=0.18\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.12it/s, train_loss=1.19, val_loss=0.088\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.17it/s, train_loss=1.19, val_loss=0.088\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.17it/s, train_loss=1.19, val_loss=0.088\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.17it/s, train_loss=0.602, val_loss=0.11\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.17it/s, train_loss=0.602, val_loss=0.11\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 34.17it/s, train_loss=0.656, val_loss=0.18\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.17it/s, train_loss=0.656, val_loss=0.18\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.17it/s, train_loss=1.01, val_loss=0.064\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.17it/s, train_loss=1.01, val_loss=0.064\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    29: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.17it/s, train_loss=0.373, val_loss=0.08\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 33.78it/s, train_loss=0.373, val_loss=0.08\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.78it/s, train_loss=0.373, val_loss=0.08\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.78it/s, train_loss=0.469, val_loss=0.13\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.78it/s, train_loss=0.469, val_loss=0.13\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.78it/s, train_loss=0.719, val_loss=0.05\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.78it/s, train_loss=0.719, val_loss=0.05\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.78it/s, train_loss=0.327, val_loss=0.10\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.78it/s, train_loss=0.327, val_loss=0.10\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.78it/s, train_loss=0.537, val_loss=0.09\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.01it/s, train_loss=0.537, val_loss=0.09\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.01it/s, train_loss=0.537, val_loss=0.09\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.01it/s, train_loss=0.48, val_loss=0.040\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.01it/s, train_loss=0.48, val_loss=0.040\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.01it/s, train_loss=0.215, val_loss=0.09\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.01it/s, train_loss=0.215, val_loss=0.09\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.01it/s, train_loss=0.491, val_loss=0.05\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.01it/s, train_loss=0.491, val_loss=0.05\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.01it/s, train_loss=0.335, val_loss=0.03\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.25it/s, train_loss=0.335, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.25it/s, train_loss=0.335, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.25it/s, train_loss=0.232, val_loss=0.07\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.25it/s, train_loss=0.232, val_loss=0.07\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.25it/s, train_loss=0.375, val_loss=0.03\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.25it/s, train_loss=0.375, val_loss=0.03\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.25it/s, train_loss=0.204, val_loss=0.03\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.25it/s, train_loss=0.204, val_loss=0.03\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.25it/s, train_loss=0.307, val_loss=0.03\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.44it/s, train_loss=0.307, val_loss=0.03\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.44it/s, train_loss=0.307, val_loss=0.03\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.44it/s, train_loss=0.325, val_loss=0.02\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.44it/s, train_loss=0.325, val_loss=0.02\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 33.74it/s, train_loss=0.18, val_loss=0.054\u001b[A\n",
      " 87%|████████████████████████████████████▌     | 87/100 [02:12<00:19,  1.50s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=9.15, val_loss=78.8, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=9.15, val_loss=78.8, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=410, val_loss=5.34, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=410, val_loss=5.34, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=34.3, val_loss=16.6, train_cin\u001b[A\n",
      "Epoch 2:   6%| | 3/50 [00:00<00:01, 27.84it/s, train_loss=34.3, val_loss=16.6, t\u001b[A\n",
      "Epoch 3:   6%| | 3/50 [00:00<00:01, 27.84it/s, train_loss=34.3, val_loss=16.6, t\u001b[A\n",
      "Epoch 3:   6%| | 3/50 [00:00<00:01, 27.84it/s, train_loss=88.8, val_loss=2.4, tr\u001b[A\n",
      "Epoch 4:   6%| | 3/50 [00:00<00:01, 27.84it/s, train_loss=88.8, val_loss=2.4, tr\u001b[A\n",
      "Epoch 4:   6%| | 3/50 [00:00<00:01, 27.84it/s, train_loss=13.1, val_loss=10.6, t\u001b[A\n",
      "Epoch 5:   6%| | 3/50 [00:00<00:01, 27.84it/s, train_loss=13.1, val_loss=10.6, t\u001b[A\n",
      "Epoch 5:   6%| | 3/50 [00:00<00:01, 27.84it/s, train_loss=55.5, val_loss=6.66, t\u001b[A\n",
      "Epoch 6:   6%| | 3/50 [00:00<00:01, 27.84it/s, train_loss=55.5, val_loss=6.66, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.2920475892298059\n",
      "Its' val AUC : 0.2912872094603779\n",
      "Its external  AUC: 0.40633370096108395\n",
      "Curent best Test AUC: 0.7629304946775204\n",
      "Its' val AUC : 0.6498572787821123\n",
      "Its external  AUC: 0.6648810461635418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6:   6%| | 3/50 [00:00<00:01, 27.84it/s, train_loss=33.8, val_loss=1.8, tr\u001b[A\n",
      "Epoch 6:  14%|▏| 7/50 [00:00<00:01, 30.30it/s, train_loss=33.8, val_loss=1.8, tr\u001b[A\n",
      "Epoch 7:  14%|▏| 7/50 [00:00<00:01, 30.30it/s, train_loss=33.8, val_loss=1.8, tr\u001b[A\n",
      "Epoch 7:  14%|▏| 7/50 [00:00<00:01, 30.30it/s, train_loss=13.2, val_loss=1.24, t\u001b[A\n",
      "Epoch 8:  14%|▏| 7/50 [00:00<00:01, 30.30it/s, train_loss=13.2, val_loss=1.24, t\u001b[A\n",
      "Epoch 8:  14%|▏| 7/50 [00:00<00:01, 30.30it/s, train_loss=9.61, val_loss=2.23, t\u001b[A\n",
      "Epoch 9:  14%|▏| 7/50 [00:00<00:01, 30.30it/s, train_loss=9.61, val_loss=2.23, t\u001b[A\n",
      "Epoch 9:  14%|▏| 7/50 [00:00<00:01, 30.30it/s, train_loss=12.1, val_loss=1.03, t\u001b[A\n",
      "Epoch 10:  14%|▏| 7/50 [00:00<00:01, 30.30it/s, train_loss=12.1, val_loss=1.03, \u001b[A\n",
      "Epoch 10:  14%|▏| 7/50 [00:00<00:01, 30.30it/s, train_loss=7.44, val_loss=0.815,\u001b[A\n",
      "Epoch 10:  22%|▏| 11/50 [00:00<00:01, 31.16it/s, train_loss=7.44, val_loss=0.815\u001b[A\n",
      "Epoch 11:  22%|▏| 11/50 [00:00<00:01, 31.16it/s, train_loss=7.44, val_loss=0.815\u001b[A\n",
      "Epoch 11:  22%|▏| 11/50 [00:00<00:01, 31.16it/s, train_loss=5.54, val_loss=0.66,\u001b[A\n",
      "Epoch 12:  22%|▏| 11/50 [00:00<00:01, 31.16it/s, train_loss=5.54, val_loss=0.66,\u001b[A\n",
      "Epoch 12:  22%|▏| 11/50 [00:00<00:01, 31.16it/s, train_loss=4.37, val_loss=0.434\u001b[A\n",
      "Epoch 13:  22%|▏| 11/50 [00:00<00:01, 31.16it/s, train_loss=4.37, val_loss=0.434\u001b[A\n",
      "Epoch 13:  22%|▏| 11/50 [00:00<00:01, 31.16it/s, train_loss=2.69, val_loss=0.404\u001b[A\n",
      "Epoch 14:  22%|▏| 11/50 [00:00<00:01, 31.16it/s, train_loss=2.69, val_loss=0.404\u001b[A\n",
      "Epoch 14:  22%|▏| 11/50 [00:00<00:01, 31.16it/s, train_loss=2.45, val_loss=0.385\u001b[A\n",
      "Epoch 14:  30%|▎| 15/50 [00:00<00:01, 31.66it/s, train_loss=2.45, val_loss=0.385\u001b[A\n",
      "Epoch 15:  30%|▎| 15/50 [00:00<00:01, 31.66it/s, train_loss=2.45, val_loss=0.385\u001b[A\n",
      "Epoch 15:  30%|▎| 15/50 [00:00<00:01, 31.66it/s, train_loss=2.51, val_loss=0.534\u001b[A\n",
      "Epoch 16:  30%|▎| 15/50 [00:00<00:01, 31.66it/s, train_loss=2.51, val_loss=0.534\u001b[A\n",
      "Epoch 16:  30%|▎| 15/50 [00:00<00:01, 31.66it/s, train_loss=2.22, val_loss=0.275\u001b[A\n",
      "Epoch 17:  30%|▎| 15/50 [00:00<00:01, 31.66it/s, train_loss=2.22, val_loss=0.275\u001b[A\n",
      "Epoch 17:  30%|▎| 15/50 [00:00<00:01, 31.66it/s, train_loss=1.6, val_loss=0.3, t\u001b[A\n",
      "Epoch 18:  30%|▎| 15/50 [00:00<00:01, 31.66it/s, train_loss=1.6, val_loss=0.3, t\u001b[A\n",
      "Epoch 18:  30%|▎| 15/50 [00:00<00:01, 31.66it/s, train_loss=1.93, val_loss=0.655\u001b[A\n",
      "Epoch 18:  38%|▍| 19/50 [00:00<00:00, 32.19it/s, train_loss=1.93, val_loss=0.655\u001b[A\n",
      "Epoch 19:  38%|▍| 19/50 [00:00<00:00, 32.19it/s, train_loss=1.93, val_loss=0.655\u001b[A\n",
      "Epoch 19:  38%|▍| 19/50 [00:00<00:00, 32.19it/s, train_loss=2.97, val_loss=0.696\u001b[A\n",
      "Epoch 20:  38%|▍| 19/50 [00:00<00:00, 32.19it/s, train_loss=2.97, val_loss=0.696\u001b[A\n",
      "Epoch 20:  38%|▍| 19/50 [00:00<00:00, 32.19it/s, train_loss=4.78, val_loss=0.908\u001b[A\n",
      "Epoch 21:  38%|▍| 19/50 [00:00<00:00, 32.19it/s, train_loss=4.78, val_loss=0.908\u001b[A\n",
      "Epoch 21:  38%|▍| 19/50 [00:00<00:00, 32.19it/s, train_loss=4.63, val_loss=0.287\u001b[A\n",
      "Epoch 22:  38%|▍| 19/50 [00:00<00:00, 32.19it/s, train_loss=4.63, val_loss=0.287\u001b[A\n",
      "Epoch 22:  38%|▍| 19/50 [00:00<00:00, 32.19it/s, train_loss=2.13, val_loss=0.346\u001b[A\n",
      "Epoch 22:  46%|▍| 23/50 [00:00<00:00, 32.79it/s, train_loss=2.13, val_loss=0.346\u001b[A\n",
      "Epoch 23:  46%|▍| 23/50 [00:00<00:00, 32.79it/s, train_loss=2.13, val_loss=0.346\u001b[A\n",
      "Epoch 23:  46%|▍| 23/50 [00:00<00:00, 32.79it/s, train_loss=2.43, val_loss=0.247\u001b[A\n",
      "Epoch 24:  46%|▍| 23/50 [00:00<00:00, 32.79it/s, train_loss=2.43, val_loss=0.247\u001b[A\n",
      "Epoch 24:  46%|▍| 23/50 [00:00<00:00, 32.79it/s, train_loss=1.56, val_loss=0.374\u001b[A\n",
      "Epoch 25:  46%|▍| 23/50 [00:00<00:00, 32.79it/s, train_loss=1.56, val_loss=0.374\u001b[A\n",
      "Epoch 25:  46%|▍| 23/50 [00:00<00:00, 32.79it/s, train_loss=2.05, val_loss=0.143\u001b[A\n",
      "Epoch 26:  46%|▍| 23/50 [00:00<00:00, 32.79it/s, train_loss=2.05, val_loss=0.143\u001b[A\n",
      "Epoch 26:  46%|▍| 23/50 [00:00<00:00, 32.79it/s, train_loss=0.94, val_loss=0.245\u001b[A\n",
      "Epoch 26:  54%|▌| 27/50 [00:00<00:00, 33.28it/s, train_loss=0.94, val_loss=0.245\u001b[A\n",
      "Epoch 27:  54%|▌| 27/50 [00:00<00:00, 33.28it/s, train_loss=0.94, val_loss=0.245\u001b[A\n",
      "Epoch 27:  54%|▌| 27/50 [00:00<00:00, 33.28it/s, train_loss=1.44, val_loss=0.090\u001b[A\n",
      "Epoch 28:  54%|▌| 27/50 [00:00<00:00, 33.28it/s, train_loss=1.44, val_loss=0.090\u001b[A\n",
      "Epoch 28:  54%|▌| 27/50 [00:00<00:00, 33.28it/s, train_loss=0.526, val_loss=0.19\u001b[A\n",
      "Epoch 29:  54%|▌| 27/50 [00:00<00:00, 33.28it/s, train_loss=0.526, val_loss=0.19\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    23: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29:  54%|▌| 27/50 [00:00<00:00, 33.28it/s, train_loss=0.991, val_loss=0.08\u001b[A\n",
      "Epoch 30:  54%|▌| 27/50 [00:00<00:00, 33.28it/s, train_loss=0.991, val_loss=0.08\u001b[A\n",
      "Epoch 30:  54%|▌| 27/50 [00:00<00:00, 33.28it/s, train_loss=0.415, val_loss=0.13\u001b[A\n",
      "Epoch 30:  62%|▌| 31/50 [00:00<00:00, 33.42it/s, train_loss=0.415, val_loss=0.13\u001b[A\n",
      "Epoch 31:  62%|▌| 31/50 [00:00<00:00, 33.42it/s, train_loss=0.415, val_loss=0.13\u001b[A\n",
      "Epoch 31:  62%|▌| 31/50 [00:00<00:00, 33.42it/s, train_loss=0.743, val_loss=0.07\u001b[A\n",
      "Epoch 32:  62%|▌| 31/50 [00:00<00:00, 33.42it/s, train_loss=0.743, val_loss=0.07\u001b[A\n",
      "Epoch 32:  62%|▌| 31/50 [00:01<00:00, 33.42it/s, train_loss=0.432, val_loss=0.15\u001b[A\n",
      "Epoch 33:  62%|▌| 31/50 [00:01<00:00, 33.42it/s, train_loss=0.432, val_loss=0.15\u001b[A\n",
      "Epoch 33:  62%|▌| 31/50 [00:01<00:00, 33.42it/s, train_loss=0.681, val_loss=0.12\u001b[A\n",
      "Epoch 34:  62%|▌| 31/50 [00:01<00:00, 33.42it/s, train_loss=0.681, val_loss=0.12\u001b[A\n",
      "Epoch 34:  62%|▌| 31/50 [00:01<00:00, 33.42it/s, train_loss=0.572, val_loss=0.11\u001b[A\n",
      "Epoch 34:  70%|▋| 35/50 [00:01<00:00, 33.58it/s, train_loss=0.572, val_loss=0.11\u001b[A\n",
      "Epoch 35:  70%|▋| 35/50 [00:01<00:00, 33.58it/s, train_loss=0.572, val_loss=0.11\u001b[A\n",
      "Epoch 35:  70%|▋| 35/50 [00:01<00:00, 33.58it/s, train_loss=0.727, val_loss=0.09\u001b[A\n",
      "Epoch 36:  70%|▋| 35/50 [00:01<00:00, 33.58it/s, train_loss=0.727, val_loss=0.09\u001b[A\n",
      "Epoch 36:  70%|▋| 35/50 [00:01<00:00, 33.58it/s, train_loss=0.62, val_loss=0.146\u001b[A\n",
      "Epoch 37:  70%|▋| 35/50 [00:01<00:00, 33.58it/s, train_loss=0.62, val_loss=0.146\u001b[A\n",
      "Epoch 37:  70%|▋| 35/50 [00:01<00:00, 33.58it/s, train_loss=0.711, val_loss=0.11\u001b[A\n",
      "Epoch 38:  70%|▋| 35/50 [00:01<00:00, 33.58it/s, train_loss=0.711, val_loss=0.11\u001b[A\n",
      "Epoch 38:  70%|▋| 35/50 [00:01<00:00, 33.58it/s, train_loss=0.593, val_loss=0.08\u001b[A\n",
      "Epoch 38:  78%|▊| 39/50 [00:01<00:00, 33.39it/s, train_loss=0.593, val_loss=0.08\u001b[A\n",
      "Epoch 39:  78%|▊| 39/50 [00:01<00:00, 33.39it/s, train_loss=0.593, val_loss=0.08\u001b[A\n",
      "Epoch 39:  78%|▊| 39/50 [00:01<00:00, 33.39it/s, train_loss=0.487, val_loss=0.08\u001b[A\n",
      "Epoch 40:  78%|▊| 39/50 [00:01<00:00, 33.39it/s, train_loss=0.487, val_loss=0.08\u001b[A\n",
      "Epoch 40:  78%|▊| 39/50 [00:01<00:00, 33.39it/s, train_loss=0.54, val_loss=0.072\u001b[A\n",
      "Epoch 41:  78%|▊| 39/50 [00:01<00:00, 33.39it/s, train_loss=0.54, val_loss=0.072\u001b[A\n",
      "Epoch 41:  78%|▊| 39/50 [00:01<00:00, 33.39it/s, train_loss=0.45, val_loss=0.069\u001b[A\n",
      "Epoch 42:  78%|▊| 39/50 [00:01<00:00, 33.39it/s, train_loss=0.45, val_loss=0.069\u001b[A\n",
      "Epoch 42:  78%|▊| 39/50 [00:01<00:00, 33.39it/s, train_loss=0.352, val_loss=0.08\u001b[A\n",
      "Epoch 42:  86%|▊| 43/50 [00:01<00:00, 32.98it/s, train_loss=0.352, val_loss=0.08\u001b[A\n",
      "Epoch 43:  86%|▊| 43/50 [00:01<00:00, 32.98it/s, train_loss=0.352, val_loss=0.08\u001b[A\n",
      "Epoch 43:  86%|▊| 43/50 [00:01<00:00, 32.98it/s, train_loss=0.385, val_loss=0.06\u001b[A\n",
      "Epoch 44:  86%|▊| 43/50 [00:01<00:00, 32.98it/s, train_loss=0.385, val_loss=0.06\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    38: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44:  86%|▊| 43/50 [00:01<00:00, 32.98it/s, train_loss=0.303, val_loss=0.04\u001b[A\n",
      "Epoch 45:  86%|▊| 43/50 [00:01<00:00, 32.98it/s, train_loss=0.303, val_loss=0.04\u001b[A\n",
      "Epoch 45:  86%|▊| 43/50 [00:01<00:00, 32.98it/s, train_loss=0.248, val_loss=0.04\u001b[A\n",
      "Epoch 46:  86%|▊| 43/50 [00:01<00:00, 32.98it/s, train_loss=0.248, val_loss=0.04\u001b[A\n",
      "Epoch 46:  86%|▊| 43/50 [00:01<00:00, 32.98it/s, train_loss=0.277, val_loss=0.04\u001b[A\n",
      "Epoch 46:  94%|▉| 47/50 [00:01<00:00, 33.20it/s, train_loss=0.277, val_loss=0.04\u001b[A\n",
      "Epoch 47:  94%|▉| 47/50 [00:01<00:00, 33.20it/s, train_loss=0.277, val_loss=0.04\u001b[A\n",
      "Epoch 47:  94%|▉| 47/50 [00:01<00:00, 33.20it/s, train_loss=0.212, val_loss=0.04\u001b[A\n",
      "Epoch 48:  94%|▉| 47/50 [00:01<00:00, 33.20it/s, train_loss=0.212, val_loss=0.04\u001b[A\n",
      "Epoch 48:  94%|▉| 47/50 [00:01<00:00, 33.20it/s, train_loss=0.197, val_loss=0.05\u001b[A\n",
      "Epoch 49:  94%|▉| 47/50 [00:01<00:00, 33.20it/s, train_loss=0.197, val_loss=0.05\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 32.78it/s, train_loss=0.229, val_loss=0.04\u001b[A\n",
      " 88%|████████████████████████████████████▉     | 88/100 [02:13<00:18,  1.51s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=40.7, val_loss=3.7, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=40.7, val_loss=3.7, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=21.1, val_loss=0.666, train_ci\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=21.1, val_loss=0.666, train_ci\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=3.48, val_loss=0.618, train_ci\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=3.48, val_loss=0.618, train_ci\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=3.17, val_loss=0.211, train_ci\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.79it/s, train_loss=3.17, val_loss=0.211, \u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.79it/s, train_loss=3.17, val_loss=0.211, \u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.79it/s, train_loss=0.981, val_loss=0.0638\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.79it/s, train_loss=0.981, val_loss=0.0638\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.79it/s, train_loss=0.523, val_loss=0.0704\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.79it/s, train_loss=0.523, val_loss=0.0704\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.79it/s, train_loss=0.435, val_loss=0.0755\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.79it/s, train_loss=0.435, val_loss=0.0755\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.739261114589856\n",
      "Its' val AUC : 0.7136060894386299\n",
      "Its external  AUC: 0.6048526863084922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.79it/s, train_loss=0.457, val_loss=0.0509\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 33.98it/s, train_loss=0.457, val_loss=0.0509\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.98it/s, train_loss=0.457, val_loss=0.0509\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.98it/s, train_loss=0.297, val_loss=0.0465\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.98it/s, train_loss=0.297, val_loss=0.0465\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.98it/s, train_loss=0.229, val_loss=0.0284\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.98it/s, train_loss=0.229, val_loss=0.028\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.98it/s, train_loss=0.148, val_loss=0.029\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.98it/s, train_loss=0.148, val_loss=0.029\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.98it/s, train_loss=0.184, val_loss=0.028\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.01it/s, train_loss=0.184, val_loss=0.02\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.01it/s, train_loss=0.184, val_loss=0.02\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.01it/s, train_loss=0.196, val_loss=0.06\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.01it/s, train_loss=0.196, val_loss=0.06\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.01it/s, train_loss=0.365, val_loss=0.11\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.01it/s, train_loss=0.365, val_loss=0.11\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.01it/s, train_loss=0.639, val_loss=0.04\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.01it/s, train_loss=0.639, val_loss=0.04\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.01it/s, train_loss=0.245, val_loss=0.12\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 34.00it/s, train_loss=0.245, val_loss=0.12\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 34.00it/s, train_loss=0.245, val_loss=0.12\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 34.00it/s, train_loss=0.666, val_loss=0.10\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 34.00it/s, train_loss=0.666, val_loss=0.10\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 34.00it/s, train_loss=0.558, val_loss=0.05\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 34.00it/s, train_loss=0.558, val_loss=0.05\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 34.00it/s, train_loss=0.373, val_loss=0.06\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 34.00it/s, train_loss=0.373, val_loss=0.06\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 34.00it/s, train_loss=0.438, val_loss=0.07\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 33.96it/s, train_loss=0.438, val_loss=0.07\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.96it/s, train_loss=0.438, val_loss=0.07\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.96it/s, train_loss=0.45, val_loss=0.048\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.96it/s, train_loss=0.45, val_loss=0.048\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.96it/s, train_loss=0.318, val_loss=0.04\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.96it/s, train_loss=0.318, val_loss=0.04\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    16: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch    22: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.96it/s, train_loss=0.275, val_loss=0.04\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.96it/s, train_loss=0.275, val_loss=0.04\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.96it/s, train_loss=0.3, val_loss=0.0484\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 33.79it/s, train_loss=0.3, val_loss=0.0484\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.79it/s, train_loss=0.3, val_loss=0.0484\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.79it/s, train_loss=0.309, val_loss=0.04\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.79it/s, train_loss=0.309, val_loss=0.04\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.79it/s, train_loss=0.288, val_loss=0.03\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.79it/s, train_loss=0.288, val_loss=0.03\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.79it/s, train_loss=0.258, val_loss=0.03\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.79it/s, train_loss=0.258, val_loss=0.03\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.79it/s, train_loss=0.244, val_loss=0.03\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 33.51it/s, train_loss=0.244, val_loss=0.03\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.51it/s, train_loss=0.244, val_loss=0.03\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.51it/s, train_loss=0.249, val_loss=0.03\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.51it/s, train_loss=0.249, val_loss=0.03\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.51it/s, train_loss=0.25, val_loss=0.037\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.51it/s, train_loss=0.25, val_loss=0.037\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.51it/s, train_loss=0.244, val_loss=0.03\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.51it/s, train_loss=0.244, val_loss=0.03\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.51it/s, train_loss=0.232, val_loss=0.03\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 33.59it/s, train_loss=0.232, val_loss=0.03\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 33.59it/s, train_loss=0.232, val_loss=0.03\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 33.59it/s, train_loss=0.217, val_loss=0.03\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 33.59it/s, train_loss=0.217, val_loss=0.03\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 33.59it/s, train_loss=0.205, val_loss=0.02\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 33.59it/s, train_loss=0.205, val_loss=0.02\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    28: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 33.59it/s, train_loss=0.197, val_loss=0.02\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 33.59it/s, train_loss=0.197, val_loss=0.02\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 33.59it/s, train_loss=0.194, val_loss=0.02\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 33.66it/s, train_loss=0.194, val_loss=0.02\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.66it/s, train_loss=0.194, val_loss=0.02\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.66it/s, train_loss=0.192, val_loss=0.02\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.66it/s, train_loss=0.192, val_loss=0.02\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.66it/s, train_loss=0.189, val_loss=0.02\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.66it/s, train_loss=0.189, val_loss=0.02\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.66it/s, train_loss=0.183, val_loss=0.02\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.66it/s, train_loss=0.183, val_loss=0.02\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.66it/s, train_loss=0.176, val_loss=0.02\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 33.76it/s, train_loss=0.176, val_loss=0.02\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 33.76it/s, train_loss=0.176, val_loss=0.02\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 33.76it/s, train_loss=0.169, val_loss=0.02\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 33.76it/s, train_loss=0.169, val_loss=0.02\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 33.76it/s, train_loss=0.162, val_loss=0.02\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 33.76it/s, train_loss=0.162, val_loss=0.02\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 33.76it/s, train_loss=0.157, val_loss=0.02\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 33.76it/s, train_loss=0.157, val_loss=0.02\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 33.76it/s, train_loss=0.154, val_loss=0.02\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 33.86it/s, train_loss=0.154, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 33.86it/s, train_loss=0.154, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 33.86it/s, train_loss=0.151, val_loss=0.02\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 33.86it/s, train_loss=0.151, val_loss=0.02\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 33.86it/s, train_loss=0.148, val_loss=0.02\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 33.86it/s, train_loss=0.148, val_loss=0.02\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 33.86it/s, train_loss=0.143, val_loss=0.02\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 33.86it/s, train_loss=0.143, val_loss=0.02\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 33.86it/s, train_loss=0.138, val_loss=0.02\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 33.90it/s, train_loss=0.138, val_loss=0.02\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 33.90it/s, train_loss=0.138, val_loss=0.02\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 33.90it/s, train_loss=0.132, val_loss=0.01\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 33.90it/s, train_loss=0.132, val_loss=0.01\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 33.82it/s, train_loss=0.127, val_loss=0.01\u001b[A\n",
      " 89%|█████████████████████████████████████▍    | 89/100 [02:15<00:16,  1.51s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=32.2, val_loss=11.2, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=32.2, val_loss=11.2, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=55.5, val_loss=0.832, train_ci\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=55.5, val_loss=0.832, train_ci\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=5.29, val_loss=1.44, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=5.29, val_loss=1.44, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=8.21, val_loss=0.587, train_ci\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 31.54it/s, train_loss=8.21, val_loss=0.587, \u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.54it/s, train_loss=8.21, val_loss=0.587, \u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.54it/s, train_loss=3.74, val_loss=0.867, \u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.54it/s, train_loss=3.74, val_loss=0.867, \u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.54it/s, train_loss=4.74, val_loss=0.572, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.54it/s, train_loss=4.74, val_loss=0.572, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.54it/s, train_loss=3.28, val_loss=0.226, \u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.54it/s, train_loss=3.28, val_loss=0.226, \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.2371947401377583\n",
      "Its' val AUC : 0.27701508767160526\n",
      "Its external  AUC: 0.3896328974318576\n",
      "Curent best Test AUC: 0.45973700688791486\n",
      "Its' val AUC : 0.48239771646051377\n",
      "Its external  AUC: 0.5637308964865291\n",
      "Curent best Test AUC: 0.7547902316844083\n",
      "Its' val AUC : 0.7030039418241131\n",
      "Its external  AUC: 0.6340003151095005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.54it/s, train_loss=1.43, val_loss=0.229, \u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 32.06it/s, train_loss=1.43, val_loss=0.229, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.06it/s, train_loss=1.43, val_loss=0.229, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.06it/s, train_loss=1.37, val_loss=0.11, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.06it/s, train_loss=1.37, val_loss=0.11, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.06it/s, train_loss=0.615, val_loss=0.122,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.06it/s, train_loss=0.615, val_loss=0.122\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.06it/s, train_loss=0.597, val_loss=0.116\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.06it/s, train_loss=0.597, val_loss=0.116\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.06it/s, train_loss=0.785, val_loss=0.078\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 32.15it/s, train_loss=0.785, val_loss=0.07\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.15it/s, train_loss=0.785, val_loss=0.07\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.15it/s, train_loss=0.539, val_loss=0.05\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.15it/s, train_loss=0.539, val_loss=0.05\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.15it/s, train_loss=0.342, val_loss=0.11\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.15it/s, train_loss=0.342, val_loss=0.11\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.15it/s, train_loss=0.572, val_loss=0.07\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.15it/s, train_loss=0.572, val_loss=0.07\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.15it/s, train_loss=0.462, val_loss=0.02\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 32.34it/s, train_loss=0.462, val_loss=0.02\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.34it/s, train_loss=0.462, val_loss=0.02\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.34it/s, train_loss=0.219, val_loss=0.06\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.34it/s, train_loss=0.219, val_loss=0.06\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.34it/s, train_loss=0.421, val_loss=0.12\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.34it/s, train_loss=0.421, val_loss=0.12\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.34it/s, train_loss=0.641, val_loss=0.06\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.34it/s, train_loss=0.641, val_loss=0.06\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.34it/s, train_loss=0.425, val_loss=0.04\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 32.31it/s, train_loss=0.425, val_loss=0.04\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.31it/s, train_loss=0.425, val_loss=0.04\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 32.31it/s, train_loss=0.248, val_loss=0.04\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.31it/s, train_loss=0.248, val_loss=0.04\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 32.31it/s, train_loss=0.282, val_loss=0.02\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.31it/s, train_loss=0.282, val_loss=0.02\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 32.31it/s, train_loss=0.178, val_loss=0.01\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.31it/s, train_loss=0.178, val_loss=0.01\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 32.31it/s, train_loss=0.123, val_loss=0.05\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 31.71it/s, train_loss=0.123, val_loss=0.05\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 31.71it/s, train_loss=0.123, val_loss=0.05\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 31.71it/s, train_loss=0.329, val_loss=0.09\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 31.71it/s, train_loss=0.329, val_loss=0.09\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 31.71it/s, train_loss=0.507, val_loss=0.15\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 31.71it/s, train_loss=0.507, val_loss=0.15\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 31.71it/s, train_loss=0.743, val_loss=0.21\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 31.71it/s, train_loss=0.743, val_loss=0.21\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 31.71it/s, train_loss=1.22, val_loss=0.348\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 31.64it/s, train_loss=1.22, val_loss=0.348\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 31.64it/s, train_loss=1.22, val_loss=0.348\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 31.64it/s, train_loss=1.66, val_loss=0.165\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 31.64it/s, train_loss=1.66, val_loss=0.165\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 31.64it/s, train_loss=0.95, val_loss=0.028\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 31.64it/s, train_loss=0.95, val_loss=0.028\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 31.64it/s, train_loss=0.178, val_loss=0.12\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 31.64it/s, train_loss=0.178, val_loss=0.12\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:01<00:00, 31.64it/s, train_loss=0.626, val_loss=0.05\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:01<00:00, 31.53it/s, train_loss=0.626, val_loss=0.05\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:01<00:00, 31.53it/s, train_loss=0.626, val_loss=0.05\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:01<00:00, 31.53it/s, train_loss=0.299, val_loss=0.07\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 31.53it/s, train_loss=0.299, val_loss=0.07\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 31.53it/s, train_loss=0.391, val_loss=0.07\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 31.53it/s, train_loss=0.391, val_loss=0.07\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 31.53it/s, train_loss=0.374, val_loss=0.03\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 31.53it/s, train_loss=0.374, val_loss=0.03\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    29: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch    35: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 31.53it/s, train_loss=0.198, val_loss=0.05\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 31.80it/s, train_loss=0.198, val_loss=0.05\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 31.80it/s, train_loss=0.198, val_loss=0.05\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 31.80it/s, train_loss=0.287, val_loss=0.01\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 31.80it/s, train_loss=0.287, val_loss=0.01\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 31.80it/s, train_loss=0.0766, val_loss=0.0\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 31.80it/s, train_loss=0.0766, val_loss=0.0\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 31.80it/s, train_loss=0.119, val_loss=0.04\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 31.80it/s, train_loss=0.119, val_loss=0.04\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 31.80it/s, train_loss=0.234, val_loss=0.01\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 31.93it/s, train_loss=0.234, val_loss=0.01\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 31.93it/s, train_loss=0.234, val_loss=0.01\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 31.93it/s, train_loss=0.0837, val_loss=0.0\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 31.93it/s, train_loss=0.0837, val_loss=0.0\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 31.93it/s, train_loss=0.0718, val_loss=0.0\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 31.93it/s, train_loss=0.0718, val_loss=0.0\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 31.93it/s, train_loss=0.175, val_loss=0.01\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 31.93it/s, train_loss=0.175, val_loss=0.01\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 31.93it/s, train_loss=0.0822, val_loss=0.0\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 32.10it/s, train_loss=0.0822, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 32.10it/s, train_loss=0.0822, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 32.10it/s, train_loss=0.0545, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 32.10it/s, train_loss=0.0545, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 32.10it/s, train_loss=0.141, val_loss=0.01\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 32.10it/s, train_loss=0.141, val_loss=0.01\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 32.10it/s, train_loss=0.0793, val_loss=0.0\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 32.10it/s, train_loss=0.0793, val_loss=0.0\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 32.10it/s, train_loss=0.0459, val_loss=0.0\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 32.28it/s, train_loss=0.0459, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 32.28it/s, train_loss=0.0459, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 32.28it/s, train_loss=0.109, val_loss=0.01\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 32.28it/s, train_loss=0.109, val_loss=0.01\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 32.04it/s, train_loss=0.0658, val_loss=0.0\u001b[A\n",
      " 90%|█████████████████████████████████████▊    | 90/100 [02:16<00:15,  1.53s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=6.96, val_loss=112, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=6.96, val_loss=112, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=634, val_loss=0.61, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=634, val_loss=0.61, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=3.39, val_loss=0.0777, train_c\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=3.39, val_loss=0.0777, train_c\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=0.361, val_loss=0.0338, train_\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 30.85it/s, train_loss=0.361, val_loss=0.0338\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 30.85it/s, train_loss=0.361, val_loss=0.0338\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 30.85it/s, train_loss=0.179, val_loss=0.124,\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 30.85it/s, train_loss=0.179, val_loss=0.124,\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 30.85it/s, train_loss=0.658, val_loss=0.0996\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 30.85it/s, train_loss=0.658, val_loss=0.0996\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 30.85it/s, train_loss=0.557, val_loss=0.0667\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 30.85it/s, train_loss=0.557, val_loss=0.0667\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.32210394489668126\n",
      "Its' val AUC : 0.29917085768655705\n",
      "Its external  AUC: 0.39168110918544197\n",
      "Curent best Test AUC: 0.3557921102066374\n",
      "Its' val AUC : 0.3016175071360609\n",
      "Its external  AUC: 0.37435008665511266\n",
      "Curent best Test AUC: 0.7273638071383844\n",
      "Its' val AUC : 0.6929454940872638\n",
      "Its external  AUC: 0.627540570348196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 30.85it/s, train_loss=0.382, val_loss=0.0209\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 31.54it/s, train_loss=0.382, val_loss=0.0209\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 31.54it/s, train_loss=0.382, val_loss=0.0209\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 31.54it/s, train_loss=0.118, val_loss=0.044,\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 31.54it/s, train_loss=0.118, val_loss=0.044,\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 31.54it/s, train_loss=0.205, val_loss=0.0799\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 31.54it/s, train_loss=0.205, val_loss=0.079\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 31.54it/s, train_loss=0.377, val_loss=0.037\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 31.54it/s, train_loss=0.377, val_loss=0.037\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 31.54it/s, train_loss=0.178, val_loss=0.004\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 32.63it/s, train_loss=0.178, val_loss=0.00\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.63it/s, train_loss=0.178, val_loss=0.00\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.63it/s, train_loss=0.0328, val_loss=0.0\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.63it/s, train_loss=0.0328, val_loss=0.0\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.63it/s, train_loss=0.136, val_loss=0.03\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.63it/s, train_loss=0.136, val_loss=0.03\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.63it/s, train_loss=0.19, val_loss=0.015\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.63it/s, train_loss=0.19, val_loss=0.015\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.63it/s, train_loss=0.0836, val_loss=0.0\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 33.19it/s, train_loss=0.0836, val_loss=0.0\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.19it/s, train_loss=0.0836, val_loss=0.0\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 33.19it/s, train_loss=0.0438, val_loss=0.0\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.19it/s, train_loss=0.0438, val_loss=0.0\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 33.19it/s, train_loss=0.119, val_loss=0.02\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.19it/s, train_loss=0.119, val_loss=0.02\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 33.19it/s, train_loss=0.111, val_loss=0.01\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.19it/s, train_loss=0.111, val_loss=0.01\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 33.19it/s, train_loss=0.0614, val_loss=0.0\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 33.75it/s, train_loss=0.0614, val_loss=0.0\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.75it/s, train_loss=0.0614, val_loss=0.0\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7337507827175955\n",
      "Its' val AUC : 0.7277422862579855\n",
      "Its external  AUC: 0.6172995115802742\n",
      "Epoch    18: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.75it/s, train_loss=0.0249, val_loss=0.0\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.75it/s, train_loss=0.0249, val_loss=0.0\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.75it/s, train_loss=0.0344, val_loss=0.0\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.75it/s, train_loss=0.0344, val_loss=0.0\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.75it/s, train_loss=0.0629, val_loss=0.0\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.75it/s, train_loss=0.0629, val_loss=0.0\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.75it/s, train_loss=0.0665, val_loss=0.0\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 33.91it/s, train_loss=0.0665, val_loss=0.0\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.91it/s, train_loss=0.0665, val_loss=0.0\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.91it/s, train_loss=0.0412, val_loss=0.0\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.91it/s, train_loss=0.0412, val_loss=0.0\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.91it/s, train_loss=0.02, val_loss=0.004\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.91it/s, train_loss=0.02, val_loss=0.004\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.91it/s, train_loss=0.0254, val_loss=0.0\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.91it/s, train_loss=0.0254, val_loss=0.0\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.91it/s, train_loss=0.0424, val_loss=0.0\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.27it/s, train_loss=0.0424, val_loss=0.0\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.27it/s, train_loss=0.0424, val_loss=0.0\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.27it/s, train_loss=0.0433, val_loss=0.0\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.27it/s, train_loss=0.0433, val_loss=0.0\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.27it/s, train_loss=0.026, val_loss=0.00\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.27it/s, train_loss=0.026, val_loss=0.00\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.27it/s, train_loss=0.0134, val_loss=0.0\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.27it/s, train_loss=0.0134, val_loss=0.0\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.27it/s, train_loss=0.0187, val_loss=0.0\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.44it/s, train_loss=0.0187, val_loss=0.0\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.44it/s, train_loss=0.0187, val_loss=0.0\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.44it/s, train_loss=0.03, val_loss=0.005\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.44it/s, train_loss=0.03, val_loss=0.005\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 34.44it/s, train_loss=0.0289, val_loss=0.0\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.44it/s, train_loss=0.0289, val_loss=0.0\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.44it/s, train_loss=0.0177, val_loss=0.0\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.44it/s, train_loss=0.0177, val_loss=0.0\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.44it/s, train_loss=0.0123, val_loss=0.0\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.48it/s, train_loss=0.0123, val_loss=0.0\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.48it/s, train_loss=0.0123, val_loss=0.0\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.48it/s, train_loss=0.0179, val_loss=0.0\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.48it/s, train_loss=0.0179, val_loss=0.0\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.48it/s, train_loss=0.0232, val_loss=0.0\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.48it/s, train_loss=0.0232, val_loss=0.0\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.48it/s, train_loss=0.0193, val_loss=0.0\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.48it/s, train_loss=0.0193, val_loss=0.0\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.48it/s, train_loss=0.0119, val_loss=0.0\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.29it/s, train_loss=0.0119, val_loss=0.0\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.29it/s, train_loss=0.0119, val_loss=0.0\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.29it/s, train_loss=0.0112, val_loss=0.0\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.29it/s, train_loss=0.0112, val_loss=0.0\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.29it/s, train_loss=0.0158, val_loss=0.0\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.29it/s, train_loss=0.0158, val_loss=0.0\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.29it/s, train_loss=0.017, val_loss=0.00\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.29it/s, train_loss=0.017, val_loss=0.00\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.29it/s, train_loss=0.0126, val_loss=0.0\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.37it/s, train_loss=0.0126, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.37it/s, train_loss=0.0126, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.37it/s, train_loss=0.00924, val_loss=0.\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.37it/s, train_loss=0.00924, val_loss=0.\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.37it/s, train_loss=0.0111, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.37it/s, train_loss=0.0111, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.37it/s, train_loss=0.0135, val_loss=0.0\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.37it/s, train_loss=0.0135, val_loss=0.0\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.37it/s, train_loss=0.0121, val_loss=0.0\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.47it/s, train_loss=0.0121, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.47it/s, train_loss=0.0121, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.47it/s, train_loss=0.00907, val_loss=0.\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.47it/s, train_loss=0.00907, val_loss=0.\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 33.94it/s, train_loss=0.0088, val_loss=0.0\u001b[A\n",
      " 91%|██████████████████████████████████████▏   | 91/100 [02:18<00:13,  1.52s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=12.5, val_loss=12.4, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=12.5, val_loss=12.4, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=65.8, val_loss=11.1, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=65.8, val_loss=11.1, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=55, val_loss=2.9, train_cindex\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=55, val_loss=2.9, train_cindex\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=18.2, val_loss=4.62, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.05it/s, train_loss=18.2, val_loss=4.62, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.05it/s, train_loss=18.2, val_loss=4.62, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.05it/s, train_loss=24.7, val_loss=0.952, \u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.05it/s, train_loss=24.7, val_loss=0.952, \u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.05it/s, train_loss=5.62, val_loss=2.34, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.05it/s, train_loss=5.62, val_loss=2.34, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.05it/s, train_loss=13.2, val_loss=1.86, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.05it/s, train_loss=13.2, val_loss=1.86, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.23782091421415152\n",
      "Its' val AUC : 0.29278238412396357\n",
      "Its external  AUC: 0.3823853789191744\n",
      "Curent best Test AUC: 0.754539762053851\n",
      "Its' val AUC : 0.7305967106157402\n",
      "Its external  AUC: 0.5985504962974634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.05it/s, train_loss=10.7, val_loss=1.09, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 34.15it/s, train_loss=10.7, val_loss=1.09, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.15it/s, train_loss=10.7, val_loss=1.09, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.15it/s, train_loss=6.75, val_loss=1.87, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.15it/s, train_loss=6.75, val_loss=1.87, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.15it/s, train_loss=8.1, val_loss=1.59, tr\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.15it/s, train_loss=8.1, val_loss=1.59, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.15it/s, train_loss=6.92, val_loss=0.46, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.15it/s, train_loss=6.92, val_loss=0.46, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.15it/s, train_loss=2.62, val_loss=0.601,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 33.40it/s, train_loss=2.62, val_loss=0.601\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.40it/s, train_loss=2.62, val_loss=0.601\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.40it/s, train_loss=3.62, val_loss=0.408\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.40it/s, train_loss=3.62, val_loss=0.408\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.40it/s, train_loss=2.45, val_loss=0.188\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.40it/s, train_loss=2.45, val_loss=0.188\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.40it/s, train_loss=0.937, val_loss=0.25\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.40it/s, train_loss=0.937, val_loss=0.25\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.40it/s, train_loss=1.63, val_loss=0.272\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 32.79it/s, train_loss=1.63, val_loss=0.272\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.79it/s, train_loss=1.63, val_loss=0.272\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.79it/s, train_loss=1.82, val_loss=0.14,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.79it/s, train_loss=1.82, val_loss=0.14,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.79it/s, train_loss=0.971, val_loss=0.10\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.79it/s, train_loss=0.971, val_loss=0.10\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.79it/s, train_loss=0.721, val_loss=0.21\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.79it/s, train_loss=0.721, val_loss=0.21\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.79it/s, train_loss=1.3, val_loss=0.211,\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 33.45it/s, train_loss=1.3, val_loss=0.211,\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.45it/s, train_loss=1.3, val_loss=0.211,\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.45it/s, train_loss=1.13, val_loss=0.072\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.45it/s, train_loss=1.13, val_loss=0.072\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.45it/s, train_loss=0.392, val_loss=0.06\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.45it/s, train_loss=0.392, val_loss=0.06\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.45it/s, train_loss=0.373, val_loss=0.09\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.45it/s, train_loss=0.373, val_loss=0.09\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.45it/s, train_loss=0.701, val_loss=0.07\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 33.88it/s, train_loss=0.701, val_loss=0.07\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.88it/s, train_loss=0.701, val_loss=0.07\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.88it/s, train_loss=0.514, val_loss=0.03\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.88it/s, train_loss=0.514, val_loss=0.03\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.88it/s, train_loss=0.271, val_loss=0.07\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.88it/s, train_loss=0.271, val_loss=0.07\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.88it/s, train_loss=0.482, val_loss=0.09\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.88it/s, train_loss=0.482, val_loss=0.09\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.88it/s, train_loss=0.648, val_loss=0.05\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.21it/s, train_loss=0.648, val_loss=0.05\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.21it/s, train_loss=0.648, val_loss=0.05\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.21it/s, train_loss=0.412, val_loss=0.03\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.21it/s, train_loss=0.412, val_loss=0.03\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.21it/s, train_loss=0.265, val_loss=0.06\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.21it/s, train_loss=0.265, val_loss=0.06\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.21it/s, train_loss=0.415, val_loss=0.06\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.21it/s, train_loss=0.415, val_loss=0.06\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.21it/s, train_loss=0.379, val_loss=0.03\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.34it/s, train_loss=0.379, val_loss=0.03\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.34it/s, train_loss=0.379, val_loss=0.03\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.34it/s, train_loss=0.241, val_loss=0.02\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.34it/s, train_loss=0.241, val_loss=0.02\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 34.34it/s, train_loss=0.128, val_loss=0.02\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.34it/s, train_loss=0.128, val_loss=0.02\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.34it/s, train_loss=0.155, val_loss=0.03\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.34it/s, train_loss=0.155, val_loss=0.03\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.34it/s, train_loss=0.206, val_loss=0.03\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.206, val_loss=0.03\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.206, val_loss=0.03\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.188, val_loss=0.01\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.188, val_loss=0.01\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    31: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.105, val_loss=0.01\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.105, val_loss=0.01\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.0682, val_loss=0.0\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.0682, val_loss=0.0\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.55it/s, train_loss=0.102, val_loss=0.02\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.66it/s, train_loss=0.102, val_loss=0.02\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.66it/s, train_loss=0.102, val_loss=0.02\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.66it/s, train_loss=0.157, val_loss=0.02\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.66it/s, train_loss=0.157, val_loss=0.02\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.66it/s, train_loss=0.149, val_loss=0.01\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.66it/s, train_loss=0.149, val_loss=0.01\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.66it/s, train_loss=0.104, val_loss=0.01\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.66it/s, train_loss=0.104, val_loss=0.01\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.66it/s, train_loss=0.0796, val_loss=0.0\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.73it/s, train_loss=0.0796, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.73it/s, train_loss=0.0796, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.73it/s, train_loss=0.103, val_loss=0.01\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.73it/s, train_loss=0.103, val_loss=0.01\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.73it/s, train_loss=0.11, val_loss=0.015\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.73it/s, train_loss=0.11, val_loss=0.015\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.73it/s, train_loss=0.103, val_loss=0.01\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.73it/s, train_loss=0.103, val_loss=0.01\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.73it/s, train_loss=0.0864, val_loss=0.0\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.78it/s, train_loss=0.0864, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.78it/s, train_loss=0.0864, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.78it/s, train_loss=0.069, val_loss=0.01\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.78it/s, train_loss=0.069, val_loss=0.01\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.24it/s, train_loss=0.0629, val_loss=0.0\u001b[A\n",
      " 92%|██████████████████████████████████████▋   | 92/100 [02:19<00:12,  1.51s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    44: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=40.9, val_loss=54.8, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=40.9, val_loss=54.8, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=288, val_loss=2.39, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=288, val_loss=2.39, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=14.5, val_loss=9.48, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=14.5, val_loss=9.48, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=52.5, val_loss=2.34, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 32.92it/s, train_loss=52.5, val_loss=2.34, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.92it/s, train_loss=52.5, val_loss=2.34, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.92it/s, train_loss=10.9, val_loss=5.61, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.92it/s, train_loss=10.9, val_loss=5.61, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.92it/s, train_loss=27.1, val_loss=0.386, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.92it/s, train_loss=27.1, val_loss=0.386, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.92it/s, train_loss=2.46, val_loss=2.67, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.92it/s, train_loss=2.46, val_loss=2.67, t\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.319724483406387\n",
      "Its' val AUC : 0.2976756830229713\n",
      "Its external  AUC: 0.38774224042854893\n",
      "Curent best Test AUC: 0.7294927989981215\n",
      "Its' val AUC : 0.584613293462009\n",
      "Its external  AUC: 0.3593823853789192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.92it/s, train_loss=13.2, val_loss=1.52, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 33.85it/s, train_loss=13.2, val_loss=1.52, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.85it/s, train_loss=13.2, val_loss=1.52, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.85it/s, train_loss=9.5, val_loss=5.4, tra\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.85it/s, train_loss=9.5, val_loss=5.4, tra\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.85it/s, train_loss=28.1, val_loss=1.47, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.85it/s, train_loss=28.1, val_loss=1.47, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.85it/s, train_loss=8.35, val_loss=0.654,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.85it/s, train_loss=8.35, val_loss=0.654,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.85it/s, train_loss=3, val_loss=0.541, tr\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.23it/s, train_loss=3, val_loss=0.541, t\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.23it/s, train_loss=3, val_loss=0.541, t\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.23it/s, train_loss=2.32, val_loss=0.332\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.23it/s, train_loss=2.32, val_loss=0.332\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.23it/s, train_loss=1.46, val_loss=0.149\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.23it/s, train_loss=1.46, val_loss=0.149\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.23it/s, train_loss=0.888, val_loss=0.10\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.23it/s, train_loss=0.888, val_loss=0.10\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.23it/s, train_loss=0.921, val_loss=0.09\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.43it/s, train_loss=0.921, val_loss=0.09\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.43it/s, train_loss=0.921, val_loss=0.09\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.43it/s, train_loss=0.926, val_loss=0.15\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.43it/s, train_loss=0.926, val_loss=0.15\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.43it/s, train_loss=1.07, val_loss=0.184\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.43it/s, train_loss=1.07, val_loss=0.184\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    12: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.43it/s, train_loss=1.13, val_loss=0.136\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.43it/s, train_loss=1.13, val_loss=0.136\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.43it/s, train_loss=0.907, val_loss=0.09\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.56it/s, train_loss=0.907, val_loss=0.09\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.56it/s, train_loss=0.907, val_loss=0.09\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.56it/s, train_loss=0.614, val_loss=0.09\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.56it/s, train_loss=0.614, val_loss=0.09\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.56it/s, train_loss=0.603, val_loss=0.10\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.56it/s, train_loss=0.603, val_loss=0.10\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.56it/s, train_loss=0.647, val_loss=0.10\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.56it/s, train_loss=0.647, val_loss=0.10\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.56it/s, train_loss=0.647, val_loss=0.08\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.63it/s, train_loss=0.647, val_loss=0.08\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.63it/s, train_loss=0.647, val_loss=0.08\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.63it/s, train_loss=0.523, val_loss=0.07\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.63it/s, train_loss=0.523, val_loss=0.07\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.63it/s, train_loss=0.415, val_loss=0.06\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.63it/s, train_loss=0.415, val_loss=0.06\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.63it/s, train_loss=0.341, val_loss=0.07\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.63it/s, train_loss=0.341, val_loss=0.07\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.63it/s, train_loss=0.442, val_loss=0.08\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.66it/s, train_loss=0.442, val_loss=0.08\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.66it/s, train_loss=0.442, val_loss=0.08\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.66it/s, train_loss=0.388, val_loss=0.09\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.66it/s, train_loss=0.388, val_loss=0.09\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.66it/s, train_loss=0.39, val_loss=0.058\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.66it/s, train_loss=0.39, val_loss=0.058\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.66it/s, train_loss=0.27, val_loss=0.056\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.66it/s, train_loss=0.27, val_loss=0.056\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.66it/s, train_loss=0.307, val_loss=0.04\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.67it/s, train_loss=0.307, val_loss=0.04\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.67it/s, train_loss=0.307, val_loss=0.04\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.67it/s, train_loss=0.244, val_loss=0.05\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.67it/s, train_loss=0.244, val_loss=0.05\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.67it/s, train_loss=0.24, val_loss=0.039\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.67it/s, train_loss=0.24, val_loss=0.039\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.67it/s, train_loss=0.197, val_loss=0.02\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.67it/s, train_loss=0.197, val_loss=0.02\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.67it/s, train_loss=0.163, val_loss=0.02\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.71it/s, train_loss=0.163, val_loss=0.02\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.71it/s, train_loss=0.163, val_loss=0.02\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.71it/s, train_loss=0.174, val_loss=0.02\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.71it/s, train_loss=0.174, val_loss=0.02\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.71it/s, train_loss=0.166, val_loss=0.02\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.71it/s, train_loss=0.166, val_loss=0.02\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.71it/s, train_loss=0.179, val_loss=0.02\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.71it/s, train_loss=0.179, val_loss=0.02\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.71it/s, train_loss=0.163, val_loss=0.02\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.71it/s, train_loss=0.163, val_loss=0.02\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.71it/s, train_loss=0.163, val_loss=0.02\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.71it/s, train_loss=0.155, val_loss=0.02\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.71it/s, train_loss=0.155, val_loss=0.02\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.71it/s, train_loss=0.151, val_loss=0.02\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.71it/s, train_loss=0.151, val_loss=0.02\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.71it/s, train_loss=0.144, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.71it/s, train_loss=0.144, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.71it/s, train_loss=0.148, val_loss=0.02\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.71it/s, train_loss=0.148, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.71it/s, train_loss=0.148, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.71it/s, train_loss=0.134, val_loss=0.02\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.71it/s, train_loss=0.134, val_loss=0.02\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.71it/s, train_loss=0.139, val_loss=0.02\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.71it/s, train_loss=0.139, val_loss=0.02\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.71it/s, train_loss=0.132, val_loss=0.02\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.71it/s, train_loss=0.132, val_loss=0.02\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.71it/s, train_loss=0.13, val_loss=0.026\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.72it/s, train_loss=0.13, val_loss=0.026\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.72it/s, train_loss=0.13, val_loss=0.026\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.72it/s, train_loss=0.128, val_loss=0.02\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.72it/s, train_loss=0.128, val_loss=0.02\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.56it/s, train_loss=0.122, val_loss=0.02\u001b[A\n",
      " 93%|███████████████████████████████████████   | 93/100 [02:21<00:10,  1.49s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=8.06, val_loss=8.73, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=8.06, val_loss=8.73, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=43.4, val_loss=10.1, train_cin\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    46: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Curent best Test AUC: 0.25660613650594866\n",
      "Its' val AUC : 0.27157808889493\n",
      "Its external  AUC: 0.3890026784307547\n",
      "Curent best Test AUC: 0.7668127739511584\n",
      "Its' val AUC : 0.7280141361968193\n",
      "Its external  AUC: 0.6035922483062864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=43.4, val_loss=10.1, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=52.1, val_loss=11, train_cinde\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=52.1, val_loss=11, train_cinde\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=57.5, val_loss=1.48, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.55it/s, train_loss=57.5, val_loss=1.48, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.55it/s, train_loss=57.5, val_loss=1.48, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.55it/s, train_loss=6.56, val_loss=8.82, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.55it/s, train_loss=6.56, val_loss=8.82, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.55it/s, train_loss=51.5, val_loss=0.804, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.55it/s, train_loss=51.5, val_loss=0.804, \u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.55it/s, train_loss=5.4, val_loss=5.87, tr\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.55it/s, train_loss=5.4, val_loss=5.87, tr\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.55it/s, train_loss=28.6, val_loss=2.22, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 34.33it/s, train_loss=28.6, val_loss=2.22, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.33it/s, train_loss=28.6, val_loss=2.22, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.33it/s, train_loss=11.2, val_loss=2.5, tr\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.33it/s, train_loss=11.2, val_loss=2.5, tr\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.33it/s, train_loss=13, val_loss=3.23, tra\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.33it/s, train_loss=13, val_loss=3.23, tr\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.33it/s, train_loss=15.9, val_loss=0.273,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.33it/s, train_loss=15.9, val_loss=0.273,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.33it/s, train_loss=1.45, val_loss=2.13, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.56it/s, train_loss=1.45, val_loss=2.13,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.56it/s, train_loss=1.45, val_loss=2.13,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.56it/s, train_loss=10.3, val_loss=0.55,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.56it/s, train_loss=10.3, val_loss=0.55,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.56it/s, train_loss=2.79, val_loss=1.47,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.56it/s, train_loss=2.79, val_loss=1.47,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.56it/s, train_loss=8.73, val_loss=0.112\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.56it/s, train_loss=8.73, val_loss=0.112\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.56it/s, train_loss=0.604, val_loss=0.43\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.68it/s, train_loss=0.604, val_loss=0.43\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.68it/s, train_loss=0.604, val_loss=0.43\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.68it/s, train_loss=2.21, val_loss=0.262\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.68it/s, train_loss=2.21, val_loss=0.262\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.68it/s, train_loss=1.64, val_loss=0.416\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.68it/s, train_loss=1.64, val_loss=0.416\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.68it/s, train_loss=2.27, val_loss=0.196\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.68it/s, train_loss=2.27, val_loss=0.196\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.68it/s, train_loss=0.969, val_loss=0.15\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.73it/s, train_loss=0.969, val_loss=0.15\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.73it/s, train_loss=0.969, val_loss=0.15\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.73it/s, train_loss=0.614, val_loss=0.28\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.73it/s, train_loss=0.614, val_loss=0.28\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.73it/s, train_loss=1.38, val_loss=0.215\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.73it/s, train_loss=1.38, val_loss=0.215\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.73it/s, train_loss=1.11, val_loss=0.178\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.73it/s, train_loss=1.11, val_loss=0.178\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.73it/s, train_loss=0.936, val_loss=0.13\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.82it/s, train_loss=0.936, val_loss=0.13\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.82it/s, train_loss=0.936, val_loss=0.13\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.82it/s, train_loss=0.641, val_loss=0.06\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.82it/s, train_loss=0.641, val_loss=0.06\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.82it/s, train_loss=0.277, val_loss=0.08\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.82it/s, train_loss=0.277, val_loss=0.08\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.82it/s, train_loss=0.564, val_loss=0.10\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.82it/s, train_loss=0.564, val_loss=0.10\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    21: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.82it/s, train_loss=0.633, val_loss=0.08\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.85it/s, train_loss=0.633, val_loss=0.08\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.85it/s, train_loss=0.633, val_loss=0.08\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.85it/s, train_loss=0.516, val_loss=0.06\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.85it/s, train_loss=0.516, val_loss=0.06\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.85it/s, train_loss=0.346, val_loss=0.05\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.85it/s, train_loss=0.346, val_loss=0.05\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.85it/s, train_loss=0.269, val_loss=0.03\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.85it/s, train_loss=0.269, val_loss=0.03\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.85it/s, train_loss=0.167, val_loss=0.04\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.76it/s, train_loss=0.167, val_loss=0.04\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.76it/s, train_loss=0.167, val_loss=0.04\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.76it/s, train_loss=0.236, val_loss=0.08\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.76it/s, train_loss=0.236, val_loss=0.08\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.76it/s, train_loss=0.384, val_loss=0.06\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.76it/s, train_loss=0.384, val_loss=0.06\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.76it/s, train_loss=0.322, val_loss=0.04\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.76it/s, train_loss=0.322, val_loss=0.04\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.76it/s, train_loss=0.225, val_loss=0.03\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.80it/s, train_loss=0.225, val_loss=0.03\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.80it/s, train_loss=0.225, val_loss=0.03\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.80it/s, train_loss=0.188, val_loss=0.03\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.80it/s, train_loss=0.188, val_loss=0.03\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.80it/s, train_loss=0.137, val_loss=0.03\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.80it/s, train_loss=0.137, val_loss=0.03\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.80it/s, train_loss=0.188, val_loss=0.04\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.80it/s, train_loss=0.188, val_loss=0.04\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.80it/s, train_loss=0.25, val_loss=0.037\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.80it/s, train_loss=0.25, val_loss=0.037\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.80it/s, train_loss=0.25, val_loss=0.037\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.80it/s, train_loss=0.19, val_loss=0.030\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.80it/s, train_loss=0.19, val_loss=0.030\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.80it/s, train_loss=0.143, val_loss=0.02\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.80it/s, train_loss=0.143, val_loss=0.02\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.80it/s, train_loss=0.122, val_loss=0.02\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.80it/s, train_loss=0.122, val_loss=0.02\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.80it/s, train_loss=0.109, val_loss=0.02\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.61it/s, train_loss=0.109, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.61it/s, train_loss=0.109, val_loss=0.02\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.61it/s, train_loss=0.159, val_loss=0.03\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.61it/s, train_loss=0.159, val_loss=0.03\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.61it/s, train_loss=0.167, val_loss=0.02\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.61it/s, train_loss=0.167, val_loss=0.02\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.61it/s, train_loss=0.124, val_loss=0.02\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.61it/s, train_loss=0.124, val_loss=0.02\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.61it/s, train_loss=0.107, val_loss=0.01\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.75it/s, train_loss=0.107, val_loss=0.01\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.75it/s, train_loss=0.107, val_loss=0.01\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.75it/s, train_loss=0.0871, val_loss=0.0\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.75it/s, train_loss=0.0871, val_loss=0.0\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.69it/s, train_loss=0.0972, val_loss=0.0\u001b[A\n",
      " 94%|███████████████████████████████████████▍  | 94/100 [02:22<00:08,  1.48s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=8.3, val_loss=27.9, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=8.3, val_loss=27.9, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=137, val_loss=67.4, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=137, val_loss=67.4, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=361, val_loss=24.6, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=361, val_loss=24.6, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=132, val_loss=1.2, train_cinde\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.35it/s, train_loss=132, val_loss=1.2, tra\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.35it/s, train_loss=132, val_loss=1.2, tra\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.35it/s, train_loss=8.3, val_loss=6.05, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.35it/s, train_loss=8.3, val_loss=6.05, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.35it/s, train_loss=33.9, val_loss=1.09, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.35it/s, train_loss=33.9, val_loss=1.09, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.35it/s, train_loss=7.3, val_loss=7.37, tr\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.35it/s, train_loss=7.3, val_loss=7.37, tr\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7345021916092673\n",
      "Its' val AUC : 0.7024602419464455\n",
      "Its external  AUC: 0.6092642193162123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.35it/s, train_loss=39.7, val_loss=0.787, \u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 33.96it/s, train_loss=39.7, val_loss=0.787, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.96it/s, train_loss=39.7, val_loss=0.787, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.96it/s, train_loss=4.79, val_loss=2.14, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.96it/s, train_loss=4.79, val_loss=2.14, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.96it/s, train_loss=10.9, val_loss=1.44, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.96it/s, train_loss=10.9, val_loss=1.44, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.96it/s, train_loss=7.48, val_loss=1.17, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.96it/s, train_loss=7.48, val_loss=1.17, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.96it/s, train_loss=6.66, val_loss=0.732,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.39it/s, train_loss=6.66, val_loss=0.732\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.39it/s, train_loss=6.66, val_loss=0.732\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.39it/s, train_loss=4.19, val_loss=0.47,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.39it/s, train_loss=4.19, val_loss=0.47,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.39it/s, train_loss=2.69, val_loss=0.266\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.39it/s, train_loss=2.69, val_loss=0.266\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.39it/s, train_loss=1.76, val_loss=0.236\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.39it/s, train_loss=1.76, val_loss=0.236\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.39it/s, train_loss=1.65, val_loss=0.312\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.50it/s, train_loss=1.65, val_loss=0.312\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.50it/s, train_loss=1.65, val_loss=0.312\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.50it/s, train_loss=1.99, val_loss=0.241\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.50it/s, train_loss=1.99, val_loss=0.241\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.50it/s, train_loss=1.53, val_loss=0.253\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.50it/s, train_loss=1.53, val_loss=0.253\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.50it/s, train_loss=1.48, val_loss=0.2, \u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.50it/s, train_loss=1.48, val_loss=0.2, \u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.50it/s, train_loss=1.08, val_loss=0.193\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.64it/s, train_loss=1.08, val_loss=0.193\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.64it/s, train_loss=1.08, val_loss=0.193\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.64it/s, train_loss=1.03, val_loss=0.168\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.64it/s, train_loss=1.03, val_loss=0.168\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.64it/s, train_loss=0.877, val_loss=0.12\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.64it/s, train_loss=0.877, val_loss=0.12\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.64it/s, train_loss=0.667, val_loss=0.10\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.64it/s, train_loss=0.667, val_loss=0.10\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.64it/s, train_loss=0.64, val_loss=0.076\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.73it/s, train_loss=0.64, val_loss=0.076\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.73it/s, train_loss=0.64, val_loss=0.076\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.73it/s, train_loss=0.458, val_loss=0.07\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.73it/s, train_loss=0.458, val_loss=0.07\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.73it/s, train_loss=0.461, val_loss=0.05\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.73it/s, train_loss=0.461, val_loss=0.05\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.73it/s, train_loss=0.377, val_loss=0.08\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.73it/s, train_loss=0.377, val_loss=0.08\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.73it/s, train_loss=0.455, val_loss=0.07\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.77it/s, train_loss=0.455, val_loss=0.07\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.77it/s, train_loss=0.455, val_loss=0.07\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.77it/s, train_loss=0.476, val_loss=0.08\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.77it/s, train_loss=0.476, val_loss=0.08\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.77it/s, train_loss=0.528, val_loss=0.08\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.77it/s, train_loss=0.528, val_loss=0.08\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.77it/s, train_loss=0.504, val_loss=0.07\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.77it/s, train_loss=0.504, val_loss=0.07\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.77it/s, train_loss=0.464, val_loss=0.06\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.79it/s, train_loss=0.464, val_loss=0.06\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.79it/s, train_loss=0.464, val_loss=0.06\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.79it/s, train_loss=0.444, val_loss=0.05\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.79it/s, train_loss=0.444, val_loss=0.05\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.79it/s, train_loss=0.405, val_loss=0.05\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.79it/s, train_loss=0.405, val_loss=0.05\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.79it/s, train_loss=0.374, val_loss=0.05\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.79it/s, train_loss=0.374, val_loss=0.05\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.79it/s, train_loss=0.356, val_loss=0.04\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.79it/s, train_loss=0.356, val_loss=0.04\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.79it/s, train_loss=0.356, val_loss=0.04\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.79it/s, train_loss=0.3, val_loss=0.0508\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.79it/s, train_loss=0.3, val_loss=0.0508\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.79it/s, train_loss=0.281, val_loss=0.04\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.79it/s, train_loss=0.281, val_loss=0.04\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    32: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.79it/s, train_loss=0.255, val_loss=0.04\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.79it/s, train_loss=0.255, val_loss=0.04\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.79it/s, train_loss=0.227, val_loss=0.04\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.76it/s, train_loss=0.227, val_loss=0.04\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.76it/s, train_loss=0.227, val_loss=0.04\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.76it/s, train_loss=0.225, val_loss=0.04\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.76it/s, train_loss=0.225, val_loss=0.04\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.76it/s, train_loss=0.198, val_loss=0.04\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.76it/s, train_loss=0.198, val_loss=0.04\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.76it/s, train_loss=0.185, val_loss=0.04\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.76it/s, train_loss=0.185, val_loss=0.04\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.76it/s, train_loss=0.169, val_loss=0.03\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.82it/s, train_loss=0.169, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.82it/s, train_loss=0.169, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.82it/s, train_loss=0.145, val_loss=0.03\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.82it/s, train_loss=0.145, val_loss=0.03\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.82it/s, train_loss=0.138, val_loss=0.02\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.82it/s, train_loss=0.138, val_loss=0.02\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.82it/s, train_loss=0.122, val_loss=0.03\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.82it/s, train_loss=0.122, val_loss=0.03\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.82it/s, train_loss=0.12, val_loss=0.029\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.83it/s, train_loss=0.12, val_loss=0.029\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.83it/s, train_loss=0.12, val_loss=0.029\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.83it/s, train_loss=0.119, val_loss=0.02\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.83it/s, train_loss=0.119, val_loss=0.02\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.66it/s, train_loss=0.115, val_loss=0.02\u001b[A\n",
      " 95%|███████████████████████████████████████▉  | 95/100 [02:24<00:07,  1.48s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=42.5, val_loss=10.3, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=42.5, val_loss=10.3, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=54.2, val_loss=18.9, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=54.2, val_loss=18.9, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=95.3, val_loss=0.324, train_ci\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=95.3, val_loss=0.324, train_ci\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=2.04, val_loss=4.66, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 32.68it/s, train_loss=2.04, val_loss=4.66, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.68it/s, train_loss=2.04, val_loss=4.66, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.68it/s, train_loss=25.7, val_loss=1.13, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.68it/s, train_loss=25.7, val_loss=1.13, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.68it/s, train_loss=5.85, val_loss=0.15, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.68it/s, train_loss=5.85, val_loss=0.15, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.68it/s, train_loss=0.791, val_loss=1.54, \u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.68it/s, train_loss=0.791, val_loss=1.54, \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.25710707576706326\n",
      "Its' val AUC : 0.2919668343074623\n",
      "Its external  AUC: 0.3782889554120057\n",
      "Curent best Test AUC: 0.7115842204132748\n",
      "Its' val AUC : 0.7161886638575506\n",
      "Its external  AUC: 0.6161966283283441\n",
      "Curent best Test AUC: 0.7169693174702567\n",
      "Its' val AUC : 0.7152371890716325\n",
      "Its external  AUC: 0.5906727587836773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.68it/s, train_loss=6.91, val_loss=0.875, \u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 32.64it/s, train_loss=6.91, val_loss=0.875, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.64it/s, train_loss=6.91, val_loss=0.875, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.64it/s, train_loss=5.03, val_loss=1.18, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.64it/s, train_loss=5.03, val_loss=1.18, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.64it/s, train_loss=7.18, val_loss=0.583, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.64it/s, train_loss=7.18, val_loss=0.583,\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.64it/s, train_loss=3.32, val_loss=0.428,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.64it/s, train_loss=3.32, val_loss=0.428,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.64it/s, train_loss=1.96, val_loss=0.113,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 32.70it/s, train_loss=1.96, val_loss=0.113\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.70it/s, train_loss=1.96, val_loss=0.113\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.70it/s, train_loss=0.558, val_loss=0.03\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.70it/s, train_loss=0.558, val_loss=0.03\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.70it/s, train_loss=0.196, val_loss=0.22\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.70it/s, train_loss=0.196, val_loss=0.22\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.70it/s, train_loss=1.23, val_loss=0.39,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.70it/s, train_loss=1.23, val_loss=0.39,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.70it/s, train_loss=2.05, val_loss=0.314\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=2.05, val_loss=0.314\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=2.05, val_loss=0.314\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=1.65, val_loss=0.119\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=1.65, val_loss=0.119\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=0.64, val_loss=0.020\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=0.64, val_loss=0.020\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=0.141, val_loss=0.09\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=0.141, val_loss=0.09\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.96it/s, train_loss=0.53, val_loss=0.206\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 33.34it/s, train_loss=0.53, val_loss=0.206\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.34it/s, train_loss=0.53, val_loss=0.206\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.34it/s, train_loss=1.08, val_loss=0.187\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.34it/s, train_loss=1.08, val_loss=0.187\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.34it/s, train_loss=0.961, val_loss=0.07\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.34it/s, train_loss=0.961, val_loss=0.07\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.34it/s, train_loss=0.355, val_loss=0.00\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.34it/s, train_loss=0.355, val_loss=0.00\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.34it/s, train_loss=0.0528, val_loss=0.0\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 33.72it/s, train_loss=0.0528, val_loss=0.0\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.72it/s, train_loss=0.0528, val_loss=0.0\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.72it/s, train_loss=0.229, val_loss=0.08\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.72it/s, train_loss=0.229, val_loss=0.08\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.72it/s, train_loss=0.484, val_loss=0.07\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.72it/s, train_loss=0.484, val_loss=0.07\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.72it/s, train_loss=0.439, val_loss=0.02\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.72it/s, train_loss=0.439, val_loss=0.02\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.72it/s, train_loss=0.132, val_loss=0.00\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.08it/s, train_loss=0.132, val_loss=0.00\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.08it/s, train_loss=0.132, val_loss=0.00\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.08it/s, train_loss=0.0413, val_loss=0.0\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.08it/s, train_loss=0.0413, val_loss=0.0\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.08it/s, train_loss=0.0963, val_loss=0.0\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.08it/s, train_loss=0.0963, val_loss=0.0\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.08it/s, train_loss=0.164, val_loss=0.02\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.08it/s, train_loss=0.164, val_loss=0.02\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.08it/s, train_loss=0.121, val_loss=0.00\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.30it/s, train_loss=0.121, val_loss=0.00\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.30it/s, train_loss=0.121, val_loss=0.00\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.30it/s, train_loss=0.0407, val_loss=0.0\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.30it/s, train_loss=0.0407, val_loss=0.0\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 34.30it/s, train_loss=0.0457, val_loss=0.0\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.30it/s, train_loss=0.0457, val_loss=0.0\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.30it/s, train_loss=0.0962, val_loss=0.0\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.30it/s, train_loss=0.0962, val_loss=0.0\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.30it/s, train_loss=0.088, val_loss=0.00\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.52it/s, train_loss=0.088, val_loss=0.00\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.52it/s, train_loss=0.088, val_loss=0.00\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.52it/s, train_loss=0.0514, val_loss=0.0\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.52it/s, train_loss=0.0514, val_loss=0.0\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.52it/s, train_loss=0.0213, val_loss=0.0\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.52it/s, train_loss=0.0213, val_loss=0.0\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.52it/s, train_loss=0.0208, val_loss=0.0\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.52it/s, train_loss=0.0208, val_loss=0.0\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.52it/s, train_loss=0.0419, val_loss=0.0\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.64it/s, train_loss=0.0419, val_loss=0.0\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.64it/s, train_loss=0.0419, val_loss=0.0\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    34: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.64it/s, train_loss=0.0573, val_loss=0.0\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.64it/s, train_loss=0.0573, val_loss=0.0\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.64it/s, train_loss=0.05, val_loss=0.005\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.64it/s, train_loss=0.05, val_loss=0.005\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.64it/s, train_loss=0.0288, val_loss=0.0\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.64it/s, train_loss=0.0288, val_loss=0.0\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.64it/s, train_loss=0.0146, val_loss=0.0\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.68it/s, train_loss=0.0146, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.68it/s, train_loss=0.0146, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.68it/s, train_loss=0.0186, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.68it/s, train_loss=0.0186, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.68it/s, train_loss=0.0317, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.68it/s, train_loss=0.0317, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.68it/s, train_loss=0.0374, val_loss=0.0\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.68it/s, train_loss=0.0374, val_loss=0.0\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.68it/s, train_loss=0.0298, val_loss=0.0\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.78it/s, train_loss=0.0298, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.78it/s, train_loss=0.0298, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.78it/s, train_loss=0.0176, val_loss=0.0\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.78it/s, train_loss=0.0176, val_loss=0.0\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.10it/s, train_loss=0.0127, val_loss=0.0\u001b[A\n",
      " 96%|████████████████████████████████████████▎ | 96/100 [02:25<00:05,  1.48s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=24.1, val_loss=22.8, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=24.1, val_loss=22.8, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=116, val_loss=30.9, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=116, val_loss=30.9, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=160, val_loss=0.375, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=160, val_loss=0.375, train_cin\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=2.06, val_loss=7.07, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 31.98it/s, train_loss=2.06, val_loss=7.07, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.98it/s, train_loss=2.06, val_loss=7.07, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.98it/s, train_loss=40.4, val_loss=3.47, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.98it/s, train_loss=40.4, val_loss=3.47, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.98it/s, train_loss=20.8, val_loss=1.2, tr\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.98it/s, train_loss=20.8, val_loss=1.2, tr\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.98it/s, train_loss=6.4, val_loss=3.76, tr\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.98it/s, train_loss=6.4, val_loss=3.76, tr\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.2603631809643081\n",
      "Its' val AUC : 0.27946173712110917\n",
      "Its external  AUC: 0.3945170946904049\n",
      "Curent best Test AUC: 0.6917971195992486\n",
      "Its' val AUC : 0.7047709664265326\n",
      "Its external  AUC: 0.6206081613360643\n",
      "Curent best Test AUC: 0.715090795241077\n",
      "Its' val AUC : 0.685741470708169\n",
      "Its external  AUC: 0.6239168110918544\n",
      "Curent best Test AUC: 0.7809643080776456\n",
      "Its' val AUC : 0.7103438901726247\n",
      "Its external  AUC: 0.6102095478178667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.98it/s, train_loss=19, val_loss=0.944, tr\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 32.09it/s, train_loss=19, val_loss=0.944, tr\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.09it/s, train_loss=19, val_loss=0.944, tr\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 32.09it/s, train_loss=5.48, val_loss=1.7, tr\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.09it/s, train_loss=5.48, val_loss=1.7, tr\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 32.09it/s, train_loss=8.97, val_loss=1.97, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.09it/s, train_loss=8.97, val_loss=1.97, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 32.09it/s, train_loss=10.2, val_loss=0.908,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.09it/s, train_loss=10.2, val_loss=0.908,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 32.09it/s, train_loss=5.01, val_loss=0.239,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 32.30it/s, train_loss=5.01, val_loss=0.239\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.30it/s, train_loss=5.01, val_loss=0.239\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.30it/s, train_loss=1.64, val_loss=0.559\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.30it/s, train_loss=1.64, val_loss=0.559\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.30it/s, train_loss=3.11, val_loss=0.767\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.30it/s, train_loss=3.11, val_loss=0.767\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.30it/s, train_loss=3.78, val_loss=0.355\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.30it/s, train_loss=3.78, val_loss=0.355\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     9: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.30it/s, train_loss=1.63, val_loss=0.139\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 32.69it/s, train_loss=1.63, val_loss=0.139\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.69it/s, train_loss=1.63, val_loss=0.139\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.69it/s, train_loss=0.986, val_loss=0.42\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.69it/s, train_loss=0.986, val_loss=0.42\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.69it/s, train_loss=2.97, val_loss=0.546\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.69it/s, train_loss=2.97, val_loss=0.546\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.69it/s, train_loss=3.72, val_loss=0.275\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.69it/s, train_loss=3.72, val_loss=0.275\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.69it/s, train_loss=1.97, val_loss=0.125\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 33.03it/s, train_loss=1.97, val_loss=0.125\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.03it/s, train_loss=1.97, val_loss=0.125\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.03it/s, train_loss=0.67, val_loss=0.294\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.03it/s, train_loss=0.67, val_loss=0.294\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.03it/s, train_loss=1.25, val_loss=0.357\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.03it/s, train_loss=1.25, val_loss=0.357\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.03it/s, train_loss=1.66, val_loss=0.171\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.03it/s, train_loss=1.66, val_loss=0.171\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.03it/s, train_loss=0.918, val_loss=0.12\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 33.45it/s, train_loss=0.918, val_loss=0.12\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.45it/s, train_loss=0.918, val_loss=0.12\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.45it/s, train_loss=0.762, val_loss=0.28\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.45it/s, train_loss=0.762, val_loss=0.28\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.45it/s, train_loss=1.51, val_loss=0.271\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.45it/s, train_loss=1.51, val_loss=0.271\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.45it/s, train_loss=1.42, val_loss=0.16,\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.45it/s, train_loss=1.42, val_loss=0.16,\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.45it/s, train_loss=0.862, val_loss=0.05\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 33.88it/s, train_loss=0.862, val_loss=0.05\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.88it/s, train_loss=0.862, val_loss=0.05\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.88it/s, train_loss=0.32, val_loss=0.037\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.88it/s, train_loss=0.32, val_loss=0.037\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.88it/s, train_loss=0.239, val_loss=0.09\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.88it/s, train_loss=0.239, val_loss=0.09\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.88it/s, train_loss=0.499, val_loss=0.12\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.88it/s, train_loss=0.499, val_loss=0.12\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.88it/s, train_loss=0.637, val_loss=0.09\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.11it/s, train_loss=0.637, val_loss=0.09\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.11it/s, train_loss=0.637, val_loss=0.09\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    26: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.11it/s, train_loss=0.492, val_loss=0.04\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.11it/s, train_loss=0.492, val_loss=0.04\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 34.11it/s, train_loss=0.288, val_loss=0.03\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.11it/s, train_loss=0.288, val_loss=0.03\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.11it/s, train_loss=0.297, val_loss=0.06\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.11it/s, train_loss=0.297, val_loss=0.06\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.11it/s, train_loss=0.482, val_loss=0.07\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.30it/s, train_loss=0.482, val_loss=0.07\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.30it/s, train_loss=0.482, val_loss=0.07\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.30it/s, train_loss=0.524, val_loss=0.06\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.30it/s, train_loss=0.524, val_loss=0.06\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.30it/s, train_loss=0.463, val_loss=0.04\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.30it/s, train_loss=0.463, val_loss=0.04\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.30it/s, train_loss=0.331, val_loss=0.02\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.30it/s, train_loss=0.331, val_loss=0.02\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.30it/s, train_loss=0.199, val_loss=0.01\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.44it/s, train_loss=0.199, val_loss=0.01\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.44it/s, train_loss=0.199, val_loss=0.01\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.44it/s, train_loss=0.124, val_loss=0.01\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.44it/s, train_loss=0.124, val_loss=0.01\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    35: reducing learning rate of group 0 to 1.2500e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.44it/s, train_loss=0.123, val_loss=0.02\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.44it/s, train_loss=0.123, val_loss=0.02\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.44it/s, train_loss=0.164, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.44it/s, train_loss=0.164, val_loss=0.03\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.44it/s, train_loss=0.197, val_loss=0.03\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.53it/s, train_loss=0.197, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.53it/s, train_loss=0.197, val_loss=0.03\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.53it/s, train_loss=0.193, val_loss=0.02\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.53it/s, train_loss=0.193, val_loss=0.02\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.53it/s, train_loss=0.158, val_loss=0.01\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.53it/s, train_loss=0.158, val_loss=0.01\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.53it/s, train_loss=0.123, val_loss=0.01\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.53it/s, train_loss=0.123, val_loss=0.01\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.53it/s, train_loss=0.113, val_loss=0.01\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.56it/s, train_loss=0.113, val_loss=0.01\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.56it/s, train_loss=0.113, val_loss=0.01\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.56it/s, train_loss=0.123, val_loss=0.01\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.56it/s, train_loss=0.123, val_loss=0.01\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 33.85it/s, train_loss=0.142, val_loss=0.02\u001b[A\n",
      " 97%|████████████████████████████████████████▋ | 97/100 [02:27<00:04,  1.48s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=30.6, val_loss=20.3, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=30.6, val_loss=20.3, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=109, val_loss=64.4, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=109, val_loss=64.4, train_cind\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Curent best Test AUC: 0.7298685034439574\n",
      "Its' val AUC : 0.7038194916406144\n",
      "Its external  AUC: 0.6220261540885458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=337, val_loss=9.45, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=337, val_loss=9.45, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=49, val_loss=23.1, train_cinde\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 33.26it/s, train_loss=49, val_loss=23.1, tra\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.26it/s, train_loss=49, val_loss=23.1, tra\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 33.26it/s, train_loss=122, val_loss=2.31, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.26it/s, train_loss=122, val_loss=2.31, tr\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 33.26it/s, train_loss=16.7, val_loss=18.4, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.26it/s, train_loss=16.7, val_loss=18.4, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 33.26it/s, train_loss=94.3, val_loss=5.9, tr\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.26it/s, train_loss=94.3, val_loss=5.9, tr\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 33.26it/s, train_loss=30.7, val_loss=9.89, t\u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 34.18it/s, train_loss=30.7, val_loss=9.89, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.18it/s, train_loss=30.7, val_loss=9.89, t\u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 34.18it/s, train_loss=58.7, val_loss=3.35, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.18it/s, train_loss=58.7, val_loss=3.35, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 34.18it/s, train_loss=21.6, val_loss=5.62, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.18it/s, train_loss=21.6, val_loss=5.62, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 34.18it/s, train_loss=26.7, val_loss=5.32, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.18it/s, train_loss=26.7, val_loss=5.32, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 34.18it/s, train_loss=25.8, val_loss=1.14, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 34.56it/s, train_loss=25.8, val_loss=1.14,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.56it/s, train_loss=25.8, val_loss=1.14,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 34.56it/s, train_loss=5.89, val_loss=1.49,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.56it/s, train_loss=5.89, val_loss=1.49,\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 34.56it/s, train_loss=8.32, val_loss=6.04,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.56it/s, train_loss=8.32, val_loss=6.04,\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 34.56it/s, train_loss=32.1, val_loss=3.25,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.56it/s, train_loss=32.1, val_loss=3.25,\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 34.56it/s, train_loss=15.5, val_loss=6.73,\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.70it/s, train_loss=15.5, val_loss=6.73,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.70it/s, train_loss=15.5, val_loss=6.73,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.70it/s, train_loss=37.3, val_loss=5.91,\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.70it/s, train_loss=37.3, val_loss=5.91,\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    11: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch    18: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.70it/s, train_loss=32.9, val_loss=1.87,\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.70it/s, train_loss=32.9, val_loss=1.87,\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.70it/s, train_loss=8.53, val_loss=2.82,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.70it/s, train_loss=8.53, val_loss=2.82,\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.70it/s, train_loss=12.8, val_loss=0.777\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.65it/s, train_loss=12.8, val_loss=0.777\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.65it/s, train_loss=12.8, val_loss=0.777\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.65it/s, train_loss=4.67, val_loss=0.884\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.65it/s, train_loss=4.67, val_loss=0.884\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.65it/s, train_loss=5.46, val_loss=0.487\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.65it/s, train_loss=5.46, val_loss=0.487\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.65it/s, train_loss=3.3, val_loss=0.149,\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.65it/s, train_loss=3.3, val_loss=0.149,\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.65it/s, train_loss=1.23, val_loss=0.209\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.74it/s, train_loss=1.23, val_loss=0.209\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.74it/s, train_loss=1.23, val_loss=0.209\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.74it/s, train_loss=1.32, val_loss=0.472\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.74it/s, train_loss=1.32, val_loss=0.472\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.74it/s, train_loss=2.61, val_loss=0.605\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.74it/s, train_loss=2.61, val_loss=0.605\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.74it/s, train_loss=3.35, val_loss=0.481\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.74it/s, train_loss=3.35, val_loss=0.481\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.74it/s, train_loss=2.8, val_loss=0.273,\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.79it/s, train_loss=2.8, val_loss=0.273,\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.79it/s, train_loss=2.8, val_loss=0.273,\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.79it/s, train_loss=1.76, val_loss=0.222\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.79it/s, train_loss=1.76, val_loss=0.222\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.79it/s, train_loss=1.46, val_loss=0.261\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.79it/s, train_loss=1.46, val_loss=0.261\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.79it/s, train_loss=1.63, val_loss=0.286\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.79it/s, train_loss=1.63, val_loss=0.286\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.79it/s, train_loss=1.73, val_loss=0.269\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.84it/s, train_loss=1.73, val_loss=0.269\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.84it/s, train_loss=1.73, val_loss=0.269\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.84it/s, train_loss=1.6, val_loss=0.218,\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.84it/s, train_loss=1.6, val_loss=0.218,\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.84it/s, train_loss=1.31, val_loss=0.153\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.84it/s, train_loss=1.31, val_loss=0.153\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.84it/s, train_loss=0.968, val_loss=0.09\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.84it/s, train_loss=0.968, val_loss=0.09\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    29: reducing learning rate of group 0 to 1.2500e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.84it/s, train_loss=0.688, val_loss=0.07\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.85it/s, train_loss=0.688, val_loss=0.07\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.85it/s, train_loss=0.688, val_loss=0.07\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.85it/s, train_loss=0.543, val_loss=0.07\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.85it/s, train_loss=0.543, val_loss=0.07\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.85it/s, train_loss=0.546, val_loss=0.09\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.85it/s, train_loss=0.546, val_loss=0.09\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.85it/s, train_loss=0.647, val_loss=0.12\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.85it/s, train_loss=0.647, val_loss=0.12\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.85it/s, train_loss=0.766, val_loss=0.13\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.87it/s, train_loss=0.766, val_loss=0.13\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.87it/s, train_loss=0.766, val_loss=0.13\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.87it/s, train_loss=0.833, val_loss=0.13\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.87it/s, train_loss=0.833, val_loss=0.13\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.87it/s, train_loss=0.823, val_loss=0.11\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.87it/s, train_loss=0.823, val_loss=0.11\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.87it/s, train_loss=0.754, val_loss=0.10\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.87it/s, train_loss=0.754, val_loss=0.10\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.87it/s, train_loss=0.683, val_loss=0.08\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.85it/s, train_loss=0.683, val_loss=0.08\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.85it/s, train_loss=0.683, val_loss=0.08\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.85it/s, train_loss=0.623, val_loss=0.07\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.85it/s, train_loss=0.623, val_loss=0.07\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.85it/s, train_loss=0.584, val_loss=0.07\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.85it/s, train_loss=0.584, val_loss=0.07\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.85it/s, train_loss=0.564, val_loss=0.07\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.85it/s, train_loss=0.564, val_loss=0.07\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.85it/s, train_loss=0.551, val_loss=0.07\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.87it/s, train_loss=0.551, val_loss=0.07\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.87it/s, train_loss=0.551, val_loss=0.07\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.87it/s, train_loss=0.532, val_loss=0.06\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.87it/s, train_loss=0.532, val_loss=0.06\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.73it/s, train_loss=0.501, val_loss=0.06\u001b[A\n",
      " 98%|█████████████████████████████████████████▏| 98/100 [02:28<00:02,  1.48s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=24.6, val_loss=164, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=24.6, val_loss=164, train_cind\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=906, val_loss=6.32, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=906, val_loss=6.32, train_cind\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=31, val_loss=0.446, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=31, val_loss=0.446, train_cind\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=2.53, val_loss=1.27, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 31.90it/s, train_loss=2.53, val_loss=1.27, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.90it/s, train_loss=2.53, val_loss=1.27, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 31.90it/s, train_loss=6.98, val_loss=1.78, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.90it/s, train_loss=6.98, val_loss=1.78, t\u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 31.90it/s, train_loss=9.63, val_loss=3.25, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.90it/s, train_loss=9.63, val_loss=3.25, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 31.90it/s, train_loss=18.1, val_loss=0.545, \u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.90it/s, train_loss=18.1, val_loss=0.545, \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.2846587351283657\n",
      "Its' val AUC : 0.2896561098273753\n",
      "Its external  AUC: 0.3890026784307547\n",
      "Curent best Test AUC: 0.29805886036318097\n",
      "Its' val AUC : 0.28707353540845454\n",
      "Its external  AUC: 0.4276035922483063\n",
      "Curent best Test AUC: 0.5302442078897933\n",
      "Its' val AUC : 0.5620497485388066\n",
      "Its external  AUC: 0.6357334173625335\n",
      "Curent best Test AUC: 0.6435817157169693\n",
      "Its' val AUC : 0.6301481582166644\n",
      "Its external  AUC: 0.6228139278399244\n",
      "Curent best Test AUC: 0.6771446462116468\n",
      "Its' val AUC : 0.655973902405872\n",
      "Its external  AUC: 0.6213959350874428\n",
      "Curent best Test AUC: 0.6860363180964308\n",
      "Its' val AUC : 0.6644012505097187\n",
      "Its external  AUC: 0.6190326138333071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 31.90it/s, train_loss=3.07, val_loss=0.301, \u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 31.64it/s, train_loss=3.07, val_loss=0.301, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 31.64it/s, train_loss=3.07, val_loss=0.301, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 31.64it/s, train_loss=1.66, val_loss=0.541, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 31.64it/s, train_loss=1.66, val_loss=0.541, \u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 31.64it/s, train_loss=2.8, val_loss=0.677, t\u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 31.64it/s, train_loss=2.8, val_loss=0.677, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 31.64it/s, train_loss=3.56, val_loss=0.575,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 31.64it/s, train_loss=3.56, val_loss=0.575,\u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 31.64it/s, train_loss=3.06, val_loss=0.382,\u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 32.60it/s, train_loss=3.06, val_loss=0.382\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.60it/s, train_loss=3.06, val_loss=0.382\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 32.60it/s, train_loss=2.14, val_loss=0.261\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.60it/s, train_loss=2.14, val_loss=0.261\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 32.60it/s, train_loss=1.57, val_loss=0.265\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.60it/s, train_loss=1.57, val_loss=0.265\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 32.60it/s, train_loss=1.67, val_loss=0.305\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.60it/s, train_loss=1.67, val_loss=0.305\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 32.60it/s, train_loss=1.89, val_loss=0.267\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:01, 32.86it/s, train_loss=1.89, val_loss=0.267\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.86it/s, train_loss=1.89, val_loss=0.267\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:01, 32.86it/s, train_loss=1.66, val_loss=0.175\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.86it/s, train_loss=1.66, val_loss=0.175\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:01, 32.86it/s, train_loss=1.14, val_loss=0.138\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.86it/s, train_loss=1.14, val_loss=0.138\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:01, 32.86it/s, train_loss=0.9, val_loss=0.15, \u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.86it/s, train_loss=0.9, val_loss=0.15, \u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:01, 32.86it/s, train_loss=0.934, val_loss=0.15\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 33.34it/s, train_loss=0.934, val_loss=0.15\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.34it/s, train_loss=0.934, val_loss=0.15\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 33.34it/s, train_loss=0.915, val_loss=0.11\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.34it/s, train_loss=0.915, val_loss=0.11\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.6862867877269881\n",
      "Its' val AUC : 0.5958950659236102\n",
      "Its external  AUC: 0.6355758626122577\n",
      "Curent best Test AUC: 0.687413901064496\n",
      "Its' val AUC : 0.588011417697431\n",
      "Its external  AUC: 0.6358909721128092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 33.34it/s, train_loss=0.749, val_loss=0.09\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.34it/s, train_loss=0.749, val_loss=0.09\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 33.34it/s, train_loss=0.626, val_loss=0.10\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.34it/s, train_loss=0.626, val_loss=0.10\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 33.34it/s, train_loss=0.692, val_loss=0.10\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 33.53it/s, train_loss=0.692, val_loss=0.10\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.53it/s, train_loss=0.692, val_loss=0.10\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 33.53it/s, train_loss=0.69, val_loss=0.072\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.53it/s, train_loss=0.69, val_loss=0.072\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 33.53it/s, train_loss=0.512, val_loss=0.06\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.53it/s, train_loss=0.512, val_loss=0.06\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 33.53it/s, train_loss=0.448, val_loss=0.07\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.53it/s, train_loss=0.448, val_loss=0.07\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 33.53it/s, train_loss=0.453, val_loss=0.05\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 33.36it/s, train_loss=0.453, val_loss=0.05\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.36it/s, train_loss=0.453, val_loss=0.05\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 33.36it/s, train_loss=0.319, val_loss=0.03\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.36it/s, train_loss=0.319, val_loss=0.03\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curent best Test AUC: 0.7127113337507828\n",
      "Its' val AUC : 0.5049612613837162\n",
      "Its external  AUC: 0.616826847329447\n",
      "Curent best Test AUC: 0.7178459611772072\n",
      "Its' val AUC : 0.5084953105885551\n",
      "Its external  AUC: 0.6193477233338586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 33.36it/s, train_loss=0.215, val_loss=0.04\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.36it/s, train_loss=0.215, val_loss=0.04\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 33.36it/s, train_loss=0.252, val_loss=0.03\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.36it/s, train_loss=0.252, val_loss=0.03\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 33.36it/s, train_loss=0.225, val_loss=0.02\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 33.49it/s, train_loss=0.225, val_loss=0.02\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 33.49it/s, train_loss=0.225, val_loss=0.02\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 33.49it/s, train_loss=0.141, val_loss=0.03\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 33.49it/s, train_loss=0.141, val_loss=0.03\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:01<00:00, 33.49it/s, train_loss=0.151, val_loss=0.03\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 33.49it/s, train_loss=0.151, val_loss=0.03\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 33.49it/s, train_loss=0.157, val_loss=0.01\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 33.49it/s, train_loss=0.157, val_loss=0.01\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 33.49it/s, train_loss=0.0939, val_loss=0.0\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 33.75it/s, train_loss=0.0939, val_loss=0.0\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.75it/s, train_loss=0.0939, val_loss=0.0\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 33.75it/s, train_loss=0.0802, val_loss=0.0\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.75it/s, train_loss=0.0802, val_loss=0.0\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 33.75it/s, train_loss=0.113, val_loss=0.01\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.75it/s, train_loss=0.113, val_loss=0.01\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 33.75it/s, train_loss=0.0929, val_loss=0.0\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.75it/s, train_loss=0.0929, val_loss=0.0\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 33.75it/s, train_loss=0.0728, val_loss=0.0\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.00it/s, train_loss=0.0728, val_loss=0.0\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.00it/s, train_loss=0.0728, val_loss=0.0\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.00it/s, train_loss=0.098, val_loss=0.01\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.00it/s, train_loss=0.098, val_loss=0.01\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.00it/s, train_loss=0.0932, val_loss=0.0\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.00it/s, train_loss=0.0932, val_loss=0.0\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.00it/s, train_loss=0.0656, val_loss=0.0\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.00it/s, train_loss=0.0656, val_loss=0.0\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.00it/s, train_loss=0.0764, val_loss=0.0\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.24it/s, train_loss=0.0764, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.24it/s, train_loss=0.0764, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.24it/s, train_loss=0.0855, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.24it/s, train_loss=0.0855, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.24it/s, train_loss=0.0701, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.24it/s, train_loss=0.0701, val_loss=0.0\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.24it/s, train_loss=0.0768, val_loss=0.0\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.24it/s, train_loss=0.0768, val_loss=0.0\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.24it/s, train_loss=0.0887, val_loss=0.0\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.40it/s, train_loss=0.0887, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.40it/s, train_loss=0.0887, val_loss=0.0\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.40it/s, train_loss=0.0744, val_loss=0.0\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.40it/s, train_loss=0.0744, val_loss=0.0\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 33.66it/s, train_loss=0.0682, val_loss=0.0\u001b[A\n",
      " 99%|█████████████████████████████████████████▌| 99/100 [02:30<00:01,  1.48s/it]\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:   0%| | 0/50 [00:00<?, ?it/s, train_loss=33.9, val_loss=9.77, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=33.9, val_loss=9.77, train_cin\u001b[A\n",
      "Epoch 1:   0%| | 0/50 [00:00<?, ?it/s, train_loss=49.3, val_loss=3.25, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=49.3, val_loss=3.25, train_cin\u001b[A\n",
      "Epoch 2:   0%| | 0/50 [00:00<?, ?it/s, train_loss=16, val_loss=6.2, train_cindex\u001b[A\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=16, val_loss=6.2, train_cindex\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    48: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Curent best Test AUC: 0.24834063869755793\n",
      "Its' val AUC : 0.27130623895609624\n",
      "Its external  AUC: 0.3937293209390263\n",
      "Curent best Test AUC: 0.29530369442705073\n",
      "Its' val AUC : 0.26790811472067416\n",
      "Its external  AUC: 0.41326610997321567\n",
      "Curent best Test AUC: 0.7328741390106449\n",
      "Its' val AUC : 0.6824792714421639\n",
      "Its external  AUC: 0.6141484165747597\n",
      "Curent best Test AUC: 0.7437695679398872\n",
      "Its' val AUC : 0.6718771238276472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3:   0%| | 0/50 [00:00<?, ?it/s, train_loss=33.9, val_loss=3.64, train_cin\u001b[A\n",
      "Epoch 3:   8%| | 4/50 [00:00<00:01, 32.72it/s, train_loss=33.9, val_loss=3.64, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.72it/s, train_loss=33.9, val_loss=3.64, t\u001b[A\n",
      "Epoch 4:   8%| | 4/50 [00:00<00:01, 32.72it/s, train_loss=20.6, val_loss=0.734, \u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.72it/s, train_loss=20.6, val_loss=0.734, \u001b[A\n",
      "Epoch 5:   8%| | 4/50 [00:00<00:01, 32.72it/s, train_loss=4.32, val_loss=4.28, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.72it/s, train_loss=4.32, val_loss=4.28, t\u001b[A\n",
      "Epoch 6:   8%| | 4/50 [00:00<00:01, 32.72it/s, train_loss=21.9, val_loss=2.43, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.72it/s, train_loss=21.9, val_loss=2.43, t\u001b[A\n",
      "Epoch 7:   8%| | 4/50 [00:00<00:01, 32.72it/s, train_loss=12.5, val_loss=0.399, \u001b[A\n",
      "Epoch 7:  16%|▏| 8/50 [00:00<00:01, 33.36it/s, train_loss=12.5, val_loss=0.399, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.36it/s, train_loss=12.5, val_loss=0.399, \u001b[A\n",
      "Epoch 8:  16%|▏| 8/50 [00:00<00:01, 33.36it/s, train_loss=2.76, val_loss=1.56, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.36it/s, train_loss=2.76, val_loss=1.56, t\u001b[A\n",
      "Epoch 9:  16%|▏| 8/50 [00:00<00:01, 33.36it/s, train_loss=8.48, val_loss=0.637, \u001b[A\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.36it/s, train_loss=8.48, val_loss=0.637,\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its external  AUC: 0.6191901685835828\n",
      "Curent best Test AUC: 0.7506574827802129\n",
      "Its' val AUC : 0.6495854288432785\n",
      "Its external  AUC: 0.6188750590830313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10:  16%|▏| 8/50 [00:00<00:01, 33.36it/s, train_loss=3.7, val_loss=0.473, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.36it/s, train_loss=3.7, val_loss=0.473, \u001b[A\n",
      "Epoch 11:  16%|▏| 8/50 [00:00<00:01, 33.36it/s, train_loss=2.6, val_loss=0.771, \u001b[A\n",
      "Epoch 11:  24%|▏| 12/50 [00:00<00:01, 33.83it/s, train_loss=2.6, val_loss=0.771,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.83it/s, train_loss=2.6, val_loss=0.771,\u001b[A\n",
      "Epoch 12:  24%|▏| 12/50 [00:00<00:01, 33.83it/s, train_loss=3.99, val_loss=0.112\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.83it/s, train_loss=3.99, val_loss=0.112\u001b[A\n",
      "Epoch 13:  24%|▏| 12/50 [00:00<00:01, 33.83it/s, train_loss=0.69, val_loss=0.458\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.83it/s, train_loss=0.69, val_loss=0.458\u001b[A\n",
      "Epoch 14:  24%|▏| 12/50 [00:00<00:01, 33.83it/s, train_loss=2.61, val_loss=0.206\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.83it/s, train_loss=2.61, val_loss=0.206\u001b[A\n",
      "Epoch 15:  24%|▏| 12/50 [00:00<00:01, 33.83it/s, train_loss=1.2, val_loss=0.121,\u001b[A\n",
      "Epoch 15:  32%|▎| 16/50 [00:00<00:00, 34.22it/s, train_loss=1.2, val_loss=0.121,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.22it/s, train_loss=1.2, val_loss=0.121,\u001b[A\n",
      "Epoch 16:  32%|▎| 16/50 [00:00<00:00, 34.22it/s, train_loss=0.698, val_loss=0.29\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.22it/s, train_loss=0.698, val_loss=0.29\u001b[A\n",
      "Epoch 17:  32%|▎| 16/50 [00:00<00:00, 34.22it/s, train_loss=1.64, val_loss=0.869\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.22it/s, train_loss=1.64, val_loss=0.869\u001b[A\n",
      "Epoch 18:  32%|▎| 16/50 [00:00<00:00, 34.22it/s, train_loss=4.43, val_loss=0.603\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.22it/s, train_loss=4.43, val_loss=0.603\u001b[A\n",
      "Epoch 19:  32%|▎| 16/50 [00:00<00:00, 34.22it/s, train_loss=3.42, val_loss=0.325\u001b[A\n",
      "Epoch 19:  40%|▍| 20/50 [00:00<00:00, 34.19it/s, train_loss=3.42, val_loss=0.325\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.19it/s, train_loss=3.42, val_loss=0.325\u001b[A\n",
      "Epoch 20:  40%|▍| 20/50 [00:00<00:00, 34.19it/s, train_loss=1.91, val_loss=0.049\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.19it/s, train_loss=1.91, val_loss=0.049\u001b[A\n",
      "Epoch 21:  40%|▍| 20/50 [00:00<00:00, 34.19it/s, train_loss=0.355, val_loss=0.16\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.19it/s, train_loss=0.355, val_loss=0.16\u001b[A\n",
      "Epoch 22:  40%|▍| 20/50 [00:00<00:00, 34.19it/s, train_loss=0.82, val_loss=0.366\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.19it/s, train_loss=0.82, val_loss=0.366\u001b[A\n",
      "Epoch 23:  40%|▍| 20/50 [00:00<00:00, 34.19it/s, train_loss=1.83, val_loss=0.229\u001b[A\n",
      "Epoch 23:  48%|▍| 24/50 [00:00<00:00, 34.35it/s, train_loss=1.83, val_loss=0.229\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.35it/s, train_loss=1.83, val_loss=0.229\u001b[A\n",
      "Epoch 24:  48%|▍| 24/50 [00:00<00:00, 34.35it/s, train_loss=1.19, val_loss=0.038\u001b[A\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.35it/s, train_loss=1.19, val_loss=0.038\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    19: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25:  48%|▍| 24/50 [00:00<00:00, 34.35it/s, train_loss=0.272, val_loss=0.11\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.35it/s, train_loss=0.272, val_loss=0.11\u001b[A\n",
      "Epoch 26:  48%|▍| 24/50 [00:00<00:00, 34.35it/s, train_loss=0.746, val_loss=0.17\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.35it/s, train_loss=0.746, val_loss=0.17\u001b[A\n",
      "Epoch 27:  48%|▍| 24/50 [00:00<00:00, 34.35it/s, train_loss=1.04, val_loss=0.079\u001b[A\n",
      "Epoch 27:  56%|▌| 28/50 [00:00<00:00, 34.56it/s, train_loss=1.04, val_loss=0.079\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.56it/s, train_loss=1.04, val_loss=0.079\u001b[A\n",
      "Epoch 28:  56%|▌| 28/50 [00:00<00:00, 34.56it/s, train_loss=0.521, val_loss=0.04\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.56it/s, train_loss=0.521, val_loss=0.04\u001b[A\n",
      "Epoch 29:  56%|▌| 28/50 [00:00<00:00, 34.56it/s, train_loss=0.228, val_loss=0.12\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.56it/s, train_loss=0.228, val_loss=0.12\u001b[A\n",
      "Epoch 30:  56%|▌| 28/50 [00:00<00:00, 34.56it/s, train_loss=0.611, val_loss=0.12\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.56it/s, train_loss=0.611, val_loss=0.12\u001b[A\n",
      "Epoch 31:  56%|▌| 28/50 [00:00<00:00, 34.56it/s, train_loss=0.594, val_loss=0.07\u001b[A\n",
      "Epoch 31:  64%|▋| 32/50 [00:00<00:00, 34.61it/s, train_loss=0.594, val_loss=0.07\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.61it/s, train_loss=0.594, val_loss=0.07\u001b[A\n",
      "Epoch 32:  64%|▋| 32/50 [00:00<00:00, 34.61it/s, train_loss=0.333, val_loss=0.02\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.61it/s, train_loss=0.333, val_loss=0.02\u001b[A\n",
      "Epoch 33:  64%|▋| 32/50 [00:00<00:00, 34.61it/s, train_loss=0.116, val_loss=0.01\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:00<00:00, 34.61it/s, train_loss=0.116, val_loss=0.01\u001b[A\n",
      "Epoch 34:  64%|▋| 32/50 [00:01<00:00, 34.61it/s, train_loss=0.133, val_loss=0.04\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.61it/s, train_loss=0.133, val_loss=0.04\u001b[A\n",
      "Epoch 35:  64%|▋| 32/50 [00:01<00:00, 34.61it/s, train_loss=0.282, val_loss=0.05\u001b[A\n",
      "Epoch 35:  72%|▋| 36/50 [00:01<00:00, 34.67it/s, train_loss=0.282, val_loss=0.05\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.67it/s, train_loss=0.282, val_loss=0.05\u001b[A\n",
      "Epoch 36:  72%|▋| 36/50 [00:01<00:00, 34.67it/s, train_loss=0.34, val_loss=0.037\u001b[A\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.67it/s, train_loss=0.34, val_loss=0.037\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    31: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37:  72%|▋| 36/50 [00:01<00:00, 34.67it/s, train_loss=0.241, val_loss=0.01\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.67it/s, train_loss=0.241, val_loss=0.01\u001b[A\n",
      "Epoch 38:  72%|▋| 36/50 [00:01<00:00, 34.67it/s, train_loss=0.122, val_loss=0.01\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.67it/s, train_loss=0.122, val_loss=0.01\u001b[A\n",
      "Epoch 39:  72%|▋| 36/50 [00:01<00:00, 34.67it/s, train_loss=0.121, val_loss=0.03\u001b[A\n",
      "Epoch 39:  80%|▊| 40/50 [00:01<00:00, 34.79it/s, train_loss=0.121, val_loss=0.03\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.79it/s, train_loss=0.121, val_loss=0.03\u001b[A\n",
      "Epoch 40:  80%|▊| 40/50 [00:01<00:00, 34.79it/s, train_loss=0.205, val_loss=0.04\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.79it/s, train_loss=0.205, val_loss=0.04\u001b[A\n",
      "Epoch 41:  80%|▊| 40/50 [00:01<00:00, 34.79it/s, train_loss=0.239, val_loss=0.03\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.79it/s, train_loss=0.239, val_loss=0.03\u001b[A\n",
      "Epoch 42:  80%|▊| 40/50 [00:01<00:00, 34.79it/s, train_loss=0.166, val_loss=0.01\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.79it/s, train_loss=0.166, val_loss=0.01\u001b[A\n",
      "Epoch 43:  80%|▊| 40/50 [00:01<00:00, 34.79it/s, train_loss=0.0749, val_loss=0.0\u001b[A\n",
      "Epoch 43:  88%|▉| 44/50 [00:01<00:00, 34.80it/s, train_loss=0.0749, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.80it/s, train_loss=0.0749, val_loss=0.0\u001b[A\n",
      "Epoch 44:  88%|▉| 44/50 [00:01<00:00, 34.80it/s, train_loss=0.0649, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.80it/s, train_loss=0.0649, val_loss=0.0\u001b[A\n",
      "Epoch 45:  88%|▉| 44/50 [00:01<00:00, 34.80it/s, train_loss=0.122, val_loss=0.02\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.80it/s, train_loss=0.122, val_loss=0.02\u001b[A\n",
      "Epoch 46:  88%|▉| 44/50 [00:01<00:00, 34.80it/s, train_loss=0.156, val_loss=0.01\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.80it/s, train_loss=0.156, val_loss=0.01\u001b[A\n",
      "Epoch 47:  88%|▉| 44/50 [00:01<00:00, 34.80it/s, train_loss=0.123, val_loss=0.01\u001b[A\n",
      "Epoch 47:  96%|▉| 48/50 [00:01<00:00, 34.87it/s, train_loss=0.123, val_loss=0.01\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.87it/s, train_loss=0.123, val_loss=0.01\u001b[A\n",
      "Epoch 48:  96%|▉| 48/50 [00:01<00:00, 34.87it/s, train_loss=0.0754, val_loss=0.0\u001b[A\n",
      "Epoch 49:  96%|▉| 48/50 [00:01<00:00, 34.87it/s, train_loss=0.0754, val_loss=0.0\u001b[A\n",
      "Epoch 49: 100%|█| 50/50 [00:01<00:00, 34.47it/s, train_loss=0.0724, val_loss=0.0\u001b[A\n",
      "100%|█████████████████████████████████████████| 100/100 [02:31<00:00,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    50: reducing learning rate of group 0 to 1.2500e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(100)):    \n",
    "    save_dic = {\"model\":\"SAGE\", \n",
    "                \"hid_feats\":256, \n",
    "                'out_feats':16, \n",
    "                'reg_l2':0.00001,\n",
    "                \"aggregator_type\":'mean',\n",
    "                \"save_path\":\"/home/jielian/lung-graph-project/Tumor_tranformer/logs/OS_new_data/\"}\n",
    "    model = SAGE(g_sh.ndata['h'].shape[1],hid_feats=save_dic[\"hid_feats\"],out_feats=save_dic['out_feats'], \n",
    "                       activation = F.leaky_relu, aggregator_type=save_dic['aggregator_type'])\n",
    "    train(g_sh, g_external, model, device, save_dic, idx_train, idx_val, idx_test, 50, patience=5, reg_l2=save_dic[\"reg_l2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dcae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dic = {\"model\":\"TAG\", \n",
    "#             \"hid_feats\":128, \n",
    "#             'out_feats':12, \n",
    "#             'reg_l2':0.00001,\n",
    "#             \"aggregator_type\":'mean',\n",
    "#             \"save_path\":\"/home/jielian/lung-graph-project/Tumor_tranformer/logs/OS_new_data/\"}\n",
    "# model = TAG(g_sh.ndata['h'].shape[1],hid_feats=save_dic[\"hid_feats\"],out_feats=save_dic['out_feats'], \n",
    "#                    activation = F.leaky_relu)\n",
    "# train(g_sh, g_external, model, device,save_dic, idx_train,idx_val, idx_test, 50, patience=10, reg_l2=save_dic[\"reg_l2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3bddd3",
   "metadata": {},
   "source": [
    "# Testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fc5b02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 0.6707406797711475\n",
      "Validation: 0.6620905260296317\n",
      "Testing: 0.7703193487789606\n",
      "External: 0.6563730896486529\n",
      "External: 0.6563730896486529\n"
     ]
    }
   ],
   "source": [
    "g = g_sh\n",
    "#test!\n",
    "save_dic = {\"model\":\"SAGE\", \n",
    "            \"hid_feats\":128, \n",
    "            'out_feats':32, \n",
    "            \"aggregator_type\":'mean',\n",
    "            \"save_path\":\"/home/jielian/lung-graph-project/Tumor_tranformer/logs/OS_new_data/\"}\n",
    "features = g.ndata['h']\n",
    "e_feature = g.edata['w']\n",
    "labels = g.ndata['label']\n",
    "events = g.ndata['event']\n",
    "model = SAGE(g.ndata['h'].shape[1],hid_feats=save_dic[\"hid_feats\"],out_feats=save_dic['out_feats'], \n",
    "                   activation = F.relu, aggregator_type=save_dic['aggregator_type'])\n",
    "pre_train='SAGE128321e-05mean_ep9_val0.667_test0.745_exte0.649.pth.gz'\n",
    "state_dict=torch.load(os.path.join(save_dic[\"save_path\"]+pre_train), map_location='cpu')\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "outputs = model.forward(g, features,e_feature)\n",
    "auc_train = c_index(-outputs[idx_train], g_all.ndata['label'][idx_train],g_all.ndata['event'][idx_train])\n",
    "print(\"Training:\", auc_train)\n",
    "auc_val = c_index(-outputs[idx_val], g_all.ndata['label'][idx_val],g_all.ndata['event'][idx_val])\n",
    "print(\"Validation:\", auc_val)\n",
    "auc_test = c_index(-outputs[idx_test], g_all.ndata['label'][idx_test],g_all.ndata['event'][idx_test])\n",
    "print(\"Testing:\", auc_test)\n",
    "outputs_val= model.forward(g_external, g_external.ndata['h'],g_external.edata['w'])\n",
    "Exter_test = c_index(-outputs_val, g_all.ndata['label'][idx_external_val],g_all.ndata['event'][idx_external_val])\n",
    "print(\"External:\",Exter_test)\n",
    "Exter_test_val = c_index(-outputs_val, g_external.ndata['label'],g_external.ndata['event'])\n",
    "print(\"External:\",Exter_test_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b57b855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7611772072636193\n",
      "0.7953200645508338\n",
      "0.6584213014022373\n",
      "0.6646026831785344\n"
     ]
    }
   ],
   "source": [
    "result1, result2 = np.load(\"logs/OS_new_data/SAGE256161e-05mean_ep19_val0.72_test0.761_exte0.658.npy\", allow_pickle=True)\n",
    "\n",
    "auc_test = c_index(-result1[idx_test], g_all.ndata['label'][idx_test],g_all.ndata['event'][idx_test])\n",
    "print(auc_test)\n",
    "print(roc_auc_score(g_all.ndata['event'][idx_test],result1[idx_test]))\n",
    "\n",
    "print(c_index(-result2, g_external.ndata['label'],g_external.ndata['event']))\n",
    "print(roc_auc_score(g_external.ndata['event'],result2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d91ea89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7703193487789606\n",
      "0.6563730896486529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_190523/3655968121.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  patient_info['risk'] = risk_score\n"
     ]
    }
   ],
   "source": [
    "from lifelines.utils import concordance_index\n",
    "risk_score = []\n",
    "for i in range(len(outputs)):\n",
    "    risk_score.append(outputs[i].item())\n",
    "patient_info['risk'] = risk_score\n",
    "test = patient_info.iloc[test_id,:]\n",
    "print(concordance_index(test['OS_Month'], -test['risk'], test['OS_Status']))\n",
    "\n",
    "exter_risk_score = []\n",
    "for i in range(len(outputs_val)):\n",
    "    exter_risk_score.append(outputs_val[i].item())\n",
    "external_info['risk'] = exter_risk_score\n",
    "print(concordance_index(external_info['OS_Month'], -external_info['risk'], external_info['OS_Status']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47a19112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_result_path = \"/home/jielian/lung-graph-project/Tumor_tranformer/logs/results/\"\n",
    "# test_result_name = pre_train[:-7]+\".csv\"\n",
    "# test.to_csv(test_result_path+test_result_name, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7655eede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTtest_result_path = \"/home/jielian/lung-graph-project/Tumor_tranformer/logs/results/\"\n",
    "# EXTtest_result_name = pre_train[:-7]+\"_EXTERNAL.csv\"\n",
    "# external_info.to_csv(EXTtest_result_path+EXTtest_result_name, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eba785fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========AUC=========\n",
      "OS on Testing Set: 0.8004303388918773\n",
      "OS on External Set: 0.6590987272101823\n",
      "RFS on Testing Set: 0.7244111820383006\n",
      "RFS on External Set: 0.6633320507887649\n"
     ]
    }
   ],
   "source": [
    "print(\"========AUC=========\")\n",
    "print(\"OS on Testing Set:\", roc_auc_score(test['OS_Status'], test['risk']))\n",
    "print(\"OS on External Set:\",roc_auc_score(external_info['OS_Status'], external_info['risk']))\n",
    "print(\"RFS on Testing Set:\",roc_auc_score(test['RFS_Status'], test['risk']))\n",
    "print(\"RFS on External Set:\",roc_auc_score(external_info['RFS_Status'], external_info['risk']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93326ee3",
   "metadata": {},
   "source": [
    "# Subanalysis: High and Low Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d877bf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import KaplanMeierFitter, CoxPHFitter, calibration\n",
    "from lifelines.statistics import logrank_test\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f58c171c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC_value: 0.8004303388918773\n",
      "Threshold value is: 3.053135633468628\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_roc_curve(fpr, tpr):\n",
    "    plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "y_true = test['OS_Status']\n",
    "y_scores = test['risk']\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "print(\"AUC_value:\",roc_auc_score(y_true, y_scores))\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(\"Threshold value is:\", optimal_threshold)\n",
    "plot_roc_curve(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe76b832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_190523/927916738.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['risk_label'] = risk_label\n"
     ]
    }
   ],
   "source": [
    "thred = optimal_threshold\n",
    "risk_label = []\n",
    "for risk_score in test['risk']:\n",
    "    if risk_score <= thred:\n",
    "        risk_label.append(0)\n",
    "    else:\n",
    "        risk_label.append(1)\n",
    "test['risk_label'] = risk_label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f92c84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28e8c3a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Stage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/gnn/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Stage'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_190523/3608923548.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Stage as reference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mTs1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Stage'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'OS_Month'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mEs1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Stage'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'OS_Status'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mkmfs1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKaplanMeierFitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"TNM Low Risk\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mkmfs1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTs1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEs1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gnn/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gnn/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Stage'"
     ]
    }
   ],
   "source": [
    "# Stage as reference\n",
    "Ts1 = test[test['Stage']==0]['OS_Month']\n",
    "Es1 = test[test['Stage']==0]['OS_Status']\n",
    "kmfs1 = KaplanMeierFitter(label=\"TNM Low Risk\")\n",
    "kmfs1.fit(Ts1, Es1)\n",
    "kmfs1.survival_function_\n",
    "kmfs1.cumulative_density_\n",
    "kmfs1.plot_survival_function(ci_show =True)\n",
    "# kmf2.plot_cumulative_density()\n",
    "\n",
    "Ts2 = test[test['Stage']==1]['OS_Month']\n",
    "Es2 = test[test['Stage']==1]['OS_Status']\n",
    "kmfs2 = KaplanMeierFitter(label=\"TNM High Risk\")\n",
    "kmfs2.fit(Ts2, Es2)\n",
    "kmfs2.survival_function_\n",
    "kmfs2.cumulative_density_\n",
    "kmfs2.plot_survival_function(ci_show =True)\n",
    "# kmf2.plot_cumulative_density()\n",
    "stage_results=logrank_test(Ts1,Ts2,event_observed_A=Es1, event_observed_B=Es2)\n",
    "stage_results.print_summary()\n",
    "\n",
    "\n",
    "TGCN_low = test[test['risk_label']==0]['OS_Month']\n",
    "EGCN_low = test[test['risk_label']==0]['OS_Status']\n",
    "kmGCN_low = KaplanMeierFitter(label=\"GCN Low Risk\")\n",
    "kmGCN_low.fit(TGCN_low, EGCN_low)\n",
    "kmGCN_low.survival_function_\n",
    "kmGCN_low.cumulative_density_\n",
    "kmGCN_low.plot_survival_function(ci_show =True)\n",
    "# kmf2.plot_cumulative_density()\n",
    "\n",
    "TGCN_high = test[test['risk_label']==1]['OS_Month']\n",
    "EGCN_high = test[test['risk_label']==1]['OS_Status']\n",
    "kmGCN_high = KaplanMeierFitter(label=\"GCN High Risk\")\n",
    "kmGCN_high.fit(TGCN_high, EGCN_high)\n",
    "kmGCN_high.survival_function_\n",
    "kmGCN_high.cumulative_density_\n",
    "kmGCN_high.plot_survival_function(ci_show =True)\n",
    "# kmf2.plot_cumulative_density()\n",
    "gcn_results=logrank_test(TGCN_low,TGCN_high,event_observed_A=EGCN_low, event_observed_B=EGCN_high)\n",
    "gcn_results.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ecb68e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.offsetbox import AnchoredText\n",
    "fig = plt.figure(figsize=(20,7))\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "kmGCN_low.plot_survival_function(ci_show =False)\n",
    "kmGCN_high.plot_survival_function(ci_show =False, color='r')\n",
    "ax1.title.set_text('Transformer Graph Model Kaplan-Meier Curve ')\n",
    "ax1.title.set_fontsize(15)\n",
    "ax1.set_ylim([0.4, 1.0])\n",
    "ax1.set_xlim([0.0, 65.0])\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "p1 = gcn_results.p_value\n",
    "ax1.add_artist(AnchoredText(\"p = %.3e\" % round(p1, 10), loc=4, frameon=False))\n",
    "# ax1.title(\"GCN Kaplan-Meier Curve\")\n",
    "ax1.legend(loc='lower left',prop={'size': 10})\n",
    "\n",
    "\n",
    "ax2 =  fig.add_subplot(132)\n",
    "kmfs1.plot_survival_function(ci_show =False)\n",
    "kmfs2.plot_survival_function(ci_show =False, color='r')\n",
    "ax2.title.set_text('TNM Model Kaplan-Meier Curve ')\n",
    "ax2.title.set_fontsize(15)\n",
    "ax2.set_ylim([0.4, 1.0])\n",
    "ax2.set_xlim([0.0, 65.0])\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "p2 = stage_results.p_value\n",
    "ax2.add_artist(AnchoredText(\"p = %.3e\" % round(p2,10 ), loc=4, frameon=False))\n",
    "ax2.legend(loc='lower left',prop={'size': 10})\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0b044f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>lifelines.CoxPHFitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duration col</th>\n",
       "      <td>'OS_Month'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event col</th>\n",
       "      <td>'OS_Status'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline estimation</th>\n",
       "      <td>breslow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number of observations</th>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number of events observed</th>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partial log-likelihood</th>\n",
       "      <td>-218.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time fit was run</th>\n",
       "      <td>2022-01-18 04:44:27 UTC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th style=\"min-width: 12px;\"></th>\n",
       "      <th style=\"min-width: 12px;\">coef</th>\n",
       "      <th style=\"min-width: 12px;\">exp(coef)</th>\n",
       "      <th style=\"min-width: 12px;\">se(coef)</th>\n",
       "      <th style=\"min-width: 12px;\">coef lower 95%</th>\n",
       "      <th style=\"min-width: 12px;\">coef upper 95%</th>\n",
       "      <th style=\"min-width: 12px;\">exp(coef) lower 95%</th>\n",
       "      <th style=\"min-width: 12px;\">exp(coef) upper 95%</th>\n",
       "      <th style=\"min-width: 12px;\">z</th>\n",
       "      <th style=\"min-width: 12px;\">p</th>\n",
       "      <th style=\"min-width: 12px;\">-log2(p)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>risk</th>\n",
       "      <td>6.90</td>\n",
       "      <td>996.70</td>\n",
       "      <td>1.37</td>\n",
       "      <td>4.22</td>\n",
       "      <td>9.59</td>\n",
       "      <td>67.75</td>\n",
       "      <td>14663.18</td>\n",
       "      <td>5.03</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>20.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><br><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Concordance</th>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Partial AIC</th>\n",
       "      <td>438.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-likelihood ratio test</th>\n",
       "      <td>20.58 on 1 df</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-log2(p) of ll-ratio test</th>\n",
       "      <td>17.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/latex": [
       "\\begin{tabular}{lrrrrrrrrrr}\n",
       "\\toprule\n",
       "{} &  coef &  exp(coef) &  se(coef) &  coef lower 95\\% &  coef upper 95\\% &  exp(coef) lower 95\\% &  exp(coef) upper 95\\% &    z &    p &  -log2(p) \\\\\n",
       "covariate &       &            &           &                 &                 &                      &                      &      &      &           \\\\\n",
       "\\midrule\n",
       "risk      &  6.90 &     996.70 &      1.37 &            4.22 &            9.59 &                67.75 &             14663.18 & 5.03 & 0.00 &     20.98 \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "<lifelines.CoxPHFitter: fitted with 213 total observations, 169 right-censored observations>\n",
       "             duration col = 'OS_Month'\n",
       "                event col = 'OS_Status'\n",
       "      baseline estimation = breslow\n",
       "   number of observations = 213\n",
       "number of events observed = 44\n",
       "   partial log-likelihood = -218.37\n",
       "         time fit was run = 2022-01-18 04:44:27 UTC\n",
       "\n",
       "---\n",
       "            coef  exp(coef)   se(coef)   coef lower 95%   coef upper 95%  exp(coef) lower 95%  exp(coef) upper 95%\n",
       "covariate                                                                                                         \n",
       "risk        6.90     996.70       1.37             4.22             9.59                67.75             14663.18\n",
       "\n",
       "             z      p   -log2(p)\n",
       "covariate                       \n",
       "risk      5.03 <0.005      20.98\n",
       "---\n",
       "Concordance = 0.74\n",
       "Partial AIC = 438.75\n",
       "log-likelihood ratio test = 20.58 on 1 df\n",
       "-log2(p) of ll-ratio test = 17.41"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICI =  0.022461336857945203\n",
      "E50 =  0.016288437251466448\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "GCN_cox = test[['OS_Month','OS_Status', 'risk']]\n",
    "cph_GCN = CoxPHFitter()\n",
    "cph_GCN.fit(GCN_cox, 'OS_Month', event_col='OS_Status')\n",
    "cph_GCN.print_summary()\n",
    "axGCN, ICI, E50 = calibration.survival_probability_calibration(cph_GCN, GCN_cox, t0=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af4b981c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>lifelines.CoxPHFitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duration col</th>\n",
       "      <td>'OS_Month'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event col</th>\n",
       "      <td>'OS_Status'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline estimation</th>\n",
       "      <td>breslow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number of observations</th>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number of events observed</th>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partial log-likelihood</th>\n",
       "      <td>-219.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time fit was run</th>\n",
       "      <td>2022-01-18 04:44:27 UTC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th style=\"min-width: 12px;\"></th>\n",
       "      <th style=\"min-width: 12px;\">coef</th>\n",
       "      <th style=\"min-width: 12px;\">exp(coef)</th>\n",
       "      <th style=\"min-width: 12px;\">se(coef)</th>\n",
       "      <th style=\"min-width: 12px;\">coef lower 95%</th>\n",
       "      <th style=\"min-width: 12px;\">coef upper 95%</th>\n",
       "      <th style=\"min-width: 12px;\">exp(coef) lower 95%</th>\n",
       "      <th style=\"min-width: 12px;\">exp(coef) upper 95%</th>\n",
       "      <th style=\"min-width: 12px;\">z</th>\n",
       "      <th style=\"min-width: 12px;\">p</th>\n",
       "      <th style=\"min-width: 12px;\">-log2(p)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Stage</th>\n",
       "      <td>1.36</td>\n",
       "      <td>3.90</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.96</td>\n",
       "      <td>2.15</td>\n",
       "      <td>7.07</td>\n",
       "      <td>4.48</td>\n",
       "      <td>&lt;0.005</td>\n",
       "      <td>17.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><br><div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Concordance</th>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Partial AIC</th>\n",
       "      <td>441.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>log-likelihood ratio test</th>\n",
       "      <td>18.05 on 1 df</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-log2(p) of ll-ratio test</th>\n",
       "      <td>15.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/latex": [
       "\\begin{tabular}{lrrrrrrrrrr}\n",
       "\\toprule\n",
       "{} &  coef &  exp(coef) &  se(coef) &  coef lower 95\\% &  coef upper 95\\% &  exp(coef) lower 95\\% &  exp(coef) upper 95\\% &    z &    p &  -log2(p) \\\\\n",
       "covariate &       &            &           &                 &                 &                      &                      &      &      &           \\\\\n",
       "\\midrule\n",
       "Stage     &  1.36 &       3.90 &      0.30 &            0.77 &            1.96 &                 2.15 &                 7.07 & 4.48 & 0.00 &     17.04 \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "<lifelines.CoxPHFitter: fitted with 213 total observations, 169 right-censored observations>\n",
       "             duration col = 'OS_Month'\n",
       "                event col = 'OS_Status'\n",
       "      baseline estimation = breslow\n",
       "   number of observations = 213\n",
       "number of events observed = 44\n",
       "   partial log-likelihood = -219.64\n",
       "         time fit was run = 2022-01-18 04:44:27 UTC\n",
       "\n",
       "---\n",
       "            coef  exp(coef)   se(coef)   coef lower 95%   coef upper 95%  exp(coef) lower 95%  exp(coef) upper 95%\n",
       "covariate                                                                                                         \n",
       "Stage       1.36       3.90       0.30             0.77             1.96                 2.15                 7.07\n",
       "\n",
       "             z      p   -log2(p)\n",
       "covariate                       \n",
       "Stage     4.48 <0.005      17.04\n",
       "---\n",
       "Concordance = 0.63\n",
       "Partial AIC = 441.27\n",
       "log-likelihood ratio test = 18.05 on 1 df\n",
       "-log2(p) of ll-ratio test = 15.51"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICI =  0.021696812855151695\n",
      "E50 =  0.015424934145033764\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_stage = test[['OS_Month','OS_Status', 'Stage']]\n",
    "cph_stage = CoxPHFitter()\n",
    "cph_stage.fit(test_stage, 'OS_Month', event_col='OS_Status')\n",
    "cph_stage.print_summary()\n",
    "ax_stage, ICI, E50 = calibration.survival_probability_calibration(cph_stage, test_stage, t0=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eefc57",
   "metadata": {},
   "source": [
    "# Subanalysis: Inside the Graph\n",
    "### Turn to Networkx Plotting and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eaa3bb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thred(risk, best_thresh = optimal_threshold):\n",
    "    if risk < best_thresh:\n",
    "        label = -1\n",
    "    else:\n",
    "        label = 1\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ab19bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213\n",
      "21213\n",
      "99.59154929577464\n"
     ]
    }
   ],
   "source": [
    "# Get the trained graph\n",
    "g_test = dgl.node_subgraph(g_sh, idx_test)\n",
    "g_test.ndata['h'] = torch.from_numpy(np.array(test['risk']))\n",
    "print(g_test.number_of_nodes())\n",
    "print(g_test.number_of_edges())\n",
    "print(g_test.number_of_edges()/g_test.number_of_nodes())\n",
    "nx_G = g_test.to_networkx()\n",
    "\n",
    "# add node and edge data to the graph\n",
    "for i in range(len(nx_G.nodes())):\n",
    "    nx_G.nodes[i][\"Risk\"] = g_test.ndata['h'][i].item()\n",
    "    nx_G.nodes[i][\"Risk_Label\"] = thred(g_test.ndata['h'][i].item())\n",
    "    nx_G.nodes[i][\"E\"] = g_test.ndata['event'][i].item()\n",
    "    nx_G.nodes[i][\"T\"] = g_test.ndata['label'][i].item()\n",
    "\n",
    "w = g_test.edata['w'].numpy()\n",
    "for i in range(len(nx_G.edges())):\n",
    "    edge = list(nx_G.edges())[i]\n",
    "    nx_G.edges[edge[0],edge[1],0][\"w\"] = g_test.edata['w'][i].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3cb5d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.readwrite import json_graph\n",
    "import json\n",
    "# import json\n",
    "data = json_graph.node_link_data(nx_G)\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NpEncoder, self).default(obj)\n",
    "jsodata = json.dumps(data, cls=NpEncoder)\n",
    "with open('dense_data.json', 'w') as f:\n",
    "    json.dump(jsodata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b090e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = dict(nx_G.nodes(data=\"Risk\"))\n",
    "# low, *_, high = sorted(d.values())\n",
    "# norm = mpl.colors.Normalize(vmin=low, vmax=high, clip=True)\n",
    "# mapper = mpl.cm.ScalarMappable(norm=norm, cmap=mpl.cm.coolwarm)\n",
    "# plt.figure(figsize=[20,10])\n",
    "# nx.draw_spring(nx_G, nodelist=d, node_size= 400,\n",
    "#         node_color=[mapper.to_rgba(i) for i in d.values()], \n",
    "#         edge_color = 'grey', with_labels=False, width= 0.2, font_color='white')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88762d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict(nx_G.nodes(data=\"Risk_Label\"))\n",
    "low, *_, high = sorted(d.values())\n",
    "norm = mpl.colors.Normalize(vmin=low-2, vmax=high+2, clip=True)\n",
    "mapper = mpl.cm.ScalarMappable(norm=norm, cmap=mpl.cm.coolwarm)\n",
    "plt.figure(figsize=[20,10])\n",
    "nx.draw(nx_G, nodelist=d, node_size= 400,\n",
    "        node_color=[mapper.to_rgba(i) for i in d.values()], \n",
    "        edge_color = 'grey', with_labels=False, width= 0.2, font_color='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0f64a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict(nx_G.nodes(data=\"E\"))\n",
    "low, *_, high = sorted(d.values())\n",
    "norm = mpl.colors.Normalize(vmin=low-1, vmax=high+1, clip=True)\n",
    "mapper = mpl.cm.ScalarMappable(norm=norm, cmap=mpl.cm.coolwarm)\n",
    "plt.figure(figsize=[20,10])\n",
    "nx.draw_spring(nx_G, nodelist=d, node_size= 400,\n",
    "        node_color=[mapper.to_rgba(i) for i in d.values()], \n",
    "        edge_color = 'grey', with_labels=False, width= 0.2, font_color='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74639741",
   "metadata": {},
   "source": [
    "# Make analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "72522d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# g_sh\n",
    "risk_label = [thred(i) for i in patient_info['risk']]\n",
    "g_sh.ndata['risk'] = torch.from_numpy(patient_info['risk'].to_numpy())\n",
    "g_sh.ndata['risk_label'] =torch.from_numpy(np.array(risk_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61606d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_np(values):\n",
    "    neg = 0\n",
    "    pos = 0\n",
    "    for i in values:\n",
    "        if i>0:\n",
    "            pos += i\n",
    "        else:\n",
    "            neg +=i\n",
    "    return neg, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ca1f0aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all neighboor of a target node: here is: dx_test[0]\n",
    "def neighboor_analysis(idx): \n",
    "    node_survival = g_sh.ndata['event'][idx]\n",
    "    node_survival_month = g_sh.ndata['label'][idx]\n",
    "    node_risk = g_sh.ndata['risk'][idx]\n",
    "    node_risk_label = g_sh.ndata['risk_label'][idx]\n",
    "    print(\"node risk:\", node_risk)\n",
    "    print(\"node risk label:\", node_risk_label)\n",
    "    print(\"node survival:\", node_survival)\n",
    "    neighboor = g_sh.in_edges(idx)\n",
    "    # find the neiboors's risk:\n",
    "    neighboor_risk = g_sh.ndata['risk'][neighboor[0]]\n",
    "    neighboor_risk_label = g_sh.ndata['risk_label'][neighboor[0]]\n",
    "    uni, count = torch.unique(neighboor_risk_label, return_counts=True)\n",
    "    print(\"node neriboor risk label:\", uni, count)\n",
    "    neighboor_survival_label = g_sh.ndata['event'][neighboor[0]]\n",
    "    print(\"node neriboor sum survival\",sum(neighboor_survival_label))\n",
    "    # find the neighboor with weight\n",
    "    neighboor_weight = g_sh.edges[neighboor][0]['w']\n",
    "    neighboor_weight_label = torch.mul(neighboor_risk_label, neighboor_weight)\n",
    "    neg, pos = sum_np(neighboor_weight_label)\n",
    "#     print(\"low risk imformation:\", neg)\n",
    "#     print(\"High risk imformation:\", pos)  \n",
    "    sum_weights = neg + pos\n",
    "    if sum_weights >0:\n",
    "        print(\"high risk provide more wights!\")\n",
    "    else:\n",
    "        print(\"==========low risk provide more wights!=============\")\n",
    "    return node_survival, node_survival_month, node_risk, node_risk_label, count[0], count[1], neighboor_risk, neighboor_risk_label, neighboor_weight, neighboor_weight_label, -neg, pos, sum_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d7ec13de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node risk: tensor(0.2921, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([721, 236])\n",
      "node neriboor sum survival tensor(173)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.4732, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([112, 395])\n",
      "node neriboor sum survival tensor(168)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.1946, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([823, 171])\n",
      "node neriboor sum survival tensor(161)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3069, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([626, 234])\n",
      "node neriboor sum survival tensor(160)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2730, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([743, 285])\n",
      "node neriboor sum survival tensor(208)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3341, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([285, 185])\n",
      "node neriboor sum survival tensor(113)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3044, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([582, 267])\n",
      "node neriboor sum survival tensor(186)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.5374, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([744, 131])\n",
      "node neriboor sum survival tensor(116)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2813, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([663, 272])\n",
      "node neriboor sum survival tensor(186)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3902, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([152, 379])\n",
      "node neriboor sum survival tensor(174)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.2400, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([826, 169])\n",
      "node neriboor sum survival tensor(151)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.4442, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([128, 382])\n",
      "node neriboor sum survival tensor(169)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.3417, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([102, 125])\n",
      "node neriboor sum survival tensor(60)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.3094, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([709, 247])\n",
      "node neriboor sum survival tensor(179)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3050, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([719, 246])\n",
      "node neriboor sum survival tensor(178)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3674, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([623, 101])\n",
      "node neriboor sum survival tensor(100)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3323, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([503, 280])\n",
      "node neriboor sum survival tensor(193)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3307, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([584, 290])\n",
      "node neriboor sum survival tensor(195)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3834, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([241, 411])\n",
      "node neriboor sum survival tensor(192)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.3105, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([564, 274])\n",
      "node neriboor sum survival tensor(182)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3066, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([815, 120])\n",
      "node neriboor sum survival tensor(135)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2999, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([703, 143])\n",
      "node neriboor sum survival tensor(113)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.4231, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([152, 392])\n",
      "node neriboor sum survival tensor(178)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.3425, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([713, 248])\n",
      "node neriboor sum survival tensor(171)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2964, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([663, 285])\n",
      "node neriboor sum survival tensor(196)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3183, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([702, 245])\n",
      "node neriboor sum survival tensor(168)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.5587, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([820, 129])\n",
      "node neriboor sum survival tensor(137)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2613, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([19, 64])\n",
      "node neriboor sum survival tensor(34)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.2874, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([677, 131])\n",
      "node neriboor sum survival tensor(102)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3144, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([488, 287])\n",
      "node neriboor sum survival tensor(187)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.5121, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([663, 286])\n",
      "node neriboor sum survival tensor(197)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2569, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([701, 157])\n",
      "node neriboor sum survival tensor(148)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3474, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([203, 397])\n",
      "node neriboor sum survival tensor(184)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.5283, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([727, 189])\n",
      "node neriboor sum survival tensor(158)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2721, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([610,  95])\n",
      "node neriboor sum survival tensor(86)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2984, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([786, 144])\n",
      "node neriboor sum survival tensor(133)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3357, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([88, 39])\n",
      "node neriboor sum survival tensor(15)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.4057, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([210, 148])\n",
      "node neriboor sum survival tensor(69)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3605, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([199, 398])\n",
      "node neriboor sum survival tensor(180)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.3455, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([222, 397])\n",
      "node neriboor sum survival tensor(185)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.2566, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([828, 168])\n",
      "node neriboor sum survival tensor(156)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3422, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([542, 288])\n",
      "node neriboor sum survival tensor(193)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2690, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([743, 285])\n",
      "node neriboor sum survival tensor(208)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2693, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([773, 167])\n",
      "node neriboor sum survival tensor(146)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.5941, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([551, 291])\n",
      "node neriboor sum survival tensor(192)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.4439, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([828, 140])\n",
      "node neriboor sum survival tensor(137)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2732, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([797, 144])\n",
      "node neriboor sum survival tensor(143)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3110, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([814, 162])\n",
      "node neriboor sum survival tensor(142)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3628, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([313, 185])\n",
      "node neriboor sum survival tensor(119)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2977, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([679, 122])\n",
      "node neriboor sum survival tensor(118)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3008, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([716, 121])\n",
      "node neriboor sum survival tensor(104)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3241, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([734, 247])\n",
      "node neriboor sum survival tensor(176)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2693, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([618, 235])\n",
      "node neriboor sum survival tensor(172)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3000, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([731, 287])\n",
      "node neriboor sum survival tensor(203)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3090, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([463, 234])\n",
      "node neriboor sum survival tensor(154)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3303, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([568, 190])\n",
      "node neriboor sum survival tensor(149)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3449, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([503, 279])\n",
      "node neriboor sum survival tensor(192)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3191, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([631, 114])\n",
      "node neriboor sum survival tensor(91)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2844, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([652, 111])\n",
      "node neriboor sum survival tensor(114)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.5396, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([714, 271])\n",
      "node neriboor sum survival tensor(189)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.5129, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([721, 234])\n",
      "node neriboor sum survival tensor(171)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3720, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([ 38, 357])\n",
      "node neriboor sum survival tensor(144)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.4102, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([ 65, 386])\n",
      "node neriboor sum survival tensor(163)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.3103, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([830, 131])\n",
      "node neriboor sum survival tensor(139)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3981, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([265, 414])\n",
      "node neriboor sum survival tensor(202)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.2845, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([848, 161])\n",
      "node neriboor sum survival tensor(146)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2754, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([625, 109])\n",
      "node neriboor sum survival tensor(107)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2695, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([773, 167])\n",
      "node neriboor sum survival tensor(146)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2669, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([686, 284])\n",
      "node neriboor sum survival tensor(200)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2944, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([640, 122])\n",
      "node neriboor sum survival tensor(97)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3150, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([660, 117])\n",
      "node neriboor sum survival tensor(94)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2911, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([779, 149])\n",
      "node neriboor sum survival tensor(138)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2739, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([788, 150])\n",
      "node neriboor sum survival tensor(137)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2974, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([842, 166])\n",
      "node neriboor sum survival tensor(157)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2774, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([836, 140])\n",
      "node neriboor sum survival tensor(139)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3882, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([210, 407])\n",
      "node neriboor sum survival tensor(195)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.5841, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([308, 415])\n",
      "node neriboor sum survival tensor(200)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.2811, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([834, 167])\n",
      "node neriboor sum survival tensor(148)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2952, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([746, 272])\n",
      "node neriboor sum survival tensor(197)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3014, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([806, 125])\n",
      "node neriboor sum survival tensor(128)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3254, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([465, 284])\n",
      "node neriboor sum survival tensor(185)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3115, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([638, 286])\n",
      "node neriboor sum survival tensor(193)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.4377, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([267,  63])\n",
      "node neriboor sum survival tensor(45)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3220, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([639, 275])\n",
      "node neriboor sum survival tensor(184)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3928, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([307, 417])\n",
      "node neriboor sum survival tensor(197)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.3058, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([630, 108])\n",
      "node neriboor sum survival tensor(89)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3238, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([706, 137])\n",
      "node neriboor sum survival tensor(132)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3070, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([722, 286])\n",
      "node neriboor sum survival tensor(210)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3049, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([722, 285])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node neriboor sum survival tensor(209)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2966, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([800, 150])\n",
      "node neriboor sum survival tensor(136)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2859, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([633, 101])\n",
      "node neriboor sum survival tensor(105)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3217, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([206, 169])\n",
      "node neriboor sum survival tensor(101)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3118, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([469, 277])\n",
      "node neriboor sum survival tensor(187)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3774, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([237, 178])\n",
      "node neriboor sum survival tensor(107)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3629, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([663, 285])\n",
      "node neriboor sum survival tensor(196)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2949, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([663, 285])\n",
      "node neriboor sum survival tensor(196)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2845, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([629, 270])\n",
      "node neriboor sum survival tensor(194)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2634, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([828, 168])\n",
      "node neriboor sum survival tensor(156)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3202, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([817, 123])\n",
      "node neriboor sum survival tensor(134)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2922, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([719, 245])\n",
      "node neriboor sum survival tensor(177)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2770, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([703, 143])\n",
      "node neriboor sum survival tensor(113)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2364, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([839, 145])\n",
      "node neriboor sum survival tensor(141)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.5013, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([ 84, 394])\n",
      "node neriboor sum survival tensor(165)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.3095, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([620, 285])\n",
      "node neriboor sum survival tensor(193)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3802, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([211, 398])\n",
      "node neriboor sum survival tensor(183)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.3758, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([265, 413])\n",
      "node neriboor sum survival tensor(201)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.3006, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([620, 285])\n",
      "node neriboor sum survival tensor(193)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.6218, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([606,  87])\n",
      "node neriboor sum survival tensor(82)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3565, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([ 93, 397])\n",
      "node neriboor sum survival tensor(167)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.3743, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([308, 413])\n",
      "node neriboor sum survival tensor(200)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.1982, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([722, 143])\n",
      "node neriboor sum survival tensor(115)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2150, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([823, 171])\n",
      "node neriboor sum survival tensor(161)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3893, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([186, 397])\n",
      "node neriboor sum survival tensor(183)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.3043, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([510, 236])\n",
      "node neriboor sum survival tensor(158)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3531, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([219, 170])\n",
      "node neriboor sum survival tensor(103)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2749, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([836, 140])\n",
      "node neriboor sum survival tensor(139)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2768, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([606,  81])\n",
      "node neriboor sum survival tensor(87)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3063, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([732, 275])\n",
      "node neriboor sum survival tensor(194)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2995, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([223, 398])\n",
      "node neriboor sum survival tensor(184)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.5462, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([300, 418])\n",
      "node neriboor sum survival tensor(194)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.3954, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([229,  51])\n",
      "node neriboor sum survival tensor(35)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3761, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([156, 381])\n",
      "node neriboor sum survival tensor(175)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.2770, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([812, 152])\n",
      "node neriboor sum survival tensor(149)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3654, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([593, 253])\n",
      "node neriboor sum survival tensor(165)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.4917, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([ 86, 395])\n",
      "node neriboor sum survival tensor(166)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.3005, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([743, 286])\n",
      "node neriboor sum survival tensor(209)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3021, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([724, 285])\n",
      "node neriboor sum survival tensor(201)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.4764, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([703, 283])\n",
      "node neriboor sum survival tensor(207)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2259, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([820, 172])\n",
      "node neriboor sum survival tensor(159)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3898, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([199, 398])\n",
      "node neriboor sum survival tensor(184)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.3110, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([582, 267])\n",
      "node neriboor sum survival tensor(186)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2920, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([716, 132])\n",
      "node neriboor sum survival tensor(107)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3377, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([743, 114])\n",
      "node neriboor sum survival tensor(123)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3150, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([730, 269])\n",
      "node neriboor sum survival tensor(197)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2902, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([ 30, 350])\n",
      "node neriboor sum survival tensor(140)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.3935, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([304, 184])\n",
      "node neriboor sum survival tensor(118)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3146, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([823, 172])\n",
      "node neriboor sum survival tensor(162)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2706, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([646, 109])\n",
      "node neriboor sum survival tensor(89)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2980, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([792, 125])\n",
      "node neriboor sum survival tensor(126)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.4126, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([ 31, 349])\n",
      "node neriboor sum survival tensor(145)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.2890, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([247,  74])\n",
      "node neriboor sum survival tensor(42)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3360, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([158, 392])\n",
      "node neriboor sum survival tensor(177)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.2923, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([681, 141])\n",
      "node neriboor sum survival tensor(106)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2545, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([732, 232])\n",
      "node neriboor sum survival tensor(174)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2515, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([847, 161])\n",
      "node neriboor sum survival tensor(151)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2935, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([828, 140])\n",
      "node neriboor sum survival tensor(137)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3001, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([631, 114])\n",
      "node neriboor sum survival tensor(91)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3833, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([214, 408])\n",
      "node neriboor sum survival tensor(197)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.2916, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([847, 161])\n",
      "node neriboor sum survival tensor(151)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3380, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([604, 270])\n",
      "node neriboor sum survival tensor(192)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3294, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([218, 398])\n",
      "node neriboor sum survival tensor(184)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.2841, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([791, 140])\n",
      "node neriboor sum survival tensor(128)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.4565, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([157, 380])\n",
      "node neriboor sum survival tensor(175)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.3005, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([836, 140])\n",
      "node neriboor sum survival tensor(139)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.5528, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([ 35, 354])\n",
      "node neriboor sum survival tensor(143)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.2982, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([839, 146])\n",
      "node neriboor sum survival tensor(142)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3683, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([593, 248])\n",
      "node neriboor sum survival tensor(172)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2890, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([669, 134])\n",
      "node neriboor sum survival tensor(130)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3235, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([618, 272])\n",
      "node neriboor sum survival tensor(183)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2925, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([711, 233])\n",
      "node neriboor sum survival tensor(177)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3058, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([802, 169])\n",
      "node neriboor sum survival tensor(152)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3202, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([647, 192])\n",
      "node neriboor sum survival tensor(148)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.4003, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([609,  97])\n",
      "node neriboor sum survival tensor(82)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2951, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([731, 284])\n",
      "node neriboor sum survival tensor(208)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2875, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([820, 172])\n",
      "node neriboor sum survival tensor(159)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3211, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([706, 192])\n",
      "node neriboor sum survival tensor(161)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.4381, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([ 29, 350])\n",
      "node neriboor sum survival tensor(140)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.3086, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([607, 291])\n",
      "node neriboor sum survival tensor(197)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3936, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([270, 417])\n",
      "node neriboor sum survival tensor(192)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.2962, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([528, 282])\n",
      "node neriboor sum survival tensor(197)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3023, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([706, 191])\n",
      "node neriboor sum survival tensor(160)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2929, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([721, 269])\n",
      "node neriboor sum survival tensor(197)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.4930, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([ 87, 145])\n",
      "node neriboor sum survival tensor(65)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.4734, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([ 75, 394])\n",
      "node neriboor sum survival tensor(165)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.3945, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([ 75, 394])\n",
      "node neriboor sum survival tensor(165)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.2999, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([826, 169])\n",
      "node neriboor sum survival tensor(151)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.4288, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([259, 415])\n",
      "node neriboor sum survival tensor(192)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.3031, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([631, 111])\n",
      "node neriboor sum survival tensor(93)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.4176, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([246, 413])\n",
      "node neriboor sum survival tensor(199)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.5489, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([650, 124])\n",
      "node neriboor sum survival tensor(95)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2451, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([828, 168])\n",
      "node neriboor sum survival tensor(156)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2976, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([781, 138])\n",
      "node neriboor sum survival tensor(134)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2905, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([721, 269])\n",
      "node neriboor sum survival tensor(197)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3596, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([272, 186])\n",
      "node neriboor sum survival tensor(118)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2383, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([790, 166])\n",
      "node neriboor sum survival tensor(150)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3417, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([109, 147])\n",
      "node neriboor sum survival tensor(56)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.3013, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([539, 273])\n",
      "node neriboor sum survival tensor(181)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2665, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([842, 165])\n",
      "node neriboor sum survival tensor(156)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3155, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([690, 112])\n",
      "node neriboor sum survival tensor(118)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3197, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([675, 131])\n",
      "node neriboor sum survival tensor(98)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2947, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([697, 124])\n",
      "node neriboor sum survival tensor(102)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2712, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([720, 231])\n",
      "node neriboor sum survival tensor(175)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3142, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([697, 131])\n",
      "node neriboor sum survival tensor(103)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3153, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([719, 245])\n",
      "node neriboor sum survival tensor(177)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3137, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([496, 288])\n",
      "node neriboor sum survival tensor(188)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3024, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([679, 122])\n",
      "node neriboor sum survival tensor(118)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2982, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([722, 285])\n",
      "node neriboor sum survival tensor(209)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3113, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([714, 233])\n",
      "node neriboor sum survival tensor(170)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.5794, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([510, 250])\n",
      "node neriboor sum survival tensor(160)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3315, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([651, 247])\n",
      "node neriboor sum survival tensor(165)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.4276, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([116, 378])\n",
      "node neriboor sum survival tensor(173)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.4172, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([162, 394])\n",
      "node neriboor sum survival tensor(178)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.3537, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([223, 399])\n",
      "node neriboor sum survival tensor(185)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.2955, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([730, 269])\n",
      "node neriboor sum survival tensor(197)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3289, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([503, 263])\n",
      "node neriboor sum survival tensor(181)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2852, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([660, 117])\n",
      "node neriboor sum survival tensor(94)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2352, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([724, 284])\n",
      "node neriboor sum survival tensor(200)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3277, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([731, 284])\n",
      "node neriboor sum survival tensor(208)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2950, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([661, 285])\n",
      "node neriboor sum survival tensor(205)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.3836, dtype=torch.float64)\n",
      "node risk label: tensor(1)\n",
      "node survival: tensor(1)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([107, 393])\n",
      "node neriboor sum survival tensor(168)\n",
      "high risk provide more wights!\n",
      "node risk: tensor(0.2504, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([799, 158])\n",
      "node neriboor sum survival tensor(139)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2184, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([750, 139])\n",
      "node neriboor sum survival tensor(122)\n",
      "==========low risk provide more wights!=============\n",
      "node risk: tensor(0.2839, dtype=torch.float64)\n",
      "node risk label: tensor(-1)\n",
      "node survival: tensor(0)\n",
      "node neriboor risk label: tensor([-1,  1]) tensor([783, 127])\n",
      "node neriboor sum survival tensor(129)\n",
      "==========low risk provide more wights!=============\n"
     ]
    }
   ],
   "source": [
    "# analysis from edge information\n",
    "\n",
    "idxs = [] \n",
    "survivals = [] \n",
    "survival_months = []\n",
    "risks = []\n",
    "risk_labels = []\n",
    "neighboor_risks = []\n",
    "low_counts  = []\n",
    "high_counts = []\n",
    "num_neighboors = []\n",
    "neighboor_risk_labels = []\n",
    "neighboor_weights = []\n",
    "neighboor_weight_labels = []\n",
    "negs = []\n",
    "poss = []\n",
    "sum_weightss = []\n",
    "for idx in idx_test:    \n",
    "    node_survival, node_survival_m, node_risk, node_risk_label, low_count, high_count, neighboor_risk, _, neighboor_weight, neighboor_weight_label, neg, pos,sum_weights = neighboor_analysis(idx)\n",
    "    idxs.append(idx.item())\n",
    "    survivals.append(node_survival.item())\n",
    "    survival_months.append(node_survival_m.item())\n",
    "    risks.append(node_risk.item())\n",
    "    risk_labels.append(node_risk_label.item())\n",
    "    neighboor_risks.append(neighboor_risk)\n",
    "    low_counts.append(low_count.item())\n",
    "    high_counts.append(high_count.item())\n",
    "    num_neighboors.append((low_count+high_count).item())\n",
    "    neighboor_weights.append(neighboor_weight)\n",
    "    neighboor_weight_labels.append(neighboor_weight_label)\n",
    "    negs.append(neg.item())\n",
    "    poss.append(pos.item())\n",
    "    sum_weightss.append(sum_weights.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5fc735b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_df = pd.DataFrame()\n",
    "clinical_df['node'] = idxs\n",
    "clinical_df['survival'] = survivals\n",
    "clinical_df['survival_month'] =\n",
    "clinical_df['num_neighboors'] = num_neighboors\n",
    "clinical_df['risks'] = risks\n",
    "clinical_df['risk_labels'] = risk_labels\n",
    "clinical_df['neighboor_risks'] = neighboor_risks\n",
    "clinical_df['neighboor_low_counts'] = [100*low_counts[i]/num_neighboors[i] for i in range(len(num_neighboors))]\n",
    "clinical_df['neighboor_high_counts'] = [100*high_counts[i]/num_neighboors[i] for i in range(len(num_neighboors))]\n",
    "\n",
    "\n",
    "clinical_df['low_weights'] = negs\n",
    "clinical_df['high_weights'] = poss\n",
    "clinical_df['sum_weights'] = sum_weightss\n",
    "clinical_df['sum_weights_abs'] = clinical_df['low_weights'] + clinical_df['low_weights']\n",
    "clinical_df['survival'] = clinical_df['survival'].replace({0:-1})\n",
    "clinical_df['mean_sum_weights_abs'] = clinical_df['sum_weights_abs']/clinical_df['num_neighboors']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ae97c55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1cb0ccbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value annotation legend:\n",
      "ns: 5.00e-02 < p <= 1.00e+00\n",
      "*: 1.00e-02 < p <= 5.00e-02\n",
      "**: 1.00e-03 < p <= 1.00e-02\n",
      "***: 1.00e-04 < p <= 1.00e-03\n",
      "****: p <= 1.00e-04\n",
      "\n",
      "-1 v.s. 1: Mann-Whitney-Wilcoxon test two-sided with Bonferroni correction, P_val=2.155e-17 U_stat=9.233e+03\n",
      "p-value annotation legend:\n",
      "ns: 5.00e-02 < p <= 1.00e+00\n",
      "*: 1.00e-02 < p <= 5.00e-02\n",
      "**: 1.00e-03 < p <= 1.00e-02\n",
      "***: 1.00e-04 < p <= 1.00e-03\n",
      "****: p <= 1.00e-04\n",
      "\n",
      "-1 v.s. 1: Mann-Whitney-Wilcoxon test two-sided with Bonferroni correction, P_val=8.746e-06 U_stat=5.338e+03\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[Text(0, 0, 'Survival'), Text(1, 0, 'Death')]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 7000x2450 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20,7), dpi = 350)\n",
    "sns.set(style=\"whitegrid\")\n",
    "# fig.subplots_adjust(top=0.15)\n",
    "ax1 = fig.add_subplot(121)\n",
    "x1 = \"risk_labels\"\n",
    "y = \"num_neighboors\"\n",
    "order1 = [-1, 1]\n",
    "# ax1 = sns.boxplot(data=clinical_df, x=x1, y=y, order=order1, palette=sns.color_palette(['#FF5720', '#18C288']))\n",
    "ax1 = sns.boxplot(data=clinical_df, x=x1, y=y, order=order1, palette=sns.color_palette(['#FF5720', '#18C288']))\n",
    "\n",
    "add_stat_annotation(ax1, data=clinical_df, x=x1, y=y,  box_pairs=[(-1,1)],\n",
    "                    test='Mann-Whitney', text_format='star', loc='outside', verbose=2)\n",
    "\n",
    "ax1.set(xlabel='risk', ylabel='number of neighboors')\n",
    "ax1.set(xticklabels=[\"low risk\", \"high risk\"])\n",
    "# fig.subplots_adjust(bottom = 0.5)\n",
    "# fig.savefig(\"box_plot.jpg\")\n",
    "                  \n",
    "ax2 = fig.add_subplot(122)\n",
    "x2 = \"survival\"\n",
    "y = \"num_neighboors\"\n",
    "order2= [-1, 1]\n",
    "ax2 = sns.boxplot(data=clinical_df, x=x2, y=y, order=order2, palette=sns.color_palette(['#FF5720', '#18C288']))\n",
    "add_stat_annotation(ax2, data=clinical_df, x=x2, y=y, box_pairs=[(-1,1)],\n",
    "                    test='Mann-Whitney', text_format='star', loc='outside', verbose=2)\n",
    "ax2.set(xlabel='Survival', ylabel='number of neighboors')\n",
    "ax2.set(xticklabels=[\"Survival\", \"Death\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "719e7e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value annotation legend:\n",
      "ns: 5.00e-02 < p <= 1.00e+00\n",
      "*: 1.00e-02 < p <= 5.00e-02\n",
      "**: 1.00e-03 < p <= 1.00e-02\n",
      "***: 1.00e-04 < p <= 1.00e-03\n",
      "****: p <= 1.00e-04\n",
      "\n",
      "-1 v.s. 1: Mann-Whitney-Wilcoxon test two-sided with Bonferroni correction, P_val=2.152e-08 U_stat=5.758e+03\n",
      "p-value annotation legend:\n",
      "ns: 5.00e-02 < p <= 1.00e+00\n",
      "*: 1.00e-02 < p <= 5.00e-02\n",
      "**: 1.00e-03 < p <= 1.00e-02\n",
      "***: 1.00e-04 < p <= 1.00e-03\n",
      "****: p <= 1.00e-04\n",
      "\n",
      "-1 v.s. 1: Mann-Whitney-Wilcoxon test two-sided with Bonferroni correction, P_val=1.566e-08 U_stat=5.778e+03\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[Text(0, 0, 'Survival'), Text(1, 0, 'Death')]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAFnkAAAjICAYAAAC5wA52AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAADXUAAA11AFeZeUIAAEAAElEQVR4nOzdeXSV5dU34J2EwyAQA4pQxUqlDi8OiMUZFZYVQawDFolSUByKClL7KTiAiNZqRbHiVHFAkS4FtVbqAFqs2LrEqhXpWyyOFRAcoIxhDDnn++NdZjU9AZInCQfida3VP9zPc997B1xNe7L5kZfJZDIBAAAAAAAAAAAAAAAAAAAAAAAAAAAAQLXk53oAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgB2RkGcAAAAAAAAAAAAAAAAAAAAAAAAAAACABIQ8AwAAAAAAAAAAAAAAAAAAAAAAAAAAACQg5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASHPAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAkIeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIQMgzAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAJCngEAAAAAAAAAAAAAAAAAAAAAAAAAAAASEPIMAAAAAAAAAAAAAAAAAAAAAAAAAAAAkICQZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEhDwDAAAAAAAAAAAAAAAAAAAAAAAAAAAAJCDkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACABIc8AAAAAAAAAAAAAAAAAAAAAAAAAAAAACQh5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEhAyDMAAAAAAAAAAAAAAAAAAAAAAAAAAABAAkKeAQAAAAAAAAAAAAAAAAAAAAAAAAAAABIQ8gwAAAAAAAAAAAAAAAAAAAAAAAAAAACQgJBnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASEPAMAAAAAAAAAAAAAAAAAAAAAAAAAAAAkIOQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEhzwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJCHkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASEDIMwAAAAAAAAAAAAAAAAAAAAAAAAAAAEACQp4BAAAAAAAAAAAAAAAAAAAAAAAAAAAAEhDyDAAAAAAAAAAAAAAAAAAAAAAAAAAAAJCAkGcAAAAAAAAAAAAAAAAAAAAAAAAAAACABIQ8AwAAAAAAAAAAAAAAAAAAAAAAAAAAACQg5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASHPAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAkIeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIQMgzAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAJCngEAAAAAAAAAAAAAAAAAAAAAAAAAAAASEPIMAAAAAAAAAAAAAAAAAAAAAAAAAAAAkICQZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEhDwDAAAAAAAAAAAAAAAAAAAAAAAAAAAAJCDkGQAAAACAb7X99tuv/D9//etf67xf//79y/vdfffddd4vIuLuu+8u79m/f/9t0vOvf/1rhV9bAAAAAAAAAAAAAIBcsjteN+yOAwAAAICQZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIBEhDwDAAAAAAAAAAAAAAAAAAAAAAAAAAAAJCDkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACABIc8AAAAAAAAAAAAAAAAAAAAAAAAAAAAACQh5BgAAAACg3ikrK4tBgwbFjBkzavXeDz/8MAYOHBiff/551rNbb701HnjggSgtLa21fsuWLYtrr702XnzxxUqfT58+Pa655ppYtmxZrfUsLS2NBx98MH71q19V+nzx4sVx3nnnxQcffFBrPSMiXnnllfjpT38aZWVltXovAAAAAAAAAAAAAFB/2R1Pzu44AAAAANQeIc8AAAAAANQ7kydPjpkzZ8bgwYNj0KBBsXDhwhrdt3bt2rj11lvjjDPOiDfeeCPGjBlT4fkHH3wQEydOjLFjx8bpp58eb731Vo36ZTKZeOqpp6Jnz57xu9/9Lm677bZYv359hXfWr18fY8aMiWeeeSZ69uwZTz75ZGQymRr1feedd+KMM86I22+/PR577LGYN29e1ju33nprzJo1K3r37h233nprrFmzpkY9P//887j44ovj0ksvjddeey0mT55co/sAAAAAAAAAAAAAgG8Pu+PJ2B0HAAAAgNol5BkAAAAAgHqlpKQk7rrrrvJ/njlzZpxyyilx//33x8aNG6t93x//+Mc4+eSTY8KECbFp06aIiHjppZfib3/7W/k7t956a5SVlUVExMcffxz9+/ePq666Kv79739Xu9+8efPi7LPPjpEjR8aKFSsiImLx4sXx6KOPVnjv0UcfjUWLFkVExIoVK+K6666Ls88+u9Ll2q1ZtmxZXH311dGvX7/46KOPIiKirKwsayH53XffjenTp0dExKZNm2LChAlx8sknx0svvVTtnqWlpTF+/Pg45ZRT4tVXXy2v33XXXVFSUlLt+wAAAAAAAAAAAACAbxe743bHAQAAAGB7IeQZAAAAAIB6pVmzZjFmzJjYd999y2vr16+PX//613HaaafFm2++WaV7Fi5cGD/96U9jyJAh8cUXX5TXW7ZsGdddd10cfPDB5bURI0ZEt27dKpx/9tlno2fPnjF58uRIp9Nb7VdSUhK33HJLnHnmmTF79uzyeiqVigEDBkTfvn0rvN+3b98YMGBApFKp8trs2bOjd+/ecfPNN1dp2TWTycSUKVOiR48e8fvf/77Cs27dusWIESMq1A466KC47rrromXLluW1L7/8MoYOHRoXXXRRLFy4cKs9IyL++te/xmmnnRZ33HFHrFu3rry+7777xpgxY6JZs2ZVugcAAAAAAAAAAAAA+PayO253HAAAAAC2F0KeAQAAAACod44//viYOnVq/PKXv4zWrVuX1z/99NM499xzY9iwYbF06dJKz27cuDHuu+++OOWUU+K1114rrzdu3Dguvvji+OMf/xg/+clPKizItm/fPu6///6YNGlSHHTQQeX1lStXxvXXXx/FxcXx/vvvb3be6dOnx8knnxyPPvpobNq0KSIi8vLyomfPnjFt2rQYMWJEtGjRosKZFi1axIgRI2LatGnRs2fPyMvLi4iIsrKymDhxYvnZzXn//fejb9++MWrUqFi5cmV5/aCDDopJkybF/fffH+3bt69wJpVKxU9+8pP44x//GBdffHE0bty4/Nmf//zn6NWrV9x3332xcePGSnsuXbo0hg0bFgMGDIhPPvmkvN66dev45S9/GVOnTo3jjz9+szMDAAAAAAAAAAAAAPwnu+N2xwEAAABge5CXyWQyuR4CAAAAAADqyrp16+LRRx+NBx98MNasWVNeLywsjJ///Odxww03lNeGDBkSL7zwQvzrX/8qr+Xn58fpp58el19+eYWl383JZDLx4osvxh133BGff/55eb2goCDOOeecmD17dvzjH/+IiIjTTz89li5dGq+//nqFOw477LAYPnx4HHzwwVX+Ov/+97/HmDFj4u23365Q79KlS+y6667x7LPPRkTEgQceGJ06dYrHH388ysrKyt9r27Zt/L//9//i5JNPLl/63Zqvvvoq7rzzznj22WcjnU6X17/3ve9Fr1694p577imvjRo1Ku68885YtWpVea1p06Zx0UUXxcCBAyss/QIAAAAAAAAAAAAAVJfdcbvjAAAAAJArQp4BAAAAAPhWWLZsWdxzzz0xZcqU2LRpU5XOdOnSJYYPHx777bdftftt3LgxHn/88fjNb34TK1asqNKZ9u3bxxVXXBEnnHBCtft945VXXonbb789Pv300yq9X1RUFJdcckmcc8450bBhw0Q9P/jggxgzZkzWwvHmNGjQIPr27RtDhgyJli1bJuoJAAAAAAAAAAAAAFAZu+OVszsOAAAAAHVHyDMAAAAAAN8qn332WYwdOzZefvnlzb7ToUOHGDZsWBx99NE17rdq1aq4//77Y9KkSbFx48ZK32nVqlUMGTIk+vTpEwUFBTXuWVZWFk899VTcc889sWTJkkrfadiwYfTv3z8uvvjiKCwsrHHPiIg33ngjxowZE//85z83+0737t3jiiuuiHbt2tVKTwAAAAAAAAAAAACAytgd/z92xwEAAACg7gl5BgAAAADgW+nhhx+OMWPGZNVPOOGEuPvuu2tlYfY/vfPOO3H++efHhg0bKtS/+93vxhNPPBG77rprrfaLiFi6dGmcffbZsWDBggr1Ro0axYQJE6Jz58613rOsrCwuu+yyeOWVV7KeDR8+PC644IJa7wkAAAAAAAAAAAAAsDl2x+2OAwAAAEBdE/IMAAAAAMC3yvz582Ps2LHx0ksvbfadAw44IIYPHx5HHnlkjfutXr06xo8fH4899ljWku43WrVqFUOHDo0zzzyzVhaEy8rK4ne/+13cddddsWTJkkrfadSoUQwYMCAGDRoUzZs3r3HPiIg333wzxowZE3Pnzt3sOyeddFJcccUVsddee9VKTwAAAAAAAAAAAACAytgd/z92xwEAAACg7gl5BgAAAADgW2H58uVx7733xuTJk6O0tLRKZ7p27RrDhg2L73//+9XuV1paGpMnT4577703li9fXqUz++yzTwwbNiyOP/74avf7xmuvvRa33XZbfPTRR1V6v0WLFjF48OAoLi6OVCqVqOfHH38ct912W8ycObNK76dSqSguLo7BgwdHixYtEvUEAAAAAAAAAAAAAKiM3fHK2R0HAAAAgLoj5BkAAAAAgHptw4YN8dhjj8X48eNj9erV5fXCwsK4/PLL48YbbyyvDR48OF544YX47LPPymsFBQXx4x//OC677LJo1apVlXq+9NJLMXbs2Jg/f36Fe84555x49913Y+7cuRERcfrpp8fSpUvj9ddfr3D+qKOOiuHDh0eHDh2q/HW+//77ceutt8abb75Zod6lS5fYdddd49lnn42IiAMOOCAOPfTQePzxx6OsrKz8vb322iuuuOKKOOmkk6rcc8mSJXH33XfH008/XeGudu3aRa9eveLee+8tr40aNSruvPPOWLVqVXmtefPmMWjQoBgwYEA0atSoyn0BAAAAAAAAAAAAAP6b3XG74wAAAACQK/m5HgAAAAAAAOpCJpOJP/zhD9GjR4+4/fbbKyzp/uhHP4pp06ZFv379Kpw54ogj4rnnnouhQ4dG48aNIyKirKwspkyZEt27d4977rkn1q5du9mes2fPjuLi4hg6dGiFJd2DDz44nn766Rg5cmQ0bdq0vN62bdt4+OGH484774zWrVuX12fNmhW9e/eO4cOHx+LFi7f4dS5evDiGDx8evXv3rrCku9tuu8Wvf/3rePjhh6Nt27bl9aZNm8bIkSPj6aefjo4dO5bX58+fH0OHDo3i4uKYPXv2FnuuXbs27rnnnujevXtMmTKlfEm3UaNGMXTo0HjuuefiiCOOqHCmX79+MW3atPjRj35UXlu9enXcfvvt0bNnz/jDH/4Q/l5KAAAAAAAAAAAAAKC67I7bHQcAAACAXBPyDAAAAABAvfPmm2/GmWeeGcOGDauw6Lr33nvHxIkT4/bbb49dd9210rMNGzaMwYMHx/PPPx/HHXdceX3t2rVx9913R/fu3eOpp54qX06NiFiwYEGlS64777xzjB49OqZMmRIdOnTY7Lw9e/aMF198Mc4999woKCiIiP9bNJ46dWr06NEjxo4dGyUlJRXOrF69OsaOHRs9evSIqVOnli+5FhQUxIABA2LatGlx8sknb7Znhw4dYvLkyXHDDTfEzjvvXF7/z2XjBQsWVDhTVlYWTz31VHTv3j3uvvvuCkvLxx13XLzwwgsxePDgaNiwYaU9d91117j99ttj4sSJsffee5fXFy1aFMOGDYszzzyzwrIxAAAAAAAAAAAAAMCW2B23Ow4AAAAA2wMhzwAAAAAA1CslJSUxdOjQmDt3bnmtcePGcfnll8fUqVPjyCOPrNI9e+65Zzz44INx9913R5s2bcrrS5YsiZEjR1ZYyB01alS89NJLFc6fdtppMW3atDj77LMjP3/rH8c3a9Ysrr322njmmWeiU6dO5fUNGzbEAw88EI899liF9ydNmhQPPPBAbNiwobx2yCGHxO9+97sYMWJENGvWbKs98/Pzo7i4OKZPnx6nn356hWcvvfRSjBo1qkJt9uzZMXLkyFiyZEl5rU2bNjFu3Lh48MEHY88999xqz4iII488MqZOnRo///nPo0mTJuX1uXPnxtChQ7OWkgEAAAAAAAAAAAAA/pvdcbvjAAAAALC9EPIMAAAAAEC90qxZs7jsssvK/7lr167x/PPPxyWXXBINGzas9n3du3ePF198Mc4///xo0KBBea1z587l71x99dVRUFAQERHt27ePSZMmxZgxY2KXXXapdr/9998/nnjiibjpppuiqKgoIv5vGXbgwIEV3hs4cGB85zvfiYiIoqKiuPHGG2Py5MnxP//zP9Xu2bJly7j11lvjt7/9beyzzz4R8X9LvFdddVWF9zp37hzdu3ePiIgGDRrEwIED48UXX4wePXpUu2fDhg3j4osvjueffz66detWXr/sssuqtGQMAAAAAAAAAAAAAHy72R23Ow4AAAAA24sGuR4AAAAAAABq2znnnBN/+ctfok+fPnHiiSfW+L6mTZvGVVddFaeffnrccsstMXz48ArP999//zj33HOjqKgozj///EilUjXql5eXF3369IkTTjghbrvttujSpUs0adKkwjtNmjSJq666Kl577bUYPnx4tGzZskY9IyIOO+yw+P3vfx+PPPJI/Pvf/6506ffqq6+O1atXx9VXXx37779/jXu2bds27r///pgxY0ZMmTIlzj777BrfCQAAAAAAAAAAAAB8O9gdT8buOAAAAADULiHPAAAAAADUOwUFBfHAAw/U+r377bdfPProo5U+u+qqq2q9X8uWLeOWW27Z7POePXtGz549a7VnKpWKn/70p5t9vscee2z216AmfvjDH8YPf/jDWr8XAAAAAAAAAAAAAKi/7I4nZ3ccAAAAAGpPfq4HAAAAAAAAAAAAAAAAAAAAAAAAAAAAANgRCXkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASEDIMwAAAAAAAAAAAAAAAAAAAAAAAAAAAEACQp4BAAAAAAAAAAAAAAAAAAAAAAAAAAAAEsjLZDKZXA8BAAAAAAAAAAAAAAAAAAAAAAAAAAAAsKPJz/UAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADsiIc8AAAAAAAAAAAAAAAAAAAAAAAAAAAAACQh5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEhAyDMAAAAAAAAAAAAAAAAAAAAAAAAAAABAAkKeAQAAAAAAAAAAAAAAAAAAAAAAAAAAABIQ8gwAAAAAAAAAAAAAAAAAAAAAAAAAAACQgJBnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASEPAMAAAAAAAAAAAAAAAAAAAAAAAAAAAAkIOQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEhzwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJCHkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASEDIMwAAAAAAAAAAAAAAAAAAAAAAAAAAAEACQp4BAAAAAAAAAAAAAAAAAAAAAAAAAAAAEhDyDAAAAAAAAAAAAAAAAAAAAAAAAAAAAJCAkGcAAAAAAAAAAAAAAAAAAAAAAAAAAACABIQ8AwAAAAAAAAAAAAAAAAAAAAAAAAAAACQg5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASHPAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAkIeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIQMgzAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAJCngEAAAAAAAAAAAAAAAAAAAAAAAAAAAASEPIMAAAAAAAAAAAAAAAAAAAAAAAAAAAAkICQZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEhDwDAAAAAAAAAAAAAAAAAAAAAAAAAAAAJCDkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACABIc8AAAAAAAAAAAAAAAAAAAAAAAAAAAAACQh5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEhAyDMAAAAAAAAAAAAAAAAAAAAAAAAAAABAAg1yPQAAAADbr3Q6HV999VWUlpbmehQAAAAA2G6lUqlo3bp15Of7u7YBAAAAAADYMdkdBwAAAICtszsOAMDmCHkGAAAgS1lZWUyaNCkeeeSRWLNmTa7HAQAAAIDtXtOmTWPgwIHRv3//KCgoyPU4AAAAAAAAUCV2xwEAAACgeuyOAwBQmbxMJpPJ9RAAAABsXx599NG45557cj0GAAAAAOxwhgwZEuedd16uxwAAAAAAAIAqsTsOAAAAAMnYHQcA4D8JeQYAAKCCdDod3bp1izVr1uR6FAAAAADY4TRr1iz+9Kc/RX5+fq5HAQAAAAAAgC2yOw4AAAAAydkdBwDgP/lfhQAAAFTw1VdfWdIFAAAAgIRKSkriq6++yvUYAAAAAAAAsFV2xwEAAAAgObvjAAD8JyHPAAAAVFBaWprrEQAAAABgh+YzNgAAAAAAAHYEfq4FAAAAADXjMzYAAL4h5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASHPAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAk0yPUAAAAA7Fjuueee2H333XM9BgAAAADkzOLFi2PIkCG5HgMAAAAAAADqlN1xAAAAAL7t7I4DAFBVQp4BAAColt133z2++93v5noMAAAAAAAAAAAAAADqkN1xAAAAAAAAgKrJz/UAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADsiIc8AAAAAAAAAAAAAAAAAAAAAAAAAAAAACQh5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEhAyDMAAAAAAAAAAAAAAAAAAAAAAAAAAABAAkKeAQAAAAAAAAAAAAAAAAAAAAAAAAAAABIQ8gwAAAAAAAAAAAAAAAAAAAAAAAAAAACQgJBnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASEPAMAAAAAAAAAAAAAAAAAAAAAAAAAAAAkIOQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEhzwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJCHkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASEDIMwAAAAAAAAAAAAAAAAAAAAAAAAAAAEACQp4BAAAAAAAAAAAAAAAAAAAAAAAAAAAAEhDyDAAAAAAAAAAAAAAAAAAAAAAAAAAAAJCAkGcAAAAAAAAAAAAAAAAAAAAAAAAAAACABIQ8AwAAAAAAAAAAAAAAAAAAAAAAAAAAACQg5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASHPAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAkIeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIQMgzAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAJCngEAAAAAAAAAAAAAAAAAAAAAAAAAAAASEPIMAAAAAAAAAAAAAAAAAAAAAAAAAAAAkICQZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEhDwDAAAAAAAAAAAAAAAAAAAAAAAAAAAAJCDkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACABIc8AAAAAAAAAAAAAAAAAAAAAAAAAAAAACQh5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEhAyDMAAAAAAAAAAAAAAAAAAAAAAAAAAABAAkKeAQAAAAAAAAAAAAAAAAAAAAAAAAAAABIQ8gwAAAAAAAAAAAAAAAAAAAAAAAAAAACQgJBnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASEPAMAAFBBw4YNa/QcAAAAAOo7n6EBAAAAAABQH/i5FwAAAABsmc/QAACoKiHPAAAAVNC6desoKiqq9FmLFi2idevW23YgAAAAANjO+AwNAAAAAACA+sDPvQAAAABgy3yGBgBAVQl5BgAAoIK8vLw499xzK3127rnnRl5e3jaeCAAAAAC2Lz5DAwAAAAAAoD7wcy8AAAAA2DKfoQEAUFUNcj0AAAAA25+f/OQnUVZWFpMmTYqVK1fGzjvvHP37949+/frlejQAAAAA2C74DA0AAAAAAID6wM+9AAAAAGDLfIYGAEBV5GUymUyuhwAAAGD7VFpaGhs2bIhGjRpFKpXK9TgAAAAAsN3xGRoAAAAAAAD1gZ97AQAAAMCW+QwNAIAtEfIMAAAAAAAAAAAAAAAAAAAAAAAAAAAAkEB+rgcAAAAAAAAAAAAAAAAAAAAAAAAAAAAA2BEJeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIQMgzAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAJCngEAAAAAAAAAAAAAAAAAAAAAAAAAAAASEPIMAAAAAAAAAAAAAAAAAAAAAAAAAAAAkICQZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEhDwDAAAAAAAAAAAAAAAAAAAAAAAAAAAAJCDkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACABIc8AAAAAAAAAAAAAAAAAAAAAAAAAAAAACQh5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEhAyDMAAAAAAAAAAAAAAAAAAAAAAAAAAABAAkKeAQAAAAAAAAAAAAAAAAAAAAAAAAAAABIQ8gwAAAAAAAAAAAAAAAAAAAAAAAAAAACQgJBnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASEPAMAAAAAAAAAAAAAAAAAAAAAAAAAAAAkIOQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEhzwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJCHkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASEDIMwAAAAAAAAAAAAAAAAAAAAAAAAAAAEACQp4BAAAAAAAAAAAAAAAAAAAAAAAAAAAAEhDyDAAAAAAAAAAAAAAAAAAAAAAAAAAAAJCAkGcAAAAAAAAAAAAAAAAAAAAAAAAAAACABIQ8AwAAAAAAAAAAAAAAAAAAAAAAAAAAACQg5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASHPAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAkIeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIQMgzAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAJCngEAAAAAAAAAAAAAAAAAAAAAAAAAAAASEPIMAAAAAAAAAAAAAAAAAAAAAAAAAAAAkICQZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEhDwDAAAAAAAAAAAAAAAAAAAAAAAAAAAAJCDkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACABIc8AAAAAAAAAAAAAAAAAAAAAAAAAAAAACQh5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEhAyDMAAAAAAAAAAAAAAAAAAAAAAAAAAABAAkKeAQAAAAAAAAAAAAAAAAAAAAAAAAAAABIQ8gwAAAAAAAAAAAAAAAAAAAAAAAAAAACQgJBnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASEPAMAAAAAAAAAAAAAAAAAAAAAAAAAAAAkIOQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEhzwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJCHkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASEDIMwAAAAAAAAAAAAAAAAAAAAAAAAAAAEACQp4BAAAAAAAAAAAAAAAAAAAAAAAAAAAAEhDyDAAAAAAAAAAAAAAAAAAAAAAAAAAAAJCAkGcAAAAAAAAAAAAAAAAAAAAAAAAAAACABBrkegCAdDod6XS60md5eXmRl5e3jScCAAAAAAAAAABgR5LJZCKTyVT6LD8/P/Lz87fxRABQuXQ6HZ988kl89NFHsXLlyli9enVERBQWFkZhYWHss88+0b59+3r7vcvuOAAAAAAAAAAAADWxve6OC3kGci6dTsecOXNyPQYAAAAAAAAAAAD1UMeOHettUCYAO4aysrL485//HE8++WS8+eabsXbt2i2+37Rp0zjqqKPirLPOimOPPbZefR+zOw4AAAAAAAAAAEBdyeXuuJBnAAAAAAAAAAAAAACAOjBnzpwYMWJEfPTRR1U+s2bNmpgxY0bMmDEj9t9//7j55pvjgAMOqMMpAQAAAAAAAAAAgJrITbQ0AAAAAAAAAAAAAABAPfbYY49FcXFxtQKe/9u8efPirLPOiieffLIWJwMAAAAAAAAAAABqU4NcDwAAAAAAAAAAAAAAAFCfPPHEE/HLX/5ys8/btGkTHTp0iJYtW0Y6nY7ly5fHP/7xj1iyZEnWu5s2bYpRo0ZF48aN49RTT63LsQEAAAAAAAAAAIAEhDwDAAAAAAAAAAAAAADUkoULF8Ytt9xS6bMuXbrEz372szj44IMrff7OO+/EuHHj4q233qpQz2Qycf3118cRRxwRrVu3rvWZAQAAAAAAAAAAgOTycz0AAAAAAAAAAAAAAABAfTF27NjYsGFDVv2iiy6Khx56aLMBzxERnTt3jokTJ0b//v2znq1duzbuvPPO2hwVAAAAAAAAAAAAqAV5mUwmk+shgG+3srKyeO+99yp91rFjx8jPl0cPAAAAAAAAAADA5qXT6ZgzZ06lzw455JAoKCjYxhMB8G21bt26OPLII2P9+vUV6sccc0xMmDChyvdkMpkYMGBAvPXWWxXqzZo1i1mzZkXDhg1rZd5tze44AAAAAAAAAAAANbG97o43yElXgP+Ql5e32Wf5+fkWdQEAAAAAAAAAAEhsSztqAFDb3n777ayA54iISy+9tFr35OXlxZAhQ2LAgAEV6iUlJTF79uw44ogjajRnrtgdBwAAAAAAAAAAoK7kcnfc9hsAAAAAAAAAAAAAAEAt+OKLL7JqhYWFceihh1b7rs6dO0fz5s2z6osWLUo0GwAAAAAAAAAAAFA3hDwDAAAAAAAAAAAAAADUguXLl2fV9thjj8jPr/4f3ygoKIg99tgjq7506dJEswEAAAAAAAAAAAB1Q8gzAAAAAAAAAAAAAABALWjcuHGValXVpEmTrFoqlUp8HwAAAAAAAAAAAFD7hDwDAAAAAAAAAAAAAADUgrZt22bVli9fnvi+ZcuWZdVatWqV+D4AAAAAAAAAAACg9gl5BgAAAAAAAAAAAAAAqAWHHnpo5OdX/KMaCxYsSBT0vGzZsliwYEFW/ZBDDkk6HgAAAAAAAAAAAFAHhDwDAAAAAAAAAAAAAADUgpYtW0a3bt0q1NLpdDz77LPVvuvpp5+OTCZTodaxY8do27ZtTUYEAAAAAAAAAAAAapmQZwAAAAAAAAAAAAAAgFpy+eWXRyqVqlD7zW9+E/Pnz6/yHR9//HGMHz8+q37JJZfUeD4AAAAAAAAAAACgdjXI9QBsW+l0Oj755JP46KOPYuXKlbF69eqIiCgsLIzCwsLYZ599on379pGfX7f538uXL4+5c+fG/Pnzo6SkJPLy8qKwsDDatWsXBx54YDRr1qxO+5sDAAAAAAAAAAAAAIC6sO+++8bIkSPj+uuvL6+tXLkyBg4cGHfccUcccsghWzz/1ltvxRVXXBElJSUV6r17945u3brVxcgAAAAAAAAAAABADQh5/hYoKyuLP//5z/Hkk0/Gm2++GWvXrt3i+02bNo2jjjoqzjrrrDj22GNrLfA5k8nEtGnT4vHHH4+//e1vkU6nK30vlUrF0UcfHQMGDIguXbrUSm9zAAAAAAAAAAAAAACwrRQXF0d+fn7cdNNNsWHDhoiIWLRoURQXF8fxxx8fJ554Yhx44IHRokWLyGQysWzZspg7d25Mnz49Xn/99az7TjrppPjFL36xrb+MbWrOnDmRl5eX6zEAAAAAAAAAAADYjmUymVyPUKm8zPY6GbVizpw5MWLEiPjoo48Snd9///3j5ptvjgMOOKBGc3z22Wdx1VVXxXvvvVetc127do2bb745dtlllxr1N8f2LZ1Ox+zZsyt91qlTp1oLGgcAAAAAAAAAAKB+socGwPbqk08+iXHjxsXLL7+c6A+WFBUVxZAhQ6J///51MN22t6Xv2QAAAAAAAAAAAFATudwdt7Fejz322GNRXFycOOA5ImLevHlx1llnxZNPPpn4jnfeeSf69OlT7UDjiIiZM2fGmWeeGZ9++mni/uYAAAAAAAAAAAAAACAX2rdvH3fddVdMmDAh2rVrV+Vz7dq1i9GjR8crr7xSbwKeAQAAAAAAAAAAoL7Ky2QymVwPQe174oknYvTo0Zt93qZNm+jQoUO0bNky0ul0LF++PP7xj3/EkiVLKn0/Ly8vxowZE6eeemq15pg3b17069cvSkpKsp41bNgwDjzwwNhzzz2jtLQ0Fi5cGHPnzo10Op31buvWreOpp56K1q1bV6u/OXYM6XQ6Zs+eXemzXKbgAwAAAAAAAAAAsGOwhwbA9mrGjBlx3333xdy5c6t99nvf+16cdtpp0a9fvygsLKyD6ba9LX3PBgAAAAAAAAAAgJrI5e64kOd6aOHChdGrV6/YsGFD1rMuXbrEz372szj44IMrPfvOO+/EuHHj4q233sp6ttNOO8X06dOrHCy8bt26OOOMM+Jf//pXhXpeXl6cd955ccEFF0SrVq0qPJs/f37ce++9MXXq1Kz7Dj/88HjsscciLy+vSv3NsePwh2sAAAAAAAAAAACoCXtoAGxvSkpKYuTIkTFt2rQa31VYWBjXXXddnHrqqbUwWW5t6Xt2fn5+vdiPBwAAAAAAAAAAoO5kMplIp9OVPsvl7niDnHSlTo0dO7bSgOeLLroorrjiii0uPXbu3DkmTpwYN998c0yaNKnCs7Vr18add94Zt9xyS5XmuP/++7MCjQsKCuLmm2+O008/vdIze+21V4wZMybat28fd9xxR4Vnb731Vjz99NPRp0+fKvU3BwAAAAAAAAAAAAAA29qaNWviwgsvrDTMuFmzZtG7d+/o0qVL7L///lFUVBTpdDpWrlwZ8+bNi9dffz2eeeaZWLNmTfmZVatWxbBhw+LDDz+MK6+8clt+KdtUx44d/cUMAAAAAAAAAAAAbFE6na50Py/XbL/VM+vWrYtXX301q37MMcfElVdeucWA52/k5+fHiBEj4vDDD8969vLLL8fGjRu3ese///3veOSRR7LqF1xwwWYDjf/ToEGD4uSTT86qjxs3rkr9zQEAAAAAAAAAAAAAQC7ccMMNlf4Bkl69esXMmTNjxIgRcfzxx0fr1q2jUaNG0aRJk2jTpk107do1Ro4cGTNnzowePXpknX/wwQdj4sSJ2+JLAAAAAAAAAAAAAKpByHM98/bbb8f69euz6pdeemm17snLy4shQ4Zk1UtKSqqUVv7b3/42NmzYUKH23e9+t9I7N+e6666LwsLCCrUlS5bE888/X+U7zAEAAAAAAAAAAAAAwLYya9asmDp1ala9b9++cccdd0Tz5s23ekdhYWGMGzcufvzjH2c9Gzt2bMyfP79WZgUAAAAAAAAAAABqh5DneuaLL77IqhUWFsahhx5a7bs6d+5c6QLpokWLtnguk8lUupR6/vnnR6NGjarcv2XLltGnT5+s+u9///sqnTcHAAAAAAAAAAAAAADb0iOPPJJVa9euXYwaNarad40ePTr22muvCrUNGzbEww8/nHg+AAAAAAAAAAAAoPYJea5nli9fnlXbY489Ij+/+r/VBQUFsccee2TVly5dusVzf//737OCoFOpVPTq1avaM5xxxhlZtbfffnurM5gDAAAAAAAAAAAAAIBtaf369fHGG29k1S+88MJo0KBBte9LpVJx0UUXZdVfeOGFSKfTiWYEAAAAAAAAAAAAap+Q53qmcePGVapVVZMmTbJqqVRqi2def/31rNoPfvCDKCwsrHb/ffbZJ9q2bVuhlslkKl18NQcAAAAAAAAAAAAAALny/vvvR2lpaVa9a9euie887rjjsmolJSXxwQcfJL4TAAAAAAAAAAAAqF1CnuuZ/w4AjohYvnx54vuWLVuWVWvVqtUWz7z77rtZtcMOOyzxDIcffnhW7Z133tnqOXMAAAAAAAAAAAAAALCtLF26NKu20047bXUHf0tat24dTZo0yap/8cUXie8EAAAAAAAAAAAAapeQ53rm0EMPjfz8ir+tCxYsSBT0vGzZsliwYEFW/ZBDDtniuffffz+rdsABB1S7/zc6dOiQVfvnP/+51XPmAAAAAAAAAAAAAABgW9m4cWNWrVmzZjW+t3nz5lm1NWvW1PheAAAAAAAAAAAAoHYIea5nWrZsGd26datQS6fT8eyzz1b7rqeffjoymUyFWseOHaNt27abPbNs2bJYtmxZVn3vvfeudv9vfO9738uqffLJJ1s8Yw4AAAAAAAAAAAAAALaloqKirNrq1atrfO+qVauyaoWFhTW+FwAAAAAAAAAAAKgdQp7rocsvvzxSqVSF2m9+85uYP39+le/4+OOPY/z48Vn1Sy65ZIvnPv/886xaXl5e7L777lXu/d8qC5Ves2ZNLF++3BxVnAMAAAAAAAAAAAAAgLrVsmXLrNq6deti8eLFie/8/PPPY/369Vn1XXbZJfGdAAAAAAAAAAAAQO1qkOsBqH377rtvjBw5Mq6//vry2sqVK2PgwIFxxx13xCGHHLLF82+99VZcccUVUVJSUqHeu3fv6Nat2xbPfv3111m1oqKirNDp6mjVqlWl9a+++ipatGhhjirMsSObM2dO5OXl5XoMAAAAAAAAAAAAtmOZTCbXIwBA7L333tGoUaPYsGFDhfqrr74a/fr1S3TnK6+8klVLpVLRrl27RPcBAAAAAAAAAAAAtU/Icz1VXFwc+fn5cdNNN5UviC5atCiKi4vj+OOPjxNPPDEOPPDAaNGiRWQymVi2bFnMnTs3pk+fHq+//nrWfSeddFL84he/2Grf5cuXZ9WKiopq9LU0bdo0UqlUlJaWVqivWLHCHFWcY0eWTqdzPQIAAAAAAAAAAAAAwFY1btw4DjvssKyd/Icffjj69OkTDRs2rNZ969ati4ceeiir3qlTp2jWrFmNZgUAAAAAAAAAAABqj5Dneuyss86KH/zgBzFu3Lh4+eWXI5PJRCaTiZkzZ8bMmTOrdEdRUVEMGTIk+vfvX6X3V69enVVr2rRpdcauVNOmTbNCjCvrZQ4AAAAAAAAAAAAAAHKlV69eWSHPixYtimuuuSZuv/32yMvLq9I96XQ6rrzyyvj666+znvXo0aNWZgUAAAAAAAAAAABqR36uB6ButW/fPu66666YMGFCtGvXrsrn2rVrF6NHj45XXnmlygHPERGlpaVZtVQqVeXzm1PZHRs3bjRHFecAAAAAAAAAAAAAAKDunXbaadG+ffus+vPPPx+DBg2qNLT5vy1evDjOO++8mDFjRtazPffcM/r06VMrswIAAAAAAAAAAAC1o0GuB6BuzZgxI+67776YO3dutc599tlnMXHixFixYkX069cvCgsLq3SuslDjBg1q/q9ZZaHGlfUyR/2Tn58feXl5uR4DAAAAAAAAAACA7Vgmk4l0Op3rMQAgCgoKYvTo0XH++edn7Xi/9tprccIJJ0TPnj3jmGOOif322y+Kiooik8nEihUrYt68efGXv/wlXn755Ur3wwsKCmLUqFHRsGHDbfXlAAAAAAAAAAAAAFUg5LmeKikpiZEjR8a0adMS3/Gvf/0r7rzzzpgwYUJcd911ceqpp271TH5+flZt06ZNiWf4RmULqpX1Mkf907Fjx3r7tQEAAAAAAAAAAFA70ul0zJ49O9djAEBERBx++OHxq1/9Kq688srIZDIVnm3cuDGmTp0aU6dOrfa9v/jFL+K4446rrTEBAAAAAAAAAACAWiLkuR5as2ZNXHjhhZX+YYVmzZpF7969o0uXLrH//vtHUVFRpNPpWLlyZcybNy9ef/31eOaZZ2LNmjXlZ1atWhXDhg2LDz/8MK688sot9m7QIPtfqQ0bNtT4a6rsjlQqZY4qzgEAAAAAAAAAAAAAwLZzyimnRMuWLeOaa66JL7/8skZ3tWrVKm666abo2rVr7QwHAAAAAAAAAAAA1Cohz/XQDTfcUGnAc69eveKGG26I5s2bZz1r0qRJtGnTJrp27RpDhw6N6667LqZPn17hnQcffDBatWoV55577mZ777TTTlm12gg1Xr9+faUzm6NqcwAAAAAAAAAAAAAAsG0dffTR8dxzz8Wjjz4aTz31VHz99dfVOr/rrrvGmWeeGQMHDowWLVrU0ZQAAAAAAAAAAABATQl5rmdmzZoVU6dOzar37ds3brzxxirdUVhYGOPGjYsRI0bE008/XeHZ2LFjo2vXrrHXXntVenbnnXfOqq1bt65KfTenrKwsNm7cmFUvKira7BlzAAAAAAAAAAAAAACQa4WFhTF06NC49NJLY9asWfHee+/FnDlzYuHChbF69epYvXp1REQ0b948mjdvHm3bto2DDjooOnXqFEcffXSkUqkcfwUAAAAAAAAAAADA1gh5rmceeeSRrFq7du1i1KhR1b5r9OjR8fbbb8f8+fPLaxs2bIiHH354s4HRu+yyS1ZtyZIlkU6nIz8/v9ozRER89dVXVe5lDgAAAAAAAAAAAAAAtjcNGjSIY489No499thcjwIAAAAAAAAAAADUsmQps2yX1q9fH2+88UZW/cILL4wGDaqf551KpeKiiy7Kqr/wwguRTqcrPbP77rtn1UpLS2PJkiXV7v+NxYsXZ9Xy8/OjTZs2mz1jDgAAAAAAAAAAAAAAAAAAAAAAAAAAAOqakOd65P3334/S0tKseteuXRPfedxxx2XVSkpK4oMPPqj0/d13373SQOkvvvgi8QxffvllVu073/lOpFKpzZ4xBwAAAAAAAAAAAAAAAAAAAAAAAAAAAHVNyHM9snTp0qzaTjvtFK1atUp8Z+vWraNJkyZZ9c2FFDds2DD23nvvrPr777+feIbKzu63335bPGMOAAAAAAAAAAAAAAAAAAAAAAAAAAAA6pqQ53pk48aNWbVmzZrV+N7mzZtn1dasWbPZ9w866KCs2rvvvpu4f2VnK+thDgAAAAAAAAAAAAAAAAAAAAAAAAAAALYlIc/1SFFRUVZt9erVNb531apVWbXCwsLNvn/kkUdm1f72t78l6r1u3bqYO3duVv2oo47a6llzAAAAAAAAAAAAAAAAAAAAAAAAAAAAUJeEPNcjLVu2zKqtW7cuFi9enPjOzz//PNavX59V32WXXTZ7pkuXLtGgQYMKtcWLF8fbb79d7f7Tpk2LjRs3Vqi1aNEiDj744K2eNQcAAAAAAAAAAAAAAAAAAAAAAAAAAAB1SchzPbL33ntHo0aNsuqvvvpq4jtfeeWVrFoqlYp27dpt9kzLli3jmGOOyapPmTKl2v2ffPLJrNrJJ58cBQUFWz1rDgAAAAAAAAAAAAAAAAAAAAAAAAAAAOqSkOd6pHHjxnHYYYdl1R9++OHYuHFjte9bt25dPPTQQ1n1Tp06RbNmzbZ49pxzzsmqvfjii/G///u/Ve7/8ssvx+zZsyvU8vLyKr3bHAAAAAAAAAAAAAAAAAAAAAAAAAAAAGxrQp7rmV69emXVFi1aFNdcc01kMpkq35NOp+PKK6+Mr7/+OutZjx49tnr++OOPjw4dOlSolZWVxbXXXhtr1qzZ6vklS5bEjTfemFX/4Q9/GN///ve3et4cAAAAAAAAAAAAAAAAAAAAAAAAAAAA1DUhz/XMaaedFu3bt8+qP//88zFo0KBKQ5v/2+LFi+O8886LGTNmZD3bc889o0+fPlu9Iy8vL0aMGBF5eXkV6h9++GH0798/lixZstmzn376aRQXF2e906hRo7jqqqu22tscAAAAAAAAAAAAAAAAAAAAAAAAAAAAbAsNcj0AtaugoCBGjx4d559/fpSWllZ49tprr8UJJ5wQPXv2jGOOOSb222+/KCoqikwmEytWrIh58+bFX/7yl3j55Zezzn5z96hRo6Jhw4ZVmqVz585xwQUXxEMPPVShPnfu3OjevXv06dMnunXrFm3bto1NmzbFggULYvr06fHcc89V2v/aa6+NPffcsxq/GuYAAAAAAAAAAAAAAAAAAAAAAAAAAACg7uRlMplMroeg9j3//PNx5ZVXRm3+9t58881x5plnVutMWVlZ/PznP4+XXnqpRr3PPffcuPbaaxOfN8f2LZ1Ox+zZsyt91qlTp8jPz9/GEwEAAAAAAAAAALAjsYcGADsG37MBAAAAAAAAAACoie11D832Wz11yimnxIQJE6JNmzY1vqtVq1Yxfvz4agc8R0QUFBTE2LFjo7i4OFHvvLy8GDp0aI0Djc0BAAAAAAAAAAAAAAAAAAAAAAAAAABAbRPyXI8dffTR8dxzz8XgwYNjt912q/b5XXfdNQYNGhTPPfdcdO3aNfEcqVQqbrjhhhg/fny0b9++yuc6duwYTzzxRAwePDhxb3MAAAAAAAAAAAAAAAAAAAAAAAAAAABQV/IymUwm10NQ9zZt2hSzZs2K9957L+bMmRMLFy6M1atXx+rVqyMionnz5tG8efNo27ZtHHTQQdGpU6c4+uijI5VK1eocmUwmZs2aFX/605/i73//e8yfPz/WrFkTERGFhYXRrl27OPTQQ+PEE0+Mjh071mpvc2y/0ul0zJ49u9JnnTp1ivx8efQAAAAAAAAAAABsnj00ANgx+J4NAAAAAAAAAABATWyve2hCnoGc217/CxIAAAAAAAAAAIAdgz00ANgx+J4NAAAAAAAAAABATWyve2i23wAAAAAAAAAAAAAAAAAAAAAAAAAAAAASEPIMAAAAAAAAAAAAAAAAAAAAAAAAAAAAkICQZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEhDwDAAAAAAAAAAAAAAAAAAAAAAAAAAAAJCDkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACABIc8AAAAAAAAAAAAAAAAAAAAAAAAAAAAACQh5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEhAyDMAAAAAAAAAAAAAAAAAAAAAAAAAAABAAkKeAQAAAAAAAAAAAAAAAAAAAAAAAAAAABIQ8gwAAAAAAAAAAAAAAAAAAAAAAAAAAACQgJBnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASEPAMAAAAAAAAAAAAAAAAAAAAAAAAAAAAkIOQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEhzwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJCHkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASEDIMwAAAAAAAAAAAAAAAAAAAAAAAAAAAEACQp4BAAAAAAAAAAAAAAAAAAAAAAAAAAAAEhDyDAAAAAAAAAAAAAAAAAAAAAAAAAAAAJCAkGcAAAAAAAAAAAAAAAAAAAAAAAAAAACABIQ8AwAAAAAAAAAAAAAAAAAAAAAAAAAAACQg5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASHPAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAkIeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIQMgzAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAJCngEAAAAAAAAAAAAAAAAAAAAAAAAAAAASEPIMAAAAAAAAAAAAAAAAAAAAAAAAAAAAkECDXA8AAED9kclk4uuvv47S0tJcjwLUkk2bNkVpaWmkUqlo0MD/hYT6JJVKxW677RZ5eXm5HgUAAAAAAAAAAIAdjN1xqH/sjkP9ZXccAAAAAACg7vkpKwAANZbJZOLJJ5+MyZMnx8qVK3M9DgBQRTvvvHMUFxfHWWedZWEXAAAAAAAAAACArbI7DgA7JrvjAAAAAAAAdUvIMwAANTZ58uR48MEHcz0GAFBNK1eujPHjx0c6nY6zzz471+MAAAAAAAAAAACwnbM7DgA7JrvjAAAAAAAAdSs/1wMAALBjKy0tjSlTpuR6DACgBqZMmRKlpaW5HgMAAAAAAAAAAIDtmN1xANjx2R0HAAAAAACoG0KeAQCokY0bN8aqVatyPQYAUAOrVq2KjRs35noMAAAAAAAAAAAAtmN2xwFgx2d3HAAAAAAAoG4IeQYAoEYaNmwYhYWFuR4DAKiBwsLCaNiwYa7HAAAAAAAAAAAAYDtmdxwAdnx2xwEAAAAAAOqGkGcAAGoklUpF3759cz0GAFADffv2jVQqlesxAAAAAAAAAAAA2I7ZHQeAHZ/dcQAAAAAAgLrRINcDAACw4ysuLo78/PyYMmVKrFixItfjAABVVFRUFMXFxdGnT59cjwIAAAAAAAAAAMAOwO44AOyY7I4DAAAAAADULSHPAADUWF5eXvTt2zfOOuus+Prrr6O0tDTXIwE18OWXX8bw4cM3+3zMmDHRpk2bbTgRUBdSqVTstttukZeXl+tRAAAAAAAAAAAA2EHYHYf6xe44fDvYHQcAAAAAAKh7Qp4BAKg1eXl50bp161yPAdSxNm3aRNu2bXM9BgAAAAAAAAAAAMD/Z+/+Yuuu6z+Ov89Zz4jrMrYSYDRFERgGhA2mGC1LgE1NOhKG5RSmiUrcBTfDGGMGYnahF1ws0cQLE40agzGG4RllwUmVYBwBSWSAzAwICThkjj9FNrCz0tO1v4tfws/+vpWdvtv1266Px+W77ee8SLjw4viEkvjuOCwMvjsOAAAAAAAA0Jpq2QMAAAAAAAAAAAAAAAAAAAAAAAAAAAAA5iORZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACABJFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgASRZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIAEkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACAhLayBwAAAAAAAAAAAAAAAJwK7rjjjujv7z+pn7F169a47bbbTupnAAAAAAAAAAAAAK2rlj0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAYD4SeQYAAAAAAAAAAAAAAJgnLrroorInAAAAAAAAAAAAAP9B5BkAAAAAAAAAAAAAAGAe6OjoiPXr15c9AwAAAAAAAAAAAPgPbWUPAAAAAAAAAAAAAAAAOBVs3749tm3bNu13hoaGYuPGjdFsNifcN23aFLVabdrvAwAAAAAAAAAAADNH5BkAAAAAAAAAAAAAAGAGtLe3R3t7+7TfGRgYKASeIyLq9fq03wYAAAAAAAAAAABmVrXsAQAAAAAAAAAAAAAAAPyfRqNRuF1xxRVx4YUXlrAGAAAAAAAAAAAAeD9tZQ9gYTpy5EgcOHAgXn755RgaGopKpRLLli2L8847Ly699NJYunSpHSXsAAAAAAAAAAAAAACgXM8//3wcOHCgcK/X6yWsAQAAAAAAAAAAAE5E5PkUc8cdd0R/f/9J/YytW7fGbbfdNuW/Gx8fjwcffDB++ctfxpNPPhljY2OT/l6tVovu7u740pe+FOvWrZvuXDsAAAAAAAAAAAAAAJg3Go1G4bZkyZLo6ekpYQ0AAAAAAAAAAABwIiLPzIqDBw/G7bffHn/+859P+LvNZjP27t0be/fujWuuuSbuuuuuOOOMM+w4CTsAAAAAAAAAAAAAAJg7RkZG4oEHHijcr7vuumhvby9hEQAAAAAAAAAAAHAi1bIHMP9cdNFFU/r9ffv2RV9fX0tB4//vD3/4Q9x4443x0ksvTflv7QAAAAAAAAAAAAAAYD556KGH4ujRo4V7vV6f/TEAAAAAAAAAAABAS9rKHsD80tHREevXr2/5959//vm49dZbY2hoqPCzxYsXx6WXXhrnnntuNJvNeOWVV+LAgQMxNjY24fdeffXVuOWWW+JXv/pVnH322anddgAAAAAAAAAAAAAAMNc1Go3CbdWqVXH55ZfP/hgAAAAAAAAAAACgJSLPp5jt27fHtm3bpv3O0NBQbNy4MZrN5oT7pk2bolartfTG8PBwfO1rXysEjSuVStxyyy2xZcuWOPPMMyf87OWXX44f/OAHsXv37gn3119/Pb7xjW/Ez3/+86hUKlP6Z7EDAAAAAAAAAAAAAIC57tChQ/H4448X7vV6vYQ1AAAAAAAAAAAAQKtEnk8x7e3t0d7ePu13BgYGCoHniKl9OfSHP/xh/PWvf51wW7RoUdx1111xww03TPo3H/rQh2LHjh1xwQUXxPe+970JP/vTn/4UjUYj+vr6Wt5gBwAAAAAAAAAAAAAA88F9990X4+PjE261Wi2uv/76khYBAAAAAAAAAAAAraiWPYC5qdFoFG5XXHFFXHjhhS39/T/+8Y/42c9+Vrhv2bLlvwaN/9Ott94aGzduLNy///3vx8jISEsb7AAAAAAAAAAAAAAAYD4YGxuL/v7+wn3Dhg3R0dFRwiIAAAAAAAAAAACgVW1lD2Duef755+PAgQOFe71eb/mNX/ziF/Huu+9OuH3wgx+MrVu3tvzG9u3b49FHH4133nnnvdvg4GD8+te/jt7eXjsSOwAAAAAAAAAAAAAAmHsee+yxOHz4cOE+le/xz3fPPPNMVCqVsmcAACwog4OD7/vz5557Lt58881ZWgMAAAAAAABwYuPj42VPmJTIMwWNRqNwW7JkSfT09LT09+Pj47F79+7C/Stf+UqcdtppLe/o6OiIvr6++OlPfzrh3t/f31LU2A4AAAAAAAAAAAAAAOaDyb7H39nZGVdddVUJa8oxNjZW9gQAgAXn+PHjJ/z5iX4HAAAAAAAAgIhq2QOYW0ZGRuKBBx4o3K+77rpob29v6Y39+/fH3//+9wm3Wq0W11133ZT3fO5znyvcnnjiiZb+y892AAAAAAAAAAAAAAAw1x05ciQefvjhwr23tzeqVf+3DwAAAAAAAAAAAJjrfNuPCR566KE4evRo4V6v11t+49FHHy3cPvaxj8WyZcumvGfVqlXR1dU14TY+Ph5//OMf7ZjiDgAAAAAAAAAAAAAA5p7du3dHs9mccKtWq3HjjTeWtAgAAAAAAAAAAACYirayBzC3NBqNwm3VqlVx+eWXt/zGU089VbhdeeWV6U2f+MQn4tChQxNu+/bti+uvv96OKewAAAAAAAAAAAAAAGDu2bVrV+HW3d0dnZ2dJawpT7VajUqlUvYMAIAFZdGiRSf8+Yl+BwAAAAAAAGA2jY+Px9jYWNkzCkSeec+hQ4fi8ccfL9zr9fqU3nn22WcLt49+9KPpXZdcckncd999E27PPfecHVPcAQAAAAAAAAAAAADA3LJ///544YUXCvepfo//VLBmzZqoVqtlzwAAWFAOHTr0vj+/+OKLo6ura5bWAAAAAAAAAJzY2NhYPP3002XPKPDtN95z3333xfj4+IRbrVaL66+/vuU33nrrrXjrrbcK9/PPPz+968Mf/nDh9uKLL9oxhR0AAAAAAAAAAAAAAMw9jUajcFuxYkVs2LChhDUAAAAAAAAAAABAhsgzEfG/FfL+/v7CfcOGDdHR0dHyO5P9V5srlUp0dnamt032X3k+duxYHDlyxI4WdwAAAAAAAAAAAAAAMLcMDw/Hnj17CvdNmzbF4sWLS1gEAAAAAAAAAAAAZLSVPYC54bHHHovDhw8X7vV6fUrvvPHGG4Xb8uXLo1arpbedeeaZk95ff/31WLFihR0t7JjPnnnmmahUKmXPAABYUAYHB9/3588991y8+eabs7QGAAAAAAAA4MTGx8fLngAAUzYwMBBDQ0OF+1S/xw8AAAAAAAAAAACUS+SZiIhoNBqFW2dnZ1x11VVTeufIkSOF2/Lly7OzIiKivb09arVaNJvNCfejR4/a0eKO+WxsbKzsCQAAC87x48dP+PMT/Q4AAAAAAAAAAPD+Jvse/5o1a2LVqlUlrAEAAAAAAAAAAACyqmUPoHxHjhyJhx9+uHDv7e2NanVq/4r885//LNza29vT297vjck+yw4AAAAAAAAAAAAAAOa6gwcPxr59+wr3vr6+EtYAAAAAAAAAAAAA0yHyTOzevTuazeaEW7VajRtvvHHKb/3/dyIiarVaetv7vTEyMmJHizsAAAAAAAAAAAAAAJg7Go1G4bZkyZLo6ekpYQ0AAAAAAAAAAAAwHW1lD6B8u3btKty6u7ujs7Nzym9NFjVua5v+v2aTRY0n+yw7Tj3VajUqlUrZMwAAFpRFixad8Ocn+h0AAAAAAACA2TQ+Ph5jY2NlzwCAloyOjsb9999fuPf09MTSpUtnfxAAAAAAAAAAAAAwLSLPC9z+/fvjhRdeKNzr9XrqvWq1WriNjo6m3vpPkwWMJ/ssO049a9asOWX/2QAA5qpDhw69788vvvji6OrqmqU1AAAAAAAAACc2NjYWTz/9dNkzAKAle/fujcHBwcI9+z1+AAAAAAAAAAAAoFzKqQtco9Eo3FasWBEbNmxIvdfWVuyGv/vuu6m3TvRGrVazo8UdAAAAAAAAAAAAAADMDZN9j/+CCy6ItWvXlrAGAAAAAAAAAAAAmC6R5wVseHg49uzZU7hv2rQpFi9enHpzyZIlhdtMRI3//e9/F24f+MAH7GhxBwAAAAAAAAAAAAAA5RscHIxHHnmkcK/X6yWsAQAAAAAAAAAAAGaCyPMCNjAwEENDQ4X7dL4cevrppxduw8PD6fciIo4fPx4jIyOF+/Lly+1ocQcAAAAAAAAAAAAAAOXr7++P0dHRCbdarRY33HBDOYMAAAAAAAAAAACAaRN5XsAajUbhtmbNmli1alX6zTPOOKNwGxwcjLGxsfSbr7/+esufZQcAAAAAAAAAAAAAAHPVrl27Crf169dHR0dHCWsAAAAAAAAAAACAmSDyvEAdPHgw9u3bV7j39fVN693Ozs7CrdlsxuDgYPrNw4cPF27VajVWrlxpR4s7AAAAAAAAAAAAAAAo1759++LgwYOFe71en/0xAAAAAAAAAAAAwIwReV6gGo1G4bZkyZLo6emZ1rudnZ3R1tZWuL/66qvpN1977bXC7ZxzzolarWZHizsAAAAAAAAAAAAAACjXZN/jP+ecc2LdunUlrAEAAAAAAAAAAABmisjzAjQ6Ohr3339/4d7T0xNLly6d1tuLFy+O888/v3B/9tln029O9rcf+chH7JjCDgAAAAAAAAAAAAAAyjM0NBQDAwOFe29vb1Sr/q8dAAAAAAAAAAAAMJ/5JuACtHfv3hgcHCzc6/X6jLx/2WWXFW5PPfVU+r3J/nayz7ADAAAAAAAAAAAAAIC5aM+ePTE8PDzhVqlUore3t6RFAAAAAAAAAAAAwEwReV6AGo1G4XbBBRfE2rVrZ+T9T37yk4Xbk08+mXpreHg4Dhw4ULh/6lOfsmOKOwAAAAAAAAAAAAAAKMdk3+Pv7u6Orq6uEtYAAAAAAAAAAAAAM6mt7AHMrsHBwXjkkUcK93q9PmOfsW7dumhra4vR0dH3bocPH44nnngirrzyyim99eCDD8bIyMiE24oVK2L16tV2THEHAAAAAAAAAAAAAACzb2hoKFavXh2XXXbZhHtPT09JiwAAAAAAAAAAAICZJPK8wPT390+IDUdE1Gq1uOGGG2bsMzo6OuKqq66KvXv3Trjv3LlzylHje++9t3DbuHFjLFq0yI4p7gAAAAAAAAAAAAAAYPYtXbo0tm/fXvYMAAAAAAAAAAAA4CSplj2A2bVr167Cbf369dHR0TGjn/OFL3yhcPvNb34Tf/nLX1p+43e/+108/fTTE26VSmXSt+0AAAAAAAAAAAAAAAAAAAAAAAAAAABgtok8LyD79u2LgwcPFu71en3GP+vqq6+OSy65ZMLt+PHjceedd8axY8dO+PeDg4Pxne98p3D/9Kc/HRdeeKEdyR0AAAAAAAAAAAAAAAAAAAAAAAAAAADMHJHnBaTRaBRu55xzTqxbt27GP6tSqcS3vvWtqFQqE+4vvPBCfPGLX4zBwcH/+rcvvfRSbN68ufA7p512Wtx+++12TGMHAAAAAAAAAAAAAAAAAAAAAAAAAAAAM6et7AHMjqGhoRgYGCjce3t7o1o9Oa3vj3/847Fly5b4yU9+MuF+4MCB+OxnPxt9fX1x7bXXRldXV4yOjsbf/va3GBgYiAceeCCazWbhvTvvvDPOPfdcO6a5AwAAAAAAAAAAAAAAAAAAAAAAAAAAgJkh8rxA7NmzJ4aHhyfcKpVK9Pb2ntTP/frXvx6vvPJK/Pa3v51w/9e//hV333133H333S298+Uvfzk2b95sxwztAAAAAAAAAAAAAAAAAAAAAAAAAAAAYPqqZQ9gdjQajcKtu7s7urq6TurnLlq0KL773e+mg8SVSiW++tWvxp133mnHDO4AAAAAAAAAAAAAAAAAAAAAAAAAAABg+trKHsDJNzQ0FKtXr47LLrtswr2np2dWPr9Wq8W3v/3tuPbaa2PHjh3x4osvtvR3a9asiW9+85txxRVX2HESdgAAAAAAAAAAAAAAAAAAAAAAAAAAADA9Is8LwNKlS2P79u1lz4hrrrkmrr766nj88cfj97//fezfvz9efvnlOHbsWERELFu2LM4777xYu3ZtfOYzn4k1a9bYMQs7AAAAAAAAAAAAAAAAAAAAAAAAAAAAyBF5ZlZVKpXo7u6O7u5uO+bQDgAAAAAAAAAAAAAAAAAAAAAAAAAAAKauWvYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgPlI5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgQeQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIEHkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACBB5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgQeQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIEHkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACBB5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgQeQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIEHkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACBB5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgQeQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIEHkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACBB5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgQeQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIEHkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACBB5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgQeQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIEHkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACBB5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgQeQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIEHkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACBB5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgQeQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIEHkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACBB5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgQeQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIEHkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACBB5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgQeQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIEHkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACBB5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgQeQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIEHkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACBB5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgQeQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIEHkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACBB5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgQeQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIEHkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACBB5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgQeQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIEHkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACBB5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgQeQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIEHkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACBB5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgQeQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIEHkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACBB5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgQeQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIEHkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACBB5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgQeQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIEHkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACBB5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgQeQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIEHkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACBB5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgQeQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIEHkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACBB5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgQeQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIEHkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACBB5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgQeQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIEHkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACBB5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgQeQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIEHkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACChrewBAAAAAAAAAAAAAAAAAAAAAAAAcKoYHx+PN954I5rNZtlTgBkyOjoazWYzarVatLVJ+MGppFarxVlnnRWVSqXsKQDMY/4XIgAAAAAAAAAAAAAAAAAAAAAAAEzT+Ph43HvvvXHPPffE22+/XfYcAKBFp59+emzevDluuukmsWcAUkSeAQAAAAAAAAAAAAAAAAAAAAAAYJruueee+PGPf1z2DABgit5+++340Y9+FGNjY/H5z3++7DkAzEPVsgcAAAAAAAAAAAAAAAAAAAAAAADAfNZsNmPnzp1lzwAApmHnzp3RbDbLngHAPCTyDAAAAAAAAAAAAAAAAAAAAAAAANMwMjIS77zzTtkzAIBpeOedd2JkZKTsGQDMQyLPAAAAAAAAAAAAAAAAAAAAAAAAMA2LFy+OZcuWlT0DAJiGZcuWxeLFi8ueAcA8JPIMAAAAAAAAAAAAAAAAAAAAAAAA01Cr1eLmm28uewYAMA0333xz1Gq1smcAMA+1lT0AAAAAAAAAAAAAAAAAAAAAAAAA5rvNmzdHtVqNnTt3xtGjR8ueAwC0aPny5bF58+bo6+srewoA85TIMwAAAAAAAAAAAAAAAAAAAAAAAExTpVKJm2++OW666aZ44403otlslj0JmIbXXnsttm3b9l9/vmPHjli5cuUsLgJOhlqtFmeddVZUKpWypwAwj4k8AwAAAAAAAAAAAAAAAAAAAAAAwAypVCpx9tlnlz0DOMlWrlwZXV1dZc8AAGAOqJY9AAAAAAAAAAAAAAAAAAAAAAAAAAAAAGA+EnkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASBB5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEgQeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIEHkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASBB5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEgQeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIEHkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASBB5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEgQeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIEHkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASBB5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEgQeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIEHkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASBB5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEgQeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIEHkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASBB5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEgQeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIEHkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASBB5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEgQeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIEHkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASBB5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEgQeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIEHkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASBB5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEgQeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIEHkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASBB5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEgQeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIEHkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASBB5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEgQeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIEHkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASBB5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEgQeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIEHkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASBB5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEgQeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIEHkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASBB5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEgQeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIEHkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASBB5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEgQeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIEHkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASBB5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEgQeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIEHkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASBB5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEgQeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIEHkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASBB5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEgQeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIEHkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASBB5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEgQeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIEHkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASBB5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEgQeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIEHkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASBB5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEgQeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIEHkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASBB5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEgQeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIEHkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASBB5BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEgQeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIEHkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASBB5Bvgf9u48uqry3B/4s0MSIIQZZRAEBFSsolWkOFRwqB20VW8rxVqtxfYWb2mrXdW2qL+q13apHfVqrddWi0PrUKc60eq12MEBwQkFVLAqMsk8hEBCsn9/dOWUQwIkJ8NJwuez1lmc/Zz97vd7WP5D1puvAAAAAAAAAAAAAAAAAAAAAAAAAAAAOVDyDAAAAAAAAAAAAAAAAAAAAAAAAAAAAJADJc8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAOVDyDAAAAAAAAAAAAAAAAAAAAAAAAAAAAJADJc8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAOVDyDAAAAAAAAAAAAAAAAAAAAAAAAAAAAJADJc8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAOVDyDAAAAAAAAAAAAAAAAAAAAAAAAAAAAJADJc8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAOVDyDAAAAAAAAAAAAAAAAAAAAAAAAAAAAJCDwnwHAAAAAAAAANgdpGkaH3zwQVRWVuY7CtCEtm7dGpWVlVFUVBSFhY5hQHtSVFQUe+65ZyRJku8oAAAAAAAAAAAAAAAAQCvmtwsBAAAAAAAAmlGapnHPPffEXXfdFevWrct3HACgAbp37x4TJ06MCRMmKHsGAAAAAAAAAAAAAAAA6qTkGQAAAAAAAKAZ3XXXXXHzzTfnOwYAkIN169bFTTfdFNXV1XHGGWfkOw4AAAAAAAAAAAAAAADQChXkOwAAAAAAAABAe1VZWRl33313vmMAAI109913R2VlZb5jAAAAAAAAAAAAAAAAAK2QkmcAAAAAAACAZlJRURHr16/PdwwAoJHWr18fFRUV+Y4BAAAAAAAAAAAAAAAAtEJKngEAAAAAAACaSXFxcXTr1i3fMQCARurWrVsUFxfnOwYAAAAAAAAAAAAAAADQCil5BgAAAAAAAGgmRUVF8fnPfz7fMQCARvr85z8fRUVF+Y4BAAAAAAAAAAAAAAAAtEKF+Q4AAAAAAAAA0J5NnDgxCgoK4u677461a9fmOw4A0AA9evSIiRMnxumnn57vKAAAAAAAAAAAAAAAAEArpeQZAAAAAAAAoBklSRKf//znY8KECfHBBx9EZWVlviMBjbRs2bK46KKLdvj5NddcE/369WvBREBzKCoqij333DOSJMl3FAAAAAAAAAAAAAAAAKAVU/IMAAAAAAAA0AKSJIm+ffvmOwbQAvr16xcDBw7MdwwAAAAAAAAAAAAAAAAAoAUU5DsAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQFuk5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgB0qeAQAAAAAAAAAAAAAAAAAAAAAAAAAAAHKg5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgB0qeAQAAAAAAAAAAAAAAAAAAAAAAAAAAAHKg5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgB4X5DgAAAAAAAAAAAAAAALA7WrhwYbz99tuxbNmyKCsri4KCgujcuXPssccesffee8ewYcOiY8eO+Y4JAAAAAAAAAAAA7ISS591cvg6ErlmzJl5//fV49913Y+PGjZEkSXTr1i2GDBkSBx54YJSWljb5nnIAAAAAAAAAAAAAAJBvr776atxzzz3xl7/8JVauXLnTe4uKimK//faLI444IsaNGxeHHXZYFBQUtFBSAAAAAAAAAAAAoD6UPO+G8nUgNE3TePzxx+N3v/tdzJ49O6qrq3e455FHHhlnn312HH300TntJQcAAAAAAAAAAAAAAK3JO++8E1dddVX85S9/qfeaysrKeO211+K1116Lm2++Of7v//4vBg4c2IwpAQAAAAAAAAAAgIZS8rwbyeeB0HfeeSe++93vxssvv1yvPZ9++ul4+umnY/z48fGjH/0oevfu3eA95QAAAAAAAAAAAAAAoDV48MEH4/LLL49NmzblOwoAAAAAAAAAAADQxAryHYCW8eCDD8Zpp53WoILnpjJr1qw4/fTT61VovL0ZM2bEZz/72Xj77bflaOIcAAAAAAAAAAAAAAA0v5tuuim++93v7rDgefDgwXH00UfHKaecEhMmTIhPfOIT8eEPfzi6dOnSwkkBAAAAAAAAAACAXBTmOwDN76abboqf/exnO/x88ODBMWjQoOjdu3d07Ngx1q9fH8uXL48333wzysrKGrX3/Pnz42tf+1ps3Lix1mfFxcVx4IEHxqBBg6KysjIWLVoUr7/+elRXV2fdt3Tp0jjnnHPi3nvvjb59+8rRBDkAAAAAAAAAAAAAAGh+d955Z53n+UtKSuLcc8+Nk046KYYOHVrn2jRN44033ogZM2bEY489Fm+88UZzxwUAAAAAAAAAAAByoOS5ncvngdDy8vI4//zzaxUaJ0kS55xzTpx77rmxxx57ZH327rvvxg033BAPPfRQ1nz58uXxne98J2677bZIkkSORuQAAAAAAAAAAAAAAKD5zZo1K374wx/Wmh9zzDFx5ZVXRt++fXe6PkmS2H///WP//fePyZMnx6xZs6J79+7NFRcAAAAAAAAAAADIUUG+A9B8dnYgdPr06TFlypQdFjxH/PtA6OTJk+OPf/xj3HnnnQ06EPqrX/0q/vnPf2bNOnToEFdddVV873vfq1VoHBExePDguOaaa+Lb3/52rc9mzpwZf/jDH+q9vxwAAAAAAAAAAAAAAORDeXl5XHTRRVFVVZU1P+644+KGG27YZcFzXUaPHh1du3ZtqogAAAAAAAAAAABAE1Hy3E7l+0DoqlWr4tZbb601P/fcc+PUU0/d5fqvfe1r8alPfarW/Nprr42Kiop6ZZADAAAAAAAAAAAAAIB8uPnmm2Px4sVZsyFDhsS1114bxcXFeUoFAAAAAAAAAAAANAclz+1Uvg+E3nHHHbFly5as2d577x1Tpkyp9zMuvfTS6NatW9ZsxYoV8cgjj8iRYw4AAAAAAAAAAAAAAJrX+vXr49Zbb601v/zyyxU8AwAAAAAAAAAAQDuk5LkdyveB0DRN46GHHqo1nzRpUnTs2LHez+nVq1ecfvrpteYPPPCAHDnkAAAAAAAAAAAAAACg+d19992xadOmrNkRRxwRY8eOzVMiAAAAAAAAAAAAoDkpeW6H8n0g9NVXX43FixdnzYqKiuKkk05q8LNOO+20WrMXXnghVq5cKUcDcwAAAAAAAAAAAAAA0PweeuihWrPPfe5zeUgCAAAAAAAAAAAAtITCfAeg6eX7QOjf//73WrPDDjssunXr1uBnjRgxIgYOHBjvv/9+ZpamaTzzzDPxmc98Ro4G5AAAAAAAAAAAAAAAoHktXLgw3nrrraxZSUlJfOxjH8tTIsiPysrKWL58eb5jAAC7sGzZskZ9DgC0Dn379o2ioqJ8xwAAAAAA2K0peW5nWsOB0BdffLHW7PDDD8/5eWPGjMkqNY6ImDVr1i5LjeUAAAAAAAAAAAAAAKAlPfvss7VmBx54YHTs2DEPaSB/li9fHmeffXa+YwAAjXTRRRflOwIAUA+33XZbDBw4MN8xAAAAAAB2a0qe25nWcCB07ty5tWYf+tCHcn7eAQccEPfff3/WbN68eXI0MAcAAAAAAAAAAAAAAM1r9uzZtWaHHnpo1vXy5cvj4YcfjmeeeSYWLFgQa9asicLCwujZs2f07t07DjnkkDj66KNj7NixyqEBAAAAAAAAAACgDVDy3M7k+0Do6tWrY/Xq1bXm++yzT8O+yDaGDh1aa7Zw4UI5GpADAAAAAAAAAAAAAIDm9+abb9aaHXDAARERsW7duvj5z38ef/jDH6KysjLrnoqKiti0aVMsXrw4Xn311bjttttijz32iPPOOy8mTJgQRUVFLZIfAAAAAAAAAAAAaDglz+1Mvg+Evv/++7VmSZLEgAEDcvg2/zJw4MBas7KyslizZk307NlTjnrkAAAAAAAAAAAAAACgeaVpGu+++26teb9+/WL+/PnxX//1X7F48eJ6P2/FihVxxRVXxH333Rc33nhj9O3btynjAgAAAAAAAAAAAE1EyXM70hoOhH7wwQe1Zj169Kh3SXRd9thjjzrny5cv32GpsRztxyuvvBJJkuQ7BgDAbmXFihU7/XzevHmxcuXKFkoDAAAAAK2Pn6EBALQ+aZrmOwIAxOrVq6OysrLWfN26dXHhhRfG2rVrc3ru66+/HhMmTIhbb7019tlnn0ambL2cHW9fdvUzNAAAAKDpOKsCAADNx9lxAIDWp7WeHVfy3I60hgOha9asqTXr0aNHTvvW6NKlSxQVFdX6bjv7PnK0H9XV1fmOAACw26mqqtrl57u6BwAAAADaMz9DAwAAAOqyo1/cvfzyy2ud9x47dmycfPLJ8aEPfSh69+4dGzdujPfeey/+8pe/xIMPPhhbtmzJun/ZsmXxjW98I/7whz9E586dm+sr5JWz4+2Ln48BAABAy3FWBQAAmo+z4wAA1JeS53akNRwI3bBhQ61Zly5dGv5l6njG9t+hrr3kAAAAAAAAAAAAAAAgHzZv3lzn/P3338+8Ly0tjWuuuSaOP/74rHv69u0bw4YNi2OPPTa++tWvxne+8514+eWXs+5ZsGBB/PCHP4wrr7yyybMDAAAAAAAAAAAAuVPy3I60hgOhlZWVtWZFRUUN+BZ1q+sZFRUVctQzBwAAAAAAAAAAAAAAzWtXZ7o7deoUt956a4waNWqn9w0aNChuueWW+PKXvxyvvPJK1mf3339/fO1rX4tBgwY1Oi/k00UDtkafwjTfMQCAbWxN//UqTP71AgBaj5Vbk7hmiXoQAAAAAIDWzE9x25HWcCC0rlLjwsLG/2dWV6lxXXvJ0f4UFBREkjgRAgDQkjp06LDLz3d1DwAAAAC0Z36GBgDQ+qRpGtXV1fmOAcBublfnns8///xdnuev0aVLl7jmmmvilFNOic2bN2fmVVVVccstt8QPfvCDRmVtjZwdb1929fOxPoVp9CtuoTAAAADQ5u38f5TkrAoAADQfZ8cBAFqf1np2XMlzO9IaDoQWFBTUmm3durVee+5MXQXGde0lR/tz8MEHt9vvBgDQWr3//vs7/XzkyJExcODAFkoDAAAAAK2Pn6EBALQ+1dXV8dJLL+U7BgC7ueLiHTfW7rHHHnHWWWc16HlDhgyJ//iP/4jf/e53WfPp06e3y5JnZ8fbl139DA0AAABoOs6qAABA83F2HACg9WmtZ8edfmtHmutA6PamT5++wzWFhbV7w7ds2dKgfetS1zOKiorkqGcOAAAAAAAAAAAAAACaV+fOnXf42amnnlrn+fJdOf3002vNVq9eHQsXLmzwswAAAAAAAAAAAIDmoeS5HWkNB0JLSkpqzZqi1Hjz5s21Zjv7vnIAAAAAAAAAAAAAANCSevToscPPDj/88JyeOXLkyCgtLa01f+mll3J6HgAAAAAAAAAAAND0lDy3I63hQGj37t1rzcrLy3Pau0ZVVVVUVFTUmu/s+8oBAAAAAAAAAAAAAEBL6tGjRxQU1P1rGvvvv39Oz0ySJPbdd99a85UrV+b0PAAAAAAAAAAAAKDpKXluR1rDgdDevXvXmq1YsSKqq6tz2j8iYvny5fXeSw4AAAAAAAAAAAAAAPKhqKgo+vbtW+dnPXv2zPm5da1du3Ztzs8DAAAAAAAAAAAAmpaS53akNRwIHTBgQK1ZZWVlrFixIuf9lyxZUmtWUFAQ/fr12+EaOQAAAAAAAAAAAAAAaGmDBw+uNSssLIzi4uKcn1lSUlJrVlZWlvPzAAAAAAAAAAAAgKal5LmdyfeB0AEDBkRhYWGt+dKlS3Pef9myZbVm/fv3j6Kioh2ukQMAAAAAAAAAAAAAgJY2cuTIWrOtW7fGli1bcn5mXef3S0tLc34eAAAAAAAAAAAA0LSUPLcz+T4QWlxcHPvss0+t+dy5c3Pev661++23307XyAEAAAAAAAAAAAAAQEsbNWpUnfO1a9fm/My61vbs2TPn5wEAAAAAAAAAAABNS8lzO9MaDoQedNBBtWYvvvhizvvXtbauPeQAAAAAAAAAAAAAACCfjjjiiCgoqP2rGvPmzcvpedXV1fHGG2/Umvft2zen5wEAAAAAAAAAAABNT8lzO9MaDoSOHTu21mz27Nk57V9eXh6vv/56rfkRRxyxy7VyAAAAAAAAAAAAAADQknr27BmHHHJIrfnMmTNzet7cuXOjrKys1vzwww/P6XkAAAAAAAAAAABA01Py3M60hgOhRx99dBQWFmbNlixZEi+88EKD93/88cejoqIia9azZ88YNWrULtfKAQAAAAAAAAAAAABASzv11FNrzR566KGorKxs8LPuvffeWrNBgwbFgAEDcokGAAAAAAAAAAAANAMlz+1Qvg+E9urVK4466qha87vvvrvB+99zzz21Zp/61KeiQ4cOu1wrBwAAAAAAAAAAAAAALe3Tn/509OjRI2u2cuXKmDZtWoOe889//jMeeOCBWvO6fmcAAAAAAAAAAAAAyB8lz+1QazgQ+oUvfKHW7LHHHos5c+bUe/8///nP8dJLL2XNkiSp89lyAAAAAAAAAAAAAADQGpSUlMSkSZNqza+77rp45ZVX6vWMjRs3xoUXXhhbtmzJmnfp0iXOOuusJskJAAAAAAAAAAAANA0lz+1QazgQOm7cuDjggAOyZlVVVTF16tQoKyvb5foVK1bEFVdcUWt+wgknxPDhw3e5Xg4AAAAAAAAAAAAAAPLly1/+cgwZMiRrtmXLlpg0aVI8+eSTO127aNGimDRpUsyZM6fWZ5MnT47u3bs3ZVQAAAAAAAAAAACgkZQ8t1P5PhCaJElcfPHFkSRJ1vzNN9+Ms846K1asWLHDtW+//XZMnDix1j0dO3aM7373u7vcWw4AAAAAAAAAAAAAAPKpuLg4fvGLX0THjh2z5hs3boyvf/3r8aUvfSnuvffemD9/fixfvjzefvvtmDFjRvy///f/4qSTTopXXnml1jOPPfbY+OpXv9pSXwEAAAAAAAAAAACop8J8B6B51BwI/fznPx9btmzJzGsOhI4dOzZOPvnkOOigg6Jnz55RVlYW7733Xjz11FPx4IMPZq2p0dADoaNHj45zzz03fv3rX2fNX3/99TjxxBPj9NNPj2OPPTYGDhwYW7dujffeey+mT58eDz/8cFRWVtZ63tSpU2PQoEEN+FuQAwAAAAAAAAAAAACA/Bg5cmT8/Oc/j29961u1zoQ/99xz8dxzz9X7WYceemhcc801kSRJU8cEAAAAAAAAAAAAGknJczvWGg6Efvvb345FixbFn/70p6z5pk2bYtq0aTFt2rR6PedLX/pSTJw4sUF7ywEAAAAAAAAAAAAAQD4df/zx8etf/zouuOCCWL16dU7POOWUU+LKK6+M4uLiJk4HAAAAAAAAAAAANIWCfAegedUcCO3Vq1fOzzjllFNi2rRp0a1btwav7dChQ/z0pz/NuZA4SZL45je/GVOnTs1pvRwAAAAAAAAAAAAAAOTT2LFj49FHH40zzjijQUXNo0aNiltuuSWuueYaBc8AAAAAAAAAAADQihXmOwDNr+ZA6HXXXRf33XdfVFRU1GvdqFGj4vzzz4+jjjqqUfsXFRXF5ZdfHscee2xcc801sXDhwnqtO/jgg+P73/9+fPjDH27U/nIAAAAAAAAAAAAAAJBPvXr1issuuyy++c1vxp/+9Kd49tln480334wPPvggNm/eHKWlpdGzZ8/o379/jB07No4++ug48MAD8x0bAAAAAAAAAAAAqAclz7uJ1nAgdPz48TFu3Lh49tln46mnnopXX3013n333SgrK4uIiG7dusWQIUPi0EMPjY997GNx8MEHN+n+cgAAAAAAAAAAAAAAkE+9evWKM844I84444x8RwEAAAAAAAAAAACaiJLn3Uy+D4QmSRJHHnlkHHnkkXnZXw4AAAAAAAAAAAAAAAAAAAAAAAAAAACaSkG+AwAAAAAAAAAAAAAAAAAAAAAAAAAAAAC0RUqeAQAAAAAAAAAAAAAAAAAAAAAAAAAAAHKg5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgB0qeAQAAAAAAAAAAAAAAAAAAAAAAAAAAAHKg5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgB0qeAQAAAAAAAAAAAAAAAAAAAAAAAAAAAHKg5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgB0qeAQAAAAAAAAAAAAAAAAAAAAAAAAAAAHKg5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgB0qeAQAAAAAAAAAAAAAAAAAAAAAAAAAAAHKg5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgB0qeAQAAAAAAAAAAAAAAAAAAAAAAAAAAAHKg5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgB0qeAQAAAAAAAAAAAAAAAAAAAAAAAAAAAHKg5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgB0qeAQAAAAAAAAAAAAAAAAAAAAAAAAAAAHKg5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgB0qeAQAAAAAAAAAAAAAAAAAAAAAAAAAAAHKg5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgB4X5DgAAAAAAAAAAAAAAAOzeqqqq4qWXXooXX3wxVq1aFRUVFbHnnnvGkCFDYvz48dG5c+d8RwQAAAAAAAAAAACok5JnAAAAAAAAAAAAAAAgb+6999644YYbYvny5XV+3qlTp/jsZz8b3/72t6OkpKSF0wEAAAAAAAAAAADsnJJnAAAAAAAAAAAAAACgwR588MG4+uqrs2b/8R//ERdeeGG91qdpGt///vfjoYceijRNd3hfeXl53HnnnfHUU0/FLbfcEkOGDGlMbAAAAAAAAAAAAIAmVZDvAAAAAAAAAAAAAAAAQNvzyCOPxJo1azKvdevWxWc/+9l6r7/22mvjwQcfjDRNI0mSnb7SNI0lS5bEpEmTYtmyZc34rQAAAAAAAAAAAAAaRskzAAAAAAAAAAAAAADQIJs2bYrnnnsukiSJiIgkSWLs2LGxzz771Gv9/Pnz46abbsqUOG8vTdOs65p7lixZEhdffHEj0wMAAAAAAAAAAAA0HSXPAAAAAAAAAAAAAABAg8yZMye2bt2aNTv++OPrvf7aa6/NFDlv+2eaptGvX7845JBDYuDAgVllzzVFz88880z8+c9/buxXAAAAAAAAAAAAAGgSSp4BAAAAAAAAAAAAAIAGeeWVV2rNjjvuuHqtXbx4ccyYMSNT2lxjr732ijvuuCP+8pe/xF133RVPPPFE3HfffTF8+PCssuc0TePWW29t3BcAAAAAAAAAAAAAaCJKngEAAAAAAAAAAAAAgAZZuHBh1nWvXr2if//+9Vr78MMPZ5U2R0R07Ngxbr311hg9enTW/EMf+lD89re/jb59+0ZEZIqhX3755Vi0aFGu8QEAAAAAAAAAAACaTGG+AwCw+6isrIzly5fnOwYAsAvLli1r1OcAQOvQt2/fKCoqyncMAAAAAAAA2qn3338/8z5Jkth3333rvfbJJ5/MvE/TNJIkidNPPz323nvvOu/v3bt3fOMb34hLLrkkU/IcEfHXv/41zjzzzBzSAwAAAAAAAAAAADQdJc8AtJjly5fH2Wefne8YAEAjXXTRRfmOAADUw2233RYDBw7MdwwAAAAAAADaqaVLl0aSJJmS5kGDBtVr3YYNG2Lu3LlZZc0RERMnTtzpupNPPjmuvPLK2LJlS2Y2d+7chgcHAAAAAAAAAAAAaGIF+Q4AAAAAAAAAAAAAAAC0LWVlZVnXpaWl9Vo3c+bMqK6uzpoNGTIkhg0bttN1nTp1ioMOOihTKp2mabzxxhsNCw0AAAAAAAAAAADQDJQ8AwAAAAAAAAAAAAAADbJ58+as6y5dutRr3ezZszPvawqbx48fX6+1Q4cOzbpetWpVvdYBAAAAAAAAAAAANCclzwAAAAAAAAAAAAAAQINUVVVlXVdUVNRr3YsvvlhrNmbMmHqt7dmzZ9b1xo0b67UOAAAAAAAAAAAAoDkpeQYAAAAAAAAAAAAAABqkS5cuWdf1KVzetGlTvPbaa5EkSWaWJEmMHj26XnsWFRVlXZeXl9drHQAAAAAAAAAAAEBzUvIMAAAAAAAAAAAAAAA0SGlpadb1e++9t8s1M2fOjK1bt0ZERJqmERExYsSI6Nq1a7323L5IevvSZwAAAAAAAAAAAIB8UPIMAAAAAAAAAAAAAAA0yIABAyJN00iSJNI0jddff32Xa5544oms6yRJ4vDDD6/3nuvWrcu67tKlS73XAgAAAAAAAAAAADSXwnwHAIAaFw3YGn0K03zHAAC2sTX916sw+dcLAGg9Vm5N4polfsQLAAAAAABAfhxwwAHxwgsvZK7XrFkTM2fOjDFjxtR5f3l5efzpT3+KJMk+hDJ27Nh677ls2bKs6549ezYgMQAAAAAAAAAAAEDz0AACQKvRpzCNfsX5TgEAAABthf9REgAAAAAAAPlz8MEH15pdd911cfvtt9cqco6IuO2222Ljxo1Zn3Xo0CE+8pGP1HvPuXPnRpIkkaZpJEkSgwYNyi08AAAAAAAAAAAAQBMqyHcAAAAAAAAAAAAAAACgbTnuuOOipKQkIiJTvDx79uy46KKLoqysLOveJ554Iq6//vpMwXNNSfO4ceOia9eu9drvvffei3Xr1mXNBg8e3ATfBAAAAAAAAAAAAKBxCvMdAAAAAAAAAAAAAAAAaFs6deoUJ598ctxzzz2RJEmm6PmRRx6J//u//4vRo0dHt27dYuHChTF//vxMsfO2JkyYUO/9nnnmmVqz/fffv9HfAwAAAAAAAAAAAKCxlDwDAAAAAAAAAAAAAAANdv7558ef/vSnWL9+fUREpuh506ZN8be//S0iItI0zXxWc50kSRx66KExbty4eu/12GOP1Zodeuihjf0KAAAAAAAAAAAAAI2m5HkblZWVMXfu3Fi5cmVUVFTEnnvuGUOGDInevXvnOxoAAAAAAAAAAAAAALQqvXr1iiuuuCK+/e1vR3V1dURklznXqJnVKCkpiSuuuKLe+6xYsSJmzZqV9ZzevXvHoEGDGhMfAAAAAAAAAAAAoEkoeY6IhQsXxvXXXx8zZsyIzZs3Z32WJEkceuihcfbZZ8eJJ56Yp4QAAAAAAAAAAAAAAND6fPzjH4+rr746Lr744tiyZUumiHn7YueIfxU/l5aWxrXXXhvDhg2r9x533XVXVFdXR5IkkaZpJEkSRx55ZJN9BwAAAAAAAAAAAIDGaNMlz7NmzYrbb789azZ69Og466yz6v2M++67Ly677LLYunVrpGla6/M0TWPWrFkxe/bsOOGEE+Lqq6+OkpKSRmcHAAAAAAAAAAAAAID24OSTT46DDz44fvzjH8eMGTOioqKi1j0FBQXxiU98Is4///wYPHhwvZ+9efPmuPPOOzMFzzXGjx/fFNEBAAAAAAAAAAAAGq1Nlzzfc8898ac//SlzWDNJkpg4cWK91//5z3+Oiy++OHOdJEmd96VpGmmaxpNPPhn/9V//Ff/7v/8bxcXFjc4PAAAAAAAAAAAAAADtwaBBg+K6666LsrKymDVrVixdujTWrVsXJSUlMWDAgDj88MOjW7duDX7urFmzYsSIEVmzJEnimGOOaaroAAAAAAAAAAAAAI3SZkueq6qq4qmnnsoUPEdEjBgxIo444oh6rV+1alVMnTo1InZc7lyj5vM0TeP555+Pq6++Oi699NJGpAcAAAAAAAAAAAAAgPanS5cuMW7cuCZ73tFHHx1HH310kz0PAAAAAAAAAAAAoKkV5DtArubPnx8bN27MXCdJEieeeGK91//qV7+KjRs3ZhU41/y5/WvbPdI0jbvuuivmzZvXRN8EAAAAAAAAAAAAAAAAAAAAAAAAAAAAaIvabMnzyy+/XGt2wgkn1Gvtxo0b45577skUPNcoLi6Ob37zm/HEE0/EnDlzYsaMGXHRRRdFp06dsu6rrq6OG2+8MefsAAAAAAAAAAAAAAAAAAAAAAAAAAAAQNtXmO8AuXrjjTeyrjt37hz7779/vdY+/vjjsWXLlkzJc5qmkSRJXHfddTFu3LjMff369YtJkybF6NGj46yzzoqKiopIkiTSNI0ZM2bE+vXro1u3bk33pQAAAAAAAAAAAAAAAAAAAAAAAAAAAIA2o82WPC9evDjzPkmS2Hfffeu9dvr06Zn3NQXPJ5xwQlbB87ZGjRoVkyZNihtvvDFTDF1ZWRlPPfVUnHrqqbl9AQAAAAAAAAAAAAAAaKMeeuihuO+++zLXSZLEtGnT2s1+AAAAAAAAAAAAAPXVZkue33///UiSJFPSPHTo0Hqtq6ysjNmzZ2fKmmt88Ytf3Om6L37xi3HTTTdFmqaZ2euvv67kGQAAAAAAAAAAAACA3c6SJUti5syZWef629N+AAAAAAAAAAAAAPVVkO8AuVq/fn3Wdbdu3eq17uWXX47NmzdnzXr37h0f+chHdrqud+/esf/++2cdBp0/f34DEgMAAAAAAAAAAAAAAAAAAAAAAAAAAADtSZsted6+qLlr1671WvfCCy9k3tcUNh9zzDH1WrvvvvtmrV26dGm91gEAAAAAAAAAAAAAAAAAAAAAAAAAAADtT5stea6oqMi6rq6urte6l156qdbsIx/5SL3W9unTJ+t648aN9VoHAAAAAAAAAAAAAAAAAAAAAAAAAAAAtD9ttuS5c+fOWdebNm3a5ZqqqqqYPXt2JEmSNR8zZky99uzUqVPWtZJnAAAAAAAAAAAAAAAAAAAAAAAAAAAA2H212ZLn0tLSrOslS5bscs2rr75aqwy6f//+0b9//3rtWV5ennXdoUOHeq0DAAAAAAAAAAAAAAByt3nz5qzrTp065SkJAAAAAAAAAAAAQLY2W/Lcp0+fSNM0kiSJNE1j/vz5u1zz1FNPZd7XrD388MPrvef69euzrktKSuofGAAAAAAAAAAAAAAAyMmKFSuyrrt06ZKnJAAAAAAAAAAAAADZ2mzJ88iRI7OuFy1aFG+//fYO70/TNB599NFIkiRr/pGPfKTee65cuTLrulu3bvVeCwAAAAAAAAAAAAAA5ObVV1/N+n2Anj175jENAAAAAAAAAAAAwL+12ZLnD33oQ7Vmv/71r3d4//Tp02PJkiW15kcccUS995w/f34kSRJpmkaSJDFw4MB6rwUAAAAAAAAAAAAAABrur3/9ayxYsCAiInOef999981zKgAAAAAAAAAAAIB/abMlz8cff3x06NAhIiJTvPzAAw/EXXfdVeve999/P370ox9FkiQR8e9DnaNGjYr+/fvXa79Vq1bF0qVLs2Z77713I78FAAAAAAAAAAAAAACwrS1btsTy5cvjueeei6uvvjq+8Y1vZH4foMYBBxyQp3QAAAAAAAAAAAAA2QrzHSBXe+yxRxx11FHx17/+NZIkyRQ9X3755fHoo4/GuHHjonv37rFgwYK4//77Y8OGDbUOdX72s5+t936zZs2qNRs+fHijvwcAAAAAAAAAAAAAALQm119/fdxwww31ujdN08yfI0eObJY8aZpm/T5AkiTx8Y9/vFn2AgAAAAAAAAAAAGioNlvyHBFxwQUXxD/+8Y+orq6OiMgUPc+aNSurlHn7A50REQMGDIjTTjut3ntNnz691uzDH/5wjskBAAAAAAAAAAAAAKD1qilvbu419VHz+wA1vxtw9NFHx8CBA5tlLwAAAAAAAAAAAICGKsh3gMYYOXJkfOUrX8k6CFpT9Lzta9uC5zRNo6CgIC655JIoKiqq1z7l5eUxY8aMrOd07tw59t9//6b7MgAAAAAAAAAAAAAA0IokSbLTV0Pvz/W1rW7dusV///d/t9RfAQAAAAAAAAAAAMAutemS54iICy64ICZOnFir6LmuA50195x//vlx7LHH1nuPRx55JMrLyzPPSJIkRo8eHQUFbf6vDwAAAAAAAAAAAAAAWrU0TSNN09hvv/3i97//ffTt2zffkQAAAAAAAAAAAAAyCvMdoClcdtllcdhhh8U111wTK1as2OF9/fv3j4suuig++clPNuj5t9xyS6YsuubP8ePH55wXAAAAAAAAAAAAAABauzRNm/X+XUmSJPbaa684+OCD4zOf+Ux89KMfjYKCgibdAwAAAAAAAAAAAKCx2kXJc0TEpz/96fjkJz8Zzz//fPzjH/+IZcuWxbp166KkpCQGDBgQY8aMiWOOOSaKiooa9Nwnn3wy/vnPf9aaH3fccU0VHQAAAAAAAAAAAAAAWo3TTjstxowZs9N7HnroobjvvvsiSZJI0zSSJIlp06Y1eu+CgoIoKSmJLl26RO/evaO0tLTRzwQAAAAAAAAAAABoTu2m5DkiorCwMI466qg46qijmuyZJ5xwQsyfP7/JngcAAAAAAAAAAAAAAK3ZXnvtFXvttddO75k9e3at2a6KoQEAAAAAAAAAAADao4J8BwAAAAAAAAAAAAAAAAAAAAAAAAAAAABoi5Q8AwAAAAAAAAAAAAAAOUnTNN8RAAAAAAAAAAAAAPKqMN8BAAAAAAAAAAAAAACAtuWss86Kz3zmM/mOAQAAAAAAAAAAAJB3bbbkedasWfHcc89lzaZMmdJu9gMAAAAAAAAAAAAAgNaqtLQ0SktL8x0DAAAAAAAAAAAAIO/abMnzCy+8ENdff30kSZKZNWfpckvvBwAAAAAAAAAAAAAAAAAAAAAAAAAAALRuBfkO0BTSNG3X+wEAAAAAAAAAAAAAAAAAAAAAAAAAAACtT7soeQYAAAAAAAAAAAAAAAAAAAAAAAAAAABoaUqeAQAAAAAAAAAAAAAAAAAAAAAAAAAAAHJQmO8AAAAAAAAAAAAAAABA+7Rx48ZYt25dbNiwIcrKyqK6urrJnn344Yc32bMAAAAAAAAAAAAAcqXkuZ4qKyuzrouLi/OUBAAAAAAAAAAAAAAAWqcVK1bEH//4x5g1a1bMmzcvli9f3iz7JEkSc+fObZZnAwAAAAAAAAAAADSEkud6Wrt2bdZ1ly5d8hMEAAAAAAAAAAAAAABamUWLFsVVV10VTz/9dFRVVUVERJqmeU4FAAAAAAAAAAAA0PyUPNfTG2+8kXXdtWvXPCUBAAAAAAAAAAAAAIDW484774yf/OQnsXnz5qxi5yRJmmU/5dEAAAAAAAAAAABAa6LkuR7eeeedeOmllyJJkkjTNJIkiaFDh+Y7FgAAAAAAAAAAAAAA5NW0adPiqquuyhQvN1exMwAAAAAAAAAAAEBrpeR5JyoqKuIf//hHXHXVVVFdXZ112HTkyJF5TAYAAAAAAAAAAAAAAPn1/PPPx1VXXRURdZc71xQ/AwAAAAAAAAAAALRnrbLk+YEHHogHHnhgp/csWbKk1uzss89u9N5pmsaWLVti3bp1sXjx4qiqqoo0TWsdOB03blyj9wIAAAAAAAAAAAAAgLYoTdO4+uqr6zxvn6ZpDB06NE466aQ46KCDYtiwYdG9e/coLS2tswwaAAAAAAAAAAAAoC1rlSXPixcvjpkzZ9br8Gaappk/X3jhhSbZv+aZdUmSJIYPHx6HHHJIk+wFAAAAAAAAAAAAAABtzfPPPx9z587NOvefpmn06dMnLr744vjkJz+Zx3QAAAAAAAAAAAAALadVljxva2eFy425d1fqKphO0zQKCwvjsssua7J9AAAAAAAAAAAAAACgrXn66aezrmsKnu+4444YMmRIfkIBAAAAAAAAAAAA5EGrL3muq2y5xvalzju7t7HSNI2SkpK44oor4rDDDmu2fQAAAAAAAAAAAAAAoLWbPXt25n2appEkSUydOlXBMwAAAAAAAAAAALDbadUlz9uXODf1/fXVp0+f+MxnPhNnnnlm7LXXXs2yBwAAAAAAAAAAAAAAtBUrV66MJEky13vssUd86lOfymMiAAAAAAAAAAAAgPxolSXPY8aMiSlTpuz0nhdeeCGef/75SJIk0jSNJEni61//eqP3TpIkunTpEl26dIk+ffrE/vvvH/3792/0cwEAAAAAAAAAAAAAoL1YvXp1RETmPP/YsWPznAgAAAAAAAAAAAAgP1ptyfOYMWN2es+NN94Yzz//fNZsV8XQAAAAAAAAAAAAAABA46VpmnXdt2/fPCUBAAAAAAAAAAAAyK+CfAcAAAAAAAAAAAAAAADalu7du2ddd+jQIU9JAAAAAAAAAAAAAPKrzZc8p2ma7wgAAAAAAAAAAAAAALBb2WeffbLO869evTqPaQAAAAAAAAAAAADypzDfAXJ13HHHRb9+/fIdAwAAAAAAAAAAAAAAdjuHHXZYPPfcc5EkSUREvPXWW3lOBAAAAAAAAAAAAJAfbbbkeb/99ov99tsv3zEAAAAAAAAAAAAAAGC384lPfCJuuOGGiIhI0zTmzJkT69ati+7du+c5GQAAAAAAAAAAAEDLKsh3AAAAAAAAAAAAAAAAoG0ZMWJEHHXUUZGmaUREVFVVxW233ZbnVAAAAAAAAAAAAAAtT8kzAAAAAAAAAAAAAADQYFOnTo2ioqJIkiTSNI1bbrkl3n333XzHAgAAAAAAAAAAAGhRSp4BAAAAAAAAAAAAAIAGGzZsWFx88cWRpmkkSRLl5eXxla98JVasWJHvaAAAAAAAAAAAAAAtRskzAAAAAAAAAAAAAACQk4kTJ8a3vvWtTNHzokWL4tRTT40ZM2bkOxoAAAAAAAAAAABAi1DyDAAAAAAAAAAAAAAA5Oy8886Lq666Kjp16hRJksSqVavivPPOiwkTJsQDDzwQS5YsyXdEAAAAAAAAAAAAgGZTmO8AzWHOnDkxe/bsmDt3bixYsCDWrl0bGzdujLKysqiurm6SPZIkiblz5zbJswAAAAAAAAAAAAAAIJ/OPvvsRj+jV69esXjx4kiSJNI0jVdffTXmzJkTERE9e/aM3r17R7du3aJDhw6N3itJkpg2bVqjnwMAAAAAAAAAAADQWO2m5LmqqiruuOOOuPfee2PhwoWZeZqmeUwFAAAAAAAAAAAAAACt38yZMyNJkiZ7Xk3Rc82Z/tWrV8fq1aubZI80TZs0KwAAAAAAAAAAAEBjtIuS5zlz5sTUqVNjwYIFdZY6N/XhTcXRAAAAAAAAAAAAAAC0R01xXr7mGduf5d+29DlXyp0BAAAAAAAAAACA1qbNlzzPmjUrvvrVr8bmzZt3eBAUAAAAAAAAAAAAAADYteY8j++sPwAAAAAAAAAAANAetemS50WLFsXkyZOjvLw8kiSpdeCzpvQZAAAAAAAAAAAAAADYOWfwAQAAAAAAAAAAABquTZc8//jHP46NGzfWWe5cUFAQo0ePjlGjRsWwYcOiW7duUVpaGgUFBXlKCwAAAAAAAAAAAAAArdOUKVPyHQEAAAAAAAAAAACgTWqzJc9vvvlm/PnPf84qeK4pd544cWKcd955seeee+YxIQAAAAAAAAAAAAAAtA1KngEAAAAAAAAAAABy02ZLnmfMmJF1naZpFBUVxc9//vM44YQT8hMKAAAAAAAAAAAAAAAAAAAAAAAAAAAA2G0U5DtArv7xj39k3qdpGkmSxHnnnafgGQAAAAAAAAAAAAAAAAAAAAAAAAAAAGgRbbbkeenSpZEkSea6pKQkvvKVr+QxEQAAAAAAAAAAAAAAAAAAAAAAAAAAALA7abMlz6tXr46IiDRNI0mSGDt2bBQXF+c5FQAAAAAAAAAAAAAAAAAAAAAAAAAAALC7aLMlz5s3b866HjRoUJ6SAAAAAAAAAAAAAAAAAAAAAAAAAAAAALujNlvyXFpamnXdpUuXPCUBAAAAAAAAAAAAAAAAAAAAAAAAAAAAdkdttuR58ODBkaZp5nrNmjV5TAMAAAAAAAAAAAAAAAAAAAAAAAAAAADsbtpsyfNBBx0UERFJkkRExOLFi/MZBwAAAAAAAAAAAAAAAAAAAAAAAAAAANjNtNmS5+OPPz7zPk3TmDlzZlRUVOQxEQAAAAAAAAAAAAAAAAAAAAAAAAAAALA7Kcx3gFwdccQRMWzYsHj77bcjImLz5s3x2GOPxamnnprfYAAAAAAAAAAAAAAA0M59/etfj6eeeqpF9ywqKoqOHTtGz549Y4899oihQ4fGyJEjY/To0bHffvu1aBYAAAAAAAAAAACAGm225Dki4jvf+U6cd955kSRJpGka1113XXz84x+Pzp075zsaAAAAAAAAAAAAAAC0a2matuh+FRUVUVFRERs2bIhFixbFiy++mPlsr732ilNPPTXOPPPM6NmzZ4vmAgAAAAAAAAAAAHZvBfkO0BjHHntsnH766ZGmaSRJEkuXLo1vfvObUVVVle9oAAAAAAAAAAAAAADQ7iVJkpdXmqZZr/fffz9uuOGGOP744+OWW25p8QJqAAAAAAAAAAAAYPfVpkueIyJ+8IMfxDHHHJM5gPn3v/89Jk2aFMuXL89zMgAAAAAAAAAAAAAA2L1sX77c2PU7esaOip83bdoUP/7xj+MrX/lKlJeXN/brAAAAAAAAAAAAAOxSmy95LiwsjF/+8pdx2mmnZQ5vzpw5M04++eT46U9/GosXL85zQgAAAAAAAAAAAAAAaJ92VMhcU7xc1z07em27rmbtrtZva9uy52eeeSYuuOCCnIqmAQAAAAAAAAAAABqiMN8Balx//fWNWr/XXnvF8OHDY8GCBZEkSWzYsCF+/etfx69//esYMGBAHHTQQdGnT5/o2rVrdOjQoUkyT5kypUmeAwAAAAAAAAAAAAAAbcn5558fX/rSlzLXzz77bNx8881RVVWVmaVpGoWFhTFq1KjYb7/9Yvjw4dGjR48oLS2N4uLiKCsri40bN8bSpUtj/vz5MWfOnFiyZElEZJdEjxgxIs4///woLS2N8vLyWLduXaxcuTJeffXVmD17dqxYsSKzpubPNE3j6aefjquuuiq+//3vt9RfCwAAAAAAAAAAALAbalUlzzUHKptCzaHMiIjFixdnDno2JSXPAAAAAAAAAAAAAADsjkaMGJF5/5Of/CR+85vfZK7TNI2hQ4fGOeecE5/4xCeie/fu9X7uK6+8Evfdd1/cf//9sXXr1kiSJN5666344Q9/GL/85S9jzJgxWfenaRpPPvlk3HTTTfHaa6/VKnq+4447YuLEiTF06NBGfmMAAAAAAAAAAACAuhXkO8D20jRt1KvmGRH/OpRZ82rsc+vaBwAAAAAAAAAAAAAAdmeXXnpp/OY3v8mcte/QoUNcdNFF8fDDD8fnP//5BhU8R0QcfPDBccUVV8RDDz0UhxxySOb8/uLFi+PMM8+MefPmZd2fJEl87GMfi7vvvju+/OUv1zrvX11dHTfeeGPjviQAAAAAAAAAAADATrS6kudti5kb82qu59b1bAAAAAAAAAAAAAAA2N3cdtttce+992auO3fuHDfccENMmjQpCgsLG/XsYcOGxbRp02L8+PGRpmkkSRIbN26MyZMnx/r162vd36FDh/jud78bEydOzBQ9J0kSaZrGo48+GuvWrWtUHgAAAAAAAAAAAIAdaXUlz2matvoXAAAAAAAAAAAAAADszpYvXx4/+9nPMkXKSZLEhRdeGOPGjWuyPTp27Bi/+MUvYu+9987MPvjgg/jJT36ywzWXXHJJDB48OGtWXV0dL7zwQpPlAgAAAAAAAAAAANhWYb4D1Dj88MPzHQEAAAAAAAAAAAAAAKiHW2+9NTZv3hxJkkRExMiRI+MLX/hCk+/TqVOnmDp1akyePDlTKP3AAw/ElClTYs8996x1f2FhYXzlK1+JSy+9NJMtIuL555+PE044ocnzAQAAAAAAAAAAALSakufbb7893xEAAAAAAAAAAAAAAIB6eOSRRzKly0mSxCmnnNJsex1zzDHRp0+fWLVqVUREbN26NaZPnx5nn312nfefdNJJcdlll0V1dXVmNnfu3GbLBwAAAAAAAAAAAOzeCvIdAAAAAAAAAAAAAAAAaDveeuutWLlyZdbsyCOPbLb9CgoKYsyYMZlC6YiI5557bof3l5SUxMiRIzP3p2kaa9asabZ8AAAAAAAAAAAAwO5NyTMAAAAAAAAAAAAAAFBvb731Vq3Znnvu2ax79u3bN/M+TdNYsGDBTu8fMWJE1vW6deuaJRcAAAAAAAAAAACAkmcAAAAAAAAAAAAAAKDeVq1aVWvWuXPnZt2zY8eOWdcrV67c6f09evTIulbyDAAAAAAAAAAAADQXJc8AAAAAAAAAAAAAAEC9lZeX15p98MEHzbrn9qXOW7Zs2en9Xbp0ybru0KFDk2cCAAAAAAAAAAAAiFDyDAAAAAAAAAAAAAAANEBJSUmt2Ztvvtmse77xxhtZ1506ddrp/Zs2bWrQ/QAAAAAAAAAAAAC5UvIMAAAAAAAAAAAAAADUW79+/WrNHn/88Wbbb9GiRTFnzpxIkiTSNI2IiP79++90zYYNG7Kuu3bt2mz5AAAAAAAAAAAAgN2bkmcAAAAAAAAAAAAAAKDeDjjggMz7muLl6dOnx5tvvtks+1177bVZ10mSxMiRI3e65p133sm6f1el0AAAAAAAAAAAAAC5UvIMAAAAAAAAAAAAAADU24ABA2L48OFZs8rKyvje974XGzdubNK9Hn744XjkkUciSZKs+bhx43a6bv78+ZkC6oiIgQMHNmkuAAAAAAAAAAAAgBpKngEAAAAAAAAAAAAAgAb53Oc+lylQrilgnjdvXpx11lnxwQcfNMkev/vd7+J73/terYLnHj16xIknnrjDdfPnz48NGzZkzfbbb78myQQAAAAAAAAAAACwvcJ8B8jVL37xi3jxxRdbbL8kSaKoqCg6duwYPXv2jD322COGDh0aI0eOjBEjRrRYDgAAAAAAAAAAAAAAyLczzjgjfvvb38by5csj4l9n7tM0jXnz5sUnP/nJmDx5cpx55plRUlLS4Ge//vrrcfXVV8cLL7wQaZpmSp5r3k+ePDmKi4t3uP6JJ56oNTvkkEManAMAAAAAAAAAAACgPtpsyfNbb70VM2fOzBzWzKfu3bvHscceG6eddlqMGTMm33EAAAAAAAAAAAAAAKBZdezYMX70ox/Fueeem5nVFD2XlZXFz372s/jlL38Z48ePj49+9KOx3377xYgRI+osZ/7ggw/ijTfeiFdffTWmT58eCxYsiIjIKniuef6hhx4aZ5999k6zPfroo1nrSkpK4kMf+lBjvzIAAAAAAAAAAABAndpsyfO20jTN6/5r166NBx98MB588MEYNWpUXHzxxTFq1Ki8ZgIAAAAAAAAAAAAAgOZ05JFHxtSpU+OHP/xhplS55s80TaO8vDymT58e06dPz6zp2LFjdOnSJYqLi6OsrCzKysqiuro68/m2vx+wbVFzmqYxePDg+J//+Z+s+fb+8pe/xDvvvJP1jPHjx0dRUVGjvy8AAAAAAAAAAABAXdpFyfPODmi2lJqDpK+88kpMnDgxvvGNb8R5552X51QAAAAAAAAAAAAAANB8zjrrrCgqKoorr7wyqqqqMvNty563tXnz5ti8efNOn7n97wikaRoHHHBA/OpXv4pevXrtdO1bb70Vxx9/fNZswoQJu/weAAAAAAAAAAAAALlq8yXP2x/4rFFX8fOO7q3r/p3dW9eabd9XV1fHddddF1VVVTFlypRdPgcAAAAAAAAAAAAAANqqiRMnxqhRo+KSSy6JuXPn7vCsfUOlaRpFRUXx1a9+NSZPnhzFxcW7XPOf//mfOe8HAAAAAAAAAAAAkIs2W/J80kknxciRIzPXq1atinvvvTeqqqoys5qi5g4dOsSQIUNi+PDh0b179ygtLY3i4uIoKyuLjRs3xrJly2L+/PmxZs2aiPjXIdIkSSJN00iSJEaMGBEnnnhiRERs2rQp1q9fHytXrow5c+bEqlWrMmtq1Ky94YYbYu+9947PfOYzzf73AQAAAAAAAAAAAAAA+XLAAQfEH/7wh/j9738ft99+e7zzzjsR0fCS55rfAygqKorjjz8+vvGNb8SwYcOaOi6QZysq850AAAAA2g7/jgYAAAAAaP3abMnzpz71qcz7p59+Or773e9mCp7TNI2ioqI48cQT4+STT46jjjoqiouLd/nM999/Px577LH44x//GAsWLMiUNb/11lsxaNCguOaaa6JLly5ZaxYuXBi///3v44EHHoiysrLMAdSatVdccUWMGzcuunfv3oTfHgAAAAAAAAAAAAAAWpeCgoI488wz48wzz4xZs2bFH//4x3jppZdi4cKFUV1dvcv13bp1iwMPPDCOOeaYOOWUU6Jnz54tkBrIhx8vLcp3BAAAAAAAAAAAgCbTZkuea9x///1x6aWXZg58pmka48ePj6lTp8bee+/doGcNHDgw/vM//zO++tWvxr333hs///nPY+3atZGmaTz11FPxhS98IW6//fbo1q1bZs2wYcPikksuiXPPPTcuuOCCePnllzNFzxERZWVl8dvf/ja+9a1vNc0XBgAAAAAAAAAAAACAVm706NExevToiIjYtGlTzJs3L1asWBEbNmyIDRs2REVFRZSWlkbXrl2je/fuMWTIkBgyZEh+QwMAAAAAAAAAAADkoE2XPD/77LNx6aWXRlVVVUREJEkSkydPjvPPP79Rz02SJCZMmBBjx46NSZMmxeLFiyNN03jjjTfivPPOizvuuCOryDkion///nH77bfHl770pXjxxRcjSZJIkiTSNI3bb789zjvvvCguLm5ULgAAAAAAAAAAAAAAaGtKSkrisMMOy3cMoAVt3bo13xEAAABgt+Hf4QAAAAAA+VeQ7wC5Ki8vj6lTp2YVPJ9xxhmNLnje1t577x3Tpk2LkpKSTKnziy++GLfeemud9xcVFcV1110X3bt3z5qXlZXFSy+91GS5AAAAAAAAAAAAAAAAoLWqrKzMdwQAAADYbfh3OAAAAABA/rXZkue77747li5dmilf7tu3b1x44YVNvs9ee+0VF1xwQaRpGkmSRJqm8b//+79RXl5e5/19+vSJL3zhC5GmadZ85syZTZ4NAAAAAAAAAAAAAAAAWpuioqJ8RwAAAIDdhn+HAwAAAADkX5stef7DH/6QKV1OkiROOeWU6Ny5c7Ps9dnPfjY6deqUuV63bl08+eSTO7z/jDPOqDV76aWXmiUbAAAAAAAAAAAAAAAAtCaFhYX5jgAAAAC7Df8OBwAAAADIvzZZ8rxkyZJYsGBB1uy4445rtv06d+4cY8eOjTRNM7O//e1vO7x/zz33jMGDB0dEZIqoV65c2Wz5AAAAAAAAAAAAAAAAAAAAAAAAAAAAgJbXJv93fPPnz68122uvvZp1z4EDB0bEv0ub582bt9P7DzzwwHj33XcjSZKIiFi7dm2z5gMAAAAAAAAAAAAAAIC24ML+lbFHUb5TAAAAQNuwojLix0v9QxoAAAAAoDVrkyXPS5curTXr0aNHs+7ZrVu3rOvly5fv9P5evXplXa9fv77JMwEAAAAAAAAAAAAAAEBbs0dRRL/ifKcAAAAAAAAAAABoGgX5DpCLTZs21ZqtWrWqWfdcs2bNLjNsa/tS6K1btzZ5JgAAAAAAAAAAAAAAAAAAAAAAAAAAACB/CvMdIBdFRUW1ZosWLYq+ffs2257vvffeLjNsq7KyMuu6c+fOTZ4JAAAAAAAAAAAAAACawtlnn13nPEmSmDZtWoPWtISd5QIAAAAAAAAAAABoSW2y5LlPnz61Zk8++WSMHj26WfZbv359zJw5M5Ikycx69+69yzXbKikpaZZsAAAAAAAAAAAAAADQWNufmY+ISNO01mxXa1rCrnIBAAAAAAAAAAAAtKSCfAfIxYgRIzLvkySJNE3joYceirVr1zbLfnfeeWdUVlZGxL8Pg26boS7Lli3Luu7bt2+zZAMAAAAAAAAAAAAAgKaSpmmkaZrTmpZ4AQAAAAAAAAAAALQ2bbLkeb/99os+ffpkzdauXRtXXHFFk+/15ptvxo033hhJkmTNjzjiiJ2umzdvXqaAOkmS2GuvvZo8GwAAAAAAAAAAAAAANKUkSWqdn6/vmpZ4AQAAAAAAAAAAALQ2bbLkOSLi05/+dKRpGhGRKVN+/PHH4+KLL47q6uom2eP111+Pc845JyoqKrLmhYWFcfLJJ+9w3fLly2P58uVZs2HDhjVJJgAAAAAAAAAAAAAAaA5pmma9clnTEi8AAAAAAAAAAACA1qQw3wFydc4558Rdd90Vmzdvjoh/Fz3ff//98eabb8bFF18chxxySE7P3rJlS9x8883xm9/8JsrLyyNJkoj41+HTJEliwoQJ0atXrx2uf/LJJ2vNcs0CAAAAAAAAAAAAAADNbcqUKS2yBgAAAAAAAAAAAKC9abMlz3379o1vfetbcdVVV2UKnmv+nDNnTpxxxhlx2GGHxUknnRQf/ehHY+DAgTt93ubNm+O1116L6dOnx+OPPx6rV6/OPLOufXfm8ccfz7ru0KGDkmcAAAAAAAAAAAAAAFotJc8AAAAAAAAAAAAAuWmzJc8REeecc07MmTMnHn300UwZc03Rc5qmMXv27Jg9e3ZERJSUlMTw4cOje/fu0aVLlyguLo6ysrIoKyuLJUuWxKJFiyJN04iIzJ/bFjynaRqdO3eO6667Lrp167bDTPPnz49Zs2Zl5Rk9enSUlpY2y98BAAAAAAAAAAAAAAAAAAAAAAAAAAAAkB9tuuQ5IuLqq6+OiKhV9Bzx77LmiIiysrJ49dVX63zGtvdtu37bz7t27Ro33HBDjBo1aqd57r777ujatWvW7KSTTqrHNwEAAAAAAAAAAAAAAAAAAAAAAAAAAADakjZf8lxYWBg//elP45BDDolf/OIXUVZWVqvsucb2Zc41tr9v+zWHH354XHnllTF48OBd5vnBD34QP/jBDxrwDQAAAAAAAAAAAAAAAAAAAAAAAAAAAIC2qCDfAZrKWWedFY899liceOKJkSRJnYXOSZLU+dpemqaRpmn06dMn/vu//ztuv/32ehU8AwAAAAAAAAAAAAAAAAAAAAAAAAAAALuPwnwHaEp9+/aN6667LpYvXx733ntvPPzww/Huu+826BmdOnWKI444Ij73uc/F+PHjo0OHDs2UFgAAAAAAAAAAAAAAAAAAAAAAAAAAAGjL2lXJc42+ffvGlClTYsqUKbFmzZqYM2dOvP7667FixYrYsGFDbNiwISoqKqK0tDS6du0a3bp1i6FDh8ZBBx0U++67r2JnAAAAAAAAAAAAAAAAAAAAAAAAAAAAYJfaZcnztnr27BnHHHNMHHPMMfmOAgAAAAAAAAAAAAAAAAAAAAAAAAAAALQj7b7kGQAAAAAAAAAAAAAAaHlVVVUxd+7ceOmll2Lu3LmxevXqWL9+faxbty4qKioiImLSpElx5pln5jkpAAAAAAAAAAAAQO6UPAMAAAAAAAAAAAAAAE3m7bffjttvvz0eeuihKC8vr/V5mqYREZEkSWzYsGGnz5o/f37cfPPNWbNDDz1UMTQAAAAAAAAAAADQaih5BgAAAAAAAAAAAAAAGm3NmjXxgx/8IJ544omI+HeZ8/aSJNnhZ9sbPnx4vPjii7Fs2bLMM//+97/HhAkToqioqGmCAwAAAAAAAAAAADRCQb4DAAAAAAAAAAAAAAAAbdvf/va3+PSnPx1PPPFEpGkaaZpGkiR1vhqisLAwzj777KxS6PXr18dTTz3V1F8BAAAAAAAAAAAAICdKngEAAAAAAAAAAAAAgJw98MADMXny5Fi5cmVWuXONmtLnmldDnX766VFUVJQ1e/zxxxudGwAAAAAAAAAAAKApFOY7AAAAAAAAAAAAAAAA0DY988wzcckll0RVVVWtYueIiIEDB8bo0aNj8ODB0aNHj7j88ssbvEdpaWkcddRRMWPGjEiSJNI0jWeffbbJvgMAAAAAAAAAAABAYyh5BgAAAAAAAAAAAAAAGmz9+vVx0UUX1VnwfNRRR8V5550Xo0ePzlqTS8lzRMQnPvGJmDFjRtber732Whx44IE5PQ8AAAAAAAAAAACgqbSakufrr79+h59NmTKlwWtawo5yAQAAAAAAAAAAAABAe3fjjTfGypUrMwXPaZpGhw4d4vvf/3588YtfbNK9xo4dW2v28ssvK3kGAAAAAAAAAAAA8q5VlTzXHOzc3s5Knne0piUoeQYAAAAAAAAAAAAAYHe0adOmuOuuu7IKnpMkicsuuyxOP/30Jt+vX79+0aNHj1i3bl1mtnDhwibfBwAAAAAAAAAAAKChCvIdYHtpmma9clnTEi8AAAAAAAAAAAAAANhdPfXUU1FeXh4R/y54/vjHP94sBc81Ro4cmdkrIuLtt99utr0AAAAAAAAAAAAA6qsw3wG2V3PYMiLqXaa87ZqWoOQZAAAAAAAAAAAAAIDd2XPPPVdrdv755zfrnv369cu8T9M0li5d2qz7AQAAAAAAAAAAANRHqyt5zqVAWekyAAAAAAAAAAAAAAC0nPnz52ddDx8+PIYMGdKse3bt2jXruqysrFn3AwAAAAAAAAAAAKiPVlPyfPjhh7fIGgAAAAAAAAAAAAAAoHEWL14cSZJEmqaRJEmMGjWq2ffcvuR548aNzb4nAAAAAAAAAAAAwK60mpLn22+/vUXWAAAAAAAAAAAAAAAAjbN9wXKfPn2afc+Kioqs66qqqmbfEwAAAAAAAAAAAGBXCvIdAAAAAAAAAAAAAAAAaFvSNG3xPf8/O3ceZnVZ/4//9Z4Zhn3Yki0QEQlR2WWYQRM00z6anzQjLZfcWkz9aJuWZX3LLHP5lFqmpmmRLVpZWmpq5j7DpoAs4g7IJgMyAzhsM+/fH/7mfDgMywAzHGd4PK7rXJz7Pu/7fT/fXFw1c3mfZ2VlZda4devWezwDAAAAAAAAAAAAwJYKch2AvdM777wTs2fPjvnz58eaNWsiSZIoKiqK/fbbLw455JDo0KGDHDnIAQAAAAAAAAAAAADQEO3atYvVq1dnxqtWrWryPZcsWZI17tKlS5PvCQAAAAAAAAAAALAjSp5boEmTJsWZZ57ZqPf80pe+FF/5yld26x5pmsZDDz0Uv//972PatGlRW1u71etatWoVY8eOjTPPPDMOP/zw3dpTDgAAAAAAAAAAAACAxte9e/dYvXp1JEkSERFvvPFGk+6Xpmm88MILkSRJpGkaSZLEBz/4wSbdEwAAAAAAAAAAAKAhlDyzR7z55ptx2WWXxfTp03d47caNG+PJJ5+MJ598MsaPHx8/+tGPolu3bnI0QQ4AAAAAAAAAAAAAgF2x3377xauvvpopXZ4xY0Zs2LAhCgsLm2S/6dOnx5o1azKl0hERBx54YJPsBQAAAAAAAAAAALAz8nIdgJZv6tSpMWHChAYVGm/piSeeiJNPPjlef/11ORo5BwAAAAAAAAAAAADArho+fHjWeMOGDfHPf/6zyfa766676s2NHDmyyfYDAAAAAAAAAAAAaCglzzSpl156Kb74xS9GVVVVvc8KCwtj5MiR8YlPfCKOO+64GDJkSOTl1f8nuWTJkjjrrLNi2bJlcjRSDgAAAAAAAAAAAACA3TFu3LjM+yRJIk3TuPXWW2PDhg2NvteLL74Yjz76aCRJkpkrLCyMD3/4w42+FwAAAAAAAAAAAMDOKsh1APaM448/Pr7zne/s8vq2bdvu9Jrq6uq45JJLYs2aNVnzSZLEWWedFeeee27ss88+WZ/Nnz8/fvGLX8Tf//73rPlly5bF17/+9fjtb3+bdTBXjp3PAQAAAAAAAAAAAACwuwYOHBgHHnhgzJs3LzM3f/78uOqqq+L73/9+o+1TWVkZF198cdTW1mbKpJMkiY9+9KPRrl27RtsHAAAAAAAAAAAAYFe1+JLnNE1j3rx5MWfOnFi5cmVUVVVFZWVlbNiwISIijj322Bg/fnxuQ+4BhYWF0bVr1z265y233BJvvPFG1lx+fn786Ec/ihNPPHGra/r16xfXXHNNDBgwIP73f/8367PJkyfHn//855gwYYIcu5EDAAAAAAAAAAAAAKAxnHfeefH1r389kiTJFDDfc8890b59+7j00kt3+/6LFi2K888/PxYvXhxJkmTmkySJc845Z7fvDwAAAAAAAAAAANAYWmTJc21tbfzrX/+Kv/zlLzF9+vRYu3btNq/t16/fdkueFy5cGM8991zW3IABA+LQQw9trLgt0ooVK+LOO++sN3/uuedus9B4c1/84hfjpZdeigcffDBr/oYbbohPfOITUVhYKMcu5AAAAAAAAAAAAAAAaCwf//jHY+LEiTFz5syIiEzR85133hlz5syJb33rWzFo0KCdvu+mTZvinnvuiZtuuilWrVqVKXhO0zSSJIkTTjghDjrooEZ9FgAAAAAAAAAAAIBd1eJKnu++++64/fbbY+nSpRHx3iHObak76Lk9nTp1ip/85CdRXV2dmdt3333jX//61+6HbcF+97vfxfr167Pm9t1337jwwgsbfI8rrrginnnmmaiqqsrMLV++PP7xj3/EJz/5STl2IQcAAAAAAAAAAAAAQGP60Y9+FJ/+9Kfj3XffjYj/K3qeNGlSnHTSSVFcXBzHHntsjBgxIvbff/9t3mfdunUxc+bM+M9//hP/+te/YsmSJZnvA2x+9r9Hjx5x+eWXN+1D7aZJkybFmWee2aj3/NKXvhRf+cpXGvWeAAAAAAAAAAAAQOPIy3WAxvL222/HOeecEz/84Q8zhznTNI0kSbb6aqiioqL41Kc+lblfmqaxYMGCmDZtWhM+TfOWpmn8/e9/rzd/zjnnROvWrRt8n65du8aECRPqzd93331y7EIOAAAAAAAAAAAAAIDGNmDAgLjuuusiPz8/M1dX9FxbWxuTJk2KH/zgB3HSSSfF8OHD662/++674/DDD48RI0bE5z73ubjrrrti8eLFWd8HiHjvXHa7du3ipptuik6dOu2pxwMAAAAAAAAAAADYoRZR8jx37tw48cQTo6ysrF6xc53NS5p31uc+97mIiKz7PfDAA7sfvIWaOXNmLFq0KGuuVatWcfzxx+/0vU466aR6c1OmTImKigo5djIHAAAAAAAAAAAAAEBTOPLII+OGG26INm3aZObqzvRvfpa/trY2IiJzrj9N01i+fHlUVFRkXbe17wN06NAhbrnllhgyZMiefTgAAAAAAAAAAACAHWj2Jc9vvfVWnH322bFy5crMYc46dQc8e/XqFaWlpfFf//Vfu7THBz/4wRg2bFjm/mmaxjPPPNNYj9DibO3vZtSoUVFUVLTT9xo4cGD06dMnay5N03juuefk2MkcAAAAAAAAAAAAAABN5SMf+Uj84Q9/iAEDBmRKnCP+r+x5y+LmzW3vmjRNY+DAgfGHP/whiouLm/QZAAAAAAAAAAAAAHZFQa4D7I7a2tq45JJLYtWqVfXKnbt16xannXZanHTSSdGrV6/MZw899NAu7XXsscfGjBkzMuNFixbFwoULo2/fvrv+AC3U888/X29u9OjRu3y/4uLieOutt7Lmpk6dGv/93/8tx07kAAAAAAAAAAAAAABoSgceeGDcd9998Zvf/CbuuOOOeOeddyIiss77b6voeXN1JdHt2rWL8847L84777woLCxsmtB7yPHHHx/f+c53dnl927ZtGzENAAAAAAAAAAAA0JiadcnzH//4x5g1a1bmkGfdQc5jjjkmrrrqqujYsWOj7TVu3Li45pprsuamTJmi5Hkr5syZU2/u4IMP3uX7HXTQQfHXv/41a27u3Lly7GQOAAAAAAAAAAAAAICm1qpVqzjvvPPi9NNPjwcffDD++c9/xpQpU2LDhg0NWp8kSQwZMiSOP/74OPnkkxv1ewG5VFhYGF27ds11DAAAAAAAAAAAAKAJNNuS59ra2rjtttuyCp6TJIlPfvKTcdVVVzX6fv3794+2bdvGunXrMnOvvPJKo+/TVBYtWhS//OUv4/nnn4/58+fHypUrY926ddGpU6fo3Llz9OzZM0aMGBGHHnpojBw5MgoLC3dpn5UrV8bKlSvrze+///67nL1///715l577TU5diIHAAAAAAAAAAAAAMCe1KZNm/jkJz8Zn/zkJ2PdunUxZ86cmDdvXixevDgqKiqiuro6amtro7CwMDp16hS9evWKAw44IIYOHaoMGQAAAAAAAAAAAGhWmm3Jc3l5eSxdujSSJMkUPB900EFx5ZVXNsl+eXl5MWjQoJg+fXpm7o033miSvZrC5MmTY/LkyfXmKyoqoqKiIl599dV45plnIiKie/fuccYZZ8Spp54aRUVFO7XPW2+9VW8uSZLo3bv3rgWPiD59+tSbW7t2bbzzzjvRpUsXORqQAwAAAAAAAAAAAAAgV9q0aRMjR46MkSNH5joKAAAAAAAAAAAAQKNrtiXPTz/9dL25b3zjG5GXl9dke/bt2zemT5+eKZZesGBBk+2VS2+//XZcf/31cdddd8U111wThx9++E6t3VLnzp2jVatWu5xnn3322er8smXLtllqLEfLMWPGjEiSJNcxaCTLly/PdQQAAADYa8ydOzcqKipyHQMAAFqkHf13Lz+PAwDseWma5joCAAAAAAAAAAAAAAB7qWZb8vziiy9mjffZZ58oKSlp0j2LioqyxqtXr27S/XJtxYoVcd5558VFF10UF1xwQYPWvPPOO/XmOnfuvFs52rdvH61atYqNGzdmza9atUqOBuZozmpra3MdgUZUU1OT6wgAAACw16ipqfG7OAAANJEd/azt53EAAAAAAAAAAAAAAAAA2Hs025LnhQsXRpIkkaZpJEkSxcXFTb5nhw4dssZr165t8j13V+vWrWP06NExatSoGDhwYPTt2zc6dOgQhYWFUVVVFcuXL4/p06fHM888E1OnTq23Pk3TuPHGG6Nz585x2mmn7XC/rRVft2/ffrefo3379vVKjLdXsi0HAAAAAAAAAAAAAAAAAAAAAAAAAAAATa3ZljxXVlZmjbt3797ke+bl5WWN169f3+R77oq60uvPfOYzcdRRR0WbNm22el337t3jgAMOiNLS0jj//PNj9uzZcdVVV8W0adPqXXvVVVfFwIEDd1imvXHjxnpzrVq12rUH2cE9NmzYIEcDcwAAAAAAAAAAAAAAAAAAAAAAAAAAAND4mm3J85bluW3btm3yPbcsli4sLGzyPXdFcXFxTJw4cafXHXzwwTFx4sT4yU9+Er/5zW+yPqupqYmrr746/vKXv0SSJNu8x9ZKjQsKdv+f2dZKjbe2lxwtT15e3nb/zdG85Ofn5zoCAAAA7DXy8/P9Lg4AAE1kRz9r+3kcAGDPS9M0amtrcx0DYK9z0003RWlpaQwfPrxRzkkDAAAAAAAAAAAANEfN9hRlmzZt4t13382MV61a1eR7VlRUZI2LioqafM89LT8/Py6//PJ455134v7778/6bPbs2fHoo4/GMcccs831eXl59eY2bdq027m2VmC8tb3kaHmGDRvWYp9tb/TWW2/lOgIAAADsNQYPHhx9+vTJdQwAAGiRdvTfvfw8DgCw59XW1sYLL7yQ6xgAe51f/OIXcfPNN0ebNm1i1KhRUVJSEqWlpXHwwQfnOtr7zqJFi+KXv/xlPP/88zF//vxYuXJlrFu3Ljp16hSdO3eOnj17xogRI+LQQw+NkSNHRmFhYa4jAwAAAAAAAAAAAA3UbEueu3btmlXyvGTJkibfc8aMGZEkSUREJEkSvXr1avI9c+WKK66Ip556ql559oMPPrjdkueCgvr/pNavX7/bebZ2j1atWsnRwBwAAAAAAAAAAAAAAE2luro6nn322Xj22WcjIqKoqCjGjBkTY8aMidLS0th///1znDD3Jk+eHJMnT643X1FRERUVFfHqq6/GM888ExER3bt3jzPOOCNOPfXUKCoq2tNRAQAAAAAAAAAAgJ2Ul+sAu2rfffeNNE0jSZJI0zSmTZvWpPstWLAgli1bFhERaZpGRMQBBxzQpHvmUlFRUZx++un15p977rmoqanZ5rp27drVm2uMUuN169bVm2vbtq0cDcwBAAAAAAAAAAAAANBU6s71170qKyvj0UcfjR/+8Idx/PHHxxFHHBGXXXZZ3HfffbF06dJcx33fe/vtt+P666+Pj33sY5niZwAAAAAAAAAAAOD9qyDXAXbVIYccEs8++2xmXFVVFeXl5VFSUtIk+/3hD3+oNzds2LAm2ev9Yty4cfHzn/88a66ysjLefPPNGDBgwFbXdOrUqd5cdXX1buWoqamJDRs21Jvv3LnzNtfIAQAAAAAAAAAAAACw5yRJkjVO0zTz/u233477778/7r///oiI2HfffaO0tDRKSkpizJgx0aVLlz2atblYsWJFnHfeeXHRRRfFBRdckOs4TW7GjBn1/h3RfC1fvjzXEQAAAGCvMXfu3KioqMh1DAAAaJF29N+9/DwOALDnbX4+8f2k2ZY8jx07Nm699dasuVtvvbVJSp5XrFgRf/rTn7IOCyZJEkcccUSj7/V+cvDBB0eSJPX+8a5cuXKbJc/dunWrN7d8+fKora2NvLy8XcqxbNmyBu8lBwAAAAAAAAAAAADAnlFcXBwzZsyI9evXZ+bqzt1vr/R5/vz5sWDBgsw5/UGDBkVJSUmUlJTE6NGjo127dnvmAfaA1q1bx+jRo2PUqFExcODA6Nu3b3To0CEKCwujqqoqli9fHtOnT49nnnkmpk6dWm99mqZx4403RufOneO0007LwRPsObW1tbmOQCOqqanJdQQAAADYa9TU1PhdHAAAmsiOftb28zgAAHWabclzcXFx7LPPPlFRUZEpIi4vL48//vGPceqppzbaPrW1tfG1r30t3n333cw+SZLEqFGjokePHo22z/tRfn5+dOrUKVatWpU1v2LFim2u6d27d725jRs3xvLly3f572vx4sX15vLy8qJnz55yNDAHAAAAAAAAAAAAAEBj++1vfxsbNmyIadOmRXl5eZSVlcXs2bMzX2DdvOh5W6XPaZrG3Llz46WXXoq77ror8vPzY+jQoVFaWholJSUxfPjwaNWq1Z57qEaQJEkUFxfHZz7zmTjqqKOiTZs2W72ue/fuccABB0RpaWmcf/75MXv27Ljqqqti2rRp9a696qqrYuDAgVFcXNzU8QEAAAAAAAAAAICdlJfrALsqSZI444wzMgc76wqYf/zjH8fjjz/eKHts3LgxLr/88igvL693oPRzn/tco+zxfpeXV/+fSN3f+db07t07Cgrqd4cvWbJklzMsXbq03lyvXr22e1BXDgAAAAAAAAAAAACApldYWBilpaXxla98Je65556YNGlS3HzzzXHGGWfEAQccEGmaZl6bS5Ik84qIzDWbNm2KF154IW6++eY488wzo7i4OM4999y4/fbbY9asWds9z/5+UVxcHBMnTozjjjtumwXPW3PwwQfHxIkTt/p9hZqamrj66qubxfMDAAAAAAAAAADA3qZ++2wzcvrpp8fvf//7WLZsWUS8d8hz/fr1cdFFF8X5558fn//856N169a7dO9XXnklvvvd78b06dOzDo0mSRLDhg2Lo48+utGe4/2qtrY2Vq1aVW++a9eu21xTWFgY+++/f7z88stZ83PmzInhw4fvUo45c+bUmxs0aNB218gBAAAAAAAAAAAAALDndejQIY466qg46qijIiJixYoVUVZWFuXl5VFWVhaLFi3KXFt3Vr/uzzqbFxlXV1fHc889F88991xERBQVFUVxcXGUlJTEaaed1tSPs8fl5+fH5ZdfHu+8807cf//9WZ/Nnj07Hn300TjmmGNylK5p5eXl1fu3QPOVn5+f6wgAAACw18jPz/e7OAAANJEd/azt53EAgD0vTdOora3NdYx6mnXJc7t27eL//b//F+eff35mLkmSqKmpiV/84hfx5z//OU455ZQ45phjYsCAATu83/r166OsrCzuv//+ePjhhyNN00yxc53CwsK48sorm+R53m/mzZu31X+03bp12+66IUOG1Cs1fv755+Ozn/3sLuV4/vnnt7rHjsgBAAAAAAAAAAAAAJBb3bp1i49//OPx8Y9/PCIiFi5cmCl8njRpUqxYsSJzbUNKnysrK+PRRx+Nxx57rEWWPNe54oor4qmnnopVq1ZlzT/44IMttuR52LBhkZeXl+sYNJK33nor1xEAAABgrzF48ODo06dPrmMAAECLtKP/7uXncQCAPa+2tjZeeOGFXMeop1mXPEdEjB8/Pi644IL4+c9/nnWgM03TWLp0adx4441x4403RseOHWP//fevt/7JJ5+Ml19+ORYuXBivvPJKrF+/PiL+7xBo3T3ryp6/973vxcCBA/fQ0+XWU089VW+uffv20a9fv+2uKykpib/85S9Zc9OmTdulDNXV1TF79ux686WlpTtcKwcAAAAAAAAAAAAAwPtL3759o2/fvjFhwoSIiHj55ZejrKwsysrKYurUqbFmzZrMtQ0pfW6pioqK4vTTT4+f//znWfPPPfdc1NTURH5+fo6SAQAAAAAAAAAAAFvKy3WAxnDhhRfGueeem3VQM0mSTNlzmqZRVVUVM2bMiIj/O9CZpmlMnz49HnrooXjxxRdj3bp1mevr1m9+/de//vX45Cc/uYefLjeqq6tj4sSJ9eZLS0ujVatW2117+OGHR0FBdn/44sWLY8qUKTud46GHHooNGzZkzXXp0iWGDh26w7VyAAAAAAAAAAAAAAC8v33oQx+Kz33uc3HLLbfE5MmT409/+lNccsklMWbMmKyz63Vn/fcm48aNqzdXWVkZb7755p4PAwAAAAAAAAAAAGxTiyh5joj4xje+EVdeeWW0bt16q2XPdYXPW9r8oOfm127+edu2bePqq6+Oc889t+kf5H3i+uuvj+XLl9ebP/roo3e4tmvXrnHYYYfVm//Tn/600znuueeeenPHHXdc5Ofny7GTOQAAAAAAAAAAAAAA3s/y8vJiyJAhUVpaGiUlJTF48OC9rth5cwcffHDW9xvqrFy5MgdpAAAAAAAAAAAAgG1pMSXPERETJkyIv//973HEEUdklTfX2bzEeWuvzdWtLykpib/+9a9x4okn7sEn2XWPPPJIbNiwYbfucdttt8XEiRPrzffv3z9OOOGEBt3js5/9bL25Bx98MF588cUG53jkkUfihRdeyJpLkmSr95YDAAAAAAAAAAAAAKB5euWVV+K3v/1tnH/++VFcXBynnnpq3HjjjTFz5syIeO98/9bO/bd0+fn50alTp3rzK1asyEEaAAAAAAAAAAAAYFtaVMlzRES/fv3itttui3vuuSdOPPHEaNu2baaweWvFz3U2/zw/Pz+OOuqouPPOO+Ouu+6K/v377+Gn2HVXX311HH300XH77bfH4sWLd2rtokWL4oILLojrr79+q59feumlUVBQ0KB7jRs3Lg466KCsuZqamrj88stj7dq1O1y/fPny+MEPflBv/uijj44DDjigQRnkAAAAAAAAAAAAAAB4/1m8eHH85S9/ia997Wtx+OGHx3//93/Hj3/843jiiSdizZo1Wef7Ny933vw7AQ09297c5eXV/9rHtr4XAQAAAAAAAAAAAORGiz3VOHTo0Bg6dGhceeWVMW3atJg+fXrMmzcvFi9eHMuXL49169ZFTU1NtG7dOjp16hS9evWKAQMGxPDhw+Owww6LDh065PoRdtmyZcvi2muvjWuvvTaGDh0aY8aMiUGDBsXAgQOjS5cu0bFjxygsLIyqqqqoqKiIGTNmxNNPPx2PPfZY1NTUbPWeF154YRx11FENzpAkSXz729+O008/PesA6csvvxxnnHFG3HrrrbHPPvtsde3rr78en//852P58uVZ861bt47LLruswRnkAAAAAAAAAAAAAADIvXfeeSfKy8ujrKwsysvLY+HChZnPtiwsrit03vLzJEnioIMOipKSkigtLY1DDz206YPnWG1tbaxatarefNeuXfd8GAAAAAAAAAAAAGCbWmzJc51WrVpFSUlJlJSU5DpKTsycOTNmzpy5W/c488wz46KLLtrpdYceemice+65cfvtt2fNz549O4455piYMGFCHHnkkdGnT5/YtGlTLFiwIB5++OF44IEHYuPGjfXud/nll0ffvn3l2M0cAAAAAAAAAAAAAABNqbq6OqZMmRJlZWVRVlYWL7/8cqasuaGlzhER/fr1i9LS0igtLY0xY8ZE586dmzz7+8m8efOitra23ny3bt1ykAYAAAAAAAAAAADYlhZf8syu69KlS1x55ZXx0Y9+dJfv8dWvfjUWLlwY//rXv7Lm33333fjNb34Tv/nNbxp0n8997nNx6qmnytFIOQAAAAAAAAAAAAAAGsumTZti+vTpUV5eHmVlZTFjxoyoqamJiJ0rde7evXuUlJRkip179uzZ9OHfx5566ql6c+3bt49+/frlIA0AAAAAAAAAAACwLUqeW5gjjzwyHn300Vi2bNku36Nnz55xyimnxKmnnhpdu3bdrTz5+flx/fXXR5cuXeKPf/zjTq9PkiQuuuiiuOCCC+RoxBwAAAAAAAAAAAAAALvr17/+dZSVlcXUqVNj3bp1mfnNi5u3V+pcVFQUxcXFUVpaGiUlJTFgwICmD91MVFdXx8SJE+vNl5aWRqtWrXKQCAAAAAAAAAAAANgWJc8tzBVXXBFXXHFFzJ8/P2bOnBlz586NV199NRYvXhxLly6N1atXZ13frl276Ny5cwwcODCGDh0aI0eOjDFjxkR+fn6jZWrVqlV8//vfjyOPPDKuueaaeO211xq0btiwYfGtb30rRowYIUcT5AAAAAAAAAAAAAAA2B3XXHNNJEmSVdwckV3svPlnbdq0iVGjRkVJSUmUlpbGwQcfXK8Emvdcf/31sXz58nrzRx99dA7SAAAAAAAAAAAAANuj5LmF6tevX/Tr1y9OOOGErPna2tqorq6OmpqaaN++faOWOe/I+PHjY9y4cVFWVhaPP/54zJw5M+bPnx9r166NiIiioqLYb7/9YuTIkfHRj340hg0bJsceyAEAAAAAAAAAAAAAsDu2LHWuK3YuKCiIQw45JEpLS6O0tDSGDx8ehYWFuYq5RzzyyCMxfvz43XrO2267LSZOnFhvvn///vW+IwAAAAAAAAAAAADkXrMueZ40aVKMGDGixR/ybEx5eXnRvn37nO2fJEmMHTs2xo4dm7MMcgAAAAAAAAAAAAAANL40TaOgoCBOOOGE+NjHPhaHHnpoTs+v58LVV18dP/zhD+PMM8+M4447Lnr37t3gtYsWLYof/ehH8dhjj23180svvTQKCpr110AAAAAAAAAAAACgRWrWp/s+97nPRZs2bWLkyJFRUlISJSUlMWTIkEiSJNfRAAAAAAAAAAAAAABgr5CmaSRJEkmSxKZNm+L++++PV199NcaOHRulpaUxcuTIKCwszHXMPWbZsmVx7bXXxrXXXhtDhw6NMWPGxKBBg2LgwIHRpUuX6NixYxQWFkZVVVVUVFTEjBkz4umnn47HHnssampqtnrPCy+8MI466qg9/CQAAAAAAAAAAABAQzTrkueIiHXr1kVZWVmUlZVFRESHDh2iuLg4SktLo6SkJA444IAcJwQAAAAAAAAAAAAAgJYrSZKscU1NTcyaNStmzZoVt912WxQWFsaIESNi7NixUVJSEkOGDKm3pqWaOXNmzJw5c7fuceaZZ8ZFF13USIkAAAAAAAAAAACAxtbsS56TJIk0TTPj1atXx+OPPx6PP/54RER069YtSkpKMqXPH/zgB3MVFQAAAAAAAAAAAAAAWozWrVvH+vXrM+MkSbLKm+vO+q9fvz4mTZoUkyZNioiIjh07xujRozOlzwMGDNizwZuJLl26xJVXXhkf/ehHcx0FAAAAAAAAAAAA2I5mX/IcEVmHQCMiq/S5oqIi/vnPf8Y///nPiIjo06dPpvC5pKQkunbtukezAgAAAAAAAAAAAABASzBlypSYPn16lJWVRVlZWcyaNSs2bdqU+Xxrhc8REVVVVfH444/H448/HhERH/jAB6K0tDTGjh0bpaWl0aNHjz33EI3syCOPjEcffTSWLVu2y/fo2bNnnHLKKXHqqaf6zgMAAAAAAAAAAAA0A8265Ll169axfv36zLjuAOj2Sp8XLlwYb731Vtx7770RETFw4MAoLS2N0tLSGD16dLRv334PJAcAAAAAAAAAAAAAgOatsLAwiouLo7i4OC6++OJYu3ZtTJkyJcrLy6OsrCxefvnlzHn+7Z3zX758eTzwwAPxwAMPREREv379MqXPY8aMiaKioj33ULvpiiuuiCuuuCLmz58fM2fOjLlz58arr74aixcvjqVLl8bq1auzrm/Xrl107tw5Bg4cGEOHDo2RI0fGmDFjIj8/P0dPAAAAAAAAAAAAAOysZl3yPGXKlJg+fXqUlZVFWVlZzJo1KzZt2pT5vCGlzy+//HK88sor8dvf/jby8/PjkEMOiZKSkigtLY0RI0ZEYWHhnnkYAAAAAAAAAAAAAABoxtq3bx/jx4+P8ePHR0TEypUro7y8PPNasGBB5trtnfN/8803Y/78+fHHP/4x8vLy4sADD8yUPo8aNSpat269R55nd/Tr1y/69esXJ5xwQtZ8bW1tVFdXR01NTbRv316ZMwAAAAAAAAAAALQAzbrkubCwMIqLi6O4uDguvvjiWLt2bUyZMiXKy8ujrKwsXn755cxBz80PgG7+Pk3TzDWbNm2KGTNmxIwZM+LWW2+N1q1bx4gRI6K0tDRKSkpiyJAh9Q6SAgAAAAAAAAAAAAAA9XXt2jWOO+64OO644yIiYvHixVFWVhZlZWVRXl4eFRUVmWu3VfpcU1MTs2fPjjlz5sQdd9wRrVq1iuHDh8fYsWPjS1/60p57mEaSl5cX7du3z3UMAAAAAAAAAAAAoBE165LnLbVv3z7Gjx8f48ePj4iIlStXRnl5eea1YMGCzLV1B0C3dRA0ImLdunWZtRERHTp0iNGjR2dKnwcOHNjETwQAAAAAAAAAAAAAAC1D79694+STT46TTz45IiJee+21TOnzlClToqqqKnPt5mf90zTNnPXfsGFDTJ48OaZMmdIsS54BAAAAAAAAAACAlqdFlTxvqWvXrnHcccfFcccdFxERixcvzhwALS8vj4qKisy1DSl9Xr16dfznP/+J//znP5EkScyZM2cPPAUAAAAAAAAAAAAAALQ8AwYMiAEDBsTpp58etbW1MWvWrCgvL4/y8vJ4/vnnY926dZEkyXbP+QMAAAAAAAAAAADkWosued5S79694+STT46TTz45IiJee+21TOnzlClToqqqKnPttkqfIxwIBQAAAAAAAAAAAACAxpSXlxdDhw6NoUOHxrnnnhuTJ0+Om2++OaZMmRJJkmTO8W/tjD8AAAAAAAAAAABALu1VJc9bGjBgQAwYMCBOP/30qK2tjVmzZkV5eXmUl5fH888/H+vWrXMYFAAAAAAAAAAAAAAAmti8efOirKwsysrKYsqUKVFdXZ35rO5MPwAAAAAAAAAAAMD70V5d8ry5vLy8GDp0aAwdOjTOPffcmDx5ctx8880xZcqUTNFzmqaKngEAAAAAAAAAAAAAYDe99dZbmVLn8vLyeOeddzKfba3U2Vl+AAAAAAAAAAAA4P1KyfP/76WXXory8vIoKyuLKVOmRHV1deazugOiDoUCAAAAAAAAAAAAAMDOW7lyZZSXl0d5eXk899xzsWjRosxnW5Y6b+vs/ubXdejQoWmCAgAAAAAAAAAAAOykvbbkeeHChVFWVhZlZWUxadKkeOeddzKfbX7wU7EzAAAAAAAAAAAAAADsnHfffTemTJmSKXV+5ZVXMmf1d6XUubCwMEaMGBGlpaVRWloaQ4YMabrwAAAAAAAAAAAAADthryl5XrFiRZSXl2eKnRcvXpz5bEcHROs+z8vLi4MOOihzKBQAAAAAAAAAAAAAAIjYtGlTTJ8+PXNuf8aMGVFTUxMRDS913vza/Pz8OPjgg6O0tDRKSkpi1KhRUVhY2HQPAAAAAAAAAAAAALCLWmzJ89q1a2Py5MmZUudXX30181lDS50jIvbff/9MqXNxcXEUFRU1bXAAAAAAAAAAAAAAAGgGXnrppcyZ/alTp0Z1dXXms4YWO29+3cCBA6OkpCRzfr9Dhw5NExwAAAAAAAAAAACgEbWYkueNGzfGCy+8kDkgOmvWrKipqYmInSt17tmzZ5SWlmYOhnbv3r3pwwMAAAAAAAAAAAAAQDNz4oknZs7n70qp8wc/+MEoLS3NnOHv1q1b04UFAAAAAAAAAAAAaCLNuuR51qxZUVZWFuXl5fH888/HunXrMp9tfvBze6XOnTp1ijFjxmQOhu63335NnhsAAAAAAAAAAAAAAFqKNE0bVOrcrVu3KCkpiZKSkigtLY0+ffrsqYgAAAAAAAAAAAAATaZZlzx/6lOfyhwE3bLUefMDopt/1rZt2zj00EOjtLQ0SkpKYvDgwds8TAoAAAAAAAAAAAAAAGzfts7vd+jQIUaPHp05v/+hD30oF/EAAAAAAAAAAAAAmlSzLnmuk6bpNg+FFhQUxLBhwzKHQocPHx4FBS3isQEAAAAAAAAAAAAAIOfqzvC3bt06RowYkTm/P2TIkMjLy8txOgAAAAAAAAAAAICm1SLajusKntM0jby8vBg8eHCMGTMmSktLY/To0dG2bdscJwQAAAAAAAAAAAAAgJYlLy8vhgwZEiUlJVFSUhKjRo2KwsLCXMcCAAAAAAAAAAAA2KNaRMnz1uTl5UV+fn7k5eXlOgoAAAAAAAAAAAAAALQ4kydPjg4dOuQ6BgAAAAAAAAAAAEBOtbiS5zRNY+7cuTF37tz49a9/Ha1atYphw4ZFaWlpjB07NoYOHar4GQAAAAAAAAAAAAAAdpOCZwAAAAAAAAAAAIAWUPKcpmlERCRJEkmSZM1FRGzYsCGmTp0aU6dOjZtuuinatWsXo0ePzpQ+Dxw4MCe5AQAAAAAAAAAAAAAAAAAAAAAAAAAAgOatWZc8/+53v4vy8vIoKyuLGTNmxKZNmzKf1RU+R2SXPq9duzaefPLJePLJJyMiolu3bjFmzJhM6XPv3r333AMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAzVazLnk+9NBD49BDD40LL7wwqqurY8qUKVFWVhaTJk2KuXPnZsqdNy98jsgufa6oqIgHH3wwHnzwwYiI6Nu3b5SUlERpaWmUlJREly5d9twDAQAAAAAAAAAAAAAAAAAAAAAAAAAAAM1Gsy553lzbtm3jiCOOiCOOOCIiIlatWhWTJk2KsrKyKCsri/nz52eu3V7p84IFC2LhwoVx7733RpIkMWjQoEzp8+jRo6Nt27Z75oEAAAAAAAAAAAAAAAAAAAAAAAAAAACA97UWU/K8pc6dO8exxx4bxx57bERELFu2LFP4XF5eHsuWLctcu63S5zRNY+7cufHSSy/FXXfdFQUFBTFs2LBM6fOoUaP23AMBAAAAAAAAAAAAAAAAAAAAAAAAAAAA7ysttuR5Sz169IgTTzwxTjzxxIiIeP3116O8vDzKy8tj0qRJUVlZmbl289LnNE0zpc8bN26MadOmxbRp0+Lmm2+OOXPm7NFnAAAAAAAAAAAAAAAAAAAAAAAAAAAAAN4/9pqS5y3tv//+sf/++8dnP/vZSNM05syZE2VlZVFeXh7Tpk2L6urqiMgufK5TV/oMAAAAAAAAAAAAAAAAAAAAAAAAAAAA7L322pLnzSVJEgcffHAcfPDBcd5558XGjRtjypQp8ctf/jKmTJkSSZJEmqZbLXwGAAAAAAAAAAAAAAAAAAAAAAAAAAAA9k5Knjczd+7ceO6556KsrCymTZsW69atyxQ8R4SiZwAAAAAAAAAAAAAAAAAAAAAAAAAAACBjry55XrBgQabUedKkSVFZWZn5rK7YeXMKngEAAAAAAAAAAAAAAAAAAAAAAAAAAIA6e1XJc0VFRZSXl8dzzz0X5eXlsWTJksxnW5Y6b6vQeWvlzwAAAAAAAAAAAAAAAAAAAAAAAAAAAMDep0WXPK9ZsyamTJmSKXV+9dVXM5/tSqlzmzZtYuTIkVFSUhKlpaVNExoAAAAAAAAAAAAAAAAAAAAAAAAAAABoFlpUyfPGjRvjhRdeiLKysigrK4tZs2ZFTU1NROxaqXNBQUEccsghUVpaGiUlJTFixIgoLCxsugcAAAAAAAAAAAAAAAAAAAAAAAAAAAAAmo1mX/I8e/bsTKnz888/H+vWrct8trPFzkmSxKBBgzKlzqNHj4727ds3XXgAAAAAAAAAAAAAAAAAAAAAAAAAAACg2WrWJc9jxoyJqqqqzHhnS50jIvbdd98oKSnJvLp27do0YQEAAAAAAAAAAAAAAAAAAAAAAAAAAIAWpVmXPFdWVmaNG1LqvM8++2QKnUtLS6N3795NmhEAAAAAAAAAAAAAAAAAAAAAAAAAAABomZp1yXPE1oudNy91LioqiuLi4kyp84ABA/ZkPAAAAAAAAAAAAAAAAAAAAAAAAAAAAKCFavYlzxHZpc5t2rSJUaNGxZgxY6K0tDQOOeSQrRZBAwAAAAAAAAAAAAAAAAAAAAAAAAAAAOyOZl/ynJ+fH0OGDImSkpIoLS2N4cOHR2FhYa5jAQAAAAAAAAAAAAAAAAAAAAAAAAAAAC1csy55vvXWW+PQQw+N9u3b5zoKAAAAAAAAAAAAAAAAAAAAAAAAAAAAsJdp1iXP48aNy3UEAAAAAAAAAAAAAAAAAAAAAAAAAAAAYC+Vl+sAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAM1RQa4DAAAAAAAAAAAAAAAAe7ennnoqHn744XjhhRdixYoVsWHDhujevXvst99+8bGPfSyOPfbYaN++fa5jAgAAAAAAAAAAANSj5BkAAAAAAAAAAAAAANhpK1eujBdffDFrrkuXLjF06NAG32PZsmXxla98JV544YWIiEjTNPPZggULYuHChfH000/HT3/60/jBD34QRx55ZOOEBwAAAAAAAAAAAGgkSp4BAAAAAAAAAAAAAICdNnHixLjllluy5r71rW81uOR56dKl8elPfzqWL1+eKXdOkiTrmrr55cuXx5e//OX4zne+E6eddlojpAcAAAAAAAAAAABoHHm5DgAAAAAAAAAAAAAAADQ/Dz/8cKRpmnkVFRXFhAkTGrS2trY2Lrnkknj77bcj4r1y5y0LnjefT5Ik0jSNq666Kh577LFGfQ4AAAAAAAAAAACA3aHkGQAAAAAAAAAAAAAA2ClLliyJN954I1PMnCRJfPSjH422bds2aP39998f06dPz6xP0zTz2ebF0ZvPJ0kStbW18f3vfz/Wrl3biE8DAAAAAAAAAAAAsOsKch0AAAAAAAAAAAAAAABoXqZPn15v7qijjmrw+l/84heZgueI9wqc0zSN/v37x3/9139Fz549o6KiIh555JF46aWXsq6tqKiIX/3qV3HJJZfsziMAAAAAAAAAAAAANIq8XAcAAAAAAAAAAAAAAACalxdffDFrnJeXF2PHjm3Q2kmTJsXChQsz4zRNIyLiU5/6VPzjH/+I//mf/4lPf/rT8eUvfznuu+++OO+88zLX1JVB//Wvf83MAQAAAAAAAAAAAOSSkmcAAAAAAAAAAAAAAGCnbF7SHBGx7777Rps2bRq09oEHHqg3t++++8b3vve9yM/Pz5pPkiS+/vWvx2GHHZZV6rx8+fIoLy/fheQAAAAAAAAAAAAAjUvJMwAAAAAAAAAAAAAAsFPeeuutiIhI0zSSJIkPfehDDV779NNPR5IkWevPOuusaNWq1TbXXHTRRfXmpk2btpOpAQAAAAAAAAAAABqfkmcAAAAAAAAAAAAAAGCnLF++PFPUHBHRrVu3Bq174403YtmyZVlzBQUFccIJJ2x33fDhw6NHjx5Zcy+99FID0wIAAAAAAAAAAAA0HSXPAAAAAAAAAAAAAADATlm3bl3WuGPHjg1aN2XKlMz7NE0jSZIoLi6ODh067HDtIYccklmTpmm8/vrrOxcaAAAAAAAAAAAAoAkoeQYAAAAAAAAAAAAAAHbKliXPhYWFDVr3/PPP15sbO3Zsg9b26dMna1xVVdWgdQAAAAAAAAAAAABNSckzAAAAAAAAAAAAAACwUwoKCrLG1dXVDVo3ZcqUSJIka2706NENWtu+ffus8dq1axu0DgAAAAAAAAAAAKApKXkGAAAAAAAAAAAAAAB2SocOHbLGK1as2OGaRYsWxaJFi7Lm2rZtG4ccckiD9kzTNGu8cePGBq0DAAAAAAAAAAAAaEpKngEAAAAAAAAAAAAAgJ3SsWPHiIhIkiQiIt54440drnnyyScz79M0jSRJYsSIEZGX17CvNlRWVmaN27Vr19C4AAAAAAAAAAAAAE1GyTMAAAAAAAAAAAAAALBTDjjggEjTNCLeK2yeNWtWrF27drtr/vWvf9WbKy4ubvCeW5Y8t2/fvsFrAQAAAAAAAAAAAJqKkmcAAAAAAAAAAAAAAGCnDB48OGtcU1MTf/vb37Z5/euvvx6TJ0+OJEmy5ktKShq85+uvv555nyRJ9OjRo8FrAQAAAAAAAAAAAJqKkmcAAAAAAAAAAAAAAGCnHHbYYZn3SZJEmqbx85//PBYvXlzv2jRN48c//nGkaZo1361btxg6dGiD9tuwYUO88sormb0iIvbdd9/deAIAAAAAAAAAAACAxqHkGQAAAAAAAAAAAAAA2CnDhg2L/fbbLzNOkiTeeeed+PSnPx333XdfrFy5MjZt2hQvvfRSXHjhhfH0009HkiQR8V7pc5IkccIJJ2TmdmTOnDmxadOmrLnN9wcAAAAAAAAAAADIlYJcBwAAAAAAAAAAAAAAAJqfs88+O773ve9FkiSZ4uaKioq4/PLLd7i2VatWcfrppzd4r8cee6ze3NChQ3cqLwAAAAAAAAAAAEBTyMt1AAAAAAAAAAAAAAAAoPk55ZRTYtiwYZmC54jIFD5v+ar7vO79aaedFh/84AcbvNdDDz2UuUfdPsOHD2/U5wEAAAAAAAAAAADYFQW5DvB+8c4778STTz4ZL7zwQqxYsSI2bNgQ3bt3j/322y+OPfbY6Nu3b64jAgAAAAAAAAAAAADA+8pNN90Up59+eixYsCCr6Hl7hg4dGl/96lcbvMfzzz8fixYtyhRIR0R86EMfig4dOux6cAAAAAAAAAAAAIBG0qxLntM0jXXr1mXN5efnR2FhYYPvUVNTEzfddFP85je/qXevOtdff32MGzcuvv/970ePHj12KzMAAAAAAAAAAAAAALQU3bt3j4kTJ8bll18ezz77bERsveS5rpz5iCOOiOuvvz5atWrV4D3uvPPOzPu6e48bN253YgMAAAAAAAAAAAA0mrxcB9gdv/vd72LkyJFZr5/+9KcNXr9hw4Y477zz4tZbb43q6upI03SbryeeeCKOP/74eOGFF5rwiQAAAAAAAAAAAAAAoHnp0aNH3HHHHXHLLbfE8ccfH507d846j19UVBRHH3103HbbbXHbbbdFx44dG3zvN998M/79739HRGTuFxExfvz4pngUAAAAAAAAAAAAgJ1WkOsAu+Phhx/OHNCMiCgoKIizzjqrwet/8IMfRFlZWUREJEmy3WvTNI01a9bEF7/4xbj77rtj4MCBu5QZAAAAAAAAAAAAAABaovHjx2fKlzds2BCrVq2Kdu3aRYcOHXb5nhs3boyrrroqay5JkhgxYsTuRAUAAAAAAAAAAABoNM225Lmqqiqef/75SJIk0jSNJEli3Lhx0aNHjwatnzp1avz5z3/earnz5sXRdZ/X/VlVVRXf+ta34s9//nMjPAUAAAAAAAAAAAAAALQ8hYWF0b17992+z8CBA2PgwIGNkAgAAAAAAAAAAACgaeTlOsCumjFjRlYZc0TERz7ykQav/9nPflZvLk3TyM/Pj0MPPTSOP/74KCkpiYKCgnr7zJ49O+69995dyg0AAAAAAAAAAAAAAAAAAAAAAAAAAAC0DAW5DrCrZsyYUW9u/PjxDVr72muvxdSpUyNJkoh4r9w5SZI4+OCD44Ybbog+ffpkrn377bfjG9/4RkyaNCnr+rvvvjsmTJiw+w8CAAAAAAAAAAAAAADNzFNPPRVvvvlm1tzYsWPjgAMOyE0gAAAAAAAAAAAAgBxptiXPb7zxRta4e/fu0bVr1watvf/+++vNdezYMX71q1/Vu0f37t3jlltuiZNPPjneeOONSJIk0jSNefPmxbx582LQoEG7/hAAAAAAAAAAAAAAANAM3XjjjTF79uzMOD8/P5566qkcJgIAAAAAAAAAAADIjWZb8rxo0aLM+yRJ4sADD2zw2scffzySJImIiDRNI0mSOO2007ZZEt22bdu4+OKL4+KLL86si4goKytT8gzQiJZvzHUCAAAAaD78Hg0AAAAAAEAuLVy4MCLeO5MfETFixIhtnskHAAAAAAAAAAAAaMmabcnz0qVLI0mSTElzz549G7RuxYoV8corr2SVNUdETJgwYbvrPvKRj0SHDh1i7dq1mbk5c+bsfHAAtunaJa1yHQEAAAAAAAAAAACABtj8bH2SJNGnT58cpgEAAAAAAAAAAADInbxcB9hV1dXVWeMOHTo0aN2kSZMy79M0jYiIwYMHR+/evbe7rqCgIIYOHZoplU7TNF5++eWdTA0AAAAAAAAAAAAAAM1f27Zts8Y9evTIURIAAAAAAAAAAACA3Gq2Jc/vvvtu1rhdu3YNWvf8889njZMkiSOOOKJBa/fbb7+s8apVqxq0DoD3bNq0KdcRAAAAYK/h93AAAAAAAACa0j777JM1Xr9+fY6SAAAAAAAAAAAAAORWsy15TtM0a7xhw4YGrZs2bVq9ueLi4gat7dixY9Z49erVDVoHwHs2btyY6wgAAACw1/B7OAAAAAAAAE1p//33zzrXv3LlyhymAQAAAAAAAAAAAMidZlvy3KFDh6xxZWXlDtdUVVXFvHnzIkmSzFx+fn6MHDmyQXu2atUqa7xu3boGrQPgPVv+7ygAAADQdPweDgAAAAAAQFM69NBDIyIiSZJI0zSmT5+e20AAAAAAAAAAAAAAOdJsS547duyYNV6wYMEO1zz77LNRW1sbERFpmkZExEEHHRRt2rRp0J5VVVVZ44auA+A9BQUFuY4AAAAAew2/hwMAAAAAANCUPvKRj0SSJJnxggUL4rXXXsthIgAAAAAAAAAAAIDcaLYlz/vuu2+kaRpJkkSapjFz5syoqanZ7prHHnssa5wkSYwePbrBe1ZWVmaN27Vr1/DAAAAAAAAAAAAAAADQQvTt2zfGjx+fOdcfEXHDDTfkOBUAAAAAAAAAAADAnleQ6wC7avDgwfHss89mxmvXro0nn3wyjjrqqK1ev2rVqnjssccyh0frlJSUNHjPRYsWZY0/8IEP7ERiAHbkG702xj6tcp0CAAAAmoflGyOuXeIXaQAAAAAAAHLn0ksvjWeeeSY2bdoUaZrGo48+Gv/4xz/i4x//eK6jAQAAAAAAAAAAAOwxzbbkeeTIkVnjNE3jpz/9aRx22GHRunXretffeuutsX79+qyS5zZt2sTo0aMbtF+apjFnzpxIkiTSNI0kSaJv37679xAAZNmnVUTPwlynAAAAAAAAAAAAAKAh+vfvH9/97nfjiiuuyJy1/+Y3vxk1NTXxiU98ItfxAAAAAAAAAAAAAPaIvFwH2FVHHHFEdO7cOSIiU9z86quvxhe+8IV46623MtfV1tbGXXfdFXfddVfmurqS5qOPPjratGnToP1ef/31ePfdd7Pm+vXr1whPAgAAAAAAAAAAAAAAzdOECRPim9/8ZkS8d7Z/06ZN8c1vfjMuuOCCeO2113KcDgAAAAAAAAAAAKDpFeQ6wK4qKCiIT33qU3H77bdHkiSRJEmkaRqTJ0+OY445Jvr37x9FRUXx5ptvxqpVqzLFzpv77Gc/2+D9nnrqqXpzBx100G4/BwAAAAAAAAAAAAAANGdnnXVWDBgwIL7zne/E22+/HWmaxuOPPx6PP/54DBkyJMaNGxcHH3xwDBgwIDp27BgdO3aM/Pz8XMcGAAAAAAAAAAAAaBTNtuQ5IuLLX/5yPPDAA/H2229HRGSKntM0jddeey0zrvssIjJlz+PHj48RI0Y0eK8HH3yw3tzIkSMb4SkAAAAAAAAAAAAAAKD5GTx48FbnNz/LP3PmzHjxxRcbfe8kSWLOnDmNfl8AAAAAAAAAAACAnZWX6wC7o127dnHddddFmzZtMnNJktQrdK4b1+nWrVt8//vfb/A+CxcujBdffDHrPr169YoePXrs5hMAAAAAAAAAAAAAAEDzlKbpNl9bnu1vihcAAAAAAAAAAADA+0GzLnmOiBg9enT88pe/jK5du2Yd0txauXOaptGzZ8/41a9+Fd27d2/wHhMnTsy6R5Ikcfjhh+9+eAAAAAAAAAAAAAAAaMbqzu5veYZ/y7LnxnwBAAAAAAAAAAAAvJ80+5LniIiSkpJ4+OGH4+yzz47u3btnDoNu/urUqVOcd9558be//S0GDx7c4HtXVlbGvffeGxGRVSJ95JFHNvpzAAAAAAAAAAAAAABAc6aQGQAAAAAAAAAAANjbFOQ6QGPp2LFjXHbZZXHZZZfFq6++GkuWLInKyspo165d9O7dOwYNGrRLB0TnzZsXH/vYx+rNjx07tjFiAwAAAAAAAAAAAABAs5Wmaa4jAAAAAAAAAAAAAORUiyl53twBBxwQBxxwQKPcq7i4OIqLixvlXgAAAAAAAAAAAAAA0FL89re/zXUEAAAAAAAAAAAAgJxrkSXPAAAAAAAAAAAAAABA0youLs51BAAAAAAAAAAAAICca7Ylz0uXLo1Vq1ZlzfXo0SO6dOmSm0AAAAAAAAAAAAAAAAAAAAAAAAAAAADAXqXZljxfeumlMWXKlKy5Bx54QMkzAAAAAAAAAAAAAAAAAAAAAAAAAAAAsEc025Ln119/PdI0zYwHDRoUBxxwQA4TAQAAAAAAAAAAAAAAAAAAAAAAAAAAAHuTvFwH2FWrVq2KJEkiIiJJEgXPAAAAAAAAAAAAAAAAAAAAAAAAAAAAwB7VbEueCwsLs8Y9e/bMURIAAAAAAAAAAAAAAAAAAAAAAAAAAABgb9RsS547d+6cNS4oKMhNEAAAAAAAAAAAAAAAAAAAAAAAAAAAAGCv1Gybkfv37x+LFy/OjCsqKnKYBgAAAAAAAAAAAAAA2J7q6uqoqqqKysrKqKqqioiIoqKi6NSpUxQVFUXbnR5HMwABAABJREFUtm1znBAAAAAAAAAAAABg5zXbkudhw4bFs88+G0mSRETEq6++muNEAAAAAAAAAAAAAABAnYULF8YTTzwR06dPjxkzZsSiRYu2e33v3r1j+PDhMXz48Bg/fnz07dt3DyUFAAAAAAAAAAAA2HXNtuR5/PjxcfPNN0dERJqm8eKLL8bKlSuja9euOU4GAAAAAAAAAAAAAAB7ryeffDLuvvvueOaZZyJN04iIzJ/bs2jRoli8eHE8+OCD8eMf/zg+/OEPx2mnnRZHHHFEU0cGAAAAAAAAAAAA2GV5uQ6wq4YOHRoHHnhgZpymadx11125CwQAAAAAAAAAAAAAAHuxZcuWxTnnnBNf+tKX4umnn47a2tpI0zTSNI0kSRr0qru+trY2nnrqqfjiF78Y5557bixbtizXjwcAAAAAAAAAAACwVc225Dki4mtf+1rmsGddyfMrr7yS61gAAAAAAAAAAAAAALBX+de//hUnnHBClJWVbbXYuaG2Vvj87LPPxgknnBCPPPJIEz4BAAAAAAAAAAAAwK5p1iXPH/7wh+OUU07JHP7csGFDnH322fH666/nOhoAAAAAAAAAAAAAAOwV/vrXv8ZXv/rVqKqqyip33lxdYfOOXpvb/D5VVVVxySWXxH333bfHngsAAAAAAAAAAACgIQpyHWB3ffe7342VK1fGo48+GkmSREVFRZx88slxySWXxBlnnBF5ec26xxoAAAAAAAAAAAAAAN63Jk+eHN/97nejpqZmq8XOERHt27ePIUOGxODBg6N///7RsWPH6NixY6RpGmvWrIk1a9bE66+/HnPnzo1Zs2bFmjVrIiIy96v7s7a2Nq644oro06dPjB49eg8+JQAAAAAAAAAAAMC2NfuS5/z8/LjxxhvjZz/7Wdx+++1RW1sb1dXVcfXVV8ftt98en/jEJ2L8+PExePDgaN++fa7jAgAAAAAAAAAAAABAi/Duu+/GN7/5zdi0adNWC55Hjx4dn/nMZ+IjH/lItG7dukH33LBhQzz22GPxxz/+MSZPnpx13yRJYtOmTXHZZZfFP/7xj2jXrl2jPg8AAAAAAAAAAADArmjWJc9nnnlm1rhLly5RUVERSZJEmqaxfPnyuOOOO+KOO+6IJEmie/fu0bFjx+jYsWMUFOzeoydJEr/5zW926x4AAAAAAAAAAAAAANBc3XnnnbF48eKsIuY0TeMDH/hAXHHFFXHsscfu9D0LCwvjuOOOi+OOOy4eeeSR+OEPfxjLly/PumbJkiVx5513xgUXXLDbzwAAAAAAAAAAAACwu5p1yfPkyZOzDoNurm4+TdPMn0uXLo2lS5duc01DpWm62/cAAAAAAAAAAAAAAIDmauPGjTFx4sSss/tJksR+++0Xt99+e/Tp02e39zjmmGPikEMOiXPOOSfmz58fEe99VyBN05g4cWJ88YtfjIKCZv21CAAAAAAAAAAAAKAFyMt1gMaQpmnmtfk44r0DnJu/trx+Z18AAAAAAAAAAAAAALC3e+aZZ2LVqlVZc+3bt48777yzUQqe6/Tu3Tt+/etfR/v27bPmKysr4+mnn260fQAAAAAAAAAAAAB2VYsoed6yyHnzQueGXtvQFwAAAAAAAAAAAAAA7O3Kysoy79M0jSRJ4mtf+1r06tWr0ffq3bt3fO1rX4s0TbPmn3vuuUbfCwAAAAAAAAAAAGBnNfuS5zRN9/gLAAAAAAAAAAAAAAD2ZrNnz84at23bNk466aQm2+/EE0+Mtm3bZs3NmTOnyfYDAAAAAAAAAAAAaKiCXAfYHRdeeGGuIwAAAAAAAAAAAAAAwF7nrbfeiiRJIk3TSJIkSkpKok2bNk22X9u2bWPs2LHx73//O7PvwoULm2w/AAAAAAAAAAAAgIZS8gwAAAAAAAAAAAAAAOyUqqqqrHHv3r2bfM9evXptNwMAAAAAAAAAAABALuTlOgAAAAAAAAAAAAAAANC8bNiwIWtcVFTU5Ht27Ngxa7xx48Ym3xMAAAAAAAAAAABgR5Q8AwAAAAAAAAAAAAAAO6VNmzZZ44qKiibfc8WKFVnj1q1bN/meAAAAAAAAAAAAADui5BkAAAAAAAAAAAAAANgp3bp1i4iIJEkiIuK1115r8j3feOONrWYAAAAAAAAAAAAAyCUlzwAAAAAAAAAAAAAAwE7Zd999I03TiIhI0zReeOGFWLZsWZPt9/bbb8e0adMiSZJI0zSSJIl+/fo12X4AAAAAAAAAAAAADaXkGQAAAAAAAAAAAAAA2CnDhw/PGqdpGr/61a+abL/bb789amtrs+aGDRvWZPsBAAAAAAAAAAAANJSSZwAAAAAAAAAAAAAAYKccccQRmfdJkkSapvH73/8+ysrKGn2vSZMmxe9+97tIkiRrfty4cY2+FwAAAAAAAAAAAMDOUvIMAAAAAAAAAAAAAADslKFDh8b++++fGSdJErW1tfHlL385nnzyyUbb55lnnonzzz8/0jSNiMj8uf/++8fQoUMbbR8AAAAAAAAAAACAXVWQ6wBNbd68eTFjxoyYPn16vPLKK1FZWRmVlZWxZs2aiIjo0KFDdOrUKYqKimLgwIExYsSIGDZsWAwaNCjHyQEAAAAAAAAAAAAA4P3rC1/4Qnzzm9+MJEki4r2i5+rq6vjyl78cp556alx88cVRVFS0S/devXp13HDDDfHHP/4xNm3alNmjbp8vfOELjfIMAAAAAAAAAAAAALurRZY8V1ZWxr333ht/+MMfYvHixZn5NE23em1lZWVERMyePTv+9re/RURE796947Of/Wx86lOfik6dOu2R3AAAAAAAAAAAAAAA0FyceOKJce+998a0adOyip5ramri97//fTzwwAPxsY99LE444YQYPnx4tGrVarv327hxY8yYMSMeeOCBePjhh6OqqirSNM3cu+79qFGj4hOf+ESTPx8AAAAAAAAAAABAQ7Sokuc0TeOWW26JW2+9NdavX7/VUue6w51bW7v59YsWLYrrrrsubrrppvjSl74UX/jCFyIvL6/JsgMAAAAAAAAAAAAAQHNz3XXXxYQJE2LFihWZuSRJIk3TqKqqinvvvTfuvffeyM/PjwEDBsR+++0XHTt2jA4dOkSSJLF69epYvXp1vPnmm/H666/Hpk2bIiIy5/u3/A5At27d4rrrrttzDwgAAAAAAAAAAACwAy2m5Hn+/Plx6aWXxsyZM7d5mHN7tnZtmqaxbt26uOGGG+KJJ56Ia665Jvbdd99GywwAAAAAAAAAAAAAAM1Zr1694rbbbotzzjknKisrM/N1Z/Trzvdv2rQp5s2bFy+//PJW71N33ZbrN/+8c+fOcdttt0XPnj0b8xEAAAAAAAAAAAAAdktergM0hjlz5sQpp5ySKXhOkmSbpc3be22p7j5pmsb06dPj05/+dMydO3dPPBIAAAAAAAAAAAAAADQLBx10UNxzzz0xbNiwrZY1b37Gf3vn+be8tk6apjF8+PC455574qCDDtozDwUAAAAAAAAAAADQQAW5DrC7lixZEp///Odj1apVERFbPcwZEVFYWBgHHHBA9O/fP4qKiqJjx46RpmmsWbMmVq9eHa+//nq8+uqrsWHDhnr3qSt6XrVqVXz+85+Pe++9N3r16rVnHhAAAAAAAAAAAAAAAN7n9t133/j9738fv/rVr+KWW26J6urqeuf7txzvSJqm0bZt2zj//PPjvPPOi7y8vMaMDAAAAAAAAAAAANAomn3J89e//vVYsWLFVsudi4qK4r/+67/iv//7v2P48OGRn5+/3XvV1NTEzJkz4+9//3s89NBDUVlZmblvXdFzRUVFfOMb34jf/e53TfZMAAAAAAAAAAAAAADQ3OTl5cUXv/jF+OxnPxt/+ctf4g9/+EPMnz8/65ptFT2naZo17tevX3zmM5+Jk08+OTp27NhkmQEAAAAAAAAAAAB2V7Muef7b3/4W06ZNq3fIMy8vL0499dT4n//5n+jUqVOD75efnx8jRoyIESNGxFe/+tW48cYb4w9/+EPU1tZGxP8VPU+bNi3+/ve/xyc+8YlGfR4AAAAAAAAAAAAAAGjuOnbsGGeddVacddZZ8dprr8WMGTNi+vTp8corr0RlZWVUVVVFVVVVREQUFRVFUVFRdOrUKQYOHBjDhw+PYcOGxYABA3L8FAAAAAAAAAAAAAAN06xLnm+99dascZqm0bZt2/jpT38a48eP3617FxUVxXe+85044ogj4uKLL45169ZFxP8VPd96661KngEAAAAAAAAAAAAAYDsGDBgQAwYMiE9+8pO5jgIAAAAAAAAAAADQJPJyHWBXzZgxI954441IkiQi3it4zsvLi1/+8pe7XfC8uSOOOCJ++ctfZvap88Ybb8SMGTMabR8AAAAAAAAAAAAAAAAAAAAAAAAAAACgeWm2Jc9PP/105n2appEkSZxxxhlRUlLS6HuVlJTEGWecEWmaZs0/9dRTjb4XAAAAAAAAAAAAAAAAAAAAAAAAAAAA0Dw025Ln6dOnZ43z8vLivPPOa7L9zjvvvMjLy/7r2jIDAAAAAAAAAAAAAAAAAAAAAAAAAAAAsPcoyHWAXTV//vxIkiTSNI0kSWLkyJGxzz77NNl+++yzTxx66KExefLkzL4LFixosv0AAAAAAAAAAAAAAACgJarYlEREmusYAMBmNqXvvQqS914AwPvHe79HAwAAAADwftZsS55XrFiRNe7fv3+T79m/f/+YPHnyNjMAAAAAAAAAAAAAAAAA23fN4mb7lSYAAAAAAAAAAIB6mu2JqPXr12eNP/CBDzT5nt26ddtuBgAAAAAAAAAAAAAAoL4lS5bEyy+/HJWVlVFVVRWVlZUREdGpU6coKiqKTp06xcCBA6N37945TgoAAAAAAAAAAACwc5ptyXNhYWGsW7cuM66qqmryPbfco1WrVk2+JwAAAAAAAAAAAAAANDcbNmyIf/zjH/Gf//wnpk+fHhUVFQ1a94EPfCCGDx8eRx55ZHz84x+PwsLCJk4KAAAAAAAAAAAAsHuabclzUVFRVsnzW2+91eR7brlHp06dmnxPAAAAAAAAAAAAAABoLt555524/fbb4y9/+UtUVlZGRESapg1ev3z58njsscfisccei2uuuSY+9alPxbnnnhtdunRpqsgAAAAAAAAAAAAAuyUv1wF2Vd++fSNN00iSJNI0jUmTJsW7777bZPtVV1fHpEmTMvslSRJ9+vRpsv0AAAAAAAAAAAAAAKA5eeKJJ+KEE06IX//617Fq1apI0zRz/n5nXnXrVq1aFXfccUeccMIJ8eSTT+b68QAAAAAAAAAAAAC2qtmWPB988MFZ43Xr1sV9993XZPvdd999UV1dvd0MAAAAAAAAAAAAAACwN/rhD38Y559/flRUVNQrdt5ZWxY+V1RUxJe+9KW46qqrmiA5AAAAAAAAAAAAwO5ptiXPpaWlmfd1Bzf/93//N5YsWdLoey1ZsiT+93//t97h0rFjxzb6XgAAAAAAAAAAAAAA0Jx897vfjbvvvjur3HlzaZru1Gtzm5c9/+53v4vvfve7e/LRAAAAAAAAAAAAAHaoINcBdtVhhx0WnTt3jsrKyszc2rVr4+yzz45f//rX0bt370bZZ+nSpXHuuefGmjVrsg6adu7cOQ4//PBG2QMAAAAAAAAAAAAAAJqju+++O+65555tljvn5eVFSUlJjB49OgYPHhz9+/ePoqKi6NChQ0RErFmzJlavXh2vv/56zJ07N6ZNmxbl5eVRU1OTdb+6oud77703Bg0aFKeddtoefU5g5/To0SN++9vf5joGALADS5cujUsvvXSbn19zzTXRs2fPPZgIANgVPXr0yHUEAAAAAIC9XrMteW7VqlWcccYZcdNNN2UdBn3zzTfj05/+dFxxxRVx7LHH7tYejz76aFx55ZXx9ttvZ+6fpmkkSRKnn356FBQ0278+AAAAAAAAAAAAAADYLYsWLYrrrrtuq+XObdq0iXPPPTdOOeWU6N69+zbv0bVr1+jatWv069cvjjzyyIiIWL58efzpT3+KO+64I6qrqzP3ryt6vu6662L8+PHxwQ9+sOkeDtgtrVq1ij59+uQ6BgCwm3r27On/0wEAAAAAAAAaIC/XAXbH2WefHb169cqaS5IkKioq4pJLLonTTz89/vnPf8a6desafM/169fHgw8+GGeeeWb8z//8T7z99tv1runVq1ecffbZu50fAAAAAAAAAAAAAACaq1/84hdRXV2dGadpGmmaxujRo+Ohhx6Kiy66aLsFz9uyzz77xIUXXhgPPfRQjBkzJtI0zfp83bp1cfPNN+92fgAAAAAAAAAAAIDGUJDrALujXbt28ZOf/CTOOeecqKmpycwnSRJpmsa0adNi2rRp0aZNmzjkkEPiwAMPjP322y86duwYHTp0iCRJYvXq1bF69ep4880346WXXorZs2dnDpmmaRpJkmTum6ZpFBQUxE9+8pNo167dHn9eAAAAAAAAAAAAAAB4P6iqqooHHnggc+a+7vz90UcfHddff30UFhbu9h49e/aM22+/Pb72ta/FI488EkmSZL4v8MADD8Rll10WRUVFu70PAAAAAAAAAAAAwO5o1iXPERHFxcVx5ZVXxre//e1I0zQzX3dwMyKiuro6pk6dGlOnTt3h/ba8x+bzeXl5ceWVV8bo0aMb8QkAAAAAAAAAAAAAAKB5+fe//x0bN27MOne/7777xrXXXtsoBc91WrVqFddee23MmzcvFixYkJnfuHFj/Pvf/46TTjqp0fYCAAAAAAAAAAAA2BV5uQ7QGE466aT42c9+FkVFRfVKmuteaZo26LX5mjppmkanTp3ihhtucAAUAAAAAAAAAAAAAIC93uTJkzPv687if/vb3442bdo0+l6tW7eOb3/721nfF9gyAwAAAAAAAAAAAECutIiS54iIY445Jh544IE47LDDMoXNm9u8vHl7r83V3eewww6L+++/Pz760Y/uyUcCAAAAAAAAAAAAAID3pZdffjlr/IEPfCA+/OEPN9l+H/7wh2OfffaJiPe+H5CmacybN6/J9gMAAAAAAAAAAABoqBZT8hwR0b1797jjjjvi9ttvjyOPPDJzcHPLwuftqbs+SZI46qij4o477og77rgjevTo0YTJAQAAAAAAAAAAAACg+Vi8eHHmzH6SJFFcXBxJkjTZfnV7bP79gCVLljTZfgAAAAAAAAAAAAANVZDrAE3h8MMPj8MPPzyWLFkSTzzxRMyYMSOmT58e8+fP327hc79+/WLEiBExbNiwGDduXPTu3XsPpgYAAAAAAAAAAAAAgOZh7dq1WeMePXo0+Z5b7rFlBgAAAAAAAAAAAIBcaJElz3V69eoVn/nMZ+Izn/lMRERs3LgxqqqqMq+IiI4dO0anTp2iqKgoWrVqlcu4AAAAAAAAAAAAAADQLNTW1maN98R5/C332DIDAAAAAAAAAAAAQC606JLnLbVq1Sq6desW3bp1y3UUAAAAAAAAAAAAAABottq1axerV6/OjJctW9bke7799tv1MgAAAAAAAAAAAADkWl6uAwAAAAAAAAAAAAAAAM1Ljx49IiIiSZJI0zRmzZrV5HvOmjUrkiTJjLt3797kewIAAAAAAAAAAADsiJJnAAAAAAAAAAAAAABgp/Tv3z/SNM2MX3vttXjppZeabL958+bFK6+8EhERaZpGkiSx//77N9l+AAAAAAAAAAAAAA2l5BkAAAAAAAAAAAAAANgpo0aNqjd3zTXXNNl+W7v3yJEjm2w/AAAAAAAAAAAAgIZS8gwAAAAAAAAAAAAAAOyUj3zkI5n3SZJEmqZRVlYWt912W6Pvdfvtt8ezzz4bSZJkzR999NGNvhcAAAAAAAAAAADAzirIdYA9qbq6OqqqqqKysjKqqqoiIqKoqCg6deoURUVF0bZt2xwnBAAAAAAAAAAAAACA978+ffpEaWlplJWVRZIkmaLnn/70p7FmzZr4yle+Uq+UeVf87Gc/i1tvvTVzrzRNI0mSKC0tjT59+uz2/QEAAAAAAAAAAAB2V4sueV64cGE88cQTMX369JgxY0YsWrRou9f37t07hg8fHsOHD4/x48dH375991BSAAAAAAAAAAAAAABoXi688MIoLy/PjOuKnn/1q1/FU089Fd/61rdizJgxu3TvyZMnx9VXXx1z587NFDtvuTcAAAAAAAAAAADA+0GLLHl+8skn4+67745nnnkm0jSNiMj8uT2LFi2KxYsXx4MPPhg//vGP48Mf/nCcdtppccQRRzR1ZAAAAAAAAAAAAAAAaFZGjRoVEyZMiHvuuSdT8Fz350svvRRnnXVW9OvXLz7+8Y/H6NGjY/DgwVFUVLTVe1VVVcXcuXNj6tSp8Y9//CPefPPNiIisgue69xMmTIiRI0fuqccEAAAAAAAAAAAA2K4WVfK8bNmy+Na3vhVlZWURkV3sXHeoc0c2L4V+6qmn4qmnnoqxY/8/9u48zsq67h//+xpmBhAYFhdkEZFFQmTRsFzu0sywNPf0mxm45o6lt7tiaqWm2a09stQ0Q809t7xdcMsluQMJBVlUBEHZd5hhhGHm+v3hb04cBxBmzuEw8Hw+HufBuT7nXJ/360If3XTzmVf7xnXXXRft27fPfWgAAAAAAAAAAAAAAGikrrzyypg0aVKMHz8+c26/tug5TdP4+OOP47bbbst8v02bNtGqVato2bJlRESUl5fH8uXLY8mSJZnvrO9nAfr27RtXXnllHp8IAAAAAAAAAAAAYOMUFTpArrzwwgtx2GGHxciRIzOHQZMkybw21Jr31O7zz3/+Mw477LAYMWJEHp8AAAAAAAAAAAAAAAAal9LS0rj77rujX79+dcqZv3g2P03TWLx4ccyYMSMmTpwYEydOjBkzZsTixYuzvrO2nwVI0zT69esXd911V5SWlhbiUQEAAAAAAAAAAADWaosoeX788cfjggsuiGXLlmUd6FzTmgc+1/da05r7LFu2LH72s5/FE088scmeCwAAAAAAAAAAAAAANndlZWXx17/+NU455ZQ6Z/kjsgufv3jef32fRUTmZwROPfXUuP/++6OsrCzvzwMAAAAAAAAAAACwMYoLHaChRo0aFVdddVVUV1ev9TBnRESLFi2ib9++0bt379hll12iVatW0apVq0jTNMrLy6O8vDymTp0akyZNivfeey/Ky8sjIjL71f5aU1MTw4YNi86dO8dee+21CZ8SAAAAAAAAAAAAAAA2XyUlJXHxxRfHt7/97bjpppvinXfeiYhYa+nz+tZr1f48wIABA+Kiiy6Kr371qznNCwAAAAAAAAAAAJArjbrkecWKFXHppZfG6tWr11rwvNdee8Xxxx8f3/72t6Np06YbtOeqVavipZdeioceeihGjRqVtW+SJLF69eq45JJL4plnnoltttkmp88DAAAAAAAAAAAAAACN2Ve/+tV46KGHYtKkSXH//ffHa6+9FgsWLNioPbbbbrs44IAD4oQTTojevXvnKSkAAAAAAAAAAABAbjTqkud77rknZs2alVXEnKZpbLfddjFs2LA4+OCDN3rP0tLSOOSQQ+KQQw6JESNGxC9/+cuYP39+1ndmz54d99xzT5xzzjkNfgYAAAAAAAAAAAAAANjS9O7dO371q19FRMSnn34a7777bnzwwQexdOnSWL58eSxdujQiIsrKyqKsrCxat24dPXv2jAEDBkTnzp0LGR0AAAAAAAAAAABgozTakueqqqq47777MgXPaZpGkiTRtWvXuOuuu3JyqHPQoEGx++67xymnnBLTp0+PiIgkSSJN07jvvvvijDPOiOLiRvtbCAAAAAAAAAAAAAAAede5c+fo3LlzHHrooYWOAgAAAAAAAAAAAJBzRYUOUF9vvvlmLFmyJGutRYsWcc899+Sk4LlWx44d489//nO0aNEia33p0qXxxhtv5GwOAAAAAAAAAAAAAAAAAAAAAAAAAAAA0Lg02pLnkSNHZt6naRpJksR///d/R4cOHXI+q2PHjvHf//3fkaZp1vpbb72V81kAAAAAAAAAAAAAAAAAAAAAAAAAAABA49BoS54nTJiQdd28efM46qij8jbvyCOPjObNm2etTZw4MW/zAAAAAAAAAAAAAAAAAAAAAAAAAAAAgM1boy15/vTTTyNJkkjTNJIkib333juaNWuWt3nNmzePfffdNzMvTdP45JNP8jYPAAAAAAAAAAAAAAAAAAAAAAAAAAAA2Lw12pLnZcuWZV137Ngx7zM7dOiw3gwAAAAAAAAAAAAAAAAAAAAAAAAAAADA1qO40AHqa9WqVVnXZWVleZ/ZqlWrrOuqqqq8zwQAAAAAAAAAAAAAgMZs9uzZMW7cuJg8eXIsWbIkli1bFsuXL4+Iz8/pl5WVRZs2baJXr17Rt2/f6NSpU4ETAwAAAAAAAAAAAGy4Rlvy3KxZs1ixYkXmesGCBXmfuXDhwqzrpk2b5n0mAAAAAAAAAAAAAAA0NpMmTYpHHnkkRowYEYsWLdqoe9u2bRuDBg2KY489Nvr06ZOnhAAAAAAAAAAAAAC50WhLnrfddttYsWJFJEkSEREfffRR3mdOmzatTgYAAAAAAAAAAAAAAOBzY8aMiRtuuCHee++9iIhI03Sj91i0aFE8/PDD8fDDD8duu+0Wl112WQwcODDXUQEAAAAAAAAAAAByoqjQAeqrS5cumcOeaZrG2LFjY+7cuXmbN2/evBgzZkwkSRJpmkaSJLHzzjvnbR4AAAAAAAAAAAAAADQW5eXlcdVVV8XgwYPjvffeizRNM2fv6/OqvX/ChAkxePDgGDZsWCxfvrzQjwkAAAAAAAAAAABQR6MteR4wYEDWdZqm8ac//Slv8+66666oqanJWuvfv3/e5gEAAAAAAAAAAAAAQGPwySefxA9+8IN49NFHo6amJqvceV1qS5zXZc370zSNxx57LH7wgx/EjBkzcp4fAAAAAAAAAAAAoCEabcnzN7/5zcz7JEkiTdN44IEHYuTIkTmf9a9//Svuv//+OgdM999//5zPAgAAAAAAAAAAAACAxmLmzJkxePDg+Pjjj9da7lxb5lz72Q477BDdunWLAQMGRP/+/aNbt26xww47ZH4u4Ivlz7X7pWka06dPj8GDB8fMmTML8agAAAAAAAAAAAAAa1Vc6AD11a9fv+jWrVtMmzYtIj4/uFlTUxNnn3123HLLLTkrYH7zzTfjvPPOyxwSrf21W7du0a9fv5zMAAAAAAAAAAAAAACAxmbVqlUxdOjQmDNnTlaxc8R/zt736dMnDjvssNhjjz1i1113jebNm691r8rKyvjggw9i7Nix8fe//z0mTJgQEZHZt7boee7cuTF06NB46KGHorS0NI9PBwAAAAAAAAAAALBhGm3Jc0TE6aefHpdeemnWoc3Kyso4++yz44c//GH89Kc/jbKysnrtvXz58rj11lvjoYceitWrV2cdOE2SJE4//fScPMPWavHixTFhwoSYPn16lJeXR5IkUVZWFl27do3dd989WrZsKUcBcgAAAAAAAAAAAAAAbKi77747Jk6cuNaC56997Wtx2WWXRe/evTdor+bNm0f//v2jf//+cdJJJ8WkSZPi+uuvj1GjRmUKnmt/nTRpUtx1111x9tln5+OxAAAAAAAAAAAAADZKoy55PvLII+PRRx+NMWPGZBU9V1dXxwMPPBB///vf47vf/W4cdthhMWDAgCgpKVnvflVVVfHuu+/G3//+93j++edj2bJlmYOgEZF5/9WvfjWOOOKIvD/fpnTnnXfGzTffvNbPzj333Bg6dGiDZ6RpGs8991w88MADMWbMmKipqVnr90pKSmLfffeNIUOGxH/91381eK4cAAAAAAAAAAAAAAC5VVFREXfddVdWwXOaplFUVBRXXHFF/PjHP27Q/r17945777037rvvvrj++usz67VFz3/+85/jxBNPjBYtWjRoDgAAAAAAAAAAAEBDNeqS54iI3/zmN3HsscfGwoULM2u1hzaXLVsWjz76aDz66KPRpEmT6N69e3Tt2jVatWoVLVu2jCRJYvny5bF8+fL4+OOPY+rUqbF69eqI+Pxwae1ea9p2223jN7/5zaZ7wE1g+vTpcdttt+V1xscffxyXXHJJvPPOO1/63aqqqnjttdfitddeiwMOOCCuu+662HbbbeXIQw4AAAAAAAAAAAAAgPp4/vnno6KiInPmPk3TSJIkbrjhhjj88MNzNmfw4MHRunXruPjii7PO91dUVMSzzz4bxx57bM5mAQAAAAAAAAAAANRHoy957tChQ9x5551xyimnxNKlSzPrax4UjYhYvXp1vP/++/HBBx+sdZ/a733x/jU/b9OmTdx5552x44475vIRCipN07jyyivjs88+y9uMt99+O84666xYtmzZRt/7j3/8I4455pj485//HN26dZMjhzkAAAAAAAAAAAAAAOrr9ddfz7yvLXg+4ogjclrwXOvwww+Pf/7zn/HUU09lnfV/8803lTwDAAAAAAAAAAAABVdU6AC5sNtuu8UjjzwS/fv3X2tZc+0r4vPDo2t7re27tdI0jQEDBsQjjzwSu+2226Z5qE3k0UcfjVGjRuVt/8mTJ8cZZ5yx1kLj0tLS2HPPPeOII46IQw45JPr27RtFRXX/lZw9e3acdNJJMXfuXDlylAMAAAAAAAAAAAAAoCHef//9Omfvhw4dmrd55513XmZekiSRpmlMnjw5b/MAAAAAAAAAAAAANlRxoQPkSpcuXeKBBx6IP/3pT3H77bdHZWVlnQOjX7z+MmmaRvPmzeOss86K0047ba2Fu43ZvHnz4sYbb8xaKy0tjVWrVuVk/8rKyvjZz34W5eXlWetJksRJJ50Up556amy//fZZn02fPj1uu+22eOqpp7LW586dGxdeeGHce++9G/3PUQ4AAAAAAAAAAAAAgNyaN29eRHx+7j5Jkujfv3906tQpb/M6deoU/fv3j3feeSdzhro2AwAAAAAANDZVVVUxd+7cQscAAL7EnDlzGvQ5ALB5aN++fZSUlBQ6Blu4LabkOSKiqKgozjjjjPjRj34Uf/vb3+LBBx+M6dOnZ31nXYW4aZpmXe+8885x/PHHxzHHHBOtWrXKW+ZCuuaaa2L58uWZ63322Seqq6tj1KhROdn/9ttvj2nTpmWtNWnSJK677ro48sgj13rPzjvvHDfeeGN07949fvvb32Z9NmrUqHjsscfi2GOPlaMBOQAAAAAAAAAAAAAAGmrlypVZ1z169Mj7zJ49e8Y777yTua6qqsr7TAAAAAAAyIe5c+fGkCFDCh0DAGigiy++uNARAIANcO+990bnzp0LHYMt3BZV8lyrVatWcdJJJ8VJJ50UH330Ubz77rvxzjvvxIcffhhLly6NZcuWxbJlyyIioqysLMrKyqJ169bRs2fPGDBgQPTv3z+6d+9e4KfIr+effz5eeumlzHXTpk3j6quvjmHDhuVk/4ULF8Y999xTZ/3UU09dZ6Hxms4444yYPHlyPPvss1nrt956axxxxBFRWloqRz1yAAAAAAAAAAAAAADkwjbbbBPl5eWZ6x122CHvM7fbbrus62bNmuV9JgAAAAAAAAAAAMCX2SJLntfUvXv36N69exx99NGFjrLZWLp0afzyl7/MWjvrrLOia9euOZtx//33x8qVK7PWunTpEueee+4G7zFs2LB48803M4XcERHz58+PZ555ZoP/ecoBAAAAAAAAAAAAAJB7O+ywQ1bJ8/Lly/M+c815tRkAAAAAAAAAAAAACq2o0AHY9G644YaYP39+5rpHjx5x2mmn5Wz/NE3jqaeeqrN+yimnRNOmTTd4n3bt2sWxxx5bZ/2JJ56Qox45AAAAAAAAAAAAAABypWfPnpGmaSRJEhERs2bNyvvMNWckSRK77rpr3mcCAAAAAAAAAAAAfBklz1uZkSNHxuOPP565TpIkrrnmmigpKcnZjHHjxsXMmTOz1kpKSuLQQw/d6L2OOuqoOmujR4+OBQsWyLGROQAAAAAAAAAAAAAAcmW//fbLvE/TNP7v//4vVq5cmbd5n332Wfzf//1fJEkSaZrWyQAAAAAAAAAAAABQKEqetyKVlZUxbNiwrLXjjjsuBg4cmNM5b775Zp21r371q1FWVrbRe/Xs2TM6d+6ctZamabz11ltybGQOAAAAAAAAAAAAAIBcGTRoUDRv3jxzvWLFinj66afzNu/pp5+OioqKzHXz5s3j4IMPzts8AAAAAAAAAAAAgA2l5Hkr8rvf/S4++eSTzPV2220XF154Yc7n/Pvf/66zttdee9V7v6997Wt11t5++205NjIHAAAAAAAAAAAAAECutG7dOk444YRI0zSSJIk0TeOmm26KuXPn5nzWnDlz4qabbsrMSZIkTjjhhCgrK8v5LAAAAAAAAAAAAICNVVzoAGwa48ePj+HDh2etXX755Xk51Dpx4sQ6a3369Kn3frvttls8/vjjWWuTJk2SYyNzAAAAAAAAAAAAAADk0tChQ+Pll1+Ojz/+OJIkiWXLlsXJJ58cd999d3To0CEnM2bNmhWnnXZaLF++PJIkiYiIbt26xdChQ3OyPwAAAAAAbI6WHt4vqsuaFToGALCm6pqI6jSiSRLRpKjQaQCANTRZ9lm0fnpcoWOwldtqSp4XL14ckydPjiVLlsTy5ctj2bJlERFRVlYWrVq1ijZt2kSvXr2iXbt2BU6ae1VVVXHFFVdEdXV1Zu0b3/hGHHrooTmftWjRoli0aFGd9W7dutV7z1122aXO2kcffSTHRuQAAAAAAAAAAAAAAMi1pk2bxp133hk/+tGPYv78+ZEkSUydOjWOPfbYuOyyyxp8Zv2ZZ56JG264IRYsWBBJkkSaprHDDjvEHXfcEaWlpTl6CgAAAAAA2PxUlzWLmjbbFDoGAAAAABtoiy15Xr16dbz44osxYsSIGD9+fMycOXOD7uvQoUP069cvBg0aFN/5zneipKQkz0nz76677or3338/c928efP4+c9/npdZn376aZ21JEmiY8eO9d6zc+fOddYqKipi8eLF0bZtWzk2IAcAAAAAAAAAAAAAQD7stNNO8eijj8bZZ58dEydOjCRJYsGCBXHhhRfGPffcE8cdd1wMGjQo2rRps0H7LVmyJF544YV47LHH4r333os0TSMiIk3T6NOnT/zhD3+I9u3b5/GJAAAAAAAAAAAAADbOFlfyvGzZsrjjjjviiSeeiMWLF0dEZA51bohZs2bF7Nmz44UXXojWrVvHUUcdFWecccYGHyjd3EydOjX+8Ic/ZK2dc845sdNOO+Vl3rx58+qstWnTpkFl2dtvv/1a1+fOnbvOUmM5thzvvvtuJElS6BjkyPz58wsdAQAAALYakyZNigULFhQ6BgAAbJG+7O+9/HkcAGDT25izogCs3ZNPPtmg+48//vgYPnx4TJkyJZIkiTRN47333osJEybEtddeG127do1evXpF165do2XLltGyZcuIiCgvL4/y8vL4+OOP44MPPohp06ZFdXV11n+2J0kSPXr0iB/+8IcxcuTIzPqRRx7ZoMybkzvvvDNuvvnmtX527rnnxtChQzdxIgAAAAAAAAAAAGBDbVElz88880xcf/31sWjRojoHOjdG7b1LliyJv/zlL/HUU0/FZZddFocddlhO8+ZbmqZx5ZVXxqpVqzJrvXr1ipNPPjlvM2uLtdfU0ILsFi1aRElJSVRVVWWtL1myRI4NzNGY1dTUFDoCOVRdXV3oCAAAALDVqK6u9t/FAQAgT77sz9r+PA4AAEBjdOmll2702fv1qS16TtM0Vq9eHVOmTImPPvpovfd8sbS/Nk+apjFlypS46qqrsj7fUkqep0+fHrfddluhYwAAAAAAAAAAAAD1VFToALmwatWq+NnPfhYXXXRRLFy4MNI0jSRJMq+Ntea9aZrGokWL4uKLL47zzjsvVq5cmYcnyI8HHnggxowZk7kuKiqKX/ziF1FcnL9u7+XLl9dZa9GiRYP3Xdsea5slBwAAAAAAAAAAAABA/dUWM9f3teYeXzzXvyH3fvE8/xf3XHNtS5CmaVx55ZXx2WefFToKAAAAAAAAAAAAUE+NvuR51apV8ZOf/CReeOGFrEOga6rPodJaax4OffHFF+O0006LVatWbcpHrJc5c+bEzTffnLV2/PHHR//+/fM6t6qqqs5aSUlJg/dd2x7r++cgBwAAAAAAAAAAAADAxluzZLmhr/rsuyH3bEkeffTRGDVqVKFjAAAAAAAAAAAAAA1QXOgADXX55ZfHv/71r3WWO5eUlMR//dd/xR577BFf+cpXomvXrtGyZcto2bJlRESUl5dHeXl5fPzxxzF58uQYO3ZsvPnmm1FVVZW1X23R89tvvx2XXXZZnQLlzc3VV18dFRUVmesddtghLrjggrzPXVupcXFxw/81W1up8dpmybHlKSoq2uIOYm/NmjRpUugIAAAAsNVo0qSJ/y4OAAB58mV/1vbncQCATS9N06ipqSl0DADYKPPmzYsbb7wxa620tDRWrVpVoEQAAAAAAAAAAABAfTTqkufnnnsunnnmmbWWO7du3TrOOuusOProo6OsrGyde7Rr1y7atWsXXbp0iW9+85sREbFs2bJ4/PHH449//GMsXbo0U/Bc++uzzz4b3/72t+OQQw7J6/PV1zPPPBOvvvpq1tqwYcMyxdb5VFRUVGdt9erVDd53bQXGa5slx5anf//+W+yzbY0+/fTTQkcAAACArUbv3r2jc+fOhY4BAABbpC/7ey9/HgcA2PRqampi7NixhY4B0Kh17Nix0BG2Otdcc00sX748c73PPvtEdXV1jBo1qoCpAAAAAAAAAAAAgI3VaEuea2pq4je/+U3WWpqmERFx8MEHx7XXXhutW7eu195lZWVx0kknxdFHHx3Dhg2LF154IVMkXVv0fPPNN8f3vve9OgXThbZ48eK47rrrsta+9a1vxaBBgzbJ/OLiuv9KrVy5ssH7rm2PkpISOTYwBwAAAAAAAAAAAADA+rzyyiuFjrBVef755+Oll17KXDdt2jSuvvrqGDZsWAFTAQAAAAAAAAAAAPVRVOgA9fX666/HzJkzMyXLaZpGkiTxox/9KG655ZZ6FzyvqaysLG699dY4/vjjMwXStWbNmhWvvvpqg2fk2vXXXx8LFy7MXG+zzTZx1VVXbbL522yzTZ21XJQaf/bZZ3XWmjdvLscG5gAAAAAAAAAAAAAAYPOwdOnS+OUvf5m1dtZZZ0XXrl0LEwgAAAAAAAAAAABokEZb8vzKK69k3tcWPPft2zeuvPLKTPFzrgwbNiz69u1bp+h5cyt5fuONN+Kpp57KWvvpT38aHTt23GQZ1lauXVlZ2aA9q6urY9WqVXXW27RpI8cG5gAAAAAAAAAAAAAAYPNwww03xPz58zPXPXr0iNNOO62AiQAAAAAAAAAAAICGaLQlz+PHj6+zdsUVV0RRUe4fqaioKK644oqstTRN15qhUFasWBE///nPs9b69OkTgwcP3qQ5tt122zpr8+fPj5qamnrvOXfu3A2eJQcAAAAAAAAAAAAAAJurkSNHxuOPP565TpIkrrnmmigpKSlgKgAAAAAAAAAAAKAhigsdoL4+/fTTSJIkc92lS5fo379/3uYNGDAgdt5555gxY0YkSRJpmsbMmTPzNm9jvfjii1l5kiSJ888/P5YuXbpR+1RVVdVZq6ysjEWLFtVZb9u2bdY/g4iIjh07rnXP+fPnR/v27TcqS61Zs2bVWSsqKoodd9xxnffIAQAAAAAAAAAAAADA5qSysjKGDRuWtXbcccfFwIEDC5QIAAAAAAAAAAAAyIVGW/K8YsWKiIhI0zSSJMlrwXOtAQMGxPTp0zPFxrUZNgfV1dVZ12maxmmnnZaTve++++64++6766yPHj06ysrKstY6duwYxcXFsXr16qz12bNn17vUeM6cOXXWOnToECUlJeu8Rw4AAAAAAAAAAAAAADYnv/vd7+KTTz7JXG+33XZx4YUXFjARAAAAAAAAAAAAkAtFhQ5QX02bNs263nHHHfM+84ulvKWlpXmf2diUlpZGt27d6qxPnDix3nuu7d5evXrJsRE5AAAAAAAAAAAAAAAonPHjx8fw4cOz1i6//PIoKysrUCIAAAAAAAAAAAAgVxptyXPbtm2zrlevXp33mdXV1evNwOf69u1bZ+3f//53vfdb271rmyEHAAAAAAAAAAAAAACbm6qqqrjiiiuyfibhG9/4Rhx66KEFTAUAAAAAAAAAAADkSqMtee7evXukaZq5njt3bt5nrjkjSZLo0aNH3mc2RnvvvXedtTFjxtRrr8rKypgwYUKd9X322UeOjcwBAAAAAAAAAAAAAMCmd9ddd8X777+fuW7evHn8/Oc/L2AiAAAAAAAAAAAAIJeKCx2gvvbaa694/fXXI0mSSNO03qW5GypN03j77bcz85Ikib322iuvMzfG0UcfHUcffXSD9xk8eHCMGjUqa+3cc8+NoUOHbvAe//Vf/xXFxcWxevXqzNqsWbNi9OjRG/179txzz8WqVauy1tq2bRv9+vWTYyNzAAAAAAAAAAAAAACwaU2dOjX+8Ic/ZK2dc845sdNOOxUoEQAAAAAAAAAAAJBrjbbk+bvf/W78z//8T6RpGhERc+fOjZEjR8Y+++yTl3lvvfVWzJkzJ5IkiYiIoqKi+O53v5uXWY1du3btYr/99ovXXnsta/3hhx/e6FLjRx55pM7aIYccEk2aNJFjI3MAAAAAAAAAAAAAAGwKCxYsiLFjx8bEiRNjypQpsWTJkigvL4+Kioqorq7OyYwkSeKll17KyV75kqZpXHnllbFq1arMWq9eveLkk08uYKrN17vvvpv5mQ0AADaN+fPnr/fzSZMmxYIFCzZRGgAA2Lp82Z/HAQAAgNzx915bltou4s1Noy153mmnneKggw6KESNGRJIkkaZp/OIXv4gnn3wySktLczpr5cqV8ctf/jIzJ0mSOOigg2KnnXbK6ZwtyY9+9KM6pcbPPvtsnHjiidG3b98N2mPEiBExduzYrLUkSeJHP/qRHPXMAQAAAAAAAAAAAACQLy+99FI88sgj8c9//jNqamqyPsv1D1U0hjLgBx54IMaMGZO5Lioqil/84hdRXNxof5Qjr7747wwAAPn3Zf8jLNXV1Tn7H2oBAACy+bM2AAAAbDr+3otNoajQARri8ssvj5YtW2aup02bFhdddFFUVVXlbEZVVVVcfPHFMW3atMxay5Yt4/LLL8/ZjC3R/vvvH7vttlvWWnV1dVx++eVRUVHxpffPnz8/rr322jrrBx10UPTo0UOOeuYAAAAAAAAAAAAAAMi1WbNmxWmnnRZDhw6NN954I6qrqyNN06xXxOfFzLl4NQZz5syJm2++OWvt+OOPj/79+xcoEQAAAAAAAAAAAJAvjbrkeccdd4z/+Z//iSZNmkSSJJGmaYwYMSJOPfXUmDlzZoP3//TTT+OUU06JESNGZPYvLi6OW265Jdq3b5+DJ9hyJUkSV1xxRZ0DtB988EEMHjw45s+fv857p06dGj/84Q/rfKdp06ZxySWXyNGAHAAAAAAAAAAAAAAAufTRRx/FD37wg/jnP/+ZKXRuzOXMuXL11VdHRUVF5nqHHXaICy64oICJAAAAAAAAAAAAgHwpLnSAhvrGN74Rf/zjH+P888+P8vLySNM0Ro0aFYccckiccMIJ8YMf/CC6deu2UXtOmzYtHnnkkXjggQdi1apVkaZpRES0bNky/ud//if222+/fDzKFmfgwIFx6qmnxl133ZW1PmHChBg0aFAce+yx8a1vfSs6d+4cq1evjhkzZsTzzz8ff//736OqqqrOfpdffnnstNNOcjQwBwAAAAAAAAAAAABALixcuDBOPvnkWLRoUUTEWouca8/jb02eeeaZePXVV7PWhg0bFi1btixQosahqKhoqysDBwAotCZNmnzp51/2HQAAoH78WRsAAAA2HX/vtWVJ0zRqamoKHaOOzabkedasWfW+t3v37vGHP/whrr322pgyZUokSRIrV66Me+65J+65557o1atX7LnnnvGVr3wldt5552jZsmXmcGR5eXmUl5fH9OnT4/33348xY8bE+++/HxH/OUyaJEn06NEjhg0bFp07d85k7dixYwOfest3wQUXxCeffBIvvPBC1vqKFSti+PDhMXz48A3a58QTT4wf/vCHcuQoBwAAAAAAAAAAAABAQ91yyy0xb968TDFvmqaRJEnmLH6nTp2ib9++0b179ygrK4uWLVtGUVFRISPn3eLFi+O6667LWvvWt74VgwYNKlCixqN///5b/L8fAACbm08//XS9n/fu3Ts6d+68idIAAMDW5cv+PA4AAADkjr/32rLU1NTE2LFjCx2jjs2m5PnAAw/MHOzMhTUPhk6ePDlT3Pxlau+p3aN2bcqUKXHiiSdmfTZx4sSc5d1SNWnSJG6++eZo27ZtPPTQQxt9f5IkMXTo0DjnnHPkyGEOAAAAAAAAAAAAAICG+OSTT+Jvf/tb1s8B1J7j/+Y3vxnnnHNO9O/fv4AJC+P666+PhQsXZq632WabuOqqqwqYCAAAAAAAAAAAAMi3okIHWFOapg1+1e4T8fkB0TWLmjf0/i/et6772TAlJSVxzTXXxB133BHdu3ff4Pv69+8fDz74YM4KjeUAAAAAAAAAAAAAAMiNV155JWpqajLXtWfsL7/88rjzzju3yoLnN954I5566qmstZ/+9KfRsWPHAiUCAAAAAAAAAAAANoXiQgdYU22xckN8sei5PvuurcB5zT225ILn++67L297H3DAAbH//vvHyJEj45VXXolx48bF9OnTo6KiIiIiysrKomvXrrHnnnvGd77znbwd6pUDAAAAAAAAAAAAAKBh3nzzzcz7NE0jSZIYPHhwDBkypICpCmfFihXx85//PGutT58+MXjw4AIlAgAAAAAAAAAAADaVzarkORdyURSdiz1YuyRJYt999419991Xjs0oBwAAAAAAAAAAAADAxvjkk0+yzt6XlpbGeeedV8BEhfXiiy/GzJkzM9dJksT5558fS5cu3ah9qqqq6qxVVlbGokWL6qy3bdvWzz8AAAAAAAAAAADAZmCzKnlO07TQEQAAAAAAAAAAAAAAgC9RWzqcpmkkSRJ77bVXtGzZssCpCqe6ujrrOk3TOO2003Ky99133x133313nfXRo0dHWVlZTmYAAAAAAAAAAAAA9bfZlDy//PLLhY4AAAAAAAAAAAAAAABsgBUrVmRd77LLLgVKAgAAAAAAAAAAAFBYm03Jc6dOnQodAQAAAAAAAAAAAAAA2AAtWrSIZcuWZa5bt25dwDQAAAAAAAAAAAAAhVNU6AAAAAAAAAAAAAAAAEDj0qlTp0jTNHO9ZMmSwoUBAAAAAAAAAAAAKKDiQgcAAAAAAAAAAAAAAAAalz59+sTEiRMjSZKIiJgzZ06BExXW0UcfHUcffXSD9xk8eHCMGjUqa+3cc8+NoUOHNnhvAAAAAAAAAAAAID+KCh0AAAAAAAAAAAAAAABoXA444IDM+zRNY9SoUVFTU1O4QAAAAAAAAAAAAAAFouQZAAAAAAAAAAAAAADYKAcccEB07Ngxc718+fL4xz/+UbhAAAAAAAAAAAAAAAWi5BkAAAAAAAAAAAAAANgoTZo0iaFDh0aappEkSaRpGr/97W+jurq60NEAAAAAAAAAAAAANiklzwAAAAAAAAAAAAAAwEY76qij4oADDsgUPX/00Udx5ZVXFjoWAAAAAAAAAAAAwCal5BkAAAAAAAAAAAAAAKiXm2++OXbbbbdI0zTSNI0nn3wyLrnkkqioqCh0NAAAAAAAAAAAAIBNQskzAAAAAAAAAAAAAABQLy1atIh777039t5774iISNM0nn766Tj88MPjoYceis8++6zACQEAAAAAAAAAAADyq7jQAfJl6dKlMXHixJgyZUosXbo0li9fHhUVFVFdXZ2T/ZMkieuuuy4newEAAAAAAAAAAAAAQCE9+eSTDbr/sMMOiwULFsSUKVMiTdOYOXNmXHPNNfHrX/86+vbtG3379o3tttsuWrVqFcXFuflRhiOPPDIn+wAAAAAAAAAAAAA0xBZV8rxgwYJ48skn48knn4yPPvoob3PSNFXyDAAAAAAAAAAAAADAFuPSSy+NJElyslftPmmaRmVlZYwePTpGjx6dk73XpOQZAAAAAAAAAAAA2BxsESXPNTU1cdddd8Uf/vCHWLlyZaRpWuhIAAAAAAAAAAAAAADQ6OTiPH6appEkSVbZc67lqpB6c3PfffcVOgIAAAAAAAAAAACwkRp9yXNFRUWceuqp8e6772Yd/MzngU0l0gAAAAAAAAAAAAAAbInycRY/13s60w8AAAAAAAAAAABsThp1yfOqVavizDPPjHfeeSci1n3w0wFOAAAAAAAAAAAAAAAAAAAAAAAAAAAAINcadcnz8OHDY/To0Wstd64tdm7Xrl107949ysrKomXLllFUVLSpYwIAAAAAAAAAAAAAwGatY8eOhY4AAAAAAAAAAAAA0Cg12pLnpUuXxh133FGn4DlN09huu+1iyJAhceihh0anTp0KlBAAAAAAAAAAAAAAABqHV155pdARAAAAAAAAAAAAABqlRlvy/Oqrr0Z5eXmm5DlN00iSJA4++OD41a9+FS1btixwQgAAAAAAAAAAAAAAAAAAAAAAAAAAAGBL1mhLnt94443M+9qC53333TduueWWTPEzAAAAAAAAAAAAAAAAAAAAAAAAAAAAQL4UFTpAfX3wwQdZZc5JksRVV12l4BkAAAAAAAAAAAAAAAAAAAAAAAAAAADYJBptyfOiRYuyrvv06RM777xzgdIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAW5tGW/K8dOnSiIhI0zSSJIndd9+9wIkAAAAAAAAAAAAAAAAAAAAAAAAAAACArUmjLXlu2rRp1nW7du0KlAQAAAAAAAAAAAAAAAAAAAAAAAAAAADYGjXakuftt98+63rFihUFSgIAAAAAAAAAAAAAAAAAAAAAAAAAAABsjRptyXOvXr0iTdNIkiQiIhYsWFDgRAAAAAAAAAAAAAAAAAAAAAAAAAAAAMDWpNGWPH/jG9/IvE/TNMaMGVPANAAAAAAAAAAAAAAAAAAAAAAAAAAAAMDWprjQAerr4IMPjuuuuy4qKysjImL27NkxadKk6N27d4GTAQAAAAAAAAAAAADAlu3++++PCRMmbLJ5SZJESUlJNG3aNNq2bRvbb7997LLLLtGrV69o2bLlJssBAAAAAAAAAAAA8EWNtuS5VatWMXjw4LjjjjsiSZKIiLjlllvijjvuKHAyAAAAAAAAAAAAAADYso0cOTJeeeWVQseIoqKi6NWrVxx44IFxxBFHxE477VToSAAAAAAAAAAAAMBWpqjQARri7LPPjp133jkiItI0jddffz0efvjhAqcCAAAAAAAAAAAAAIAtX5qmBX9VV1fHxIkT47bbbovvfve7ccEFF8SsWbMK/VsDAAAAAAAAAAAAbEUadclz06ZN4/bbb4+ysrJIkiTSNI1rr702HnvssUJHAwAAAAAAAAAAAACALV6SJAV/RUSm8Pm5556LQw89NJ5++ukC/84AAAAAAAAAAAAAW4tGXfIcEbHLLrvEvffeG9tuu21ERFRXV8ewYcPivPPOi48++qjA6QAAAAAAAAAAAAAAYMuVpuk6Xxv63Y3d94v3rFn4nKZpVFZWxiWXXBKPPvpoXp4ZAAAAAAAAAAAAYE3FhQ5Qa9asWfW+t1WrVnHbbbfFFVdcEVOmTIk0TePFF1+MF198MQYOHBhf//rXo2/fvrHddttFWVlZNGnSJCeZO3bsmJN9AAAAAAAAAAAAAACgMdl7772jrKwsc71y5coYMWJEVFdXZ9bWLGNu3rx5dO/ePVq3bh0tW7aM0tLSqKioiPLy8pgzZ0588sknme+vWdicJEn06NEjdt9994iIWLFiRSxbtiwWLFgQH330UdTU1GTuWfPXNE3jmmuuiU6dOsW+++6b398MAAAAAAAAAAAAYKu22ZQ8H3jggZnDlA2x5oHMiIi333473n777Qbvu7Y5EydOzPm+AAAAAAAAAAAAAACwuRs8eHDm/YQJE+KCCy7IFDzXnucfMGBAfP/7349vfvOb0aVLl/Xut2LFihg/fnw8++yz8cILL8SSJUsyRc8fffRR7LPPPnHJJZdEkyZNMvdUVFTEmDFj4sEHH4zXXnstq1Q6SZJYvXp1XHzxxfHSSy9Fs2bNcvn4AAAAAAAAAAAAABlFhQ6wpjRNc/ZKkiRzoDNfLwAAAAAAAAAAAAAA2Jq98cYb8eMf/zhmzJgREZ//XEDv3r3j/vvvj4ceeih+/OMff2nBc0TENttsE1//+tfjmmuuiVdffTXOOOOMKCkpiSRJoqamJu677774yU9+EqtWrcrc06JFi/jmN78Zf/zjH+ORRx6Jzp071znrv3DhwnjwwQdz+9AAAAAAAAAAAAAAa9isSp5ri5kb8qr1xbLnXL4AAAAAAAAAAAAAAGBrN3HixDjvvPOisrIyc4b/sMMOi8ceeywGDhxY732bN28e559/ftx3333RunXrSJIk0jSNkSNHxoUXXrjWe3bfffd4/PHHo3v37pm12vvuvvvuqK6urnceAAAAAAAAAAAAgPXZrEqec0EhMwAAAAAAAAAAAAAA5Nfq1avjkksuicrKysz5/UGDBsVNN90UTZo0ycmM/v37xz333BPFxcWZwuYXX3wxHn/88bV+v1WrVnHbbbdFs2bNstYXLlwY48ePz0kmAAAAAAAAAAAAgC/arEqe0zRtFC8AAAAAAAAAAAAAANiaPfXUU/Hhhx9mypdbt24dV199dc7n9O7dO84444xI0zQz63e/+12sXr16rd/v2rVrHHPMMXXO/o8aNSrn2QAAAAAAAAAAAAAiIooLHaDWyy+/XOgIAAAAAAAAAAAAAADABnjwwQcjIjLly4cddli0a9cuL7MGDx4ct99+e1RXV0dExNy5c+Mf//hHHHTQQWv9/pAhQ+L+++/PWhszZkxesgEAAAAAAAAAAABsNiXPnTp1KnQEAAAAAAAAAAAAAADgSyxYsCAmTJgQSZJEmqYRETFo0KC8zWvdunV87Wtfi7feeiuztr6S5y5dukSHDh1izpw5mYxz587NWz4AAAAAAAAAAABg61ZU6AAAAAAAAAAAAAAAAEDjMWnSpEy5c60uXbrkdWbt/rWlzRMnTlzv9/v165eVcenSpXnNBwAAAAAAAAAAAGy9lDwDAAAAAAAAAAAAAAAb7NNPP62ztu222+Z1Zrt27bKuZ86cud7vb7fddlnXS5YsyXUkAAAAAAAAAAAAgIhQ8gwAAAAAAAAAAAAAAGyEioqKOmtLly7N68xly5Z9aYY1tW7dOuu6qqoq55kAAAAAAAAAAAAAIpQ8AwAAAAAAAAAAAAAAG6G4uLjO2syZM/M684v7N2nSZL3fr66uzrpu1qxZzjMBAAAAAAAAAAAARCh5BgAAAAAAAAAAAAAANkKbNm3qrL366qt5m1dZWRn/93//F0mSZNbatm273nuWL1+edd28efO8ZAMAAAAAAAAAAABQ8gwAAAAAAAAAAAAAAGywHj16ZN4nSRJpmsaTTz4Zn332WV7mPfHEE1FZWRkREWmaRpIk0b179/XeM3/+/Kzr7bffPi/ZAAAAAAAAAAAAAJQ8AwAAAAAAAAAAAAAAG2y33XaLVq1aZa3NmTMnfvOb3+R81pw5c+K3v/1tJEmStf71r399vfdNmjQpU0CdJEl07tw559kAAAAAAAAAAAAAIpQ8AwAAAAAAAAAAAAAAG6GoqCi+973vRZqmERGZMuW//vWvcdttt+VszsyZM+PEE0+M8vLyOvMPPfTQdd63ePHi+PTTT7PWdtlll5zlAgAAAAAAAAAAAFhTcaED1Ne//vWvmD179iadWVpaGqWlpdG2bdvYYYcdolOnTlFUpCcbAAAAAAAAAAAAAICty8knnxyPP/54VFdXR8R/ip5///vfx+TJk+PSSy+NTp061Xv/J554In7zm9/EwoULI0mSiIhI0zSSJInvfe976937lVdeqbPWv3//emcBAAAAAAAAAAAAWJ9GW/J87733rvXg5aZUUlISPXv2jL322iu+/e1vx1577VXQPAAAAAAAAAAAAAAAsCnssssuccopp8Sdd96ZKXiu/fWll16K119/Pb73ve/FIYccEvvss0+UlJR86Z6zZ8+O559/Pp544on48MMPI03TOt8pKyuLiy66aL37vPDCC1nXSZLEgAEDNur5AAAAAAAAAAAAADZUoy15joi1HtjclFatWhUTJkyIiRMnxvDhw6NTp05x2mmnxXHHHRdFRUUFzQYAAAAAAAAAAAAAAPl03nnnxTvvvBOjRo2KJEkiIjJFzytXroynnnoqnnrqqWjSpEnssssu0bNnz2jdunW0aNEiSktLo6KiIioqKmL27Nnx/vvvx8KFCyPiPz8rULtn7VpxcXHceOON0b59+3VmmjFjRrz55ptZefr16xft2rXL128DAAAAAAAAAAAAsJVr1CXPEdmHNgshTdPMAdJPP/00rrnmmnjooYfi5ptvju7duxc0GwAAAAAAAAAAAAAA5EtxcXH88Y9/jDPPPDNGjx6dVawc8Z+y5tWrV8eHH34YU6ZMWedetd+t9cWC55KSkrjhhhti//33X2+me+65J2pqarLWDj744A1/KAAAAAAAAAAAAICNVFToALlSW7a85iuXe61rvyRJsl5pmsbkyZPjuOOOi3HjxtU7AwAAAAAAAAAAAAAAbO5atGgRf/7zn+Okk06KoqLsH1FY86x9xLrP6qdpWudsfq00TaNr164xfPjwOPTQQ780z4UXXhijR4/Oeg0ZMiS3Dw0AAAAAAAAAAACwhkZb8rzttttGx44do0OHDtGhQ4fo2LFjFBcXf+kh0OLi4mjTpk3ssMMO0aJFi0wx85pFzmvenyRJtGvXLjOrTZs20aRJk3WWP9feW1FREWeffXbMmjVr0/7GAAAAAAAAAAAAAADAJlRSUhKXXnppPPLII7Hbbrut96z9ul5flKZplJaWxplnnhlPP/107LnnnhuUpUWLFtGqVausV5MmTXLynAAAAAAAAAAAAABrU1zoAPV17bXXZt4vX748fvGLX8TTTz+dVe7ctGnT2H///eMb3/hG9OrVK3bddddo1qxZnb3mzp0bkydPjnHjxsVzzz0XU6dOjYjIFEAXFRXF5ZdfHgcddFDmnsWLF8e4ceNi9OjR8cQTT8TChQuzDpYmSRILFiyIc889Nx5//PF8/TYAAAAAAAAAAAAAAMBmoU+fPvG3v/0txo4dG4888kg8//zzUVlZWed7a577X5tdd901fvCDH8QRRxwRrVu3zmtmAAAAAAAAAAAAgIZqtCXPtT799NM4/fTTY9q0aZlS5ubNm8dPfvKTGDJkSLRs2fJL92jfvn20b98+9t9//xg6dGi888478etf/zrGjh2bKWseOnRonHPOOXHuuedGRETbtm1j//33j/333z/OO++8eOCBB+K3v/1tVFVVZe09adKkGDFiRAwaNCgvzw8AAAAAAAAAAAAAAJuTPfbYI/bYY4+49tpr4/3334/x48fHhAkTYv78+bF8+fJYvnx5rFq1Klq0aBFlZWVRVlYWu+yyS/Tt2zf69u0b7du3L/QjAAAAAAAAAAAAAGywRl3yvGjRojjllFNixowZmbWePXvGHXfcER07dqz3vgMGDIgHH3wwhg8fHjfccEOmPPq2226L0tLSOP3007O+X1paGieddFIMHDgwTj755CgvL898lqZp/P73v1fyDAAAAAAAAAAAAADAVqWkpCR233332H333QsdBQAAAAAAAAAAACBvigodoCGGDRsWM2bMiCRJIkmS6NWrV/z1r39tUMHzmk488cS47rrrIk3TTNHzrbfeGmPHjl3r93ffffe46aabMtdJkkRExIcffhgfffRRTjIBAAAAAAAAAAAAAAAAAAAAAAAAAAAAm4dGW/I8cuTIePnllzPly02aNIlf//rXUVZWltM5Rx11VBxyyCGZoufq6uq47rrr1vn9Aw44IA488MBI0zRrfdSoUTnNBQAAAAAAAAAAAAAAAAAAAAAAAAAAABRWoy15/stf/pJ5nyRJHHzwwfGVr3wlL7POP//8rOv33nsv/v3vf6/z+6ecckqdNSXPAAAAAAAAAAAAAAAAAAAAAAAAAAAAsGVplCXPFRUV8dZbb0WSJJGmaUREfO9738vbvJ122in69OmTmRUR8dJLL63z+3vuuWeUlZVlrtM0jRkzZuQtHwAAAAAAAAAAAAAAAAAAAAAAAAAAALDpNcqS50mTJkVVVVXWWq9evfI68ytf+UpERCRJEhER48aNW+d3kySJAQMGRJqmme8vXbo0r/kAAAAAAAAAAAAAAAAAAAAAAAAAAACATatRljx//PHHddbat2+f15nbbbdd5n2apmvNsKYOHTpkXSt5BgAAAAAAAAAAAAAAAAAAAAAAAAAAgC1Loyx5Xr58eZ21zz77LK8zV61a9aUZ1tS6deus6xUrVuQ8EwAAAAAAAAAAAAAAAAAAAAAAAAAAAFA4xYUOUB/V1dV11ubNmxdlZWV5mzlv3rys65qamvV+v6SkJOu6tLQ055kAAAAAAAAAAAAAACAXnnzyyXV+duSRR270PZvCunIBAAAAAAAAAAAAbEqNsuS5devWddbeeuut6NGjR17mpWka//rXvyJJksxaq1at1nvPsmXLsq6bN2+el2wAAAAAAAAAAAAAANBQl156adaZ+TWtq0x5ffdsCkqeAQAAAAAAAAAAgM1BUaED1MfOO++cdZ2maTz99NN5m/fGG2/EggULMrOSJKmT4YsWL16cdb3tttvmLR8AAAAAAAAAAAAAAORCmqZZr/rcsyleAAAAAAAAAAAAAJuLRlnyvPvuu0dJSUlERCRJEhEREyZMiIcffjjns1auXBm/+tWvMnNq7bHHHuu974MPPogkSTKl0B07dsx5NgAAAAAAAAAAAAAAyKUkSTKv+tyzKV4AAAAAAAAAAAAAm5NGWfK8zTbbxH777RdpmkZEZMqUb7jhhhg5cmTO5lRVVcWFF14Y06dPr/PZwQcfvM77Vq5cGVOnTs1a22mnnXKWCwAAAAAAAAAAAAAAAAAAAAAAAAAAACi84kIHqK8TTzwx/vGPf2SukySJysrKOP300+Piiy+OE044IYqK6t9hPX369LjiiitizJgxkSRJRESkaRpJkkT//v2jf//+67z3rbfeitWrV2fui4jo27dvvbMAAAAAAAAAAAAAAEA+dezYcZPcAwAAAAAAAAAAALClabQlz/vss09861vfildffTWSJMkUMFdVVcV1110XjzzySJx88snxne98J1q1arXB+06YMCGeeOKJePjhh2P16tWZfWsVFRXFpZdeut49RowYUWdtzz333PCHAwAAAAAAAAAAAACATeiVV17ZJPcAAAAAAAAAAAAAbGkabclzRMSvfvWrOProo2Pu3LmZIubawucPP/wwrrjiirj66qujf//+0atXr+jRo0e0bt06WrRoEaWlpVFRUREVFRUxe/bseP/992PcuHExc+bMiIhI0zSzX+11kiRxzjnnxIABA9aZafny5fHCCy9k5enSpUvstNNOefydAAAAAAAAAAAAAAAAAAAAAAAAAAAAADa1Rl3y3K5du7j77rvjxBNPjIULF2bWa4ue0zSNVatWxdtvvx1vv/32l+5XW+xcu8cX/ehHP4qzzz57vXs8+OCDsWLFiqy1QYMGfelsAAAAAAAAAAAAAAAAAAAAAAAAAAAAoHFp1CXPERHdu3ePhx9+OH7605/Ge++9lylnXrOkec3y5vVZW7FzmqZRUlISP/vZz+LUU0/90j323HPP+P3vf5+11r9//w2aDwAAAAAAAAAAAAAAAAAAAAAAAAAAADQejb7kOSKiU6dO8eijj8bw4cPjd7/7XaxYsSKrsHlt5c1fprYYeuDAgXHttddGt27dNui+gQMHbvQsAAAAAAAAAAAAAAAAAAAAAAAAAAAAoPHZIkqeIz4vcj7ppJPi+9//fjz22GPx+OOPx4wZM7I+X5/aUueIiOLi4jjggAPi2GOPjf333z9vmQEAAAAAAAAAAAAAAAAAAAAAAAAAAIDGa4spea613XbbxZlnnhlnnnlmjB07NsaOHRvjx4+PCRMmxPz586OysjLr+02aNIlWrVpF165do2/fvtGvX7/Yd999Y9ttty3QEwAAAAAAAAAAAAAAAAAAAAAAAAAAAACNwRZX8rymPfbYI/bYY4+sterq6li2bFmsWrUqWrZsGS1atChQOgAAAAAAAAAAAAAAAAAAAAAAAAAAAKAx26JLntemSZMm0bZt20LHAAAAAAAAAAAAAACArcLy5ctj4sSJsWjRoli2bFksXbo0Vq1aFRERe++9dwwcOLDACQEAAAAAAAAAAADqb6sreQYAAAAAAAAAAAAAAPJr3Lhx8be//S3GjBkTU6dOjTRN1/q94uLi9ZY8z5s3Lz744IOstQ4dOkT37t1zmhcAAAAAAAAAAACgvpQ8AwAAAAAAAAAAAAAAOfGPf/wjbr/99nj33XcjItZZ7hwRkSTJBu151llnxerVqzPXvXr1iieffLJBOQEAAAAAAAAAAABypajQAQAAAAAAAAAAAAAAgMatsrIyrrjiijjrrLPi3XffjTRNMwXPSZLUeW2oHXbYIb7//e9n9kvTNN5///2YPHlyvh4FAAAAAAAAAAAAYKMoeQYAAAAAAAAAAAAAAOpt1qxZcdRRR8Xjjz+eKWJeW6HzmsXPG+OUU06JiMja66mnnmp4cAAAAAAAAAAAAIAcUPIMAAAAAAAAAAAAAADUy+LFi2PIkCHx8ccfZ5U7R/yn1LmkpCR22mmn6NevX71m9OzZM3r27BkRnxc9p2kar7/+es6eAQAAAAAAAAAAAKAhigsdAAAAAAAAAAAAAAAAaJwuvPDC+PTTTzPFzhGRKXY+/PDD46ijjop+/fpFaWlpRER85Stfqdecgw8+OD788MPMnKlTp8a8efNihx12aPhDAAAAAAAAAAAAADTAZlPyPGvWrHV+1rFjx42+Z1NYVy4AAAAAAAAAAAAAANjS/e///m/885//rFPw3L9//7j55pujc+fOOZt14IEHxu9///ustVGjRsX3v//9nM0AAAAAAAAAAAAAqI/NpuT5wAMPzDrYWStJkpg4ceJG3bMprC8XAAAAAAAAAAAAAABs6W677bbM+zRNI0mS2H///eO2226L4uLc/rjCrrvuGiUlJbF69erM2gcffJDTGQAAAAAAAAAAAAD1UVToAGtK03Str/rcsyleAAAAAAAAAAAAAACwNRo7dmxMnTo1kiTJrHXp0iVuvfXWnBc8R0QUFxdHz549s87yT506NedzAAAAAAAAAAAAADbWZlXynCRJ1qs+92yKFwAAAAAAAAAAAAAAbM3+8Y9/ZN6naRpJksSFF14YzZo1y9vMXXbZJSI+/zmCNE1j+vTpeZsFAAAAAAAAAAAAsKE2q5JnAAAAAAAAAAAAAABg8/fuu+9mXbdu3ToOOuigvM4sKyvLul62bFle5wEAAAAAAAAAAABsiOJCB1hTmqab5B4AAAAAAAAAAAAAAKD+ZsyYEUmSRJqmkSRJfP3rX48kSfI684slz+Xl5XmdBwAAAAAAAAAAALAhNpuS55dffnmT3AMAAAAAAAAAAAAAADTM4sWLs647dOiQ95nFxdk/AvHZZ5/lfSYAAAAAAAAAAADAl9lsSp47deq0Se4BAAAAAAAAAAAAAAAaZtWqVVnXLVu2zPvMpUuXZl2XlJTkfSYAAAAAAAAAAADAlykqdAAAAAAAAAAAAAAAAKBxadq0adb1FwuY82HRokVZ15uiWBoAAAAAAAAAAADgyyh5BgAAAAAAAAAAAAAANkqbNm2yrufNm5f3me+9914kSZK53nHHHfM+EwAAAAAAAAAAAODLKHkGAAAAAAAAAAAAAAA2SufOnSNN00iSJNI0jTFjxuR13ty5c2PGjBkREZm53bt3z+tMAAAAAAAAAAAAgA2h5BkAAAAAAAAAAAAAANgoffr0ybpeuHBhjBs3Lm/zHnvssTpr/fr1y9s8AAAAAAAAAAAAgA2l5BkAAAAAAAAAAAAAANgoe++9d521u+66Ky+zKioq4v77748kSbLWv/GNb+RlHgAAAAAAAAAAAMDGUPIMAAAAAAAAAAAAAABslH333TfKysoiIiJJkkjTNF588cV4+eWXcz5r2LBhsXjx4oiISNM0kiSJ3r17R5cuXXI+CwAAAAAAAAAAAGBjbTUlz9XV1bFo0aKYNm1avPvuuzF69OgYPXp0zJ49u9DRAAAAAAAAAAAAAACgUSkpKYnjjjsu0jSNiP8UPV922WUxfvz4nM255ZZb4tlnn40kSbLWhwwZkrMZAAAAAAAAAAAAAA1RXOgA+bJy5cp4+eWX4+23346xY8fGBx98EDU1NXW+99Of/jTOPPPMde6TpmlUVVVlrRUXF0dR0VbTjw0AAAAAAAAAAAAAAHWccsop8eijj8ayZcsi4vOi52XLlsWQIUPi5z//eRx55JH13nvRokVxzTXXxIgRIzIFz2maRpIk0b179zj88MNz8QgAAAAAAAAAAAAADbbFlTzPmzcv7r///njkkUdi6dKlEfH5Qc61qT3ouT6TJ0+Oo48+Omttv/32i7vuuqvhYQEAAAAAANZQVVUVc+fOLXQMAOBLzJkzp0GfAwCbh/bt20dJSUmhYwBAo9auXbu4+OKL44orrogkSTIlzJWVlXHZZZfFfffdFyeeeGIceOCB0bJlyw3a88MPP4ynn346/vrXv0ZlZWVmz1pFRUVx9dVXR1FRUb4eCwAAAAAAAAAAAGCjbFElz0899VT88pe/jPLy8jrFzl8sdF5X8fMX9e7dO/bee+8YOXJkZm3kyJExb9682GGHHRoeGgAAAAAA4P83d+7cGDJkSKFjAAANdPHFFxc6AgCwAe69997o3LlzoWMAQKN3zDHHxNixY+Oxxx7LnNuvLXyeMGFCXHLJJVFcXBw9evSIbt261bl/7NixcfPNN8cnn3wS48ePj1mzZkXEf8781+5ZW/Y8dOjQGDhw4CZ6OgAAAAAAAAAAAIAvt0WUPFdVVcWll14azz77bJ2DnF9Ue7BzY5x66qkxcuTIzEHTmpqaeOqpp+InP/lJg7MDAAAAAAAAAAAAAEBjdu2118aKFSvi2WefrVP0nKZpVFVVxaRJk2Ly5MkR8Z8C5zRN4/XXX4/XX389a732/i864YQT4swzz8z34wAAAAAAAAAAAABslKJCB2iompqaOPfcczMFz0mSZB3mrD0UWvuqj/322y+23377rLVXXnmlQbkBAAAAAAAAAAAAAGBLUFRUFL/97W/jjDPOyFqvPd9fe8Z/bWf61zzv/8Xv136eJEmcd955ceWVV+b3QQAAAAAAAAAAAADqodGXPP/mN7+J1157LSKizkHOLl26xNChQ2P48OHx2muvxTvvvFOvGUmSxKBBgzKHQ9M0jfHjx0dFRUUuHgEAAAAAAAAAAAAAABq9888/P+67777o0aNHpri51poFzmue/V/bWsR/yp932WWXGD58eJx99tmb7DkAAAAAAAAAAAAANkZxoQM0xLhx4+Ivf/lLnXLn1q1bx7Bhw+LQQw+tc9Czvr7zne/EX//618x1dXV1jB49Og444ICc7A8AAAAAAAAAAAAAAI3dwIED4+mnn47nnnsuHnzwwXj77bfrlD2v+eua1vxer169YvDgwXHUUUdFkyZN8h8cAAAAAAAAAAAAoJ4adcnzLbfcEjU1NZnDnWmaRpcuXWL48OHRoUOHnM7q27dvnUOkEydOVPIMAAAAAAAAAAAAAABrSJIkDjnkkDjkkENi7ty58eabb8Y777wT77//fsyaNSsWLlyYVejcvHnz6NixY3Tv3j0GDBgQ+++/f3Tv3r2ATwAAAAAAAAAAAACw4RptyfOMGTPirbfeyip4Lisri7vuuivnBc8RES1atIguXbrEjBkzMmtTp07N+RwAAAAAAAAAAAAAANhStG/fPo455pg45phjstY/++yzqK6ujqZNm0ZxcaP90QYAAAAAAAAAAACAxlvy/OKLL2bep2kaSZLEGWecEV26dMnbzF133TWmT58eSZJEmqYxbdq0vM0CAAAAAAD4oqWH94vqsmaFjgEArKm6JqI6jWiSRDQpKnQaAGANTZZ9Fq2fHlfoGADAOjRr5v/fDQAAAAAAAAAAAGwZGm3J85gxY7KuS0tL44QTTsjrzLZt22ZdL168OK/zAAAAAAAA1lRd1ixq2mxT6BgAAAAAAAAAAAAAAAAAAADA/6+o0AHqa9q0aZEkSaRpGkmSxMCBA6NZs2Z5ndmqVaus64qKirzOAwAAAAAAAAAAAAAAAAAAAAAAAAAAADZfjbbked68eVnXXbt2zfvM5s2bZ10reQYAAAAAAAAAAAAAAAAAAAAAAAAAAICtV6Mtef7ss8+yrtu2bZv3meXl5VnXRUWN9rcPAAAAAAAAAAAAAAAAAAAAAAAAAAAAaKBG21JcUlKSdV1RUZH3mUuWLMm6bt68ed5nAgAAAAAAAAAAAAAAAAAAAAAAAAAAAJunRlvy3LJly6zrxYsX533mBx98kHXdvn37vM8EAAAAAAAAAAAAAAAAAAAAAAAAAAAANk/FhQ5QX506dYoFCxZEkiSRpmm8++67eZ1XXl4e77//fmZekiTRpUuXvM4EAAAAAAAAAAAAAIDGZvXq1fHBBx/ExIkTY8qUKbF06dJYvnx5VFRURHV1dU5mJEkSw4cPz8leAAAAAAAAAAAAAA3RaEued91116xi548//jhmz54dHTp0yMu8ESNGRHV1dSRJklnbfffd8zILAAAAAAAAAAAAAAAam0mTJsUjjzwSzz77bCxbtixvc9I0zTrbDwAAAAAAAAAAAFBIRYUOUF8DBw6sszZ8+PC8zbv77rvrHALdZ5998jYPAAAAAAAAAAAAAAAag/Ly8rjqqqvimGOOiYceeiiWLl0aaZrm5QUAAAAAAAAAAACwuWm0Jc/f+ta3ori4OCIikiSJNE3j4YcfjqlTp+Z81p/+9Kf46KOPstbat28f/fv3z/ksAAAAAAAAAAAAAABoLObPnx/HHHNMPProo1FTUxNpmkaSJHl7AQAAAAAAAAAAAGxuGm3Jc1lZWRx88MGRpmlmrbKyMs4777xYunRpzua89tprccstt2QOg9YeOP1//+//5WwGAAAAAAAAAAAAAAA0NuXl5XHyySfH9OnTs8qd15Smac5fAAAAAAAAAAAAAJuT4kIHaIizzjornn/++aipqckcBJ0yZUr88Ic/jNtvvz123nnnBu3/17/+Na6//vqorq7OOmjaunXr+PGPf9ygvQEAAAAAAAAAAAAAoDG7/fbbY8qUKWstdo6IaNasWfTs2TO6d+8eZWVl0bJlyygqKipEVAAAAAAAAAAAAIC8adQlzz169IghQ4bEPffckzkUmiRJTJs2LQ477LA44YQTYsiQIdGhQ4eN2veNN96I3/3ud/Hee+9FmqaZvWvfX3DBBdGqVaucPw8AAAAAAAAAAAAAADQG8+bNi3vvvXetBc+77rprnHnmmXHggQdGs2bNCpQQAAAAAAAAAAAAYNNo1CXPERHnn39+vP322zF+/Pisw6GrVq2Kv/zlL/GXv/wl+vbtG3vssUd069atzv3z58+PN954Iz755JMYP358vP7667Fo0aKIiKyC54jPC6QPOuigOO644/L/YAAAAAAAAAAAAAAAsJl69dVXY9WqVZkz97Xn70866aS46KKLokmTJgVOCAAAAAAAAAAAALBpNPqS59LS0rjjjjtiyJAhMWXKlEiSJOuQaETEuHHjYvz48Zl7atfTNI0HHnggHnjggTqfRURWwXOapjFgwIC46aab8vo8AAAAAAAAAAAAAACwuXv99dcz72sLng855JC49NJLC5gKAAAAAAAAAAAAYNMrKnSAXGjXrl3cd999sddee9UpaV6z8HnNz2rVrte+au/5YsHzfvvtF3fffXc0a9Ys/w8EAAAAAAAAAAAAAACbsWnTpmWdu2/SpImCZwAAAAAAAAAAAGCrtEWUPEdEtG3bNv7yl7/ET3/60ygpKVlr2fOaB0jX9tkXP0/TNIqLi+O8886LP/3pT9GiRYu8PwcAAAAAAAAAAAAAAGzuFi1aFBGfn7tPkiT22GOP2H777QucCgAAAAAAAAAAAGDT22JKniMimjRpEmeddVaMGDEifvjDH0bz5s0jTdN1Fj6vrfS59vvFxcVx+OGHx7PPPhtnn312FBVtUb9VAAAAAAAAAAAAAABQb+Xl5VnXvXr1KlASAAAAAAAAAAAAgMIqLnSAfNhxxx3j6quvjosvvjheeeWVeOONN+Kdd96JGTNmZBU+f1Hbtm2jf//+8c1vfjMOPvjg2HbbbTdhagAAAAAAAAAAAAAAaByaN2+eVfTcpk2bwoUBAAAAAAAAAAAAKKAtsuS51jbbbBPf//734/vf/35ERFRWVsbs2bNjwYIFUVlZGTU1NdG0adMoKyuLjh07Rrt27QqcGAAAAAAAAAAAAAAANn877rhjTJkyJXO9bNmyAqYBAAAAAAAAAAAAKJwtuuT5i5o3bx7dunWLbt26FToKAAAAAAAAAAAAAAA0Wr17944PP/wwkiSJiIj58+cXOBEAAAAAAAAAAABAYRQVOgAAAAAAAAAAAAAAANC47L///pn3aZrG6NGjC5gGAAAAAAAAAAAAoHCUPAMAAAAAAAAAAAAAABvl29/+drRt2zZzvXDhQkXPAAAAAAAAAAAAwFZJyTMAAAAAAAAAAAAAALBRmjVrFqeffnqkaRpJkkSapvHb3/620LEAAAAAAAAAAAAANjklzwAAAAAAAAAAAAAAwEY78cQTo3///pGmaUREvPPOO3HrrbcWOBUAAAAAAAAAAADApqXkGQAAAAAAAAAAAAAA2GhFRUXx+9//Pjp27BhJkkSapnH77bfHrbfeGjU1NYWOBwAAAAAAAAAAALBJKHkGAAAAAAAAAAAAAADqZfvtt48HH3wwunfvHhGRKXo+/vjj44033ihwOgAAAAAAAAAAAID8Ky50gHz65JNP4sMPP4ylS5fG8uXLo6KiIqqrq3O2/7nnnpuzvQAAAAAAAAAAAAAAoDEZPXp05v1FF10UN910U0yZMiXSNI133303Tj/99OjYsWN87Wtfi759+8Z2220XrVq1iuLi3Pwow1577ZWTfQAAAAAAAAAAAAAaYosqea6qqoqXXnopnnzyyfj3v/8d5eXleZ2n5BkAAAAAAAAAAAAAgK3V4MGDI0mSOutJkkSappGmacycOTOefPLJePLJJ3M6O0mSmDhxYk73BAAAAAAAAAAAAKiPLabk+X//93/j+uuvj4ULF0ZERJqmeZ23toOoAAAAAAAAAAAAAACwtfni+f00TbPO3Of7fD8AAAAAAAAAAABAITX6kufVq1fHhRdeGC+88ELWwc98ljA7YAoAAAAAAAAAAAAAAJ9bW6Fz7a9JkuT8fL8z/QAAAAAAAAAAAMDmpFGXPKdpGhdddFE8//zzEbH+YmeHOAEAAAAAAAAAAAAAIL9yXegMAAAAAAAAAAAAsLlr1CXPjz32WDz33HNrPQRaW+qcJEl06NAhysrKomXLlg6MAgAAAAAAAAAAAABAjtSe3QcAAAAAAAAAAADYWjXakufKysq49dZb65Q2p2kazZo1iyOOOCIOPfTQ6Nu3bzRv3rxAKQHYGAtWJxHhoD8AbE5Wp5+/ipPPXwDA5uPz/x4NAAAAAAAAhTN58uRCRwAAAAAAAAAAAAAouEZb8vzaa6/FggULMiXPafp5KehXv/rVuPHGG6NTp06FjAdAPdw4q9H+nyUAAAAAAAAAAAAAAAAAAAAAAAAAALZCjbZN8/XXX8+8T9M0kiSJPn36xN133x3NmjUrYDIAAAAAAAAAAAAAAAAAAAAAAAAAAABga1BU6AD1NWHChDpr11xzjYJnAAAAAAAAAAAAAAAAAAAAAAAAAAAAYJNotCXPCxcujCRJMtc9evSIPn36FDARAAAAAAAAAAAAAAAAAAAAAAAAAAAAsDVptCXPS5YsiYiINE0jSZLYY489ChsIAAAAAAAAAAAAAAAAAAAAAAAAAAAA2Ko02pLn0tLSrOttt922QEkAAAAAAAAAAAAAAAAAAAAAAAAAAACArVGjLXn+YqnzqlWrCpQEAAAAAAAAAAAAAAAAAAAAAAAAAAAA2BoVFzpAffXs2TM++eSTzPWiRYsKmAaADdG+ffu49957Cx0DAPgSc+bMiYsvvnidn994442x4447bsJEAEB9tG/fvtARAAAAAAAAAAAAAAAAAAAAAGCL12hLnvfdd9945ZVXIkmSSNM0/j927jzMyrL+H/jnGWbYhBERF1BEVkUEUVTULAUNi0ox029pmkv9lHIps0yz1RLJzA3M+Gq4+y03rDR3yAUXVPZFwIVFEIh9GZhheH5/dM2Jw7DMDDNzmJnX67rO5bnv89zP/X64vGxo7vOeMGFCriMBsAMFBQWx//775zoGALCT9t13X/+bDgAAAAAAAAAAAAAAAAAAAAAAABGRl+sAVXXKKadEQUFBZvzxxx/HnDlzcpgIAAAAAAAAAAAAAAAAAAAAAAAAAAAAaEjqbMnzXnvtFV/96lcjTdNIkiQiIoYNG5bjVAAAAAAAAAAAAAAAAAAAAAAAAAAAAEBDUWdLniMifvjDH0abNm0iIiJN03j66adj9OjROU4FAAAAAAAAAAAAAAAAAAAAAAAAAAAANAR1uuS5sLAwhg0bFo0bN44kSWLTpk1x5ZVXxtixY3MdDQAAAAAAAAAAAAAAAAAAAAAAAAAAAKjn6nTJc0RE796946677oqmTZtGRERRUVF8+9vfjqFDh8aKFStyGw4AAAAAAAAAAAAAAAAAAAAAAAAAAACot+p8yXNExLHHHhuPPvpodOnSJSIiNm3aFPfee2/069cvrrnmmhg1alR88MEHsWLFiti0aVOO0wIAAAAAAAAAAAAAAAAAAAAAAAAAAAD1QX6uA+yM7t27l5tLkiQiItI0jaKiohg1alSMGjWq2vdOkiSmTZtW7fcFAAAAAAAAAAAAAAAAAAAAAAAAAAAA6oY6XfKcpulW55MkySp7BgAAAAAAAAAAAAAAAAAAAAAAAAAAAKhudbrkOSIyZc5l0jTNFDtvXvZcnRRHAwAAAAAAAAAAAAAAAAAAAAAAAAAAAHW+5HlLNVHqDAAAAAAAAAAAAAAAAAAAAAAAAAAAALClOl/ynKZpriMAAAAAAAAAAAAAAAAAAAAAAAAAAAAADVCdLnm+//77cx0BAAAAAAAAAAAAAAAAAAAAAAAAAAAAaKDqdMnz0UcfnesIAAAAAAAAAAAAAABQ57300ktZ4y5dukSHDh1ylAYAAAAAAAAAAACg7qjTJc8AAAAAAAAAAAAAAMDO+973vhdJkmTGV1xxRVxyySU5TAQAAAAAAAAAAABQNyh5BgAAAAAAAAAAAAAAIiIiTdOssudteeihh+LPf/5zZpwkSbz44os1GQ0AAAAAAAAAAABgl6TkGQAAAAAAAAAAAAAAqJRVq1bFJ598EkmSVLgYGgAAAAAAAAAAAKA+yst1AAAAAAAAAAAAAAAAAAAAAAAAAAAAAIC6SMkzAAAAAAAAAAAAAAA0cEmS5DoCAAAAAAAAAAAAQJ2k5BkAAAAAAAAAAAAAABq45s2bZ43XrVuXoyQAAAAAAAAAAAAAdYuSZwAAAAAAAAAAAAAAaOAKCwuzxgsWLMhREgAAAAAAAAAAAIC6RckzAAAAAAAAAAAAAAA0cG3bto00TSNJkkjTNN58880oLS3NdSwAAAAAAAAAAACAXZ6SZwAAAAAAAAAAAAAAaOB69eqVNV66dGnceuutuQkDAAAAAAAAAAAAUIfk5zoAAAAAAAAAAAAAAACQW8cee2zce++9ERGRJEmkaRp33313vPHGGzFgwIDo3LlztGjRIvLy8iIiYv78+eXu8c4770SaprWSN0mSOPLII2tlLwAAAAAAAAAAAIDtUfIMAAAAAAAAAAAAAAAN3Oc+97lo3759pry5rOh5ypQpMXXq1G2uKyt1TtM0zj333FrJGhHRqFGjmDJlSq3tBwAAAAAAAAAAALAtebkOAAAAAAAAAAAAAAAA5FaSJHH11VdnSpvL5iL+U+C8tdeWtnVdTbwAAAAAAAAAAAAAdhX5uQ5A7Vi3bl3Mnz8/Fi5cGIsWLYp169ZFUVFR5OfnR2FhYbRs2TI6d+4cXbp0iUaNGtV4nuXLl8fUqVNjzpw5sWbNmkiSJAoLC+PAAw+MQw89NFq0aFHjGeQAAAAAAAAAAAAAAPivk08+OS699NIYNmxYpuC57J9bs2XZ8vaurU5KngEAAAAAAAAAAIBdiZLnemj9+vUxefLkeO+992LSpEkxc+bMmD9/fmzatGmHa5s2bRpHHHFEDBo0KAYMGBDNmjWrtlxpmsY///nPePjhh+Pdd9/dZp6CgoI47rjj4rzzzovjjz++2vaXAwAAAAAAAAAAAABg+y699NLo1q1bDB06ND755JPMfG0VOAMAAAAAAAAAAADUNUqe66Hvf//7MXr06CqtXb9+fYwdOzbGjh0bQ4cOjauvvjpOO+20nc708ccfx9VXXx0TJkzY4bUlJSXxr3/9K/71r3/FiSeeGDfccEPsueeeO51BDgAAAAAAAAAAAACAHRswYECcfPLJMWbMmBg7dmxMnjw5Fi5cGKtXr46ioqJtrkvTtBZTAgAAAAAAAAAAAOwalDzXQ9V1MHbp0qXx4x//OF544YX4wx/+EI0bN67Sfd55550YPHhwrFq1qtJrx4wZE2eccUb8+c9/jk6dOlVpfzkAAAAAAAAAAAAAAConLy8v+vfvH/3799/q53/84x/jtttuiyRJIk3TSJIkpk+fXsspAQAAAAAAAAAAAHJPyXMD0rJly+jQoUO0a9cudtttt2jSpEmsXbs2li5dGu+//34sXbp0q+teeOGFuOSSS2LEiBGRn1+5f2VmzJgRF198caxZs6bcZ40bN45DDz002rdvHyUlJTFv3ryYOnVqbNq0Keu6hQsXxvnnnx+PPvpo7LPPPpXaXw4AAAAAAAAAAAAAAAAAAAAAAAAAAABqipLneuzAAw+Mz372s3HEEUdEnz59dlgIPHny5HjooYdi1KhRkaZp1mevv/563HPPPXHxxRdXeP+ioqL4/ve/X67QOEmSOP/88+Oiiy6KvfbaK+uzOXPmxPDhw+Opp57Kml+0aFFcddVVcf/990eSJBXOIAcAAAAAAAAAAAAAAAAAAAAAAAAAAAA1JS/XAah+X//61+OZZ56J5557Lq677roYOHDgDgueIyJ69uwZN954Y9xzzz3RvHnzcp8PHz48li1bVuEcd911V3z00UdZc40aNYobb7wxfvKTn5QrNI6I6NChQ/zud7+LK6+8stxnb7/9djz22GMV3l8OAAAAAAAAAAAAAAAAAAAAAAAAAAAAapKS53qoX79+0blz5yqv/8xnPhO//e1vy81v2LAhnn/++QrdY+nSpTFy5Mhy8xdddFEMGjRoh+svvvjiGDhwYLn52267LYqLiyuUQQ4AAAAAAAAAAAAAgJqVpmmuIwAAAAAAAAAAAADkVH6uA5RZs2ZN1rhJkyZRUFCQozQMHDgw/vSnP8WMGTOy5seOHRtf//rXd7j+wQcfjA0bNmTNHXDAAXHppZdWOMPPfvazeO2112LVqlWZuSVLlsQ//vGP+OpXv1qhe8gBAAAAAAAAAAAAAFD9Bg8eHIMHD851DAAAAAAAqJfyVq3PdQQAAACoM/w9ml3BLlPyfOSRR0aSJJnxFVdcEZdcckkOE3HCCSeUK3n+9NNPd7guTdN46qmnys1feOGF0aRJkwrv37p16zjzzDPjnnvuyZp/8sknK1RqLAcAAAAAAAAAAAAAAAAAAAAAUNe0+tukXEcAAAAAoBLych1gc2maRpqmFbp29OjRcc0112Re1157bQ2na3j23XffcnMrV67c4bpJkybFJ598kjVXUFAQX/rSlyqd4fTTTy83N27cuPj3v/8tRyVzAAAAAAAAAAAAAAAAAAAAAAAAAAAAUL12qZLnJEkqfO2MGTPiySefjFGjRsWTTz4ZTz75ZA0ma5hKSkrKzRUWFu5w3WuvvVZurk+fPhVau6WuXbvG/vvvnzWXpmmMHTtWjkrmAAAAAAAAAAAAAAAAAAAAAAByb+PGjbmOAAAAAA2Gv4dTG3apkmd2LR999FG5uW7duu1w3XvvvVdu7qijjqpyjqOPPrrc3DvvvCNHJXMAAAAAAAAAAAAAAOTKxo0b44MPPoh33303Ro8eHaNGjYpRo0bF6NGj4913343Zs2dHSUlJrmMCAAAAAECt8P+JAwAAQO3x93BqQ36uA7Br2rBhQ7z44ovl5vv167fDtdOmTSs316NHjypnOeSQQ+KJJ57Imps+fboclcwBAAAAAAAAAAAAAFCb3nrrrRg9enRMmDAhpk+fHsXFxdu9vqCgIA455JDo3bt39OvXL/r27VtLSQEAAAAAoHYVFBTkOgIAAAA0GP4eTm3YZUqemzZtGhs2bMiMN39P7bvllltiyZIlWXOdO3eO/v37b3fdsmXLYtmyZeXmO3XqVOUsHTt2LDf3wQcfyFGJHAAAAAAAAAAAAAAAtaG4uDgee+yxePjhh7POOadpWqG1EydOjIkTJ8Z9990XnTt3jnPOOSfOOOOMaNy4cU3GBgAAAACAWpWfv8vU/gAAAEC95+/h1Ia8XAco07Jly6zx0qVLc5SkYVu7dm1cf/31MXLkyKz5xo0bx4033hh5edv/V2b+/Pnl5pIkiXbt2lU50/7777/VnMuXL5ejgjkAAAAAAAAAAAAAAGra9OnT44wzzojrr78+Zs+eHWmaZl5JklTotfma2bNnx69//es444wzYsaMGbl+PAAAAAAAAAAAAICt2mWqxNu0aRNLlizJHMp87733ch2pQVi7dm2sWLEiZs+eHWPHjo2///3v5Qq2mzdvHrfeemv06tVrh/dbvHhxublWrVpFQUFBlTPutddeW51ftGhR7LHHHnJUIEddNnHixEiSJNcxAAAalCVLlmz38+nTp8e///3vWkoDAAANy45+HgcAAACqj9971S9pmuY6AkCDd++998bNN98cGzduzPx3uSrngDdfU1b2PGvWrDjzzDPjhz/8YZx//vnVFRkAAAAAAHZZK07tFZsKm+Y6BgAAANQJeavWR6u/Tcp1DBq4XabkuUePHjF9+vTM+IMPPojnn38+BgwYkMNU9cecOXOq9Gd5/PHHx89//vPo0KFDha5fvnx5ublWrVpVet/N7bbbblFQUBAlJSVZ8ytWrJCjgjnqsk2bNuU6AgBAg1NaWrrDz3d0DQAAUDV+1gYAAIDa4/deAFB97rzzzrjjjju2W+5c0UL+zddu/r6kpCSGDh0aRUVFMXjw4J1MDAAAAAAAu7ZNhU1jU6vmuY4BAAAAQAXtMiXPffr0icceeywi/nMQM03T+OEPfxjnnntunHLKKdG5c+do0aJFjlM2DPn5+XHWWWfFmWeeGYccckil1q5evbrc3G677bbTmXbbbbdyJcZb20sOAAAAAAAAAAAAAIDa8+yzz8btt98eSZKUK3cuK3bu0KFDHHnkkdG9e/fo2LFjtGzZMlq2bBlpmsaaNWtizZo18eGHH8b06dPjnXfeiTlz5kRE+cLnNE3j9ttvj06dOsUpp5xSew8JAAAAAAAAAAAAsB27TMnzF77whRgyZEimqDZJkigpKYmRI0fGyJEjt7mu7NBnmqbRvXv3WskaEdGoUaOYMmVKre1XmzZu3BiPPvpozJs3L84999w44YQTKry2pKSk3FxBQcFOZ9raPYqLi+WoYA4AAAAAAAAAAAAAgOq2bNmy+OUvf1luPk3TSJIkBg0aFN/4xjfisMMO2+G9jjvuuMz7SZMmxSOPPBJPPfVU5jsDEf8tev7FL34RRx11VLRu3bpangMAAAAAAAAAAABgZ+wyJc/NmjWLCy+8MG699dZIkiQi/nsAs6Iqcy3bV1JSEq+++mq8+uqrceyxx8aQIUOibdu2FVq3pfz8nf/XbGulxlvbS476Jy8vL/PfBAAAakejRo12+PmOrgEAAKrGz9oAAABQe/zeq35J0zQ2bdqU6xgADc6IESNixYoVWed90zSNzp07x5AhQ6JXr15Vum+vXr2iV69ecfbZZ8c111wTs2fPztpj5cqV8b//+79x9dVX7/QzAAAAAAAAAAAAAOysXabkOSLi4osvjnHjxsXrr7+eVfS8LVuWOtdWEWxdLJNu3759vPHGG1lzmzZtijVr1sSyZcti+vTp8eabb8bo0aPLlQW/8cYbceaZZ8b9998fnTp12u4+eXl55eY2bty40/m3VmC8tb3kqH8OO+ywevtsAAC7qvnz52/38+7du8f+++9fS2kAAKBh2dHP4wAAAED18Xuv+mXTpk0xfvz4XMcAaFCKioriL3/5S+Ycf5qmkSRJHH744XHXXXfF7rvvvtN79OzZMx555JG4+OKL47333oskSSJJkkjTNP7yl7/EFVdcEU2bNt3pfQAAAAAAAAAAAAB2xi7VnJokSfzpT3+Kb37zm5mDl3WxUHlXlJeXF61bt856tWnTJg488MA44ogj4pxzzok77rgjxowZE6eeemq59UuWLIlvf/vbsWbNmu3uk59fvjd8w4YNO51/a/coKCiQo4I5AAAAAAAAAAAAAACq05gxY6KoqChrbs8994w//vGP1VLwXKZly5Zx5513Rps2bbLmi4qK4uWXX662fQAAAAAAAAAAAACqqnwDbY7l5+fHddddF+edd148/PDDMXbs2Jg9e3Zs2rRph2sVQu+8Nm3axE033RQ9e/aM3/72t1mfffLJJ3HzzTfHL37xi22ub968ebm56ig1Xr9+fbm5Zs2ayVHBHAAAAAAAAAAAAAAA1emtt97KvE/TNJIkiWuvvTZatWpV7Xu1atUqrrnmmvjhD38YSZJkZRg4cGC171cT1q1bF/Pnz4+FCxfGokWLYt26dVFUVBT5+flRWFgYLVu2jM6dO0eXLl2iUaNGuY4LAAAAAAAAAAAAVMIuV/Jc5oADDoif/OQnERFRVFQUixcvjtWrV0dRUVGkaRpPPfVUPP7445EkSeZA6H333Vdr+TY/GFofnXfeeTFr1qz461//mjX/+OOPx2WXXRatW7fe6rrdd9+93FxRUdFOZSktLY3i4uJy89s7/CsHAAAAAAAAAAAAAEDNmTFjRta4sLAwBgwYUGP7DRgwIHbfffdYtWrVNjPsKtavXx+TJ0+O9957LyZNmhQzZ86M+fPnx6ZNm3a4tmnTpnHEEUfEoEGDYsCAAdGsWbNaSAwAAAAAAAAAAADsjF225HlzzZo1iw4dOmTNvfvuu+WuO/roo2srUoPwgx/8IJ544onYuHFjZm7Dhg0xZsyY+OpXv7rVNXvuuWe5uSVLlsSmTZsiLy+vSjkWLVpU4b3kAAAAAAAAAAAAAACoefPnz48kSSJN00iSJI455pjIz6+5rygUFBTEMcccE88991xm308++aTG9tsZ3//+92P06NFVWrt+/foYO3ZsjB07NoYOHRpXX311nHbaadWcEAAAAAAAAAAAAKhOVWuZpUFo3bp19OnTp9z8hAkTtrmmXbt25eZKSkpiyZIlVc6xYMGCcnN5eXmx7777ylHBHAAAAAAAAAAAAAAA1Wn16tVZ49o4z9y2bdvtZthVpGlaLfdZunRp/PjHP45LL700iouLq+WeAAAAAAAAAAAAQPXLz3UAdm2dOnWKt956K2tuewXF7dq1i/z8/Ni4cWPW/MKFC2OfffapUoZPP/203Fzbtm2joKBAjgrmAAAAAAAAAAAAAACoTluekW7WrFmN79m0adOscWlpaY3vWZ1atmwZHTp0iHbt2sVuu+0WTZo0ibVr18bSpUvj/fffj6VLl2513QsvvBCXXHJJjBgxIvLzfQ0EAAAAAAAAAAAAdjV1/nRfmqa5jlCvFRYWlpsrLi7e5vWNGzeOTp06xcyZM7Pmp02bFr17965ShmnTppWbO+igg7a7Rg4AAAAAAAAAAAAAgJrTrFmzWLt2bWa8ePHiGt9zyZIlWeMtS593NQceeGB89rOfjSOOOCL69OkT++yzz3avnzx5cjz00EMxatSoct+VeP311+Oee+6Jiy++uCYjAwAAAAAAAAAAAFWQl+sAVXXaaafF/fffn3ndd999uY5ULy1btqzcXJs2bba7pmfPnuXm3nvvvSpn2Nrare0hBwAAAAAAAAAAAABA7dhrr70iIiJJkkjTNN5///0a33PmzJlZ4x2dbc+Vr3/96/HMM8/Ec889F9ddd10MHDhwhwXPEf85F37jjTfGPffcE82bNy/3+fDhw7d6xh8AAAAAAAAAAADIrTpb8tyuXbs4+uijs15Uv1mzZpWb29FB2GOOOabc3Lvvvlul/YuKimLq1Knl5o899tgdrpUDAAAAAAAAAAAAAKBmHHjggZGmaWY8bdq0mDt3bo3tN2/evJg8eXKmVDpJkjjwwANrbL+d0a9fv+jcuXOV13/mM5+J3/72t+XmN2zYEM8///zORAMAAAAAAAAAAABqQJ0teabmLViwICZOnFhu/vDDD9/uuuOPPz7y8/PL3WvcuHGVzvDPf/4ziouLs+b22GOP6NWr1w7XygEAAAAAAAAAAAAAUDO2dq78tttuq7H9br/99nJzRxxxRI3tl2sDBw6Mgw8+uNz82LFjc5AGAAAAAAAAAAAA2B4lz2zT0KFDI03TrLlmzZrF8ccfv911rVu3js985jPl5v/yl79UOsNf//rXcnMDBw6MRo0a7XCtHAAAAAAAAAAAAAAANaNfv36Z90mSRJqm8cwzz8RTTz1V7Xs9/fTT8fe//z2SJNlmhvrohBNOKDf36aef5iAJAAAAAAAAAAAAsD0NsuR57dq1sXDhwli4cGGsXbs213Gq1QsvvBAlJSU7fZ/bb789nn322XLzp556ajRt2nSH688+++xyc88880xMnjy5whmef/75GD9+fNZckiRbvbccAAAAAAAAAAAAAAC1p2vXrtGzZ8/MuKzo+dprr42//OUv1bbPo48+GldffXWm4DlN00iSJHr27Bldu3attn12Rfvuu2+5uZUrV+YgCQAAAAAAAAAAALA99brkee3atfHyyy/HLbfcEt/61rfiuOOOi0MPPTSOPPLI6N+/f/Tv3z+OPPLIOPTQQ+PYY4+N8847L2655ZZ4+eWX62z585AhQ2LAgAFx7733xqJFiyq9fs6cOXHJJZfE8OHDy322xx57xJVXXlmh+5xwwglxyCGHZM2VlpbGtddeW6E/2yVLlsSvf/3rcvMnn3xydOnSpUIZ5AAAAAAAAAAAAAAAqDnf/e53I03TiPhv+XJpaWn88pe/jEsuuSTmzp1b5XvPmzcvBg8eHD//+c9j48aNW927vispKSk3V1hYmIMkAAAAAAAAAAAAwPbk5zpATfjggw/iwQcfjL/97W+xbt26zHzZ4dEtbdy4MZYvXx7jxo2LcePGRUREs2bNYtCgQXHOOedE586dayV3dVmwYEEMGTIkbrzxxujdu3f06dMnDjrooOjWrVvsscce0aJFi2jatGkUFRXF6tWr4+OPP47p06fH6NGjY9y4cVv9cyooKIghQ4ZEq1atKpQhSZL46U9/Gt/85jez7jdz5sw499xz409/+lPstddeW1374Ycfxne+851YsmRJ1nyTJk3i6quvrvgfhBwAAAAAAAAAAAAAADWmX79+cdJJJ8VLL70USZJkip7TNI1//etf8eqrr8YxxxwTX/nKV+LII4+M/ffff7v3mz9/frzzzjvxj3/8I958880oLS3N3DPiv0XSJ510Upx44om18IS59dFHH5Wb69atWw6SAAAAAAAAAAAAANtTr0qe161bFzfccEM8/vjjEVG+1LnsYOe2bH79unXr4pFHHolHHnkkvva1r8U111wTzZs3r/7QNShN0xg/fnyMHz9+p+5TUFAQt99+e/Tr169S64488si46KKL4u67786anzp1agwYMCDOPPPM6NevX+y///6xcePGmDt3bjz77LPx97//PUpKSsrd79prr4327dtXOr8cAAAAAAAAAAAAAAA1Y8iQIfG1r30t5s2blzmzX1b0XFpaGmPHjo2xY8dGRESLFi2iQ4cO0bJly2jRokUkSRKrV6+O1atXx5w5c2LNmjWZ+5ad79/yewAHHHBA3HDDDbX0dLmzYcOGePHFF8vNV/ZcPwAAAAAAAAAAAFDz6k3J8/jx4+NHP/pRfPLJJ9s8zLkjW15fdp/HHnss3nzzzbjpppuid+/e1ZK3rjj88MPjV7/6VRx00EFVWn/llVfGvHnz4rnnnsuaX7duXdx3331x3333Veg+3/rWt+LrX/96lTLIAQAAAAAAAAAAAABQMwoLC2PkyJHxrW99K+bPn59V9Bzx33P5ERGrV6+OKVOmbPPs/ua2dk379u3jz3/+cxQWFlb3Y+xybrnllliyZEnWXOfOnaN///45SgQAAAAAAAAAAABsS16uA1SHV199NS644IJMwXOSJFsteE7TdLuvLZXdJ03TmDdvXpx//vnx2muv1cYjVdnll18e/fv3j+bNm1f5HkmSxNFHHx2///3v45FHHqlywXNERKNGjeLmm2+uciFxkiRx+eWXx7XXXlvlDHIAAAAAAAAAAAAAANSc/fbbLx5//PH4whe+UO5sftm5/M3P+W/tLP/WriuTpml88YtfjEcffTT222+/2nmoHFm7dm1cf/31MXLkyKz5xo0bx4033hh5efXiayAAAAAAAAAAAABQr+TnOsDOev/99+Pyyy+P9evXb/MwZ5Iksf/++0f37t2jY8eO0bJly2jZsmWkaRpr1qyJ1atXx0cffRTTp0+P+fPnZ9aUKXu/fv36uPzyy+P//u//olu3brX6nBU1aNCgGDRoUBQXF8e0adNi0qRJMWXKlJg7d27Mnz8/li1bFqWlpZnrGzduHC1btoz99tsvunfvHj169Ijjjz++Wg++FhQUxK9+9avo169f/O53v4sPPvigQusOO+ywuOaaa+Lwww+XowZyAAAAAAAAAAAAAABUl9133z1uvfXW+Mc//hE333xzLFy4MCKi3Bn/LcfbUlb+3LZt27jqqqviS1/6UvUG3kWsXbs2VqxYEbNnz46xY8fG3//+91i6dGnWNc2bN49bb701evXqlaOUtWPixIkV/vcDAIDqsWTJku1+Pn369Pj3v/9dS2kAAKBh2dHP4wAAAED18Xuv+qXsfOGupk6XPG/cuDGuuuqqKCoq2mq5c4cOHeKMM86IU089Nfbdd98K3XPRokXxt7/9LR577LGYM2dOubLndevWxVVXXRVPPPFE5Ofvun98jRs3jt69e0fv3r3LfbZ+/frYsGFDNGvWLBo3blxrmU488cQ44YQT4o033oiXX345Jk2aFHPmzIm1a9dGRERhYWEceOCBccQRR8TnP//5OOyww+SohRwAAAAAAAAAAAAAANXly1/+cgwcODBeeumlePjhh+Ptt9+O0tLScteVndXf2pctGjVqFEcffXScc8450b9//8jLy6vx3DVtzpw5MWDAgEqvO/744+PnP/95dOjQoQZS7Vo2bdqU6wgAAA3O1n5W3/LzHV0DAABUjZ+1AQAAoPb4vRe1YddtKa6ARx55JGbNmpVVxJymaTRr1iwuu+yyOO+88ypdxLzPPvvEd77znbjgggvigQceiDvuuCOKioqyrpk1a1b83//9X3zzm9+slueobU2bNo2mTZvmZO8kSeK4446L4447Lif7ywEAAAAAAAAAAAAAULPy8vLi85//fHz+85+PdevWxeTJk2PixIkxc+bMWLlyZaxevTpWrlwZERGFhYVRWFgYu+++e3Tt2jV69+4dPXv2jObNm+f4KXInPz8/zjrrrDjzzDPjkEMOyXUcAAAAAAAAAAAAYAfqbMlzmqZxzz33lCt4bt26dYwYMSIOPfTQnbp/fn5+XHDBBdG3b9/4f//v/8XSpUsj4j+lvGV7n3POOVn7AwAAAAAAAAAAAAAA/9W8efPo27dv9O3bN9dR6oyNGzfGo48+GvPmzYtzzz03TjjhhFxHAgAAAAAAAAAAALajzpY8v/XWW/Hpp59mSpbTNI3GjRvHyJEj46CDDqq2fQ455JC455574swzz4ySkpLM/KeffhpvvvlmHHvssdW2FwAAAAAAAAAAAAAAQElJSbz66qvx6quvxrHHHhtDhgyJtm3b5jpWjcrLy8t8RwQAgNrRqFGjHX6+o2sAAICq8bM2AAAA1B6/96pf0jSNTZs25TpGOXW25Pn111/PvE/TNJIkie9+97vVWvBc5qCDDorBgwfHbbfdlnVg8PXXX1fyDAAAAAAAAAAAAAAAbFf79u3jjTfeyJrbtGlTrFmzJpYtWxbTp0+PN998M0aPHh0lJSVZ173xxhtx5plnxv333x+dOnWqzdi16rDDDou8vLxcxwAAaFDmz5+/3c+7d+8e+++/fy2lAQCAhmVHP48DAAAA1cfvveqXTZs2xfjx43Mdo5w6e/pt0qRJWeOCgoI4++yza2y/c845JwoKCrLmJk+eXGP7AQAAAAAAAAAAAAAA9UNeXl60bt0669WmTZs48MAD44gjjohzzjkn7rjjjhgzZkyceuqp5dYvWbIkvv3tb8eaNWtykB4AAAAAAAAAAADYnjpb8jxv3rxIkiTSNI0kSeLoo4+OwsLCGtuvsLAw+vbtm9kvTdOYN29eje0HAAAAAAAAAAAAAAA0LG3atImbbropfvrTn5b77JNPPombb745B6kAAAAAAAAAAACA7amzJc/Lly/PGh9wwAE1vueWe2yZAQAAAAAAAAAAAAAAYGedd955cdZZZ5Wbf/zxx2PZsmU5SAQAAAAAAAAAAABsS50ted6wYUPWuFWrVjW+55Z7FBcX1/ieAAAAAAAAAAAAAABAw/ODH/wg8vPzs+Y2bNgQY8aMyU0gAAAAAAAAAAAAYKvqbMlzkyZNssbLli2r8T2XL1+eNW7cuHGN7wkAAAAAAAAAAAAAADQ8rVu3jj59+pSbnzBhQu2HAQAAAAAAAAAAALapzpY877HHHlnjefPm1fiec+fO3W4GAAAAAAAAAAAAAACA6tKpU6dyc0uWLMlBEgAAAAAAAAAAAGBb6mzJ8wEHHBBpmkaSJJGmabz99tuxcuXKGttv5cqV8dZbb2X2S5Ik2rdvX2P7AQAAAAAAAAAAAAAADVthYWG5ueLi4hwkAQAAAAAAAAAAALalzpY89+zZM2u8cePGePDBB2tsv4cffjg2bty43QwAAAAAAAAAAAAAAADVZdmyZeXm2rRpk4MkAAAAAAAAAAAAwLbU2ZLnz3zmM5n3SZJEmqZx1113xYwZM6p9r5kzZ8add94ZSZJkzR9//PHVvhcAAAAAAAAAAAAAAEBExKxZs8rNKXkGAAAAAAAAAACAXUudLXnu27dvtG3bNjNOkiRKSkriwgsvjKlTp1bbPjNmzIgLL7wwSkpKsub33XffOOaYY6ptHwAAAAAAAAAAAAAAgDILFiyIiRMnlps//PDDc5AGAAAAAAAAAAAA2JY6W/KcJElccMEFkaZp1tyyZcvi3HPPjT//+c+xcePGKt+/tLQ07r333jjnnHPi3//+dyRJEhERaZpGkiRx4YUX7vQzAAAAAAAAAAAAAAAAbM3QoUOzvjMREdGsWbM4/vjjc5QIAAAAAAAAAAAA2Jo6W/IcEXH22WdH165ds+aSJIl169bFTTfdFAMHDowRI0bEggULKnzPhQsXxogRI2LgwIExdOjQWLt2babguez+Xbp0iW984xvV9hwAAAAAAAAAAAAAAEDd98ILL0RJSclO3+f222+PZ599ttz8qaeeGk2bNt3p+wMAAAAAAAAAAADVJz/XAXZGfn5+/P73v49vfOMbUVRUlJlPkiTSNI25c+fGLbfcErfccku0bds2Dj744DjwwAOjZcuW0aJFi0iSJFavXh2rV6+Ojz/+OGbMmBELFy6MiIg0TTP3KpOmaTRv3jxuvvnmyM+v0390AAAAAAAAAAAAAABANRsyZEjccMMN8a1vfSu++MUvxj777FOp9XPmzIkhQ4bE6NGjy322xx57xJVXXlldUQEAAAAAAAAAAIBqUuebig866KC47bbb4rLLLosNGzZk5svKmcvKmhcsWJApcN6Wsms3X7/5Z02aNInbb789unXrVl3xAQAAAAAAAAAAAACAemTBggUxZMiQuPHGG6N3797Rp0+fOOigg6Jbt26xxx57RIsWLaJp06ZRVFQUq1evjo8//jimT58eo0ePjnHjxmV9t6FMQUFBDBkyJFq1alX7DwQAAAAAAAAAAABsV50veY6I+OxnPxsjR46MH/3oRzF//vysgubN32/toOPmtix23nxd+/bt46abborevXtXS2YAAAAAAAAAAAAAAKD+StM0xo8fH+PHj9+p+xQUFMTtt98e/fr1q6ZkAAAAAAAAAAAAQHXKy3WA6nL44YfH3/72tzjrrLMiYuuFzkmSbPe1pbJ7nHXWWfHUU08peAYAAAAAAAAAAAAAAGrN4YcfHo8//nj0798/11EAAAAAAAAAAACAbcjPdYDq1Lx58/j1r38dF1xwQTz00EPx1FNPxerVqzOfb63IeXObF0O3bNkyBg0aFGeffXZ07NixxjIDAAAAAAAAAAAAAAD1w+WXXx7PPfdcvPnmm7Fu3boq3SNJkjjqqKPirLPOii9/+cs7/C4EAAAAAAAAAAAAkFv1quS5TMeOHeO6666LH/7whzFu3LiYOHFiTJgwIWbNmhUrV66MDRs2ZF3fpEmT2H333aNr167Ru3fvOOyww+Koo46KZs2a5egJAAAAAAAAAAAAAACAumbQoEExaNCgKC4ujmnTpsWkSZNiypQpMXfu3Jg/f34sW7YsSktLM9c3btw4WrZsGfvtt1907949evToEccff3zst99+OXwKAAAAAAAAAAAAoDLqZclzmWbNmsXnPve5+NznPpc1X1xcHKtWrYqIiMLCwmjcuHEu4gEAAAAAAAAAAAAAAPVQ48aNo3fv3tG7d+9yn61fvz42bNgQzZo1830GAAAAAAAAAAAAqAfqdcnztjRu3DjatGmT6xgAAAAAAAAAAAAAAEAD07Rp02jatGmuYwAAAAAAAAAAAADVJC/XAQAAAAAAAAAAAAAAAAAAAAAAAAAAAADqIiXPAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFWg5BkAAAAAAAAAAAAAAAAAAAAAAAAAAACgCvJzHQAAAAAAAAAAAAAAAKh/iouL46OPPoqVK1fG6tWrY+3atbFp06Zqu/+gQYOq7V4AAAAAAAAAAAAAVaXkGQAAAAAAAAAAAAAAqBaTJk2KUaNGxTvvvBMffvhhlJaW1theSp4BAAAAAAAAAACAXYGSZwAAAAAAAAAAAAAAYKeMHz8+fvWrX8X7778fERFpmtbofkmS1Oj9AQAAAAAAAAAAACpKyTMAAAAAAAAAAAAAAFBlv/vd7+Lee++NNE2zyp1rqoi5pgukAQAAAAAAAAAAACpDyTMAAAAAAAAAAAAAAFAlQ4cOjZEjR2bGNVXsDAAAAAAAAAAAALCrUvIMAAAAAAAAAAAAAABU2vPPPx8jR47carFzmqaZ940aNYrmzZtHixYtlEADAAAAAAAAAAAA9Y6SZwAAAAAAAAAAAAAAoFJKSkri97//fbn5NE0jSZI4/vjj40tf+lL06tUrDjzwwGjUqFEOUgIAAAAAAAAAAADUPCXPAAAAAAAAAAAAAABApbz++usxd+7cSJIkIv5T7hwR0bFjx7jxxhvjsMMOy2U8AAAAAAAAAAAAgFqj5BkAAAAAAAAAAAAAAKiUV155JfO+rOC5Q4cO8eCDD8aee+6Zq1gAAAAAAAAAAAAAtS4v1wEAAAAAAAAAAAAAAIC6ZcKECVnjJEnil7/8pYJnAAAAAAAAAAAAoMFR8gwAAAAAAAAAAAAAAFTK0qVLI0mSzLh9+/Zx7LHH5jARAAAAAAAAAAAAQG4oeQYAAAAAAAAAAAAAACpl+fLlERGRpmkkSRJHHnlkjhMBAAAAAAAAAAAA5IaSZwAAAAAAAAAAAAAAoFIaNWqUNd5rr71ylAQAAAAAAAAAAAAgt5Q8AwAAAAAAAAAAAAAAldKqVauscZqmuQkCAAAAAAAAAAAAkGNKngEAAAAAAAAAAAAAgErp0qVLVrHzsmXLcpgGAAAAAAAAAAAAIHeUPAMAAAAAAAAAAAAAAJVy9NFHR0REkiSRpmlMnTo1x4kAAAAAAAAAAAAAckPJMwAAAAAAAAAAAAAAUClf+MIXIi/vv19JmDFjRixevDiHiQAAAAAAAAAAAAByQ8kzAAAAAAAAAAAAAABQKe3bt49TTjkl0jTNzI0YMSKHiQAAAAAAAAAAAAByQ8kzAAAAAAAAAAAAAABQaT/5yU+iRYsWkSRJpGkaf/nLX2LSpEm5jgUAAAAAAAAAAABQq5Q8AwAAAAAAAAAAAAAAlbbPPvvE0KFDI0mSSJIkSkpK4uKLL45Zs2blOhoAAAAAAAAAAABArVHyDAAAAAAAAAAAAAAAVMlJJ50UN9xwQzRq1CiSJInly5fHmWeeGQ899FCUlpbmOh4AAAAAAAAAAABAjcvPdYDakKZprF27NtauXVuth0TbtWtXbfcCAAAAAAAAAAAAAIC6aNCgQbHPPvvEj370o1i6dGmsX78+fvOb38SIESPizDPPjL59+0bPnj2jadOmuY4KAAAAAAAAAAAAUO3qXclzcXFxvPzyy/HOO+/E9OnTY/bs2bFq1apq3ydJkpg2bVq13xcAAAAAAAAAAAAAAOqCk046KWtcUlISaZpGkiSRpmksWrQohg8fHsOHD4+IiGbNmkVhYWE0atRop/dOkiRefPHFnb4PAAAAAAAAAAAAwM6qNyXPK1asiDvuuCP+/ve/x+rVqzPzaZrmMBUAAAAAAAAAAAAAANRPn3zySabQOeI/xctlyt5vfqZ/3bp1sW7dumrZe/O9AAAAAAAAAAAAAHKpXpQ8P/3003H99dfHypUry5U618TBTcXRAAAAAAAAAAAAAADwH2VFz2Vn7TcvfXamHwAAAAAAAAAAAKjv6nzJ80MPPRS//e1vY9OmTRFRM6XOAAAAAAAAAAAAAADAtjnLDwAAAAAAAAAAADRUdbrk+fXXX4/rr78+IrZ+IDRN09qOBAAAAAAAAAAAAAAADYZz+wAAAAAAAAAAAEBDV2dLnouLi+OXv/xlRJQveE7TNNq1axf9+/eP7t27R4cOHaJly5bRvHnzyMvLy0FaAAAAAAAAAAAAAACoX4YMGZLrCAAAAAAAAAAAAAA5V2dLnseMGRPz5s3LFDynaRoREe3atYuf/OQnMWDAgFzGAwAAAAAAAAAAAACAeu3000/PdQQAAAAAAAAAAACAnKuzJc8vvPBC5n1ZwXOHDh3igQceiL333jtXsQAAAAAAAAAAAAAAAAAAAAAAAAAAAIAGIi/XAapq+vTpkSRJZpwkSdxwww0KngEAAAAAAAAAAAAAAAAAAAAAAAAAAIBaUWdLnpcsWZI17tKlS/Tp0ydHaQAAAAAAAAAAAAAAAAAAAAAAAAAAAICGps6WPK9duzYiItI0jSRJFDwDAAAAAAAAAAAAAAAAAAAAAAAAAAAAtarOljy3aNEia9y6descJQEAAAAAAAAAAAAAAAAAAAAAAAAAAAAaojpb8tyuXbtI0zQzLioqymEaAAAAAAAAAAAAAAAAAAAAAAAAAAAAoKGpsyXPPXr0iIiIJEkiImLx4sW5jAMAAAAAAAAAAAAAAAAAAAAAAAAAAAA0MHW25Ll///6Z92maxltvvZXDNAAAAAAAAAAAAAAAAAAAAAAAAAAAAEBDk5/rAFV1/PHHR9u2bePTTz+NiIilS5fGG2+8Eccee2yOkwEAAAAAAAAAAAAAAFtat25drF69OkpLS6vlfvvss080atSoWu4FAAAAAAAAAAAAUFV1tuS5oKAgvve978V1110XSZJEmqZx0003xeOPPx5JkuQ6HgAAAAAAAAAAAAAANEjFxcXxyiuvxIQJE2LixIkxe/bsai13johIkiSefvrp6NixY7XdEwAAAAAAAAAAAKAq6mzJc0TE1772tXjxxRdjzJgxkSRJTJ8+Pa699toYMmRIrqMBAAAAAAAAAAAAAECDsnjx4njggQfi0UcfjZUrV2bm0zTNYSoAAAAAAAAAAACAmpWX6wA76/e//3307Nkzc+hz1KhRcfnll8eKFStyGwwAAAAAAAAAAAAAABqIF154Ib7yla/E3XffHStWrIg0TTOvJEmq9QUAAAAAAAAAAACwK6nzJc8tWrSIkSNHxmc/+9lM0fMLL7wQX/rSl2L48OGxePHiHCcEAAAAAAAAAAAAAID6a9iwYXH55ZfHypUrt1rqXGbz4uey8//bUplrAQAAAAAAAAAAAHIpP9cBqkOLFi1ixIgRcffdd8edd94Z69evj6VLl8awYcNi2LBhccABB0SPHj2idevWUVhYGHl51dNtfemll1bLfQAAAAAAAAAAAAAAoC56+umnY9iwYRERWYXOEVGhIuet2bwcuuyaLa9V/AwAAAAAAAAAAADsKupFyXPEfw5xnn322bFs2bIYOXJkJEmSObQ5Z86cmDt3brXvqeQZAAAAAAAAAAAAAICGavHixfHTn/50q+XO7dq1i6985SvRt2/f2GuvveKJJ57IOuufJEm8+OKLUVJSEitXrozly5fH1KlTY/z48fHmm2/Gxo0bs8qeDz300Pj5z38erVu3zuyzzz771OrzAgAAAAAAAAAAAGxNvSh5TtM0HnjggRg2bFisXr06c4hz84OiZYXP1WXLQ6gAAAAAAAAAAAAAANCQ3HPPPbF+/frM+fqyc/sXXHBBfP/7348mTZpkri0sLCy3fr/99ssan3jiiRHxn/Lohx56KO69994oLi6OiIgpU6bEpZdeGiNGjIiDDjqoJh4HAAAAAAAAAAAAoErych1gZ61Zsya+853vxJAhQ2LVqlWRpmnWq0ySJNX2AgAAAAAAAAAAAACAhmz16tXx17/+NavgOUmS+O53vxtXX311VsFzZe29997xgx/8IB5//PHo1q1b5rsBixYtinPPPTc++OCDankGAAAAAAAAAAAAgOpQp0ueS0pKYvDgwfHaa69lDoRuq5B5y/LnnXkBAAAAAAAAAAAAAEBDNm7cuCgqKoqIyJyz79GjR1x22WXVtkeXLl3igQceiIMOOigiIpIkiVWrVsV3v/vd2LBhQ7XtAwAAAAAAAAAAALAz8nMdYGcMHz48xo0bl1XmXKbskGh+fn60a9cuWrRoEc2bN9/qtQAAAAAAAAAAAAAAQMWNGzcua5wkSfy///f/qv3MfmFhYYwYMSIGDhwY69ati4iIuXPnxp133hk/+MEPqnUvAAAAAAAAAAAAgKqosyXPS5YsiXvuuafcAdA0TWPvvfeOs846K0466aTo0qVLFBQU5CglAAAAAAAAAAAAAADUP5MnT84a77bbbnHSSSfVyF777LNPDB48OH7/+99HkiSRpmk8/PDDcckll0SzZs1qZE8AAAAAAAAAAACAisrLdYCqevLJJ6OkpCQzTtM0IiL+53/+J55//vm49NJLo3v37gqeAQAAAAAAAAAAAACgmi1dujRTuJwkSfTo0SMaNWpUqXts2LChwteeddZZkZ+fnxmvWbMmXn755UrtBwAAAAAAAAAAAFAT6mzJ85tvvpl5X3Yo9LTTTotf/epX0bRp0xwmAwAAAAAAAAAAAACA+m3lypVZ406dOm33+ry88l9fqEzJc2FhYfTp0yfSNM3MjR07tsLrAQAAAAAAAAAAAGpKnS15njVrViRJkhkXFBTET37ykxwmAgAAAAAAAAAAAACAhmHVqlVZ48LCwu1e37x583Jza9eurdSeHTp0iIiIJEkiTdOYOXNmpdYDAAAAAAAAAAAA1IQ6W/K8cuXKiIhI0zSSJIm+fftGq1atchsKAAAAAAAAAAAAAAAagIKCgu2Ot9SiRYtyc59++mml9txzzz2zxp988kml1gMAAAAAAAAAAADUhDpb8pwkSda4U6dOOUoCAAAAAAAAAAAAAAANy5alzatXr97u9bvttlu5uYULF1Zqz40bN2aN165dW6n1AAAAAAAAAAAAADWhzpY8FxYWZo23PCAKAAAAAAAAAAAAAADUjNatW2eNd1TyfMABB5SbmzJlSqX23LIUOk3TSq0HAAAAAAAAAAAAqAl1tuS5Y8eOWQcyly1blsM0AAAAAAAAAAAAAADQcJSd6U+SJCIi5s6du93rO3fuHPn5+ZlxmqbxzjvvVGrP9957L7NfRESrVq0qtR4AAAAAAAAAAACgJtTZkufDDz88IiJzQHPOnDm5jAMAAAAAAAAAAAAAAA1Gp06dMu/TNI33339/u9cXFBREp06dsoqhJ0+eHLNnz67Qfv/6179iwYIFmf0iIvbaa6+qRAcAAAAAAAAAAACoVnW25HnAgAGZ92maxjvvvBPr1q3LYSIAAAAAAAAAAAAAAGgYDjvssKzxmjVrYu7cudtd079//3JzQ4cO3eFeq1evjt/85jeZcuiIiCRJ4ogjjqhgWgAAAAAAAAAAAICaU2dLnnv06BF9+vTJjEtKSuLBBx/MYSIAAAAAAAAAAAAAAGgY+vTpE40aNcqae/nll7e75itf+UrmfZIkkaZpvPbaa3HVVVfFmjVrtrrmo48+ivPOOy/mzZtX7rPjjz++CskBAAAAAAAAAAAAqld+rgPsjOuuuy7OPPPMKC0tjTRN46677opTTjklOnTokOtoAAAAAAAAAAAAAABQb7Vo0SJ69uwZEyZMiCRJIiLixRdfjPPPP3+bazp37hzHHHNMvPnmm5EkSabo+emnn45XXnkl+vfvH127do3CwsJYsWJFvPvuu/Haa69FaWlp5h5pmkaSJNGxY8c44YQTavoxAQAAAAAAAAAAAHaoTpc8d+/ePa655pq4/vrrI0mSWLduXVxwwQXx4IMPRrt27XIdDwAAAAAAAAAAAAAA6q0BAwbEhAkTIuI/5cvjx4+POXPmRIcOHba55mc/+1mcdtppmeLmsqLnVatWxVNPPVXu+jRNM9dt7sorr6ympwAAAAAAAAAAAADYOXm5DrCzzjnnnPjRj34UEf85tLlgwYI47bTTYtSoUbkNBgAAAAAAAAAAAAAA9djAgQMzJc0REaWlpfG///u/213TuXPnuPrqqzNrIv7zXYCy+2z5Kvtsc+edd16cfPLJ1f9AAAAAAAAAAAAAAFWQn+sA1eGiiy6Kjh07xnXXXRfLly+P1atXxzXXXBPDhg2LM844I44++ujo2bNnNG7cONdRAQAAAAAAAAAAAACgXth3333j6quvjqVLl2bmKnJu/9xzz43S0tIYOnRo1vyWZc6bKyuFPv/88+PHP/5xFRMDAAAAAAAAAAAAVL86XfJ80kknZY1LS0sjTdNIkiTSNI358+fH7bffnvm8efPmUVhYGHl5eTu9d5Ik8eKLL+70fQAAAAAAAAAAAAAAoK46//zzq7zuiCOOiOuvvz4mT56c9VnZdwI216VLl/j+978fJ598clWjAgAAAAAAAAAAANSIOl3y/Mknn2Qd3kySJPNZ2fvND3auXbs21q5dWy17b74XAAAAAAAAAAAAAABQOb169YpHH300ZsyYEaNHj44pU6bE0qVLY/ny5dG8efNo3bp1HHLIIfGZz3wm+vbt6xw/AAAAAAAAAAAAsEuq0yXPZcqKnssKnTcvfa6JQ5ybF0cDAAAAAAAAAAAAAABVd/DBB8fBBx+c6xgAAAAAAAAAAAAAVVIvSp4jokbKnAEAAAAAAAAAAAAAAAAAAAAAAAAAAAC2pc6XPKdpmusIAAAAAAAAAAAAAAAAAAAAAAAAAAAAQANUp0uehwwZkusIAAAAAAAAAAAAAAAAAAAAAAAAAAAAQANVp0ueTz/99FxHAAAAAAAAAAAAAAAAAAAAAAAAAAAAABqovFwHAAAAAAAAAAAAAAAAAAAAAAAAAAAAAKiL8nMdAAAAAAAAAAAAAAAAqL9Wr14d06ZNi2XLlsWqVati5cqVUVxcHBERxxxzTBx55JE5TggAAAAAAAAAAABQdUqeAQAAAAAAAAAAAACAajVp0qR4/PHH4913340PP/ww0jTd6nX5+fnbLXlevHhxzJw5M2uubdu20blz52rNCwAAAAAAAAAAAFBVSp4BAAAAAAAAAAAAAIBqMWbMmLjrrrti4sSJERHbLHeOiEiSpEL3HDx4cGzcuDEzPuigg2LUqFE7lRMAAAAAAAAAAACguuTlOgAAAAAAAAAAAAAAAFC3FRUVxU9/+tMYPHhwTJw4MdI0zRQ8J0lS7lVRe++9d3z5y1/O3C9N03j//fdjxowZNfUoAAAAAAAAAAAAAJWi5BkAAAAAAAAAAAAAAKiyBQsWxOmnnx5PPPFEpoh5a4XOmxc/V8aFF14YEZF1r6eeemrngwMAAAAAAAAAAABUAyXPAAAAAAAAAAAAAABAlSxfvjzOO++8+Pjjj7PKnSP+W+pcUFAQ7du3j169elVpj65du0bXrl0j4j9Fz2maxiuvvFJtzwAAAAAAAAAAAACwM/JzHQAAAAAAAAAAAAAAAKibrrrqqpg/f36m2DkiMsXOp556apx++unRq1evaNy4cUREHHzwwVXa55RTTolZs2Zl9vnwww9j8eLFsffee+/8QwAAAAAAAAAAAADshDpd8rxgwYKc7t+uXbuc7g8AAAAAAAAAAAAAALny9NNPx+uvv16u4Pmwww6Lm2++Ofbff/9q26t///4xbNiwrLm33347vvzlL1fbHgAAAAAAAAAAAABVUadLnvv37591GLQ2JUkS06ZNy8neAAAAAAAAAAAAAACQa8OHD8+8T9M0kiSJE044IYYPHx75+dX7dYVu3bpFQUFBbNy4MTM3c+bMat0DAAAAAAAAAAAAoCrych1gZ6VpmrMXAAAAAAAAAAAAAAA0ROPHj48PP/wwkiTJzB1wwAFx2223VXvBc0REfn5+dO3aNess/4cffljt+wAAAAAAAAAAAABUVp0veU6SpNZfAAAAAAAAAAAAAADQkI0ZMybzPk3TSJIkrrrqqmjatGmN7dmxY8eI+M/3CNI0jTlz5tTYXgAAAAAAAAAAAAAVlZ/rALuqNE2zxsqdAQAAAAAAAAAAAADgPyZOnJg13n333ePkk0+u0T0LCwuzxqtWrarR/QAAAAAAAAAAAAAqos6XPG9Zxlxdykqdy+5fU/sAAAAAAAAAAAAAAEBdM3fu3EiSJNI0jSRJom/fvplz+DVly5LnNWvW1Oh+AAAAAAAAAAAAABVRp0ueX3rppWq5T2lpaaxYsSKWL18ekyZNirfffjvGjRsXEf8te+7SpUv85je/iTZt2lTLngAAAAAAAAAAAAAAUFctX748a9y2bdsa3zM/P/srEOvXr6/xPQEAAAAAAAAAAAB2pE6XPO+3337Vdq8DDjggIiJOOOGEiIiYNWtW3HbbbfHiiy9GkiQxe/bsuOyyy+Luu++Obt26Vdu+AAAAAAAAAAAAAABQ1xQXF2eNW7RoUeN7rly5MmtcUFBQ43sCAAAAAAAAAAAA7EhergPsqrp27RrDhg2LX/3qV5GX958/psWLF8e5554bH330UY7TAQAAAAAAAAAAAABA7jRp0iRrvGUBc01YtmxZ1rg2iqUBAAAAAAAAAAAAdkTJ8w78z//8T1x33XWRpmkkSRIrV66MSy65JNavX5/raAAAAAAAAAAAAAAAkBOtWrXKGi9evLjG95wyZUokSZIZ77vvvjW+JwAAAAAAAAAAAMCOKHmugG984xvxuc99LtI0jYiIuXPnxu23357jVAAAAAAAAAAAAAAAkBv7779/pGkaSZJEmqbx7rvv1uh+ixYtirlz50ZEZPbt3Llzje4JAAAAAAAAAAAAUBFKnivoiiuuiIjIHEB95JFHYsWKFbkNBQAAAAAAAAAAAAAAOdCjR4+s8dKlS2PSpEk1tt9jjz1Wbq5Xr141th8AAAAAAAAAAABARSl5rqAePXpEu3btMuP169fHs88+m8NEAAAAAAAAAAAAAACQG8ccc0y5ubvvvrtG9lq7dm08+OCDkSRJ1vxnP/vZGtkPAAAAAAAAAAAAoDKUPFdCnz59Ik3TzMHQ119/PceJAAAAAAAAAAAAAACg9h133HFRWFgYERFJkkSapvHCCy/ESy+9VO17/exnP4vly5dHRGTO9Hfv3j0OOOCAat8LAAAAAAAAAAAAoLKUPFdCmzZtMu/TNI2ZM2fmMA0AAAAAAAAAAAAAAORGQUFBnHXWWZGmaUT8t+j5mmuuicmTJ1fbPrfeems888wzkSRJ1vx5551XbXsAAAAAAAAAAAAA7Awlz5XQrFmzrPGSJUtylAQAAAAAAAAAAAAAAHLrwgsvjN133z0zTpIkVq1aFeedd16MGjVqp+69bNmyuOKKK+JPf/pTpuA5TdNIkiQ6d+4cp5566k7dHwAAAAAAAAAAAKC6KHmuhBUrVmSNi4uLcxMEAAAAAAAAAAAAAAByrHXr1vHjH/840jSNiP+WMBcVFcU111wTZ5xxRvztb3+LNWvWVPies2bNiptvvjlOPvnkeP755zP3LpOXlxe//OUvIy/P1yEAAAAAAAAAAACAXUN+rgPUJVOmTMkat2rVKjdBAAAAAAAAAAAAAABgF3DGGWfE+PHj47HHHoskSSIiIkmSSNM0pk6dGldffXXk5+dHly5dolOnTuXWjx8/Pm6++eaYN29eTJ48ORYsWBARkSl3LrtnWYH0ZZddFkceeWQtPR0AAAAAAAAAAADAjil5rqA5c+bE5MmTM4dNIyL22GOPHKcCAAAAAAAAAAAAAIDc+vWvfx3r1q2LZ555plzRc5qmUVJSEtOnT48ZM2ZExH8LnNM0jVdeeSVeeeWVrPmy9Vs655xz4pJLLqnpxwEAAAAAAAAAAAColLxcB6gLNm3aFNdff325A6MHH3xwDlMBAAAAAAAAAAAAAEDu5eXlxR/+8Ie4+OKLs+aTJMm8IrJLnMuUFUGnaVru+rLPkySJyy+/PK677rqafRAAAAAAAAAAAACAKlDyvANr166Nq666Kl577bWsg6IREZ/73OdylAoAAAAAAAAAAAAAAHYtP/jBD+KBBx6ILl26ZIqby2xe4Lz52fytzUX8t/y5Y8eOcd9998V3v/vdWnsOAAAAAAAAAAAAgMrIz3WAXdWCBQvimWeeifvvvz+WLFlS7vPCwsLo169fDpIBAAAAAAAAAAAAAMCu6cgjj4y//e1v8c9//jMeeeSReOedd8qVPW/+z81tft1BBx0U5557bpx++unRqFGjmg8OAAAAAAAAAAAAUEV1uuT5mmuuqbZ7pWka69atixUrVsSHH34YS5cuzcxH/PcAaZqmkSRJDB48OFq0aFFt+wMAAAAAAAAAAAAAQH2QJEkMHDgwBg4cGIsWLYrXXnstJkyYEO+//34sWLAgli5dmlXo3KxZs2jXrl107tw5evfuHSeccEJ07tw5h08AAAAAAAAAAAAAUHF1uuT5ySefzJQvV6fND4tu7f7HHHNMfPOb36z2fQEAAAAAAAAAAAAAoD7ZZ5994owzzogzzjgja379+vVRWloaTZo0ifz8Ov3VBgAAAAAAAAAAAKCBqxcnITcvZa4O2yqOTtM0jjjiiLjzzjsdIgUAAAAAAAAAAAAAgCpq2rRpriMAAAAAAAAAAAAAVIt60VS8rVLm6pKmaTRp0iS+973vxUUXXRSNGjWq0f0AAAAAAAAAAAAAAAAAAAAAAAAAAACAXV+dL3lO07RG79+5c+cYNGhQnH766dGmTZsa3QsAAAAAAAAAAAAAAAAAAAAAAAAAAACoO+p0yfPpp59ebfdKkiR22223aNGiRey+++7RtWvX6NGjR7Rq1ara9gAAAAAAAAAAAAAAAAAAAAAAAAAAAADqjzpd8jxkyJBcRwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAaqLxcBwAAAAAAAAAAAAAAAAAAAAAAAAAAAACoi5Q8AwAAAAAAAAAAAAAAAAAAAAAAAAAAAFRBfq4DAAAAAAAAAAAAAAAA9cfGjRvj/fffj+nTp8fs2bNjxYoVsWbNmli7dm2UlpZWyx5JksR9991XLfcCAAAAAAAAAAAA2BlKngEAAAAAAAAAAAAAgJ02ZcqUePTRR+PZZ5+NVatW1dg+aZpGkiQ1dn8AAAAAAAAAAACAylDyDAAAAAAAAAAAAAAAVNmyZcvid7/7XTz11FMR8Z8S5pqi3BkAAAAAAAAAAADY1Sh5BgAAAAAAAAAAAAAAqmTOnDlxwQUXxMKFCzPlzoqYAQAAAAAAAAAAgIZEyfNmFi5cGOPHj49///vfUVxcHHvvvXcceOCB0atXr1xHAwAAAAAAAAAAAACAXcqSJUvim9/8ZixZsiQitl7uXFb8DAAAAAAAAAAAAFBfKXmOiLFjx8att94akydP3urnbdu2jXPPPTfOP//8rR46BQAAAAAAAAAAAACAhua3v/1tLFmypNw5+7Ji58LCwujWrVt06NAhWrZsGc2bN4+8vLxcRAUAAAAAAAAAAACoMXW65Hn06NExfPjwrLmTTjopBg8eXOF73HHHHXHnnXdGxH8Pkm5pwYIF8bvf/S6eeeaZ+OMf/xht2rSpemgAAAAAAAAAAAAAAKjjpk2bFs8+++xWC55PPPHEuOCCC+Koo45S6gwAAAAAAAAAAADUe3W65PnJJ5+MKVOmRJIkkaZpJEkSv/jFLyq8/qGHHsoqid7ycOnm0jSNyZMnx0UXXRQPPvhgtGzZcqeyAwAAAAAAAAAAAABAXfXcc89ljdM0jfz8/Ljhhhvi1FNPzVEqAAAAAAAAAAAAgNqXl+sAVVVcXByvvPJKpuA5IuKwww6Lnj17Vmj9vHnz4sYbb4wkSTKv7Sn7fObMmZUqkgYAAAAAAAAAAAAAgPrmjTfeyLxP0zSSJIkrrrhCwTMAAAAAAAAAAADQ4NTZkuepU6fG+vXrM+MkSeLzn/98hdcPGzYsSkpKIiIyJdFl7/Pz86NNmzZRUFCQ9VlZofQ///nPePvtt6vhKQAAAAAAAAAAAAAAoO5ZtGhRJEmSGe++++5x4YUX5jARAAAAAAAAAAAAQG7U2ZLnCRMmlJvr379/hdYuW7Ysnn766cyB0rLy5sLCwhg6dGi8++678eqrr8b48ePjtttuizZt2pS7x4gRI3YqPwAAAAAAAAAAAAAA1FXLli2LiIg0TSNJkvjMZz4TjRo1ynEqAAAAAAAAAAAAgNpXZ0ueZ82alTUuLCyMjh07VmjtP/7xj9i4cWNmnKZp5Ofnx9133x2nnXZaNG7cOCIiGjVqFKeccko89NBDUVhYGBH/LYR+4403YsmSJdX0NAAAAAAAAAAAAAAAUHeUnbsv065duxwlAQAAAAAAAAAAAMitOlvyPH/+/Mz7JEmiW7duFV773HPPZd6naRpJksRXvvKV6Nmz51avP+CAA2Lw4MGRpmlmbtOmTTFmzJjKBwcAAAAAAAAAAAAAgDpuzz33zBo3atQoR0kAAAAAAAAAAAAAcqvOljwvWLAgkiTJFC936NChQuuKiopi4sSJkSRJ1vzZZ5+93XVnnHFG5OfnZ81NnTq1EokBAAAAAAAAAAAAAKB+6NatW+Y8f0TE0qVLc5gGAAAAAAAAAAAAIHfqbMnzmjVrssYtW7as0Lp33nknNm7cmDXXtm3b6Nmz53bXtWzZMg499NBI0zRTED1jxoxKJAYAAAAAAAAAAAAAgPqhb9++ERGRJEmkaRpTp07NcSIAAAAAAAAAAACA3KizJc/r16/PGu+2224VWvfuu+9m3pcVNp944okVWtu5c+estYsXL67QOgAAAAAAAAAAAAAAqE+++MUvRn5+fmY8ffr0+PTTT3OYCAAAAAAAAAAAACA36mzJc0lJSda4tLS0Quvee++9cnNHH310hdbuueeeWeO1a9dWaB0AAAAAAAAAAAAAANQnbdq0ia997WuRpmkkSRIREcOGDctxKgAAAAAAAAAAAIDaV2dLnnfbbbes8Zo1a3a4pri4OCZOnJg5QFqmoiXPjRs3zhoreQYAAAAAAAAAAAAAoKG64oorYu+9946IiDRN48knn4x//etfOU4FAAAAAAAAAAAAULvqbMlzixYtssbz58/f4Zp33303NmzYkDXXoUOHaN26dYX23LLUuaCgoELrAAAAAAAAAAAAAACgvtljjz3illtuiaZNm0aSJFFaWhpXXHFFvPzyy7mOBgAAAAAAAAAAAFBr6mzJ89577x1pmkaSJJGmaUyfPn2Ha1566aXM+7K1Rx99dIX3XLVqVda4efPmFQ8MAAAAAAAAAAAAAAD1TJ8+feLOO++M3XbbLZIkifXr18f3vve9uO6662L+/Pm5jgcAAAAAAAAAAABQ4/JzHaCqDjnkkJgwYUJmvGjRopg6dWr06NFjq9eXlJTEP//5z0iSJGu+b9++Fd5zyZIlWePdd9+94oEBAAAAAAAAAAAAAKAeOvbYY+PRRx+NK664ImbOnBlpmsbjjz8eTz75ZBxzzDFx1FFHRY8ePWLPPfeMli1bRqNGjapl33bt2lXLfQAAAAAAAAAAAAB2Rp0tee7Zs2e5ueHDh8edd9651esff/zxWLp0aVbJc5Ikceyxx1Z4z6lTp0aSJJGmaSRJEu3bt698cAAAAAAAAAAAAAAAqGc6duwYTzzxRPzhD3+IP//5zxERUVpaGmPHjo2xY8dW+35JksS0adOq/b4AAAAAAAAAAAAAlfX/2bv3KCvren/gn73ngjosBMwgxBioVMrUQC1vmR2VNDs6p4ViKsMCXIridaWkaD+zVprnlEeF1EQLytRupLUEOh7T0OSkjokXpI5clOQyCkhc57L374+zZDXtkZnZs/c87D2v11qsmM/zPJ/ve4q1mmH2fpNOOkC+/uVf/iWqq6sjInYWL//+97+P7373u5HNZtvcu3jx4viP//iPnQXP75U0f/rTn46BAwd26rw1a9bEO++802am5BkAAAAAAAAAAAAAACLWrl0bX//61+P+++/f+dr9917rX6xfAAAAAAAAAAAAALuDyqQD5Ktfv35x4oknxqOPPhqpVGrniz9nzZoV8+bNi+OOOy769esXr7/+ejzxxBPR0tKy84Wi7xk7dmynz1u0aFHO7MADD+z25wEAAAAAAAAAAAAAAKXskUceiRtuuCG2bdvWpnz5vdf6F5qCZwAAAAAAAAAAAGB3UrIlzxERV155ZTz++OOxY8eOiIidRc+rVq2KBx98cOd92Wx25wtD3/v9xz72sTj11FM7fdb8+fNzZp/61Ke6+RkAAAAAAAAAAAAAAEDp+tGPfhTf+c53dhYvv/e6/mKUOwMAAAAAAAAAAADsjtJJB+iOoUOHxlVXXbXzxaAR//eC0PdeFPrer39+cWhlZWV84xvf6PQ5mzZtiqeeeqrNnn79+sVHP/rR7n8SAAAAAAAAAAAAAABQgp566qmdBc/vvZY/Inb+5z++rr+QvwAAAAAAAAAAAAB2J5VJB+iuc845J7Zu3Rrf+9732sz/udg54v9eIFpZWRnf/OY341Of+lSnz/jVr34VLS0tO8ujU6lUfPrTn+52dgAAAAAAAAAAAAAAKEUtLS3xjW98Y+dr7P9RNpuN/fbbLz7/+c/HQQcdFLW1tdG3b9/Ya6+9Ip1OJ5QYAAAAAAAAAAAAoDhKvuQ5IuL888+P0aNHx7e+9a149dVX3/e+Qw45JL72ta/FqFGjOr27paUlfvSjH+W86PSEE07IOy8AAAAAAAAAAAAAAJSyxx57LN58882dr7XPZrMRETFkyJC49tpr48QTT0wyHgAAAAAAAAAAAECPKYuS54iIUaNGxa9+9atYsWJFPP3007FmzZp49913Y6+99oohQ4bEkUceGQcddFCX9y5cuDDS6XR86EMf2jlLpVJx/PHHFzI+AAAAAAAAAAAAAACUjP/6r//a+fv3Cp6HDx8ec+bMiX333TepWAAAAAAAAAAAAAA9rmxKnt9TW1sbtbW1Bdt3wgknxAknnFCwfQAAAAAAAAAAAAAAUOpefvnlSKVSOz9OpVLxrW99S8EzAAAAAAAAAAAA0Oukkw4AAAAAAAAAAAAAAACUlrfffrvNxx/96Edj9OjRCaUBAAAAAAAAAAAASI6SZwAAAAAAAAAAAAAAoEt27NgRERHZbDZSqVSMGjUq4UQAAAAAAAAAAAAAyVDyDAAAAAAAAAAAAAAAdElNTU2bj/fZZ5+EkgAAAAAAAAAAAAAkS8kzAAAAAAAAAAAAAADQJUOGDGnz8bZt2xJKAgAAAAAAAAAAAJAsJc8AAAAAAAAAAAAAAECXHHTQQZHNZiOVSkVExLp16xJOBAAAAAAAAAAAAJAMJc8AAAAAAAAAAAAAAECXHH/88Tt/n81m43/+538STAMAAAAAAAAAAACQHCXPAAAAAAAAAAAAAABAl3z+85+PD3zgAzs/fuedd+KPf/xjgokAAAAAAAAAAAAAkqHkGQAAAAAAAAAAAAAA6JLq6uq46KKLIpvNRiqVimw2G9/5zncik8kkHQ0AAAAAAAAAAACgRyl5BgAAAAAAAAAAAAAAuuwrX/lKHHvssTuLnv/yl7/ENddck3QsAAAAAAAAAAAAgB6l5BkAAAAAAAAAAAAAAMjLbbfdFp/85Ccjm81GNpuNRx55JC655JLYsGFD0tEAAAAAAAAAAAAAeoSSZwAAAAAAAAAAAAAAIC81NTXx4x//OE4++eSIiMhms/HYY4/FF7/4xZgxY0asXbs24YQAAAAAAAAAAAAAxVWZdAAAAAAAAAAAAAAAAKD0zJgxY+fvDzjggPjrX/8ay5cvj2w2G+vXr4+ZM2fGzJkzY//994+DDz44Bg4cGP369Yt0Ol2Q86dOnVqQPQAAAAAAAAAAAADdoeQZAAAAAAAAAAAAAADoshkzZkQqlcqZvzfLZrMREfHGG2/Em2++WfDzlTwDAAAAAAAAAAAAuwMlz71ENpuNVatWxV//+td4++23Y9OmTdHa2hr9+vWLvffeO4YNGxYHHnhgVFb2zB+JDRs2xCuvvBIrV66MzZs3RyqVin79+kVtbW0cfPDB0bdvXzkSyAEAAAAAAAAAAAAA0FXvlTn/48epVKpNAfQ/39Nd7ZVLAwAAAAAAAAAAACRByXOZymQy8eKLL8aiRYti0aJFsXjx4ti6desun9lzzz3jsMMOi7Fjx8bJJ58cVVVVBc2UzWZj3rx58dOf/jSef/75yGQy7d5XVVUVRx99dIwfPz6OPfbYgmaQAwAAAAAAAAAAAACgsDpTuFzIUuZCF0YDAAAAAAAAAAAAdIeS5zLT0NAQv/3tb+N3v/tdNDY2dunZbdu2xTPPPBPPPPNMfPCDH4zrrrsuxowZU5BcK1asiGnTpsWf//znDu9tbm6OJ598Mp588sn43Oc+F9/+9rdjn332kaMIOQAAAAAAAAAAAAAAukvpMgAAAAAAAAAAANCbKXkuM9OmTYs33nij23vWrVsXl156aZx++unxrW99K6qrq/Pe9dxzz8WUKVNi06ZNXX72iSeeiC9/+ctx3333xYgRI/LOIAcAAAAAAAAAAAAAQGEdccQRSUcAAAAAAAAAAAAASJyS515m2LBhsd9++8XAgQNjr732infffTeWLl0aK1asaPf+hx9+OLZv3x633nprVFRUdPm81157LS644ILYvHlzzrXq6uo4+OCDY//994/m5uZ4880345VXXolMJtPmvtWrV8eECRPi5z//eQwaNKjLGeQAAAAAAAAAAAAAACi8H//4x0lHAAAAAAAAAAAAAEickucyV11dHSeddFKcdNJJcfjhh8e+++7b7n1vvvlm3HvvvfHQQw/llAovWLAgbr/99rjiiiu6dPa2bdvi8ssvzyk0TqVSMWHChJg0aVJOnpUrV8bMmTPj4YcfbjNfu3ZtfPWrX405c+ZEKpWSoxs5AAAAAAAAAAAAAAAAAAAAAAAAAAAAKIx00gEojv322y+mT58eCxcujO9973txyimnvG/Bc0TE/vvvHzfccEPcd9990bdv35zr9957b6xYsaJLGe66665Yvnx5m1lFRUXcfPPN8bWvfa3dPMOGDYtbbrklrrzyypxrf/rTn+IXv/hFlzLIAQAAAAAAAAAAAAAAAAAAAAAAAAAAQLEoeS4zgwcPjq9//esxf/78GD9+fPTv379Lzx911FExY8aMSKfb/tFobm6O++67r9N73nnnnfjhD3+YM580aVKcccYZHT5/wQUXxKmnnpozv+2226KpqUmOPHMAAAAAAAAAAAAAAAAAAAAAAAAAAABQOEqey8zs2bPjnHPOierq6rx3HHXUUXH66afnzB9//PHIZrOd2vGTn/wkduzY0Wb24Q9/OKZOndrpHNdff33069evzayxsTF++9vfdnqHHAAAAAAAAAAAAAAAAAAAAAAAAAAAABSLkucyk04X5n/SM888M2fW2NgYy5Yt6/DZbDYbDz/8cM584sSJ0adPn05nGDhwYIwdOzZnPnfu3E49LwcAAAAAAAAAAAAAAAAAAAAAAAAAAADFpOSZdh122GFRWVmZM1+3bl2Hzy5evDj+9re/tZlVVVXFF7/4xS7nqKury5k9++yz8fbbb8vRxRwAAAAAAAAAAAAAAAAAAAAAAAAAAAAUlpJn2pVOp2PAgAE58/Xr13f47FNPPZUzGz16dPTr16/LOT72sY/F0KFD28yy2Wz88Y9/lKOLOQAAAAAAAAAAAAAAAAAAAAAAAAAAACgsJc+8rx07duTMampqOnyuoaEhZ3bEEUfknePII4/MmT333HNydDEHAAAAAAAAAAAAAAAAAAAAAAAAAAAAhaXkmXatXbs2Nm3alDPfd999O3z21VdfzZl94hOfyDvLxz/+8ZzZkiVL5OhiDgAAAAAAAAAAAAAAAAAAAAAAAAAAAApLyTPteuKJJ3Jme+yxR4wYMWKXz61fvz7Wr1+fM+/ouV0ZPnx4zuz111+Xows5AAAAAAAAAAAAAAAAAAAAAAAAAAAAKDwlz+TIZrNx//3358yPOuqo2HPPPXf57KpVq3JmqVQqhgwZkneeoUOH5sy2bNkSGzZskKOTOQAAAAAAAAAAAAAAAAAAAAAAAAAAACi8yqQDsPt55JFHYunSpTnz0047rcNn161blzPr379/VFVV5Z1n3333bXe+du3aGDBggBydyFHKXnzxxUilUknHAADoVRobG3d5fcmSJfH222/3UBoAAOhdOvp6HAAAACgcP/cqL9lsNukIAAAAAAAAAAAAAAD0UkqeaeOdd96Jm266KWc+YsSIOOWUUzp8fsOGDTmz/v37dytTTU1NVFVVRXNzc5v5xo0b5ehkjlKWyWSSjgAA0Ou0trZ2eL2jewAAgPz4WhsAAAB6jp97AQAAAAAAAAAAAAAAhZBOOgC7j0wmE1dddVW7xcTXXXddVFRUdLjj73//e86spqam29na29HeWXIAAAAAAAAAAAAAAAAAAAAAAAAAAADQU5Q8s9Ott94aTz/9dM78rLPOimOOOaZTO5qbm3NmVVVV3c7W3o6mpiY5OpkDAAAAAAAAAAAAAAAAAAAAAAAAAACAwqtMOgC7h1/96lfxgx/8IGd+wAEHxDXXXNPpPe2VGldWdv+PWXulxu2dJUf5SafTkUqlko4BANCrVFRUdHi9o3sAAID8+FobAAAAeo6fe5WXbDYbmUwm6RgAAAAAAAAAAAAAAPRCSp6JJ598Mq6//vqcef/+/eOOO+6IPffcs9O70ul0zqylpaVb+SLaLzBu7yw5ys+hhx5atp8bAMDuatWqVbu8PnLkyBg6dGgPpQEAgN6lo6/HAQAAgMLxc6/ykslk4oUXXkg6BgAAAAAAAAAAAAAAvZDm1F7uueeei0svvTSneLimpibuueeeqK2t7dK+ysrc3vAdO3Z0J+L77qiqqpKjkzkAAAAAAAAAAAAAAAAAAAAAAAAAAAAoPCXPvdjixYvjggsuiO3bt7eZ77HHHnHXXXfFIYcc0uWde+21V86sEKXG/5wxImLPPfeUo5M5AAAAAAAAAAAAAAAAAAAAAAAAAAAAKDwlz73Ua6+9Fueff35s3ry5zbyqqipuv/32OPLII/Pau/fee+fMtm3blteu97S2tkZTU1POvH///nJ0MgcAAAAAAAAAAAAAAAAAAAAAAAAAAACFp+S5F3r99ddj4sSJsXHjxjbzysrKuPXWW+P444/Pe/c+++yTM2tsbIxMJpP3zrVr13b6LDkAAAAAAAAAAAAAAAAAAAAAAAAAAADoKUqee5mVK1dGfX19vPPOO23m6XQ6br755jjppJO6tX/IkCE5s+bm5mhsbMx751tvvZUzS6fTMXjwYDk6mQMAAAAAAAAAAAAAAAAAAAAAAAAAAIDCU/Lci6xatSrq6+tzCoZTqVTceOON8aUvfanbZwwZMiQqKytz5qtXr85755o1a3JmH/rQh6KqqkqOTuYAAAAAAAAAAAAAAAAAAAAAAAAAAACg8JQ89xKrV6+O+vr6dsuFr7vuuhg7dmxBzqmuro4RI0bkzF999dW8d7b37IEHHihHF3IAAAAAAAAAAAAAAAAAAAAAAAAAAABQeEqee4F169ZFfX19rFq1KufatGnT4txzzy3oeZ/85CdzZg0NDXnva+/Z9s6QAwAAAAAAAAAAAAAAAAAAAAAAAAAAgJ5UmXQAimv9+vUxYcKEWLlyZc61yy67LCZOnFjwMz/zmc/EL3/5yzaz559/Pq9d27Zti1deeSVnftRRR8nRxRwAAAAAAAAAAAAAACQjm83GqlWr4q9//Wu8/fbbsWnTpmhtbY1+/frF3nvvHcOGDYsDDzwwKiu9zQMAAAAAAAAAAABKjVf/lbGNGzfGhAkT4vXXX8+5duGFF8ZFF11UlHOPPfbYqKysjJaWlp2zt956K5599tk44ogjurRr3rx50dTU1GY2YMCAOOSQQ+ToYg4AAAAAAAAAAAAAAHpGJpOJF198MRYtWhSLFi2KxYsXx9atW3f5zJ577hmHHXZYjB07Nk4++eSoqqrqobQAAAAAAAAAAABAd6STDkBxbN68OSZNmhRLly7NuTZx4sS44oorinb2wIED45hjjsmZP/TQQ13e9bOf/Sxnduqpp0ZFRYUcXcwBAAAAAAAAAAAAAEBxNTQ0xI033hif/exnY9y4cfGf//mfsWjRog4LniMitm3bFs8880xceeWV8fnPfz4WLFjQA4kBAAAAAAAAAACA7lLyXIa2bt0a559/frz88ss5184777yYNm1a0TN85StfyZk9+uij8dJLL3V6x+9+97t44YUX2sxSqVS7u+UAAAAAAAAAAAAAACBp06ZNi/vvvz8aGxu7tWfdunVx6aWXxtVXXx1NTU0FSgcAAAAAAAAAAAAUg5LnMrNjx46YMmVKNDQ05Fw766yzYvr06T2S4/jjj4+Pf/zjbWatra1x7bXXxpYtWzp8vrGxMW688cac+Yknnhgf/ehH5cgzBwAAAAAAAAAAAAAAyRk2bFgcffTRcdppp8WZZ54ZY8aMidra2ve9/+GHH46vfvWr0dra2nMhAQAAAAAAAAAAgC6pTDoAhdPS0hKXXXZZLFq0KOfaiSeeGJdffnls2LChW2f06dMnampqOrwvlUrF9OnT49xzz41sNrtz/pe//CXOO++8uPvuu2Pfffdt99lly5bF+eefH42NjTlnT5s2rUt55QAAAAAAAAAAAAAAICnV1dVx0kknxUknnRSHH374+75u/M0334x77703HnroochkMm2uLViwIG6//fa44ooreiIyAAAAAAAAAAAA0EVKnsvImjVr4ve//3271x577LF47LHHun1GXV1d3HzzzZ269/DDD49JkybFrFmz2sxfeeWVOPnkk2Ps2LFxwgknxNChQ6OlpSXeeOONmD9/fvzmN7+J5ubmnH3XXntt7L///l3OLAcAAAAAAAAAAAAAAD1pv/32iwkTJsS//uu/Rv/+/Tu8f//9948bbrghxowZE1OnTo3Nmze3uX7vvfdGXV1d1NbWFicwAAAAAAAAAAAAkDclzxTVlVdeGW+++WYsWLCgzXzr1q0xe/bsmD17dqf21NfXx7hx4+QoUA4AAAAAAAAAAAAAAApv8ODBMWHChBg7dmxUV1d3+fmjjjoqZsyYERMnToxMJrNz3tzcHPfdd1/ceOONhYwLAAAAAAAAAAAAFEA66QCUt4qKivjud7+bdyFxKpWKSy+9NK699lo5CpgDAAAAAAAAAAAAAIDCmz17dpxzzjl5FTy/56ijjorTTz89Z/74449HNpvtTjwAAAAAAAAAAACgCJQ8U3RVVVXxjW98I+6+++74yEc+0unnDj300HjggQfi4osvlqMIOQAAAAAAAAAAAAAAKKx0ujBv0zjzzDNzZo2NjbFs2bKC7AcAAAAAAAAAAAAKpzLpABTO0KFDY+nSpUnHeF+f+9zn4vjjj49nnnkmHn/88Vi8eHGsXLkytmzZEhER/fr1i9ra2hg1alScdNJJceihh8rRAzkAAAAAAAAAAAAAANi9HHbYYVFZWRktLS1t5uvWrYuPfOQjCaUCAAAAAAAAAAAA2qPkmR6VSqXi6KOPjqOPPlqO3SgHAAAAAAAAAAAAAAC7j3Q6HQMGDIjGxsY28/Xr1yeUCAAAAAAAAAAAAHg/6aQDAAAAAAAAAAAAAAAA0NaOHTtyZjU1NQkkAQAAAAAAAAAAAHZFyTMAAAAAAAAAAAAAAMBuZO3atbFp06ac+b777ptAGgAAAAAAAAAAAGBXlDwDAAAAAAAAAAAAAADsRp544omc2R577BEjRozo+TAAAAAAAAAAAADALil5BgAAAAAAAAAAAAAA2E1ks9m4//77c+ZHHXVU7LnnngkkAgAAAAAAAAAAAHZFyTMAAAAAAAAAAAAAAMBu4pFHHomlS5fmzE877bQE0gAAAAAAAAAAAAAdqUw6AAAAAAAAAAAAAAAAABHvvPNO3HTTTTnzESNGxCmnnJJAop7z4osvRiqVSjoGAECv0tjYuMvrS5YsibfffruH0gAAQO/S0dfjAAAAQOH4uVd5yWazSUdol5JnAAAAAAAAAAAAAACAhGUymbjqqqtiw4YNOdeuu+66qKioSCBVz8lkMklHAADodVpbWzu83tE9AABAfjr6Wrti0/YeSgIAdFprJqI1G1GRiqhIJ50GAPgHHX0f7ede9AQlzwAAAAAAAAAAAAAAAAm79dZb4+mnn86Zn3XWWXHMMcckkAgAAAAAgKTs/cjipCMAAAAA0AX+GRAAAAAAAAAAAAAAAIAE/epXv4of/OAHOfMDDjggrrnmmgQSAQAAAAAAAAAAAJ1VmXQAAAAAAAAAAAAAAACA3urJJ5+M66+/Pmfev3//uOOOO2LPPfdMIFXPS6fTkUqlko4BANCrVFRUdHi9o3sAAID8+FobAAAAeo6fe5WXbDYbmUwm6Rg5lDwDAAAAAAAAAAAAAAAk4LnnnotLL700Wlpa2sxramrinnvuidra2mSCJeDQQw+NdDqddAwAgF5l1apVu7w+cuTIGDp0aA+lAQCA3qWjr8cBAACAwvFzr/KSyWTihRdeSDpGDq9+AwAAAAAAAAAAAAAA6GGLFy+OCy64ILZv395mvscee8Rdd90VhxxySELJAAAAAAAAAAAAgK5Q8gwAAAAAAAAAAAAAANCDXnvttTj//PNj8+bNbeZVVVVx++23x5FHHplQMgAAAAAAAAAAAKCrlDwDAAAAAAAAAAAAAAD0kNdffz0mTpwYGzdubDOvrKyMW2+9NY4//vhkggEAAAAAAAAAAAB5qUw6AAAAAAAAAAAAAAAAQG+wcuXKqK+vj3feeafNPJ1Ox8033xwnnXRSQskAAAAAAOhJgwYNijlz5iQdAwDowJo1a+Lqq69+3+u33HJLDB48uAcTAQD5GDRoUNIR6AWUPAMAAAAAAAAAAAAAABTZqlWror6+PhobG9vMU6lU3HjjjfGlL30poWQAAAAAAPS0qqqqGDp0aNIxAIBuGjx4sP9PBwAgIiLSSQcAAAAAAAAAAAAAAAAoZ6tXr476+vpYvXp1zrXrrrsuxo4dm0AqAAAAAAAAAAAAoBCUPAMAAAAAAAAAAAAAABTJunXror6+PlatWpVzbdq0aXHuuecmkAoAAAAAAAAAAAAoFCXPAAAAAAAAAAAAAAAARbB+/fqYMGFCrFy5MufaZZddFhMnTkwgFQAAAAAAAAAAAFBISp4BAAAAAAAAAAAAAAAKbOPGjTFhwoR4/fXXc65deOGFcdFFFyWQCgAAAAAAAAAAACg0Jc8AAAAAAAAAAAAAAAAFtHnz5pg0aVIsXbo059rEiRPjiiuuSCAVAAAAAAAAAAAAUAxKngEAAAAAAAAAAAAAAApk69atcf7558fLL7+cc+28886LadOmJZAKAAAAAAAAAAAAKBYlzwAAAAAAAAAAAAAAAAWwY8eOmDJlSjQ0NORcO+uss2L69OkJpAIAAAAAAAAAAACKqTLpAAAAAAAAAAAAAAAAAKWupaUlLrvssli0aFHOtRNPPDEuv/zy2LBhQ7fO6NOnT9TU1HRrBwAAAAAAAAAAAFBYSp4BAAAAAAAAAAAAAAC6ac2aNfH73/++3WuPPfZYPPbYY90+o66uLm6++eZu7wEAAAAAAAAAAAAKJ510AAAAAAAAAAAAAAAAAAAAAAAAAAAAAIBSpOQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIA9KngEAAAAAAAAAAAAAAAAAAAAAAAAAAADyUJl0AAAAAAAAAAAAAAAAgFI3dOjQWLp0adIxAAAAAAAAAAAAgB6WTjoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQClS8gwAAAAAAAAAAAAAAAAAAAAAAAAAAACQByXPAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHlQ8gwAAAAAAAAAAAAAAAAAAAAAAAAAAACQByXPAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHlQ8gwAAAAAAAAAAAAAAAAAAAAAAAAAAACQByXPAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHlQ8gwAAAAAAAAAAAAAAAAAAAAAAAAAAACQByXPAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHlQ8gwAAAAAAAAAAAAAAAAAAAAAAAAAAACQByXPAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHlQ8gwAAAAAAAAAAAAAAAAAAAAAAAAAAACQByXPAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHmoTDoAAAAAAAAAAAAAAAAAAFA6stlsrFu3Lpqbm5OOAnTDmjVrunUdKA1VVVXxwQ9+MFKpVNJRAAAAAAAAypaSZwAAAAAAAAAAAAAAAACgQ9lsNn72s5/Fgw8+GO+++27ScYAiu/rqq5OOABTI3nvvHePGjYszzzxT2TMAAAAAAEARKHkGAAAAAAAAAAAAAAAAADr04IMPxj333JN0DACgi9599924++67I5PJxNlnn510HAAAAAAAgLKTTjoAAAAAAAAAAAAAAAAAALB7a25ujoceeijpGABANzz00EPR3NycdAwAAAAAAICyo+QZAAAAAAAAAAAAAAAAANilpqam2LRpU9IxAIBu2LRpUzQ1NSUdAwAAAAAAoOwoeQYAAAAAAAAAAAAAAAAAdqm6ujr69euXdAwAoBv69esX1dXVSccAAAAAAAAoO0qeAQAAAAAAAAAAAAAAAIBdqqqqirPOOivpGABAN5x11llRVVWVdAwAAAAAAICyU5l0AAAAAAAAAAAAAAAAAABg9zdu3LhIp9Px0EMPxcaNG5OOAwB0Uv/+/WPcuHExduzYpKMAAAAAAACUJSXPAAAAAAAAAAAAAAAAAECHUqlUnHXWWXHmmWfGunXrorm5OelIQIG0tLREc3NzVFVVRWWltx9DOamqqooPfvCDkUqlko4CAAAAAABQtvyUFQAAAAAAAAAAAAAAAADotFQqFYMGDUo6BgAAAAAAAADAbiGddAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAUqTkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACAPSp4BAAAAAAAAAAAAAAAAAAAAAAAAAAAA8qDkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACAPSp4BAAAAAAAAAAAAAAAAAAAAAAAAAAAA8qDkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACAPSp4BAAAAAAAAAAAAAAAAAAAAAAAAAAAA8qDkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACAPSp4BAAAAAAAAAAAAAAAAAAAAAAAAAAAA8qDkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACAPSp4BAAAAAAAAAAAAAAAAAAAAAAAAAAAA8qDkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACAPSp4BAAAAAAAAAAAAAAAAAAAAAAAAAAAA8qDkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACAPSp4BAAAAAAAAAAAAAAAAAKCX27JlS9IRAAAAAAAAAEqSkmcAAAAAAAAAAAAAAAAAAOjFNm/eHJdddlls3rw56SgAAAAAAAAAJUfJMwAAAAAAAAAAAAAAAAAA9GKzZ8+OZcuWxZw5c5KOAgAAAAAAAFBylDwDAAAAAAAAAAAAAAAAAEAvtWLFipg7d25ERMydOzdWrFiRbCAAAAAAAACAEqPkGQAAAAAAAAAAAAAAAAAAeqFsNhszZsyITCYTERGtra0xc+bMyGazCScDAAAAAAAAKB1KngEAAAAAAAAAAAAAAAAAoBdauHBhNDQ0tJk9//zzsXDhwoQSAQAAAAAAAJQeJc8AAAAAAAAAAAAAAAAAANDLbN++Pb7//e+3e+3OO++MHTt29HAiAAAAAAAAgNKk5BkAAAAAAAAAAAAAAAAAAHqZBx98MNatW9futbVr18aDDz7Yw4kAAAAAAAAASpOSZwAAAAAAAAAAAAAAAAAA6EXWrFnTYYnzAw88EGvWrOmhRAAAAAAAAAClqzLpAAAAAAAAAHROetP2pCMAAABAyfB9NAAAAAC8vzvvvDOampp2eU9TU1PcddddccMNN/RMKAAAAAAAAIASpeQZAAAAAACgRPR/ZHHSEQAAAAAAAAAAKHHPP/98LFy4sFP3/uEPf4iGhoYYNWpUkVMBAAAAAAAAlK500gEAAAAAAAAAAAAAAAAAAIDia2lpiRkzZnTpmTvuuCNaWlqKlAgAAAAAAACg9Cl5BgAAAAAA2E14MxwAAAD0HN+HAwAAANAbzZ07N1auXNmlZ1auXBm//vWvixMIAAAAAAAAoAwoeQYAAAAAANhNNDc3Jx0BAAAAeg3fhwMAAADQ26xfvz7mzJmT17OzZ8+ODRs2FDgRAAAAAAAAQHlQ8gwAAAAAALCbqKqqSjoCAAAA9Bq+DwcAAACgt5k1a1Zs2bIlr2e3bNkSs2bNKnAiAAAAAAAAgPKg5BkAAAAAAGA3UVlZmXQEAAAA6DV8Hw4AAABAb7JkyZKYP39+t3bMmzcvlixZUqBEAAAAAAAAAOVDyTMAAAAAAAAAAAAAAAAAAJSpTCYTt99+e0F23XHHHZHJZAqyCwAAAAAAAKBcVCYdAAAAAAAAgM7Z+K+HRKbfHknHAAAAgJKQ3rQ9+j+yOOkYAAAAAJC4BQsWxNKlSwuy67XXXosFCxbEKaecUpB9AAAAAAAAAOVAyTMAAAAAAECJyPTbIzL990o6BgAAAAAAAAAAJWLz5s1xzz33FHTnrFmz4rjjjou+ffsWdC8AAAAAAABAqUonHQAAAAAAAAAAAAAAAAAAACi82bNnx8aNGwu6c8OGDTFnzpyC7gQAAAAAAAAoZUqeAQAAAAAAAAAAAAAAAACgzCxfvjzmzp1blN1z586NFStWFGU3AAAAAAAAQKlR8gwAAAAAAAAAAAAAAAAAAGUkm83GzJkzI5PJFGV/a2trzJgxI7LZbFH2AwAAAAAAAJQSJc8AAAAAAAAAAAAAAAAAAFBGli9fHg0NDUU9o6GhIVasWFHUMwAAAAAAAABKgZJnAAAAAAAAAAAAAAAAAAAoI8OHD49Ro0YV9YzRo0dHbW1tUc8AAAAAAAAAKAVKngEAAAAAAAAAAAAAAAAAoIykUqk444wzinrGGWecEalUqqhnAAAAAAAAAJQCJc8AAAAAAAAAAAAAAAAAAFBGstls/PrXvy7qGXPnzo1sNlvUMwAAAAAAAABKgZJnAAAAAAAAAAAAAAAAAAAoI8uXL4+GhoaintHQ0BArVqwo6hkAAAAAAAAApUDJMwAAAAAAAAAAAAAAAAAAlJHhw4fHqFGjinrG6NGjo7a2tqhnAAAAAAAAAJQCJc8AAAAAAAAAAAAAAAAAAFBGUqlUXHzxxZFOF+etxBUVFXHxxRdHKpUqyn4AAAAAAACAUqLkGQAAAAAAAAAAAAAAAAAAyszw4cOjrq6uKLvr6uqitra2KLsBAAAAAAAASo2SZwAAAAAAAAAAAAAAAAAAKEP19fUxYMCAgu4cMGBAjB8/vqA7AQAAAAAAAEqZkmcAAAAAAAAAAAAAAAAAAChDffv2jcmTJxd05+TJk6Nv374F3QkAAAAAAABQypQ8AwAAAAAAAAAAAAAAAABAmRozZkwceOCBBdl10EEHxZgxYwqyCwAAAAAAAKBcKHkGAAAAAAAAAAAAAAAAAIAylU6n49JLLy3IrksuuSTSaW9PBgAAAAAAAPhHfooKAAAAAAAAAAAAAAAAAABlbOTIkfGFL3yhWztOOeWUGDlyZIESAQAAAAAAAJQPJc8AAAAAAAAAAAAAAAAAAFDmJk+eHDU1NXk9W1NTE5MnTy5wIgAAAAAAAIDyoOQZAAAAAAAAAAAAAAAAAADK3MCBA2P8+PF5PVtfXx8DBgwocCIAAAAAAACA8qDkGQAAAAAAAAAAAAAAAAAAeoG6uroYNmxYl54ZNmxYnHHGGcUJBAAAAAAAAFAGlDwDAAAAAAAAAAAAAAAAAEAvUFlZGVOnTu3SM5dccklUVlYWKREAAAAAAABA6VPyDAAAAAAAAAAAAAAAAAAAvcTo0aPjuOOO69S9n/3sZ2PUqFFFTgQAAAAAAABQ2pQ8AwAAAAAAAAAAAAAAAABALzJlypSorq7e5T3V1dVx4YUX9lAiAAAAAAAAgNKl5BkAAAAAAAAAAAAAAAAAAHqRwYMHx7hx43Z5z9lnnx2DBw/uoUQAAAAAAAAApUvJMwAAAAAAAAAAAAAAAAAA9DJnn312DBo0qN1rgwYN6rAEGgAAAAAAAID/o+QZAAAAAAAAAAAAAAAAAAB6mT59+sSUKVPavTZlypTo06dPDycCAAAAAAAAKE1KngEAAAAAAAAAAAAAAAAAoBc67rjjYtSoUW1mo0ePjuOOOy6hRAAAAAAAAAClR8kzAAAAAAAAAAAAAAAAAAD0QqlUKqZOnRrp9P+95biioiKmTp0aqVQq4WQAAAAAAAAApUPJMwAAAAAAAAAAAAAAAAAA9FK1tbVRV1cXERF1dXUxbNiwhBMBAAAAAAAAlBYlzwAAAAAAAAAAAAAAAAAA0IvV19fHiBEjYvz48UlHAQAAAAAAACg5lUkHAAAAAAAAAAAAAAAAAAAAktO3b9+47bbboqamJukoAAAAAAAAACUnnXQAAAAAAAAAAAAAAAAAAAAgWQqeAQAAAAAAAPKj5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgD0qeAQAAAAAAAAAAAAAAAAAAAAAAAAAAAPKg5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgD0qeAQAAAAAAAAAAAAAAAAAAAAAAAAAAAPKg5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgD0qeAQAAAAAAAAAAAAAAAAAAAAAAAAAAAPKg5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgD0qeAQAAAAAAAAAAAAAAAAAAAAAAAAAAAPKg5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgD0qeAQAAAAAAAAAAAAAAAAAAAAAAAAAAAPKg5BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgD0qeAQAAAAAAAAAAAAAAAAAAAAAAAAAAAPKg5BkAAAAAAAAAAAAAAAAAAHq5xsbGpCMAAAAAAAAAlCQlzwAAAAAAAAAAAAAAAAAA0IutWbMmzj333Fi7dm3SUQAAAAAAAABKjpJnAAAAAAAAAAAAAAAAAADoxaZPnx7Nzc0xffr0pKMAAAAAAAAAlBwlzwAAAAAAAAAAAAAAAAAA0Es9/fTTsXz58oiIWLZsWTz99NMJJwIAAAAAAAAoLUqeAQAAAAAAAAAAAAAAAACgF8pkMnHTTTe1md10002RyWQSSgQAAAAAAABQepQ8AwAAAAAAAAAAAAAAAABAL3T33XfH1q1b28y2bt0ad999d0KJAAAAAAAAAEqPkmcAAAAAAAAAAAAAAAAAAOhlNm7cGL/4xS/avfaLX/wiNm3a1MOJAAAAAAAAAEqTkmcAAAAAAAAAAAAAAAAAAOhlvv71r0c2m233Wjabjeuvv76HEwEAAAAAAACUJiXPAAAAAAAAAAAAAAAAAADQi7z00kvx8ssvd3jPSy+91EOJAAAAAAAAAEqXkmcAAAAAAAAAAAAAAAAAAOhF/t//+3+duu+GG24obhAAAAAAAACAMqDkGQAAAAAAAAAAAAAAAAAAeokHHnggNm7c2Kl7N2zYEA899FBxAwEAAAAAAACUOCXPAAAAAAAAAAAAAAAAAADQC2zfvj3uu+++Lj0za9as2L59e5ESAQAAAAAAAJQ+Jc8AAAAAAAAAAAAAAAAAANAL3HjjjdHa2tqlZ1pbW+Ob3/xmkRIBAAAAAAAAlD4lzwAAAAAAAAAAAAAAAAAAUOaWLVsWixYtyuvZZ555JlasWFHYQAAAAAAAAABlQskzAAAAAAAAAAAAAAAAAACUueuuu65bz0+fPr1ASQAAAAAAAADKi5JnAAAAAAAAAAAAAAAAAAAoY48++misWbOmWztWr14d8+bNK1AiAAAAAAAAgPKh5BkAAAAAAAAAAAAAAAAAAMpUS0tL3H777QXZddttt0VLS0tBdgEAAAAAAACUCyXPAAAAAAAAAAAAAAAAAABQpr773e9GU1NTQXY1NTXFd7/73YLsAgAAAAAAACgXSp4BAAAAAAAAAAAAAAAAAKAMrVmzJhYsWFDQnQsWLIi1a9cWdCcAAAAAAABAKVPyDAAAAAAAAAAAAAAAAAAAZWj69OkltRcAAAAAAACgFCl5BgAAAAAAAAAAAAAAAACAMvPUU0/F8uXLi7J72bJl8fTTTxdlNwAAAAAAAECpUfIMAAAAAAAAAAAAAAAAAABlJJPJxM0331zUM2666abIZDJFPQMAAAAAAACgFCh5BgAAAAAAAAAAAAAAAACAMrJw4cLYunVrUc/YunVrPPXUU0U9AwAAAAAAAKAUKHkGAAAAAAAAAAAAAAAAAIAyst9++5XVOQAAAAAAAAC7MyXPAAAAAAAAAAAAAAAAAABQRj7ykY/EiBEjSv4MAAAAAAAAgFKg5BkAAAAAAAAAAAAAAAAAAMpIKpWK6dOnF/WM6dOnRyqVKuoZAAAAAAAAAKVAyTMAAAAAAAAAAAAAAAAAAAAAAAAAAABAHpQ8AwAAAAAAAAAAAAAAAABAGclmszFz5syinjFjxozIZrNFPQMAAAAAAACgFCh5BgAAAAAAAAAAAAAAAACAMrJ8+fJoaGgo6hkNDQ2xYsWKop4BAAAAAAAAUAqUPAMAAAAAAAAAAAAAAAAAQBkZPnx4jBo1qqhnjB49Ompra4t6BgAAAAAAAEApUPIMAAAAAAAAAAAAAAAAAABlJJVKxcUXXxzpdHHeSlxRUREXX3xxpFKpouwHAAAAAAAAKCVKngEAAAAAAAAAAAAAAAAAoMwMHz486urqirK7rq4uamtri7IbAAAAAAAAoNQoeQYAAAAAAAAAAAAAAAAAgDJUX18fAwYMKOjOAQMGxPjx4wu6EwAAAAAAAKCUKXkGAAAAAAAAAAAAAAAAAIAy1Ldv35g8eXJBd06ePDn69u1b0J0AAAAAAAAApUzJMwAAAAAAAAAAAAAAAAAAlKkxY8bEgQceWJBdBx10UIwZM6YguwAAAAAAAADKhZJnAAAAAAAAAAAAAAAAAAAoU+l0Oi699NKC7Lrkkksinfb2ZAAAAAAAAIB/5KeoAAAAAAAAAAAAAAAAAABQxkaOHBlf+MIXurXjlFNOiZEjRxYoEQAAAAAAAED5UPIMAAAAAAAAAAAAAAAAAABlbvLkyVFTU5PXszU1NTF58uQCJwIAAAAAAAAoD0qeAQAAAAAAAAAAAAAAAACgzA0cODDOOeecvJ4999xzY8CAAQVOBAAAAAAAAFAelDwDAAAAAAAAAAAAAAAAAEAvkEqlevQ5AAAAAAAAgN5AyTMAAAAAAAAAAAAAAAAAAJS59evXx09+8pO8nv3xj38cGzZsKHAiAAAAAAAAgPKg5BkAAAAAAAAAAAAAAAAAAMrcrFmzYsuWLXk9u2XLlpg1a1aBEwEAAAAAAACUByXPAAAAAAAAAAAAAAAAAABQxpYsWRLz58/v1o558+bFkiVLCpQIAAAAAAAAoHwoeQYAAAAAAAAAAAAAAAAAgDKVyWTi9ttvL8iuO+64IzKZTEF2AQAAAAAAAJQLJc8AAAAAAAAAAAAAAAAAAFCmFixYEEuXLi3Irtdeey0WLFhQkF0AAAAAAAAA5ULJMwAAAAAAAAAAAAAAAAAAlKHNmzfHPffcU9Cds2bNis2bNxd0JwAAAAAAAEApq0w6AAAAAAAAAJ1TsWl70hEAgH/WmolozUZUpCIq/FvbALA78X00AAAAAETMnj07Nm7cWNCdGzZsiDlz5sRFF11U0L0AAAAAAAAApUrJMwAAAAAAQInY+5HFSUcAAAAAAAAAAKBELF++PObOnVuU3XPnzo1TTz01amtri7IfAAAAAAAAoJSkkw4AAAAAAAAAAAAAAAAAAAAUTjabjZkzZ0YmkynK/tbW1pgxY0Zks9mi7AcAAAAAAAAoJUqeAQAAAAAAAAAAAAAAAACgjCxfvjwaGhqKekZDQ0OsWLGiqGcAAAAAAAAAlILKpAPQO23YsCFeeeWVWLlyZWzevDlSqVT069cvamtr4+CDD46+ffvKkUAOAAAAAAAAAAAAAAAAAKD0DR8+PEaNGlXUoufRo0dHbW1t0fYDAAAAAAAAlAolz73Eli1b4tVXX42XX345Xn755XjppZfijTfeiGw22+a+qVOnxiWXXFKUDNlsNubNmxc//elP4/nnn49MJtPufVVVVXH00UfH+PHj49hjj5WjyDkAAAAAAAAAAAAAAAAAgPKSSqXi4osvjvPPP/9937/YHRUVFXHxxRdHKpUq+G4AAAAAAACAUqPkuQy1trbGSy+9tLPM+eWXX45ly5YV5YfwnbVixYqYNm1a/PnPf+7w3ubm5njyySfjySefjM997nPx7W9/O/bZZx85ipADAAAAAAAAAAAAAAAAAChPw4cPj7q6uvjlL39Z8N11dXVRW1tb8L0AAAAAAAAApSiddAAKb/Xq1XHWWWfFN7/5zfj1r38d//u//5towfNzzz0XY8eO7VSh8T974okn4stf/nIsW7ZMjgLnAAAAAAAAAAAAAAAAAADKW319fQwYMKCgOwcMGBDjx48v6E4AAAAAAACAUlaZdADK22uvvRYXXHBBbN68OedadXV1HHzwwbH//vtHc3NzvPnmm/HKK6/kFFKvXr06JkyYED//+c9j0KBBchQgBwAAAAAAu6dBgwbFnDlzko4BAHRgzZo1cfXVV7/v9VtuuSUGDx7cg4kAgHx4/RUAAAAAvUHfvn1j8uTJ8e///u8F2zl58uTo27dvwfYBAAAAAAAAlDolz71UVVVVpFKpaGpqKtoZ27Zti8svvzyn0DiVSsWECRNi0qRJse+++7a5tnLlypg5c2Y8/PDDbeZr166Nr371qzFnzpxIpVJydCMHAAAAAAC7r6qqqhg6dGjSMQCAbho8eLD/TwcAAAAAAGC3MWbMmHjkkUdi6dKl3d510EEHxZgxYwqQCgAAAAAAAKB8pJMOQPFVVlbGAQccEP/2b/8WX//61+NnP/tZNDQ0xGGHHVbUc++6665Yvnx5m1lFRUXcfPPN8bWvfS2n0DgiYtiwYXHLLbfElVdemXPtT3/6U/ziF7+Qo5s5AAAAAAAAAAAAAAAAAIDeI51Ox6WXXlqQXZdcckmk096eDAAAAAAAAPCPKpMOQOHtscceUVdXFwcffHB84hOfiJEjR8Yee+zRoxneeeed+OEPf5gznzRpUpxxxhkdPn/BBRfEa6+9Fo8++mib+W233Rann356VFdXy5FHDgAAAAAAAAAAAAAAAACg9xk5cmR84QtfiPnz5+e945RTTomRI0cWMBUAAAAAAABAefBP5ZahD3zgA3HzzTfHueeeG5/61Kd6vOA5IuInP/lJ7Nixo83swx/+cEydOrXTO66//vro169fm1ljY2P89re/lSPPHAAAAAAAAAAAAAAAAABA7zR58uSoqanJ69mampqYPHlygRMBAAAAAAAAlAclzxRcNpuNhx9+OGc+ceLE6NOnT6f3DBw4MMaOHZsznzt3rhx55AAAAAAAAAAAAAAAAAAAeq+BAwfG+PHj83q2vr4+BgwYUOBEAAAAAAAAAOVByTMFt3jx4vjb3/7WZlZVVRVf/OIXu7yrrq4uZ/bss8/G22+/LUcXcwAAAAAAAAAAAAAAAAAAvVtdXV0MGzasS88MGzYszjjjjOIEAgAAAAAAACgDSp4puKeeeipnNnr06OjXr1+Xd33sYx+LoUOHtplls9n44x//KEcXcwAAAAAAAAAAAAAAAAAAvVtlZWVMnTq1S89ccsklUVlZWaREAAAAAAAAAKVPyTMF19DQkDM74ogj8t535JFH5syee+45ObqYAwAAAAAAAAAAAAAAAABg9OjRcdxxx3Xq3s9+9rMxatSoIicCAAAAAAAAKG1Knim4V199NWf2iU98Iu99H//4x3NmS5YskaOLOQAAAAAAAAAAAAAAAAAAIiKmTJkS1dXVu7ynuro6Lrzwwh5KBAAAAAAAAFC6lDxTUOvXr4/169fnzEeMGLWP6FkAAJCSSURBVJH3zuHDh+fMXn/9dTm6kAMAAAAAAAAAAAAAAAAA4D2DBw+OcePG7fKes88+OwYPHtxDiQAAAAAAAABKl5JnCmrVqlU5s1QqFUOGDMl759ChQ3NmW7ZsiQ0bNsjRyRwAAAAAAAAAAAAAAAAAAP/o7LPPjkGDBrV7bdCgQR2WQAMAAAAAAADwfyqTDkB5WbduXc6sf//+UVVVlffOfffdt9352rVrY8CAAXJ0Ikcpe/HFFyOVSiUdAwCgV2lsbNzl9SVLlsTbb7/dQ2kAAAAAYPfj79AAAHY/2Ww26QgAAAAAJadPnz4xZcqUuOGGG3KuTZkyJfr06dPzoQAAAAAAAABKkJJnCmrDhg05s/79+3drZ01NTVRVVUVzc3Ob+caNG+XoZI5Slslkko4AANDrtLa2dni9o3sAAAAAoJz5OzQAAAAAAADKxXHHHRejRo2KhoaGnbPRo0fHcccdl2AqAAAAAAAAgNKSTjoA5eXvf/97zqympqbbe9vb0d5ZcgAAAAAAAAAAAAAAAAAAdE4qlYqpU6dGOv1/bzmuqKiIqVOnRiqVSjgZAAAAAAAAQOlQ8kxBNTc358yqqqq6vbe9HU1NTXJ0MgcAAAAAAAAAAAAAAAAAQHtqa2ujrq4uIiLq6upi2LBhCScCAAAAAAAAKC2VSQegvLRXalxZ2f0/Zu2VGrd3lhzlJ51O+xe/AQB6WEVFRYfXO7oHAAAAAMqZv0MDANj9ZLPZyGQySccAAAAAKFn19fXxwgsvxPjx45OOAgAAAAAAAFBylDxTUOl0OmfW0tLS7b3tFRi3d5Yc5efQQw8t288NAGB3tWrVql1eHzlyZAwdOrSH0gAAAADA7sffoQEA7H4ymUy88MILSccAAAAAKFl9+/aN2267LWpqapKOAgAAAAAAAFByNKdSUJWVub3hO3bs6Pbe9nZUVVXJ0ckcAAAAAAAAAAAAAAAAAAAAAAAAAAAAFJ6SZwpqr732ypkVotR4+/btObM999xTjk7mAAAAAAAAAAAAAAAAAAB4P5s3b47LLrssNm/enHQUAAAAAAAAgJKj5JmC2nvvvXNm27Zt69bO1tbWaGpqypn3799fjk7mAAAAAAAAAAAAAAAAAAB4P7Nnz45ly5bFnDlzko4CAAAAAAAAUHKUPFNQ++yzT86ssbExMplM3jvXrl3b6bPkAAAAAAAAAAAAAAAAAADovBUrVsTcuXMjImLu3LmxYsWKZAMBAAAAAAAAlBglzxTUkCFDcmbNzc3R2NiY98633norZ5ZOp2Pw4MFydDIHAAAAAAAAAAAAAAAAAMA/y2azMWPGjMhkMhER0draGjNnzoxsNptwMgAAAAAAAIDSoeSZghoyZEhUVlbmzFevXp33zjVr1uTMPvShD0VVVZUcncwBAAAAAAAAAAAAAAAAAPDPFi5cGA0NDW1mzz//fCxcuDChRAAAAAAAAAClR8kzBVVdXR0jRozImb/66qt572zv2QMPPFCOLuQAAAAAAAAAAAAAAAAAAPhH27dvj+9///vtXrvzzjtjx44dPZwIAAAAAAAAoDQpeabgPvnJT+bM/vlfce6K9p5t7ww5AAAAAAAAAAAAAAAAAAA658EHH4x169a1e23t2rXx4IMP9nAiAAAAAAAAgNKk5JmC+8xnPpMze/755/PatW3btnjllVdy5kcddZQcXcwBAAAAAAAAAAAAAAAAABARsWbNmg5LnB944IFYs2ZNDyUCAAAAAAAAKF1Knim4Y489NiorK9vM3nrrrXj22We7vGvevHnR1NTUZjZgwIA45JBD5OhiDgAAAAAAAAAAAAAAAACAiIg777wz5/2K/6ypqSnuuuuuHkoEAAAAAAAAULqUPFNwAwcOjGOOOSZn/tBDD3V5189+9rOc2amnnhoVFRVydDEHAAAAAAAAAAAAAAAAAMDzzz8fCxcu7NS9f/jDH6KhoaHIiQAAAAAAAABKm5JniuIrX/lKzuzRRx+Nl156qdM7fve738ULL7zQZpZKpdrdLQcAAAAAAAAAAAAAAAAAwK61tLTEjBkzuvTMHXfcES0tLUVKBAAAAAAAAFD6lDxTFMcff3x8/OMfbzNrbW2Na6+9NrZs2dLh842NjXHjjTfmzE888cT46Ec/KkeeOQAAAAAAAAAAAAAAAACA3mvu3LmxcuXKLj2zcuXK+PWvf12cQAAAAAAAAABlQMlzmWpqaor169fv8ldzc3POc9u2bevwuUwm0+H5qVQqpk+fHqlUqs38L3/5S5x33nnR2Nj4vs8uW7Ysxo0bl3NPnz59Ytq0aZ38b0AOAAAAAAAAAAAAAAAAAID3rF+/PubMmZPXs7Nnz44NGzYUOBEAAAAAAABAeahMOgDF8dvf/jauueaaLj937733xr333rvLe/77v/87hg4d2uGuww8/PCZNmhSzZs1qM3/llVfi5JNPjrFjx8YJJ5wQQ4cOjZaWlnjjjTdi/vz58Zvf/KbdAuprr7029t9//659QnIAAAAAAAAAAAAAAAAAAMSsWbNiy5YteT27ZcuWmDVrVlx11VUFTgUAAAAAAABQ+pQ8U1RXXnllvPnmm7FgwYI2861bt8bs2bNj9uzZndpTX18f48aNk6NAOQAAAAAAAAAAAAAAAACA3mPJkiUxf/78bu2YN29enHbaaTFy5MgCpQIAAAAAAAAoD+mkA/D/2bv32DoL+3zg33N8oY2tJA4D0ig0dro2MdCmTQZqU26dRkPo6HAgJXRdwogroCYRMC4tU/9gmioGq7aSZAPFiCWqSrisHgsjSbUy2FCRgJjbgkMlsAMUcqFxoA4pdnzO749pE/4dN8TnvMcn5/XnI6Fq33Pe5332x6qqtp+lW01NTfzwhz8sepA4k8nE6tWr45ZbbtEjwR4AAAAAAAAAAAAAAAAAwMSQy+XizjvvTCRrzZo1kcvlEskCAAAAAAAASAsjz5RdXV1d3HrrrXH33XfHpz71qaN+bt68eXHfffdFR0eHHmXoAQAAAAAAAAAAAADA+Dh48GA888wzce+998Zf/MVfxFe/+tWYO3duzJkzZ8Q/a9asqXRVAABSaNu2bfHKK68kkrVz587Ytm1bIlkAAAAAAAAAaVFb6QKUx5IlS2LJkiWVrjHCueeeG+ecc0489dRT8dhjj8WLL74Yu3btioMHD0ZExOTJk6O5uTnmz58f5513XsybN0+PcegBAAAAAAAAAAAAAEByhoeH46WXXor//u///r9/fe211yKXy1W6GgAAE9DAwECsX78+0czOzs4466yzorGxMdFcAAAAAAAAgGpl5JlxlclkYuHChbFw4UI9jqEeAAAAAAAAAAAAAAAk4+23345LL7200jUAACAiIjZs2BAHDhxINLO/vz82btwY3/nOdxLNBQAAAAAAAKhW2UoXAAAAAAAAAAAAAAAAAAAAktXb2xtdXV1lye7q6oq+vr6yZAMAAAAAAABUGyPPAAAAAAAAAAAAAAAA46iuri7q6+srXQMAgBTL5/Oxbt26yOVyZckfHh6OtWvXRj6fL0s+AAAAAAAAQDWprXQBAAAAAAAAAAAAAACAtKqtrY3Zs2fHaaed9n//tLa2xsqVK+Ppp5+udD0AAFKqt7c3uru7y/qO7u7u6Ovri5aWlrK+BwAAAAAAAOBYZ+QZAAAAAAAAAAAAAAAgIR/72Meira0tTjvttDj11FOjtbU1Pvaxj1W6FgAAE0xLS0vMnz+/rEPPCxYsiObm5rLlAwAAAAAAAFQLI88AAAAAAAAAAAAAAAAJ+b3f+7247bbbKl0DAIAJLpPJREdHR3z729+OXC6XeH5NTU10dHREJpNJPBsAAAAAAACg2mQrXQAAAAAAAAAAAAAAAAAAAEhWS0tLtLW1lSW7ra0tmpuby5INAAAAAAAAUG2MPAMAAAAAAAAAAAAAAAAAQAqtWLEimpqaEs1samqK5cuXJ5oJAAAAAAAAUM2MPAMAAAAAAAAAAAAAAAAAQAo1NjZGe3t7opnt7e3R2NiYaCYAAAAAAABANTPyDAAAAAAAAAAAAAAAAAAAKbVo0aKYM2dOIllz586NRYsWJZIFAAAAAAAAkBZGngEAAAAAAAAAAAAAAAAAIKWy2WysXr06kaxVq1ZFNuvPkwEAAAAAAAA+zE9RAQAAAAAAAAAAAAAAAAAgxVpbW+P8888vKWPx4sXR2tqaUCMAAAAAAACA9DDyDAAAAAAAAAAAAAAAAAAAKXfJJZeU9PzSpUsTagIAAAAAAACQLkaeAQAAAAAAAAAAAAAAAAAg5R566KGSnn/wwQcTagIAAAAAAACQLrWVLgAAAAAAAAAAAAAAAMDE9sILL0Qmk6l0DQCA1Nq1a1ds3bq1pIwtW7bEZz7zmZg1a1ZCrQAAAADg2LZv374jft7T0xPvvPPOOLUBACAiIp/PV7rCqIw8AwAAAAAAAAAAAAAAUFG5XK7SFQAAUiuXy8VPf/rTRLJ++tOfxjXXXBPZbDaRPAAAAAA4lg0PD3/k5x/1HQAAJgY/QQUAAAAAAAAAAAAAAAAAgJR69tln44033kgk64033ojt27cnkgUAAAAAAACQFkaeAQAAAAAAAAAAAAAAAAAghQ4dOhSPPvpoopmPPvpoHDp0KNFMAAAAAAAAgGpWW+kCAAAAAAAAAAAAAAAATGzZbDYymUylawAApM6///u/x8GDBxPNHBgYiJ///OfxJ3/yJ4nmAgAAAMCxpqam5iM//6jvAACQrHw+H7lcrtI1Chh5BgAAAAAAAAAAAAAAoKLmzZsX2Wy20jUAAFKlt7c3nnzyybJkP/nkk7FixYpobm4uSz4AAAAAHAvefPPNI37e2toaM2fOHKc2AABERORyuXjuuecqXaOA334DAAAAAAAAAAAAAAAAAIAUyefzsW7dusjlcmXJHx4ejrVr10Y+ny9LPgAAAAAAAEA1MfIMAAAAAAAAAAAAAAAAAAAp0tvbG93d3WV9R3d3d/T19ZX1HQAAAAAAAADVwMgzAAAAAAAAAAAAAAAAAACkSEtLS8yfP7+s71iwYEE0NzeX9R0AAAAAAAAA1cDIMwAAAAAAAAAAAAAAAAAApEgmk4mOjo7IZsvzp8Q1NTXR0dERmUymLPkAAAAAAAAA1cTIMwAAAAAAAAAAAAAAAAAApExLS0u0tbWVJbutrS2am5vLkg0AAAAAAABQbYw8AwAAAAAAAAAAAAAAAABACq1YsSKampoSzWxqaorly5cnmgkAAAAAAABQzYw8AwAAAAAAAAAAAAAAAABACjU2NkZ7e3uime3t7dHY2JhoJgAAAAAAAEA1M/IMAAAAAAAAAAAAAAAAAAAptWjRopgzZ04iWXPnzo1FixYlkgUAAAAAAACQFkaeAQAAAAAAAAAAAAAAAAAgpbLZbKxevTqRrFWrVkU268+TAQAAAAAAAD7MT1EBAAAAAAAAAAAAAAAAACDFWltb4/zzzy8pY/HixdHa2ppQIwAAAAAAAID0MPIMAAAAAAAAAAAAAAAAAAAp197eHg0NDUU929DQEO3t7Qk3AgAAAAAAAEgHI88AAAAAAAAAAAAAAAAAAJBy06ZNi+XLlxf17IoVK6KpqSnhRgAAAAAAAADpUFvpAgAAAAAAAAAAAAAAAGkyODgYAwMDR/zO0NBQwe3QoUOxf//+Iz43derUyGazJfUDAGDiuvDCC6Ozs3PU/zz6u9TV1cUf//Efl7EVAAAAAAAAQHUz8gwAAAAAAAAAAAAAAJCgRx55JL73ve+N+bl77rkn7rnnniN+5+c//3nMnDmz2GoAAExwmzdvHtPAc8T//D8oeeSRR+KSSy4pUysAAAAAAACA6patdAEAAAAAAAAAAAAAAAAAAKC89u/fHxs3bizq2Q0bNkR/f3/CjQAAAAAAAADSwcgzAAAAAAAAAAAAAAAAAACkXGdnZxw8eLCoZw8ePBidnZ0JNwIAAAAAAABIByPPAAAAAAAAAAAAAAAAAACQYj09PbF169aSMrZs2RI9PT0JNQIAAAAAAABIj9pKFwAAAAAAAAAAAAAAAEiTJUuWxJIlSypdAwAAIiIil8vFnXfemUjWmjVrYu3atZHNZhPJAwAAAAAAAEgDP0EFAAAAAAAAAAAAAAAAAICU2rZtW7zyyiuJZO3cuTO2bduWSBYAAAAAAABAWhh5BgAAAAAAAAAAAAAAAACAFBoYGIj169cnmtnZ2RkDAwOJZgIAAAAAAABUMyPPAAAAAAAAAAAAAAAAAACQQhs2bIgDBw4kmtnf3x8bN25MNBMAAAAAAACgmhl5BgAAAAAAAAAAAAAAAACAlOnt7Y2urq6yZHd1dUVfX19ZsgEAAAAAAACqjZFnAAAAAAAAAAAAAAAAAABIkXw+H+vWrYtcLleW/OHh4Vi7dm3k8/my5AMAAAAAAABUEyPPAAAAAAAAAAAAAAAAAACQIr29vdHd3V3Wd3R3d0dfX19Z3wEAAAAAAABQDYw8AwAAAAAAAAAAAAAAAABAirS0tMT8+fPL+o4FCxZEc3NzWd8BAAAAAAAAUA2MPAMAAAAAAAAAAAAAAAAAQIpkMpno6OiIbLY8f0pcU1MTHR0dkclkypIPAAAAAAAAUE2MPAMAAAAAAAAAAAAAAAAAQMq0tLREW1tbWbLb2tqiubm5LNkAAAAAAAAA1cbIMwAAAAAAAAAAAAAAAAAApNCKFSuiqakp0cympqZYvnx5opkAAAAAAAAA1czIMwAAAAAAAAAAAAAAAAAApFBjY2O0t7cnmtne3h6NjY2JZgIAAAAAAABUMyPPAAAAAAAAAAAAAAAAAACQUosWLYo5c+YkkjV37txYtGhRIlkAAAAAAAAAaWHkGQAAAAAAAAAAAAAAAAAAUiqbzcbq1asTyVq1alVks/48GQAAAAAAAODD/BQVAAAAAAAAAAAAAAAAAABSrLW1Nc4///ySMhYvXhytra0JNQIAAAAAAABIDyPPAAAAAAAAAAAAAAAAAACQcu3t7TFp0qSinp00aVK0t7cn3AgAAAAAAAAgHYw8AwAAAAAAAAAAAAAAAABAyk2bNi0+//nPF/XsF77whWhqakq2EAAAAAAAAEBKGHkGAAAAAAAAAAAAAAAAAICU279/fzz//PNFPfvcc89Ff39/soUAAAAAAAAAUsLIMwAAAAAAAAAAAAAAAAAApFxnZ2e8//77RT37/vvvR2dnZ8KNAAAAAAAAANLByDMAAAAAAAAAAAAAAAAAAKRYT09PbN26taSMLVu2RE9PT0KNAAAAAAAAANLDyDMAAAAAAAAAAAAAAAAAAKRULpeLO++8M5GsNWvWRC6XSyQLAAAAAAAAIC2MPAMAAAAAAAAAAAAAAAAAQEpt27YtXnnllUSydu7cGdu2bUskCwAAAAAAACAtjDwDAAAAAAAAAAAAAAAAAEAKDQwMxPr16xPN7OzsjIGBgUQzAQAAAAAAAKqZkWcAAAAAAAAAAAAAAAAAAEihDRs2xIEDBxLN7O/vj40bNyaaCQAAAAAAAFDNjDwDAAAAAAAAAAAAAAAAAEDK9Pb2RldXV1myu7q6oq+vryzZAAAAAAAAANXGyDMAAAAAAAAAAAAAAAAAAKRIPp+PdevWRS6XK0v+8PBwrF27NvL5fFnyAQAAAAAAAKqJkWcAAAAAAAAAAAAAAAAAAEiR3t7e6O7uLus7uru7o6+vr6zvAAAAAAAAAKgGRp4BAAAAAAAAAAAAAAAAACBFWlpaYv78+WV9x4IFC6K5ubms7wAAAAAAAACoBkaeAQAAAAAAAAAAAAAAAAAgRTKZTHR0dEQ2W54/Ja6pqYmOjo7IZDJlyQcAAAAAAACoJkaeAQAAAAAAAAAAAAAAAAAgZVpaWqKtra0s2W1tbdHc3FyWbAAAAAAAAIBqY+QZAAAAAAAAAAAAAAAAAABSaMWKFdHU1JRoZlNTUyxfvjzRTAAAAAAAAIBqZuQZAAAAAAAAAAAAAAAAAABSqLGxMdrb2xPNbG9vj8bGxkQzAQAAAAAAAKqZkWcAAAAAAAAAAAAAAAAAAEipRYsWxZw5cxLJmjt3bixatCiRLAAAAAAAAIC0MPIMAAAAAAAAAAAAAAAAAAAplc1mY/Xq1YlkrVq1KrJZf54MAAAAAAAA8GF+igoAAAAAAAAAAAAAAAAAACnW2toa559/fkkZixcvjtbW1oQaAQAAAAAAAKSHkWcAAAAAAAAAAAAAAAAAAEi59vb2aGhoKOrZhoaGaG9vT7gRAAAAAAAAQDoYeQYAAAAAAAAAAAAAAAAAgJSbNm1aLF++vKhnV6xYEU1NTQk3AgAAAAAAAEgHI88AAAAAAAAAAAAAAAAAADABtLW1xaxZs8b0zKxZs+Kiiy4qTyEAAAAAAACAFDDyDAAAAAAAAAAAAAAAAAAAE0BtbW1cc801Y3pm1apVUVtbW6ZGAAAAAAAAANXPyDMAAAAAAAAAAAAAAAAAAEwQCxYsiLPOOuuovnv22WfH/Pnzy9wIAAAAAAAAoLoZeQYAAAAAAAAAAAAAAAAAgAnk6quvjvr6+iN+p76+Pq666qpxagQAAAAAAABQvYw8AwAAAAAAAAAAAAAAAADABDJ9+vRYtmzZEb9z2WWXxfTp08epEQAAAAAAAED1MvIMAAAAAAAAAAAAAAAAAAATzGWXXRZTpkwZ9bMpU6Z85Ag0AAAAAAAAAP/DyDMAAAAAAAAAAAAAAAAAAEww+Xw+8vl8pWsAAAAAAAAAVD0jzwAAAAAAAAAAAAAAAAAAMMFs2rQp3nvvvVE/e/fdd2PTpk3j3AgAAAAAAACgOhl5BgAAAAAAAAAAAAAAAACACWT37t0fOeJ83333xe7du8epEQAAAAAAAED1MvIMAAAAAAAAAAAAAAAAAAATyD/+4z/G4ODgEb8zODgYd9111zg1AgAAAAAAAKheRp4BAAAAAAAAAAAAAAAAAGCC2L59e/zXf/3XUX33P//zP6O7u7vMjQAAAAAAAACqm5FnAAAAAAAAAAAAAAAAAACYAA4fPhxr164d0zNr1qyJw4cPl6kRAAAAAAAAQPUz8gwAAAAAAAAAAAAAAAAAABNAV1dX7Nq1a0zP7Nq1K/7lX/6lPIUAAAAAAAAAUsDIMwAAAAAAAAAAAAAAAAAApNz+/ftj48aNRT27YcOG6O/vT7gRAAAAAAAAQDoYeQYAAAAAAAAAAAAAAAAAgJTr7OyMgwcPFvXswYMHo7OzM+FGAAAAAAAAAOlg5BkAAAAAAAAAAAAAAAAAAFKsp6cntm7dWlLGli1boqenJ6FGAAAAAAAAAOlh5BkAAAAAAAAAAAAAAAAAAFIql8vFnXfemUjWmjVrIpfLJZIFAAAAAAAAkBZGngEAAAAAAAAAAAAAAAAAIKW2bdsWr7zySiJZO3fujG3btiWSBQAAAAAAAJAWRp4BAAAAAAAAAAAAAAAAACCFBgYGYv369YlmdnZ2xsDAQKKZAAAAAAAAANXMyDMAAAAAAAAAAAAAAAAAAKTQhg0b4sCBA4lm9vf3x8aNGxPNBAAAAAAAAKhmRp4BAAAAAAAAAAAAAAAAACBlent7o6urqyzZXV1d0dfXV5ZsAAAAAAAAgGpj5BkAAAAAAAAAAAAAAAAAAFIkn8/HunXrIpfLlSV/eHg41q5dG/l8viz5AAAAAAAAANXEyDMAAAAAAAAAAAAAAAAAAKRIb29vdHd3l/Ud3d3d0dfXV9Z3AAAAAAAAAFQDI88AAAAAAAAAAAAAAAAAAJAiLS0tMX/+/LK+Y8GCBdHc3FzWdwAAAAAAAABUAyPPAAAAAAAAAAAAAAAAAACQIplMJjo6OiKbLc+fEtfU1ERHR0dkMpmy5AMAAAAAAABUEyPPAAAAAAAAAAAAAAAAAACQMi0tLdHW1laW7La2tmhubi5LNgAAAAAAAEC1MfIMAAAAAAAAAAAAAAAAAAAptGLFimhqako0s6mpKZYvX55oJgAAAAAAAEA1M/IMAAAAAAAAAAAAAAAAAAAp1NjYGO3t7Ylmtre3R2NjY6KZAAAAAAAAANXMyDMAAAAAAAAAAAAAAAAAAKTUokWLYs6cOYlkzZ07NxYtWpRIFgAAAAAAAEBaGHkGAAAAAAAAAAAAAAAAAICUymazceGFFyaSdeGFF0Y268+TAQAAAAAAAD7MT1EBAAAAAAAAAAAAAAAAACClcrlcbN68OZGszZs3Ry6XSyQLAAAAAAAAIC2MPAMAAAAAAAAAAAAAAAAAQEpt27YtXnnllUSydu7cGdu2bUskCwAAAAAAACAtjDwDAAAAAAAAAAAAAAAAAEAKDQwMxPr16xPN7OzsjIGBgUQzAQAAAAAAAKqZkWcAAAAAAAAAAAAAAAAAAEihDRs2xIEDBxLN7O/vj40bNyaaCQAAAAAAAFDNjDwDAAAAAAAAAAAAAAAAAEDK9Pb2RldXV1myu7q6oq+vryzZAAAAAAAAANXGyDMAAAAAAAAAAAAAAAAAAKRIPp+PdevWRS6XK0v+8PBwrF27NvL5fFnyAQAAAAAAAKqJkWcAAAAAAAAAAAAAAAAAAEiR3t7e6O7uLus7uru7o6+vr6zvAAAAAAAAAKgGRp4BAAAAAAAAAAAAAAAAACBFWlpaYv78+WV9x4IFC6K5ubms7wAAAAAAAACoBkaeAQAAAAAAAAAAAAAAAAAgRTKZTHR0dEQ2W54/Ja6pqYmOjo7IZDJlyQcAAAAAAACoJkaeAQAAAAAAAAAAAAAAAAAgZVpaWqKtra0s2W1tbdHc3FyWbAAAAAAAAIBqY+QZAAAAAAAAAAAAAAAAAABSaMWKFdHU1JRoZlNTUyxfvjzRTAAAAAAAAIBqZuQZAAAAAAAAAAAAAAAAAABSqLGxMdrb2xPNbG9vj8bGxkQzAQAAAAAAAKqZkWcAAAAAAAAAAAAAAAAAAEipRYsWxZw5cxLJmjt3bixatCiRLAAAAAAAAIC0MPIMAAAAAAAAAAAAAAAAAAAplc1mY/Xq1YlkrVq1KrJZf54MAAAAAAAA8GF+igoAAAAAAAAAAAAAAAAAACnW2toa559/fkkZixcvjtbW1oQaAQAAAAAAAKSHkWcAAAAAAAAAAAAAAAAAAEi59vb2aGhoKOrZhoaGaG9vT7gRAAAAAAAAQDoYeQYAAAAAAAAAAAAAAAAAgJSbNm1aLF++vKhnV6xYEU1NTQk3AgAAAAAAAEgHI88AAAAAAAAAAAAAAAAAADABtLW1xaxZs8b0zKxZs+Kiiy4qTyEAAAAAAACAFDDyDAAAAAAAAAAAAAAAAAAAE0BtbW1cc801Y3pm1apVUVtbW6ZGAAAAAAAAANXPyDMAAAAAAAAAAAAAAAAAAEwQCxYsiLPOOuuovnv22WfH/Pnzy9wIAAAAAAAAoLoZeQYAAAAAAAAAAAAAAAAAgAnk6quvjvr6+iN+p76+Pq666qpxagQAAAAAAABQvYw8AwAAAAAAAAAAAAAAAADABDJ9+vS45JJLjvidpUuXxvTp08epEQAAAAAAAED1MvIMAAAAAAAAAAAAAAAAAAATzPDwcEmfAwAAAAAAAPA/jDwDAAAAAAAAAAAAAAAAAMAEsnv37njwwQeP+J0HH3wwdu/ePU6NAAAAAAAAAKqXkWcAAAAAAAAAAAAAAAAAAJhA/u7v/i5yudwRvzM8PBx///d/Pz6FAAAAAAAAAKqYkWcAAAAAAAAAAAAAAAAAAJggtm/fHs8888xRfffpp5+O7u7uMjcCAAAAAAAAqG5GngEAAAAAAAAAAAAAAAAAYAI4fPhw3HHHHWN65vbbb4/Dhw+XqREAAAAAAABA9TPyDAAAAAAAAAAAAAAAAAAAE8BDDz0Ue/fuHdMze/fujX/+538uUyMAAAAAAACA6mfkGQAAAAAAAAAAAAAAAAAAUm7//v3xT//0T0U9e++990Z/f3+yhQAAAAAAAABSwsgzAAAAAAAAAAAAAAAAAACk3Lp162JwcLCoZwcHB2PdunUJNwIAAAAAAABIByPPAAAAAAAAAAAAAAAAAACQYj09PfEf//EfJWU89thj0dPTk1AjAAAAAAAAgPQw8gwAAAAAAAAAAAAAAAAAACmVy+Xib/7mbxLJuv322yOXyyWSBQAAAAAAAJAWRp4BAAAAAAAAAAAAAAAAACCltm7dGq+//noiWbt27YqtW7cmkgUAAAAAAACQFkaeAQAAAAAAAAAAAAAAAAAghQYGBmLdunWJZv7DP/xDDAwMJJoJAAAAAAAAUM2MPAMAAAAAAAAAAAAAAAAAQAqtX78+Dh06lGjm+++/H/fcc0+imQAAAAAAAADVzMgzAAAAAAAAAAAAAAAAAACkTG9vb2zevLks2f/6r/8afX19ZckGAAAAAAAAqDZGngEAAAAAAAAAAAAAAAAAIEXy+Xz87d/+bVnz77jjjsjn82V7BwAAAAAAAEC1MPIMAAAAAAAAAAAAAAAAAAAp8tprr0VPT09Z39HT0xO9vb1lfQcAAAAAAABANTDyDAAAAAAAAAAAAAAAAAAAAAAAAAAAAFAEI88AAAAAAAAAAAAAAAAAAJAis2fPjtbW1rK+45RTTomWlpayvgMAAAAAAACgGhh5BgAAAAAAAAAAAAAAAACAFMlkMnHDDTdEJpOpynwAAAAAAACAamLkGQAAAAAAAAAAAAAAAAAAUqalpSUuvPDCsmR//etfj+bm5rJkAwAAAAAAAFQbI88AAAAAAAAAAAAAAAAAAJBC7e3tMWnSpEQzJ02aFCtXrkw0EwAAAAAAAKCaGXkGAAAAAAAAAAAAAAAAAIAUamxsjKuvvjrRzO985zvR2NiYaCYAAAAAAABANTPyDAAAAAAAAAAAAAAAAAAAKbV48eKYNWtWIlmzZs2K888/P5EsAAAAAAAAgLQw8gwAAAAAAAAAAAAAAAAAACmVzWbjpptuSiTrpptuimzWnycDAAAAAAAAfJifogIAAAAAAAAAAAAAAAAAQIq1trbGV77ylZIy/vAP/zBaW1sTagQAAAAAAACQHkaeAQAAAAAAAAAAAAAAAAAg5To6OqK+vr6oZ+vr66OjoyPhRgAAAAAAAADpYOQZAAAAAAAAAAAAAAAAAABSbtq0afHnf/7nRT17xRVXRFNTU8KNAAAAAAAAANLByDMAAAAAAAAAAAAAAAAAAEwAF198cZx44oljeuakk06KJUuWlKkRAAAAAAAAQPUz8gwAAAAAAAAAAAAAAAAAABNAbW1t3HjjjWN65sYbb4za2toyNQIAAAAAAACofkaeAQAAAAAAAAAAAAAAAABggliwYEGcfvrpR/XdM844I+bPn1/mRgAAAAAAAADVzcgzAAAAAAAAAAAAAAAAAABMINddd13U1NQc8Ts1NTVx7bXXjk8hAAAAAAAAgCpm5BkAAAAAAAAAAAAAAAAAACaQ6dOnx7Jly474ncsuuyymT58+To0AAAAAAAAAqpeRZwAAAAAAAAAAAAAAAAAAmGC+9a1vxZQpU0b9bMqUKfGnf/qn49wIAAAAAAAAoDoZeQYAAAAAAAAAAAAAAAAAgAnmuOOOi2uvvXbUz6677ro47rjjxrcQAAAAAAAAQJUy8gwAAAAAAAAAAAAAAAAAABPQ2WefHa2trSNup5xySpx11lkVagQAAAAAAABQfYw8AwAAAAAAAAAAAAAAAADABJTJZOLGG2+MTCYz6v8MAAAAAAAAwEcz8gwAAAAAAAAAAAAAAAAAABNUc3NzXHjhhRER8fWvfz1mzZpV4UYAAAAAAAAA1cXIMwAAAAAAAAAAAAAAAAAATGDt7e3R3NwcK1eurHQVAAAAAAAAgKpTW+kCAAAAAAAAAAAAAAAAAABA5TQ2NsaaNWuioaGh0lUAAAAAAAAAqk620gUAAAAAAAAAAAAAAAAAAIDKMvAMAAAAAAAAUBwjzwAAAAAAAAAAAAAAAAAAAAAAAAAAAABFqK10AQAAAAAAAICJIJ/Px969e2NoaKjSVYAS7d69u6TPgepQV1cXJ554YmQymUpXAQAAAAAAAAAAAAAAAI5hRp4BAEiMkRpIBwM1MDEYqAEAgPGTz+fjgQceiE2bNsW7775b6TrAOLjpppsqXQFIyJQpU2LZsmXxjW98w3+XBgAAAAAAAAAAwFGzvwDpYH8BJgb7CwAkwcgzAAAlM1IDE4uBGkgPAzUAADA+Nm3aFOvXr690DQCgCO+++27cfffdkcvl4rLLLqt0HQAAAAAAAAAAAI5x9hdgYrG/AOlhfwGAUmUrXQAAgOq3adOmuPvuu/2AAQCqzP8O1GzatKnSVQAAILWGhobi/vvvr3QNAKBE999/fwwNDVW6BgAAAAAAAAAAAMc4+wsAUJ3sLwBQKiPPAACUxEgNAFQ/AzUAAFA+g4OD8d5771W6BgBQovfeey8GBwcrXQMAAAAAAAAAAIBjmP0FAKh+9hcAKJaRZwAASmKkBgCqn4EaAAAon/r6+pg8eXKlawAAJZo8eXLU19dXugYAAAAAAAAAAADHMPsLAFD97C8AUCwjzwAAlMRIDQBUPwM1AABQPnV1dXHppZdWugYAUKJLL7006urqKl0DAAAAAAAAAACAY5j9BQCofvYXACiWkWcAAEpipAYAqp+BGgAAKK9ly5bFlVdeGVOnTq10FQBgjKZOnRpXXXVVLFu2rNJVAAAAAAAAAAAAOMbZXwCA6md/AYBi1Va6AAAA1W/ZsmWRzWbj/vvvjwMHDlS6DgBwlKZOnRrLli2LpUuXVroKAACkWiaTiUsvvTS+8Y1vxN69e2NoaKjSlYAEHT58OIaGhqKuri5qa/0aBqRJXV1dnHjiiZHJZCpdBQAAAAAAAAAAgCphfwEAqpP9BQBK5a8LAQAomZEaSC8DNZBeBmoAAGD8ZTKZOOmkkypdAwAAAAAAAAAAAACAMrG/AOllfwHSy/4CAEnwnxABAEiMkRoAAAAAAAAAAAAAAAAAAABgorO/AAAAMLFkK10AAAAAAAAAAAAAAAAAAAAAAAAAAAAAoBrVVroA8D/6+/tjx44dsWvXrhgYGIhMJhOTJ0+O5ubmOO2006KxsbHSFQEAAAAAAAAAAAAAAAAAAAAAAAAAAPgQI89QQfl8PrZs2RI/+clPYvv27ZHL5Ub9Xl1dXSxcuDCWL18eZ5555ji3BAAAAAAAAAAAAAAAAAAAAAAAAAAAYDRGnqFC+vr64uabb47nn3/+I787NDQUTzzxRDzxxBNx7rnnxg9+8IM4/vjjy18SAAAAAAAAAAAAAAAAAAAAAAAAAACA3ylb6QIwET377LOxdOnSoxp4/v89/vjjcfHFF8drr72WfDEAAAAAAAAAAAAAAAAAAAAAAAAAAACOWm2lC8BEs3PnzrjyyitjYGCg4LP6+vo47bTT4uSTT46hoaF44403YseOHZHL5UZ87+23347LL788HnzwwTjppJPGqzoAAAAAAAAAAAAAAAAAAAAAAAAAAAAfYuQZxtGhQ4fi2muvLRh4zmQycfnll8fKlSvjhBNOGPHZrl27Yt26dfHwww+PuO/ZsyduuOGG2LhxY2QymbJ3BwAAAAAAAAAAAAAAAAAAAAAAAAAAYKRspQvARHLXXXdFb2/viFtNTU3cdttt8d3vfrdg4DkiYtasWXH77bfH9ddfX/DZ008/HQ899FDZ+gIAAAAAAAAAAAAAAAAAAAAAAAAAAPC7GXmGcfLrX/867r333oL7ypUr46KLLvrI56+88sq44IILCu4/+tGPYnBwMImKAAAAAAAAAAAAAAAAAAAAAAAAAAAAjIGRZxgnP/7xj+ODDz4YcfvkJz8Z11xzzVFnfP/734/JkyePuO3bty8eeeSRRDoCAAAAAAAAAAAAAAAAAAAAAAAAAABw9Iw8wzjI5/Px8MMPF9yvuOKKOO644446Z9q0abF06dKCe1dXV0n9AAAAAAAAAAAAAAAAAAAAAAAAAAAAGDsjzzAOXnzxxfjVr3414lZXVxdf+9rXxpzV1tZWcHvmmWfinXfeKbofAAAAAAAAAAAAAAAAAAAAAAAAAAAAY2fkGcbBk08+WXBbsGBBTJ48ecxZn/70p2PmzJkjbvl8Pn7xi18U3Q8AAAAAAAAAAAAAAAAAAAAAAAAAAICxM/IM46C7u7vgdvrppxedd8YZZxTcnn322aLzAAAAAAAAAAAAAAAAAAAAAAAAAAAAGDsjzzAOXn755YLbqaeeWnTeKaecUnDr6ekpOg8AAAAAAAAAAAAAAAAAAAAAAAAAAICxM/IMZbZ///7Yv39/wX327NlFZ7a0tBTcXn311aLzAAAAAAAAAAAAAAAAAAAAAAAAAAAAGDsjz1Bmb775ZsEtk8nEjBkzis6cOXNmwe3gwYPR399fdCYAAAAAAAAAAAAAAAAAAAAAAAAAAABjU1vpApB2e/fuLbhNnTo16urqis484YQTRr3v2bMnmpqais49Fr3wwguRyWQqXQMAAAAAAAAAAIBjWD6fr3QFAAAAAAAAAAAAAAAmKCPPUGb9/f0Ft6lTp5aU2dDQEHV1dTE0NDTifuDAgZJyj0W5XK7SFQAAAAAAAAAAAAAAAAAAAAAAAAAAAEaVrXQBSLvf/OY3BbeGhoaSc0fLGO1dAAAAAAAAAAAAAAAAAAAAAAAAAAAAlIeRZyizoaGhgltdXV3JuaNlDA4OlpwLAAAAAAAAAAAAAAAAAAAAAAAAAADA0amtdAFIu9FGnmtrS/8/vdFGnkd7V7XLZrORyWQqXQMAAAAAAAAAAIBjWD6fj1wuV+kaAAAAAAAAAAAAAABMQEaeocyy2WzB7fDhwyXnjjboPNq7qt28efNS+b8XAAAAAAAAAAAAycnlcvHcc89VugYAAAAAAAAAAAAAABOQ5VQos9rawi31Dz74oOTc0TLq6upKzgUAAAAAAAAAAAAAAAAAAAAAAAAAAODoGHmGMps0aVLBLYmR59/+9rcFt49//OMl5wIAAAAAAAAAAAAAAAAAAAAAAAAAAHB0jDxDmU2ZMqXgdujQoZIyh4eHY3BwsOA+derUknIBAAAAAAAAAAAAAAAAAAAAAAAAAAA4ekaeocyOP/74gtu+ffsil8sVnblnz56jfhcAAAAAAAAAAAAAAAAAAAAAAAAAAADlYeQZymzGjBkFt6Ghodi3b1/RmW+99VbBLZvNxvTp04vOBAAAAAAAAAAAAAAAAAAAAAAAAAAAYGyMPEOZzZgxI2prawvub7/9dtGZu3fvLrh94hOfiLq6uqIzAQAAAAAAAAAAAAAAAAAAAAAAAAAAGBsjz1Bm9fX1MXv27IL7yy+/XHTmaM/OmTOn6DwAAAAAAAAAAAAAAAAAAAAAAAAAAADGzsgzjIPPfvazBbfu7u6i80Z7drR3AAAAAAAAAAAAAAAAAAAAAAAAAAAAUD5GnmEcfPGLXyy4bd++vaisQ4cOxY4dOwruX/rSl4rKAwAAAAAAAAAAAAAAAAAAAAAAAAAAoDhGnmEcnHnmmVFbWzvi9tZbb8Uzzzwz5qwtW7bE4ODgiFtTU1N87nOfK6kjAAAAAAAAAAAAAAAAAAAAAAAAAAAAY2PkGcbBtGnT4stf/nLB/f777x9z1gMPPFBwu+CCC6KmpqaobgAAAAAAAAAAAAAAAAAAAAAAAAAAABTHyDOMk29+85sFt0cffTReeumlo8742c9+Fs8999yIWyaTGTUbAAAAAAAAAAAAAAAAAAAAAAAAAACA8jLyDOPknHPOiVNOOWXEbXh4OG655ZY4ePDgRz6/b9+++Ku/+quC+x/90R/F7//+7yfWEwAAAAAAAAAAAAAAAAAAAAAAAAAAgKNj5BnGSSaTib/8y7+MTCYz4v7LX/4y/uzP/iz27dv3O5997bXXYtmyZQXfOe644+Lmm28uS18AAAAAAAAAAAAAAAAAAAAAAAAAAACOrLbSBWAi+YM/+INYuXJldHZ2jrjv2LEjvvrVr8bSpUvjK1/5SsycOTMOHz4cr7/+emzdujU2b94cQ0NDBXm33HJLnHzyyeNVHwAAAAAAAAAAAAAAAAAAAAAAAAAAgA8x8gzj7Prrr4833ngjtm3bNuL+/vvvx4YNG2LDhg1HlbNixYpYtmxZOSoCAAAAAAAAAAAAAAAAAAAAAAAAAABwFLKVLgATTU1NTfzwhz8seqA5k8nE6tWr45Zbbkm4GQAAAAAAAAAAAAAAAAAAAAAAAAAAAGNh5BkqoK6uLm699da4++6741Of+tRRPzdv3ry47777oqOjo4ztAAAAAAAAAAAAAAAAAAAAAAAAAAAAOBq1lS4AE9m5554b55xzTjz11FPx2GOPxYsvvhi7du2KgwcPRkTE5MmTo7m5OebPnx/nnXdezJs3r8KNAQAAAAAAAAAAAAAAAAAAAAAAAAAA+F9GnqHCMplMLFy4MBYuXFjpKgAAAAAAAAAAAAAAAAAAAAAAAAAAAIxBttIFAAAAAAAAAAAAAAAAAAAAAAAAAAAAAKqRkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACAIhh5BgAAAAAAAAAAAAAAAAAAAAAAAAAAACiCkWcAAAAAAAAAAAAAAAAAAAAAAAAAAACAItRWugAAAAAAAAAAAAAAAMBE0d/fHzt27Ihdu3bFwMBAZDKZmDx5cjQ3N8dpp50WjY2Nla4IAAAAAAAAAAAAjIGRZwAAAAAAAAAAAAAAgDLK5/OxZcuW+MlPfhLbt2+PXC436vfq6upi4cKFsXz58jjzzDPHuSUAAAAAAAAAAABQDCPPAAAAAAAAAAAAAAAAZdLX1xc333xzPP/88x/53aGhoXjiiSfiiSeeiHPPPTd+8IMfxPHHH1/+kgAAAAAAAAAAAEDRspUuAAAAAAAAAAAAAAAAkEbPPvtsLF269KgGnv9/jz/+eFx88cXx2muvJV8MAAAAAAAAAAAASExtpQsAAAAAAAAAAAAAAACkzc6dO+PKK6+MgYGBgs/q6+vjtNNOi5NPPjmGhobijTfeiB07dkQulxvxvbfffjsuv/zyePDBB+Okk04ar+oAAAAAAAAAAADAGBh5BgAAAAAAAAAAAAAASNChQ4fi2muvLRh4zmQycfnll8fKlSvjhBNOGPHZrl27Yt26dfHwww+PuO/ZsyduuOGG2LhxY2QymbJ3BwAAAAAAAAAAAMYmW+kCAAAAAAAAAAAAAAAAaXLXXXdFb2/viFtNTU3cdttt8d3vfrdg4DkiYtasWXH77bfH9ddfX/DZ008/HQ899FDZ+gIAAAAAAAAAAADFM/IMAAAAAAAAAAAAAACQkF//+tdx7733FtxXrlwZF1100Uc+f+WVV8YFF1xQcP/Rj34Ug4ODSVQEAAAAAAAAAAAAEmTkGQAAAAAAAAAAAAAAICE//vGP44MPPhhx++QnPxnXXHPNUWd8//vfj8mTJ4+47du3Lx555JFEOgIAAAAAAAAAAADJMfIMAAAAAAAAAAAAAACQgHw+Hw8//HDB/YorrojjjjvuqHOmTZsWS5cuLbh3dXWV1A8AAAAAAAAAAABInpFnAAAAAAAAAAAAAACABLz44ovxq1/9asStrq4uvva1r405q62treD2zDPPxDvvvFN0PwAAAAAAAAAAACB5Rp4BAAAAAAAAAAAAAAAS8OSTTxbcFixYEJMnTx5z1qc//emYOXPmiFs+n49f/OIXRfcDAAAAAAAAAAAAkmfkGQAAAAAAAAAAAAAAIAHd3d0Ft9NPP73ovDPOOKPg9uyzzxadBwAAAAAAAAAAACTPyDMAAAAAAAAAAAAAAEACXn755YLbqaeeWnTeKaecUnDr6ekpOg8AAAAAAAAAAABInpFnAAAAAAAAAAAAAACAEu3fvz/2799fcJ89e3bRmS0tLQW3V199teg8AAAAAAAAAAAAIHlGngEAAAAAAAAAAAAAAEr05ptvFtwymUzMmDGj6MyZM2cW3A4ePBj9/f1FZwIAAAAAAAAAAADJMvIMAAAAAAAAAAAAAABQor179xbcpk6dGnV1dUVnnnDCCaPe9+zZU3QmAAAAAAAAAAAAkKzaShcAAAAAAAAAAAAAAACodv39/QW3qVOnlpTZ0NAQdXV1MTQ0NOJ+4MCBknKPRS+88EJkMplK1wAAAAAAAAAAAOAYls/nK11hVEaeAQAAAAAAAAAAAAAASvSb3/ym4NbQ0FBybkNDQ8Go82jvqna5XK7SFQAAAAAAAAAAAKAo2UoXAAAAAAAAAAAAAAAAqHZDQ0MFt7q6upJzR8sYHBwsORcAAAAAAAAAAABIhpFnAAAAAAAAAAAAAACAEo028lxbW1ty7mgjz6O9CwAAAAAAAAAAAKiM0n9bEAAAAAAAAAAAAAAAYILLZrMFt8OHD5ecO9qg82jvqnbZbDYymUylawAAAAAAAAAAAHAMy+fzkcvlKl2jgJFnAAAAAAAAAAAAAACAEtXWFv6JxgcffFBy7mgZdXV1Jecea+bNm5fK8WoAAAAAAAAAAACSk8vl4rnnnqt0jQJ++w0AAAAAAAAAAAAAAKBEkyZNKrglMfL829/+tuD28Y9/vORcAAAAAAAAAAAAIBlGngEAAAAAAAAAAAAAAEo0ZcqUgtuhQ4dKyhweHo7BwcGC+9SpU0vKBQAAAAAAAAAAAJJj5BkAAAAAAAAAAAAAAKBExx9/fMFt3759kcvlis7cs2fPUb8LAAAAAAAAAAAAqAwjzwAAAAAAAAAAAAAAACWaMWNGwW1oaCj27dtXdOZbb71VcMtmszF9+vSiMwEAAAAAAAAAAIBk1Va6AEA+n/+dn+VyuXFsAgAAAAAAAAAAQDU60u+aHel31AAgSTNmzIja2to4fPjwiPvbb78dJ510UlGZu3fvLrh94hOfiLq6uqLyKs3vjgMAAAAAAAAAAFCKY/V3x408AxV3pH8TfOGFF8axCQAAAAAAAAAAAGlj5BmA8VJfXx+zZ8+OX/7ylyPuL7/8cnz+858vKvPll18uuM2ZM6eorGOB3x0HAAAAAAAAAACgXCr5u+PZir0ZAAAAAAAAAAAAAAAgRT772c8W3Lq7u4vOG+3Z0d4BAAAAAAAAAAAAVI6RZwAAAAAAAAAAAAAAgAR88YtfLLht3769qKxDhw7Fjh07Cu5f+tKXisoDAAAAAAAAAAAAysPIMwAAAAAAAAAAAAAAQALOPPPMqK2tHXF766234plnnhlz1pYtW2JwcHDErampKT73uc+V1BEAAAAAAAAAAABIlpFnAAAAAAAAAAAAAACABEybNi2+/OUvF9zvv//+MWc98MADBbcLLrggampqiuoGAAAAAAAAAAAAlIeRZwAAAAAAAAAAAAAAgIR885vfLLg9+uij8dJLLx11xs9+9rN47rnnRtwymcyo2QAAAAAAAAAAAEBlZfL5fL7SJYCJLZfLRS6XG/WzTCYTmUxmnBsBAAAAAAAAAABQTfL5fPyuX4nNZrORzWbHuREAE1k+n48lS5bEyy+/POL+mc98JjZt2hQNDQ1HfH7fvn3R1tYW+/btG3E/77zzYu3atYn3HU9+dxwAAAAAAAAAAIBSHKu/O27kGQAAAAAAAAAAAAAAIEHPPvtsfOtb3yr4Q5JTTz017r777jjhhBNGfe61116Lb3/72/Hmm2+OuB933HHxb//2b3HyySeXrTMAAAAAAAAAAABQHCPPAAAAAAAAAAAAAAAACbvjjjuis7Oz4D5p0qRYunRpfOUrX4mZM2fG4cOH4/XXX4+tW7fG5s2bY2hoqOCZW2+9NZYtWzYetQEAAAAAAAAAAIAxMvIMAAAAAAAAAAAAAACQsOHh4bjuuuti27ZtJeWsWLEibrnlloRaAQAAAAAAAAAAAEkz8gwAAAAAAAAAAAAAAFAGQ0ND8dd//dexadOmMT+byWRi1apV0dHRUYZmAAAAAAAAAAAAQFKMPAMAAAAAAAAAAAAAAJTR448/Hrfffnu8+uqrR/X9efPmxfe+9734whe+UOZmAAAAAAAAAAAAQKmMPAMAAAAAAAAAAAAAAJRZPp+Pp556Kh577LF48cUXY9euXXHw4MGIiJg8eXI0NzfH/Pnz47zzzot58+ZVuC0AAAAAAAAAAABwtIw8AwAAAAAAAAAAAAAAAAAAAAAAAAAAABQhW+kCAAAAAAAAAAAAAAAAAAAAAAAAAAAAANXIyDMAAAAAAAAAAAAAAAAAAAAAAAAAAABAEYw8AwAAAAAAAAAAAAAAAAAAAAAAAAAAABTByDMAAAAAAAAAAAAAAAAAAAAAAAAAAABAEYw8AwAAAAAAAAAAAAAAAAAAAAAAAAAAABTByDMAAAAAAAAAAAAAAAAAAAAAAAAAAABAEYw8AwAAAAAAAAAAAAAAAAAAAAAAAAAAABTByDMAAAAAAAAAAAAAAAAAAAAAAAAAAABAEYw8AwAAAAAAAAAAAAAAAAAAAAAAAAAAABTByDMAAAAAAAAAAAAAAAAAAAAAAAAAAABAEYw8AwAAAAAAAAAAAAAAAAAAAAAAAAAAABTByDMAAAAAAAAAAAAAAAAAAAAAAAAAAABAEYw8AwAAAAAAAAAAAAAAAAAAAAAAAAAAABTByDMAAAAAAAAAAAAAAAAAAAAAAAAAAABAEYw8AwAAAAAAAAAAAAAAAAAAAAAAAAAAABTByDMAAAAAAAAAAAAAAAAAAAAAAAAAAABAEYw8AwAAAAAAAAAAAAAAAAAAAAAAAAAAABTByDMAAAAAAAAAAAAAAAAAAAAAAAAAAABAEYw8AwAAAAAAAAAAAAAAAAAAAAAAAAAAABTByDMAAAAAAAAAAAAAAAAAAAAAAAAAAABAEYw8AwAAAAAAAAAAAAAAAAAAAAAAAAAAABTByDMAAAAAAAAAAAAAAAAAAAAAAAAAAABAEYw8AwAAAAAAAAAAAAAAAAAAAAAAAAAAABTByDMAAAAAAAAAAAAAAAAAAAAAAAAAAABAEYw8AwAAAAAAAAAAAAAAAAAAAAAAAAAAABTByDMAAAAAAAAAAAAAAAAAAAAAAAAAAABAEYw8AwAAAAAAAAAAAAAAAAAAAAAAAAAAABTByDMAAAAAAAAAAAAAAAAAAAAAAAAAAABAEYw8AwAAAAAAAAAAAAAAAAAAAAAAAAAAABTByDMAAAAAAAAAAAAAAAAAAAAAAAAAAABAEYw8AwAAAAAAAAAAAAAAAAAAAAAAAAAAABTByDMAAAAAAAAA/L/27jy6yvLcG/CdEKYwoyACigjiCAoKCCKKigoOSdTTY+tna6312CoerdWqtVrbYkfr59BqtVqHftSRBESBCk6gVRQRGQQVBQEVkQghYQqQ7w+XtNu9E5Kd0XBda7nWyf0+z/3ciZy16psfzwYAAAAAAAAAAAAAAAAAgDS45BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgDS55BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEiDS54BAAAAAAAAAAAAAAAAAAAAAAAAAAAA0uCSZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIA0uOQZAACAOnH77bfH/vvvn/DPq6++Wt9jfS2MHz8+6Wc3fvz4+h4LAAAAgEYm1Xuo22+/vb7HAgAAAAAA4GtGdjx9suMAAAAA1BX5cQCAmpVV3wMAAABUxrZt2+K9996L999/P9avXx9FRUVRWloaLVu2jOzs7OjUqVN07949unXrFtnZ2fU9LgAAAAAAAAAAAAAAaZAdBwAAAAAAAODrxiXPAABAg7V58+aYPHly5Ofnx1tvvRUbNmzY6Z6MjIzYe++9o2/fvtG3b98YOnRo9OnTpw6mBQAAAIDG49xzz41Zs2ZVam2TJk2iWbNm0axZs2jbtm3stttu0blz5+jZs2fsu+++ceihh0bPnj1reWIAAAAAAAC+zmTHAQAAAKD+yI8DAED1ueQZAABocMrKyuKRRx6JP/7xj7Fu3boq7122bFksW7YsJk2aFBERe+yxR1xxxRWRk5NTG+MCAAAAwC5t27ZtsXHjxti4cWOsW7culi9fnrRmt912i2OOOSZGjRoVw4YNi8zMzHqYFAAAAAAAgIZGdhwAAAAAvl7kxwEAIDWXPAMAAA1KUVFRjBkzJl555ZUa67lq1ap49913a6wfAAAAAFA1a9asifHjx8f48eNjr732iu9+97vx3//935GVJbaQrtmzZ8cPf/jDhNqpp54aP/vZz+ppIgAAAAAAgKqRHQcAAACAxkl+vObJjwMANHz+1y4AANBgFBcXx/e+97146623yl3TqVOn2H///aN9+/bRqlWr2LRpU6xduzZWrVoV7733XmzdurUOJwYAAAAAqmr58uXxi1/8Iv7xj3/EL37xixgwYEB9j/S1tHXr1li7dm1CraSkpH6GAQAAAAAAqCLZcQAAAADYNciP1wz5cQCAhs8lzwAAQINx7bXXpgzptmnTJs4777w47bTTokePHuXu37x5cyxcuDBmzJgRU6dOjffee682x6WKxowZE2PGjKnvMQAAAABI0/e+97244IILUj7bunVrbNmyJdauXRuffvppvP/++/H222/Hq6++GqtXr0655913341vf/vb8ZOf/CTOPffc2hwdAAAAAACABkZ2vHGTHQcAAAD4+pMfBwCAqnHJMwAA0CC8/PLLMXXq1KT6wIED47bbbouOHTvutEfz5s2jf//+0b9//7j00ktjyZIl8fe//z0KCgpqYWIAAAAA2LW0bNlyp+/punfvHhERxx133I7aG2+8EY888khMmjQptm7dmrC+tLQ0fvWrX8XGjRvjwgsvrPmhAQAAAAAAaHBkxwEAAACg4ZMfBwCAqsms7wEAAAAiIu65556k2n777Rf33ntvpUK6qfTq1StuuOGGeOGFF+KUU06p7ogAAAAAQBoGDBgQv/3tb2PSpElxxBFHpFxz8803x5QpU+p4MgAAAAAAAOqD7DgAAAAANF7y4wAA7Kpc8gwAANS7oqKieO2115LqV199dTRv3rza/du2bRsHHnhgtfsAAAAAAOnr2bNnPPjgg/HNb34z5fOf/vSn8cknn9TxVAAAAAAAANQl2XEAAAAA2DXIjwMAsKtxyTMAAFDv5syZE6WlpQm19u3bx5AhQ+ppIgAAAACgNjRp0iR+/vOfR05OTtKz4uLiuPnmm+thKgAAAAAAAOqK7DgAAAAA7DrkxwEA2JVk1fcAAAAAn376aVKtW7du0aRJk3qY5utl48aNMX/+/Fi6dGmsXbs2SktLIzs7O/bff39B51q2ZcuW+PDDD+P999+PNWvWRHFxcWzfvj3atm0b7dq1i7333jsOPPBAf44BAAAAUrjxxhvjzTffjGXLliXUn3zyybjkkkuiR48e1epfXFwcCxcujA8//HDHe7N27drF7rvvHnvvvXcccMAB1epfnrKysli5cmW8//778cknn0RxcXFs3rw52rRpE+3atYvOnTtH3759Izs7u1bOr0sbN26Mt956K95///0oKiqKpk2bRseOHaN79+5x6KGHRtOmTet7RAAAAAAAoAGSHU+f7Hj9kR0HAAAAqB75cflxAIBdgUueAQCAeldYWJhUy8qq+/9cufrqqyM/Pz+hNn369OjevXta/Y477rhYuXLljq+7desWzz777E73jR8/Pq655pqE2iWXXBJjxozZ8fW//vWveOCBB+Kll16KLVu2JPUYNGhQDBkyJCZNmhRXXHFFwrPBgwfHgw8+WNVvJ8lTTz0VP/rRjxJqQ4cOjb/97W8p199+++1xxx13JNQefPDBGDx4cNLazZs3x1FHHRXr16/fUcvIyIhp06al/e/jS1u2bImjjjoqioqKEno/++yz0bVr13L3lZWVxezZs2PGjBnx6quvxvz586O0tLTCs7Kzs+Pwww+Pc845J0aMGFGtuQEAAAAak5YtW8aVV14Zl1xySUK9rKwsxo0bl/R+rDI2bdoUBQUFMXHixHjzzTdj27Zt5a7t1KlTHHvssfG9730vevbsWeWz/tP7778fzz77bMyaNStmz54dxcXFFa7PysqKgw46KHJzc+Oss86K5s2bV/qsc889N2bNmlXu8/z8/KR3nKl89X1jVSxatCjuueeemDZtWmzatCnlmlatWsXIkSNjzJgx1X6fBwAAAAAANC6y44lkx2XHAQAAAHYF8uPy4wAAu4LM+h4AAAAg1UvoFStWRFlZWT1M07CtX78+Lr300jjvvPPiueeeSxnS/U8jR46MNm3aJNRmzZoVH330UbVnKSgoSKrl5uZWu2/EF38mRo8enVArKyuLCRMmVLv3tGnTEkK6ERFHHnlkhSHdv/3tb3HsscfGOeecE3fddVfMmTNnpyHdiIgNGzbEjBkz4qKLLoqcnJxYvHhxtecHAAAAaCxGjhwZ++67b1J9ypQpVe5VUFAQxx9/fNxwww0xe/bsCgO6ERGrV6+Oxx57LE499dS48cYbY8OGDVU+c+bMmZGbmxujRo2K3//+9/HCCy/sNKAbEbF169Z466234he/+EUcd9xxMXny5CqfXR+2bdsWN998c5xxxhkxadKkcgO6ERElJSVRUFAQo0aNikcffbQOpwQAAAAAABo62fHKkx2XHQcAAABoTOTH5ccBABo7lzwDAAD1rkOHDkm1NWvWxOuvv14P0zRchYWFcfbZZ8fUqVMrvad58+YxatSohFpNBF5Xr14dL730UkItOzs7TjzxxGr1/U+pQr81EdRN1WNnAeNnn302Pvnkk2qdu2jRojj77LNj2rRp1eoDAAAA0Jikei/zySefxNtvv12p/Zs2bYrLL788fvKTn8Rnn31W5fO3bt0a48aNi3PPPTfWrFlTpb1z5syp9Jzl+eyzz+Kyyy6LW265pVp9altpaWlccsklcffdd+80AP2ftmzZEj/72c/ioYceqsXpAAAAAACArxPZ8cqRHZcdBwAAAGiM5MflxwEAGrOs+h4AAADgkEMOSVn/5S9/GePGjYvWrVvX8UQNz7Zt2+KHP/xhvPfeewn1PffcMw444IDo2LFjbNmyJVatWhULFixIWJObm5v0aYcFBQXxgx/8IO15nnzyyaSX8SeffHK0bNky7Z5fNWDAgNhnn31i6dKlO2rLli2LN954IwYMGJBWzzVr1sTMmTMTaukGjDMyMmKvvfaKffbZJ9q0aROtW7eOzZs3x9q1a2PRokUpg70bNmyIyy+/PB577LE44IAD0voeAAAAABqT4cOHxx//+Mek+pw5c+LAAw+scO/mzZvj+9//fsyaNSvl8xYtWkTfvn2jc+fO0bp161i3bl0sXbo0Fi9eHGVlZQlr58+fH9/61rfi8ccfjzZt2qT/DUXE7rvvHr1794727dtHmzZtYvv27bF+/fpYsmRJfPDBB7F9+/akPXfddVfsueeecfbZZ1fr7Nry05/+NJ599tmEWpcuXeLAAw+Mjh07xtatW+Pjjz+OOXPmRGlpadL+3/72tzF48ODo06dPXY0MAAAAAAA0ULLjOyc7/gXZcQAAAIDGR378C/LjAACNk0ueAQCAeterV6/o0qVLUrBx8eLFccYZZ8S1114bxxxzTGRkZNTThPXv8ccfj9WrV+/4esSIETFmzJg4+OCDk9Zu2LAh5s2bt+Prww8/PHr06BHLli3bUVu6dGm8+eabcdhhh6U1T0FBQVItLy8vrV4VycnJiVtvvTXp7HSDuk8++WRs3bo1oXbyySdHdnZ2pfZ36tQpjj/++Bg5cmQcdthhFYbIP/jggxg3blyMGzcu4cwtW7bEZZddFhMnToxmzZql9X0AAAAANBYHHHBAZGdnx4YNGxLqCxcu3OnesWPHpgzoHnrooXHhhRfGMcccE02bNk16vmrVqvjLX/4SjzzySMJ7m6VLl8b1118ft9xyS5W+h5YtW8bw4cNj5MiRMWTIkNh9993LXVtYWBgFBQVx9913x+eff57w7KabboqBAwdGr169yt3/5z//eUcI9o033oiLL7444fkpp5wS1113XaVmrqxJkyYl/GX6kSNHxsUXX5wyRL1+/fq4884747777ksIQpeWlsbYsWPjgQceqPS5AAAAAABA4yQ7vnOy44lny44DAAAANB7y4/8mPw4A0Phk1vcAAAAAERHf+c53UtaXLVsW//M//xMnnnhi/O53v4t//etfUVxcXMfT1b8vQ7oZGRlx/fXXx1133ZUypBsRkZ2dHYMHD06o5eTkJK3Lz89Pa5a33347Fi9enFDr1q1bDBw4MK1+FcnNzU0KaE+ePDm2bNmSVr9U33Nubu5O9/Xu3Tt+85vfxHPPPRc33nhjDBs2rMKQbkREz54946c//Wk8+uijsdtuuyU8++CDD+Lpp5+u0uwAAAAAjVFGRkbst99+SfUPP/ywwn2TJ0+ORx55JKnXj370o3jkkUfihBNOSBnQjYjYY4894vrrr4+//OUv0apVq4RnTz/9dEyZMqVSs3fs2DEuvfTSeP755+O2226L0047rcKA7pd7zj///Hjqqaeif//+Cc82b94c9957b4X727RpEx07doyOHTtGmzZtkp43a9Zsx/OK/qlKSPfLgG7Tpk3jpptuijvuuCNlQPfL+a666qr4+c9/nvTslVdeSQj7AgAAAAAAuy7Z8YrJjv+b7DgAAABA4yI//m/y4wAAjY9LngEAgAbhm9/8Zuy7777lPv/www/j3nvvjfPOOy8GDhwYp512Wlx77bXx8MMPx6JFi2L79u11OG39ufTSS+Occ86p8r6aDLwWFBRUqn9N6Nq1a1LouKioKKZPn17lXosWLYpFixYl1Lp16xaDBg3a6d4bbrgh8vLyyv3FTkUOPvjguOeeeyIrKyuh/tBDD1W5FwAAAEBjtOeeeybVVq1aVe76LVu2xNixY5PqV199dfzP//xPpd9TDRs2LH73u98l1e++++5K7T/nnHPi4osvjvbt21dq/X/abbfd4q9//Wt069YtoT5p0qQoLCyscr+6cN1118WZZ55ZqbVnn312DB8+PKn+1FNP1fRYAAAAAADA15DseOXIjsuOAwAAADRG8uP/Jj8OANC4uOQZAABoEFq2bBl33XVX7Lbbbjtdu3379njnnXfiiSeeiBtuuCFycnJi8ODBcdFFF8XDDz8cRUVFdTBx3evTp09ceOGFae1NFUhdt25dPPvss1Xqs3Xr1pg0aVJSPTc3N625KiMvLy+pliosvDOp9uTk5NRKwPirDj744KRfYMyfPz8+/fTTWj8bAAAAoKHbfffdk2pr164td/2ECRNi9erVCbVjjz02zjvvvCqffcIJJ8To0aMTagsWLIjXX3+9yr2qqnXr1vGjH/0oobZ58+Z4+eWXa/3sqho2bFicffbZVdrzne98J6k2b968mhoJAAAAAAD4GpMd3znZ8X+THQcAAABoXOTH/01+HACgcXHJMwAA0GD06NEjxo8fH/3796/y3qKionjuuefihhtuiGHDhsVVV10VK1asqIUp68+3v/3tyMrKSnt/TQReZ86cGZ999llC7fDDD4+999477bl25sQTT4zs7OydzlGRbdu21XnA+KtGjRqVVJs7d26dnQ8AAADQULVs2TKptmnTpnLXP/DAA0m1K664Iu3zv/vd7ybVpk+fnna/qjj++OOjefPmCbU333yzTs6uinQuEBgyZEi0aNEiobZgwYKaGgkAAAAAAPiakx2vmOx4xXNURHYcAAAAoGGTH5cfBwBorFzyDAAANChdunSJcePGxe9+97vo0aNHWj02b94cEyZMiFGjRsUdd9wRZWVlNTxl3WvSpEnKoGdVpAq8zpgxIwoLCyvdIz8/P6mWKgBck7Kzs+Okk05KqG3dujVl8LY8M2fOTPp0zgEDBqT9Zywdffr0SarNnz+/zs4HAAAAaKiaNm2aVNuyZUvKtR9//HG8++67CbUDDzww5buXyurbt2+0b98+oTZnzpy0+1VFy5Yto3v37gm1hvbOqEOHDjFo0KAq72vSpEnst99+CbXVq1c3ive1AAAAAABAzZAdT012XHYcAAAAoDGTH5cfBwBorFzyDAAANDiZmZmRk5MTU6dOjQceeCDOPPPM2H333avcZ8uWLXH77bfHD3/4wwo/ufHroFevXtG6detq9WjVqlWceOKJCbWtW7fGk08+Wan9RUVF8dxzzyXUWrRoESeffHK15qqM3NzcpFpBQUGl96daWxsB402bNsXnn38ehYWFSf+k8tlnn9X4DAAAAABfN6kCuc2aNUu59rXXXkuqDR06tFrnZ2RkxAEHHJBQmz9/fpSWllar75e2bNkSa9euTfnOqLCwMNq0aZOwfs2aNTVybk3p379/ZGRkpLW3U6dOCV+XlZVFSUlJTYwFAAAAAAA0ErLjyWTHc5NqsuMAAAAAjYf8uPw4AEBjlVXfAwAAAJQnIyMjjjzyyDjyyCMjImLJkiXx+uuvx7x582LhwoXxzjvvVOpF+bPPPhvXXXdd/OEPf6jtkWvN/vvvXyN98vLykkKrBQUF8Z3vfGene59++unYvHlzQu2EE05I+iVCbRg8eHB069YtVq5cuaP29ttvx6JFi5J+gfJV69evj+nTpyfUmjdvHqNGjUp7nlWrVsUzzzwTCxYsiMWLF8fy5cujpKQktm3bVqU+69evT3sGAAAAgMYi1V+yb9GiRcq1c+bMSar17t272jO0b98+4evS0tJYu3ZtUsi0IiUlJfHcc8/FnDlz4p133oklS5ZEUVFRlcO+RUVFVVpf27p06ZL23latWiXViouLq30pAQAAAAAA0PjIjv+b7LjsOAAAAEBjJj+eSH4cAKDxcMkzAADwtdGrV6/o1atX/Pd//3dEfPGifN68eTFr1qyYOnVqLFy4sNy9Tz75ZAwbNixyc3PraNqa9dVfEqQrVeD1y9Bznz59Ktz71YBvRNTZzzMjIyNycnLiz3/+c0J9woQJOw3qTp48ucYCxm+++WbccsstMWvWrNi+fXuV93+VoC4AAABAxOrVq5Nq5b0P+/jjj5Nq11xzTVxzzTU1PVasW7euUiHdTz75JG655ZaYMmVKysBxVRUXF1e7R01q27Zt2nuzspJjKVu3bq3OOAAAAAAAwC5Cdrz6ZMe/IDsOAAAA0LDIjyeSHwcAaDwy63sAAACAdDVt2jQGDBgQF110UeTn58f48eNj2LBh5a7/05/+FNu2bavDCWtOqk8sTEdGRkacfvrpSfX8/PwK9y1btizpUy47d+4cQ4cOrZG5KiNVKPjJJ5/c6b/TVN9bVQPG27ZtixtuuCHOPvvseOWVV2okpBvhFxIAAAAAEREfffRRUm3PPfdMuXbdunW1PU6Vznr00Udj1KhRUVBQUCMB3YiG984oVdAWAAAAAACgrsmOV53sePm9KiI7DgAAAFC75McTNbT3RvLjAADpc8kzAADQaBx88MFx7733xhVXXJHy+YcffhivvPJKHU9VM2ryRXheXl5SbWeB14KCgqTa6aefHk2aNKmxuXamR48eMWDAgITa6tWrY+bMmeXu+fDDD+ONN95IqHXq1CmOOuqoSp+7bdu2uPzyy+Phhx+OsrKyqg0NAAAAQIW2b98e7733XlJ97733Trm+LkO6OwvL3n///fGzn/0sNmzYUEcTAQAAAAAA8CXZ8cqRHZcdBwAAAGhI5McBAGjMfFwGAADQ6Fx44YWxfPnyePTRR5Oe/etf/6pSSLMx6tGjR/Tv3z/mzJmzo7Z69ep46aWXYvjw4Unry8rKYsKECUn1VIHf2paXl5cUvC0oKIhjjjkm5fqaCBg/9NBDMXXq1KR6RkZGDB48OAYOHBgHHXRQdOnSJTp16hQtW7aMZs2aRbNmzZL27L///pU+FwAAAGBXsHDhwti4cWNS/eCDD065fmfB2bqyYMGC+O1vf5vyWZ8+feKoo46Kfv36RdeuXaNLly6RnZ0dzZs3j+bNmyetP/fcc2PWrFm1PTIAAAAAAECjJDteMdlx2XEAAACAhkR+XH4cAKAxc8kzAADQKI0ZMyYef/zx2L59e0L97bffrrMZysrK6uysqsrLy0sI6kZETJgwIWVQ97XXXouVK1cm1A455JDo3bt3rc6YyqhRo2Ls2LGxadOmHbXp06fH+vXro02bNglry8rKYuLEiUk9qhIwLioqittvvz2pPmjQoLjppptir732qnSvzZs3V3otAAAAwK5ixowZKev9+/dPWW/RokVS7f7776+VvyD91fdN/+mmm25KevfYrVu3uOmmm+LII4+s0jneGwEAAAAAAFSP7HjFZMcrR3YcAAAAoPbJj3t3BADQmLnkGQAAaJQ6d+4cBx54YCxYsCCh/vnnn5e7JyMjI6lWnbDt+vXr095b20aPHh1jx45N+AXAtGnTori4OFq3bp2wNj8/P2l/VcKuNalNmzZxwgknxKRJk3bUNm/eHJMnT45vfOMbCWtff/31WL58eULt4IMPjv3226/S5z3//PNRXFycUOvXr1/cd9990bRp0yrNvnbt2iqtBwAAAGjsysrKoqCgIKnetWvX6NOnT8o9nTp1isWLFyfUioqKomPHjrUxYkqffPJJvP766wm1tm3bxt///vfo2rVrlft5bwQAAAAAAFA9suMVkx2vHNlxAAAAgNolP/4F744AABqvzPoeAAAAoLZ07949qVZSUlLu+lSf4rhp06a0zi4tLU0KeDYkXwZe/9OmTZti8uTJCbWNGzfG1KlTE2pNmzaNU045pdZnLE9ubm5SLdUvc1LVqhowTvVJoJdffnmVQ7oREStWrKjyHgAAAIDG7JlnnomlS5cm1UeNGlXunh49eiTV3n333Zoca6dSvTP61re+lVZAt7S0NFatWlUTYwEAAAAAAOzSZMfLJzteObLjAAAAALVLflx+HACgsXPJMwAAsEvp0KFDuc/atGmTVFu3bl1a5yxcuDDKysrS2ltXKhN4feaZZ5LCzccee2yFP8faNnTo0OjcuXNCbfbs2bF8+fIdX2/atCmmTJmSsCadgPFHH32U8HVWVlYMHjy4ihP/e0YAAAAAvrBx48b4wx/+kFTPzMyMb33rW+Xu69u3b1LthRdeqNHZduar74wiIoYNG5ZWr4ULF6Z9WQAAAAAAAAAVkx3/N9nxnZMdBwAAAKg98uNfkB8HAGjcXPIMAAA0Wqlelu+2227lrm/fvn1SbcmSJWmd/a9//SutfXXpqKOOik6dOiXUvhp4nTBhQtK+vLy8Wp+tIk2aNInTTz89qf6fIeNp06ZFcXFxwvNjjz02OnbsWKWz1qxZk/B1hw4dokmTJlXq8Z8zAQAAAPCFG264IZYtW5ZUz83Nje7du5e776ijjoqMjIyE2rx581L2qi1ffWcUEbH77run1euf//xn2nOkek+1ffv2tPsBAAAAAAB8ncmOV0x2fOdkxwEAAABqj/z4F+THAQAaN5c8AwAAjdJnn30WCxYsSKr36dOn3D2pnr3xxhtVPnvr1q3x8MMPV3lfXUsVeC0rK9sRzl21alW8/PLLCc87duwYw4cPr7MZy5MqLFxQUBBlZWUREZGfn5/0PDc3t8rnNG3aNOHrkpKSKveI+CIAPXfu3LT2AgAAADQm27ZtixtvvDHlXxBv27Zt/OhHP6pwf+fOnWPw4MEJtbKysrj99ttrdM6KNGvWLKn21b80XhklJSXx2GOPpT1H69atk2qbNm1Kux8AAAAAAMDXlez4zsmO75zsOAAAAEDNkx//N/lxAIDGzyXPAABAvZs9e3Y8/fTTO0KWNeGOO+5I+amDxx13XLl7Dj744KTatGnTqhzOvO++++Ljjz+u0p76csYZZyTVvvwFycSJE5N+hqeeempSeLU+9O7dO/r27ZtQW7FiRcyePTs+/fTT+Ne//pXwrEOHDnHMMcdU+Zzddtst4esNGzbE22+/XaUemzdvjp///OdVPhsAAACgsVm6dGl85zvfiXHjxqV8/utf/zo6deq00z7f+c53kmqTJk2KadOmVXvGyujYsWNSbfbs2VXuM3bs2Fi3bl3ac7Rq1Sqptnr16rT7AQAAAAAA1AXZ8fojO14x2XEAAACAmiU/nkh+HACg8XPJMwAAUO8++eSTuPzyy+P000+PqVOnxrZt26rV75FHHomHH344qd6zZ8845JBDyt3XoUOHOPTQQxNqxcXFcccdd1T67BdffDFuvfXWyg9bz3r37p30M/nwww/j9ddfT/lpmHl5eXU12k6lmiU/Pz8mTpyY9Gco3YDxV8PAERF33nlnpfeXlpbG1VdfHe+8806VzwYAAABoLObMmRPXXHNNnHLKKfHaa6+lXHPNNdfECSecUKl+xx13XAwaNCihVlZWFldeeWXSX+Cuqi1btkR+fn4UFhaWuybVO6MHHnggNmzYUOlzHnrooXjiiSfSmvFLe+65ZzRr1iyh9u6778bWrVur1RcAAAAAAKA2yY7XH9nxismOAwAAANQM+fFk8uMAALsGlzwDAAANxjvvvBOXXnppHHvssfGHP/whlixZUqX9q1evjmuvvTauv/76KCsrS3p+7bXXRkZGRoU9zjjjjKTa3/72t3jggQcq3FdaWhr3339//OAHP9jxEjwz8+vxn1ypAq+/+93v4t13302o9enTJw466KC6GmunTjnllKTw7ZQpU1L+ciPdgPExxxyTVJs6dWr87ne/22mgfMWKFXHhhRfG008/HRGx0z97AAAAAA3dxo0bo7CwMOU/n332WaxcuTIWLlwYzz//fNx3333x4x//OI4++ug4++yzY/z48SnDo02bNo0bb7wxzjvvvCrNMnbs2GjVqlVCbcOGDXH++efHLbfcEuvXr69Sv/fffz/+9Kc/xfHHHx9XX311hYHbgQMHRnZ2dkLto48+iosvvjjWrVtX4TklJSXx61//On71q1/tqKX73igrKyv222+/hNr69evjqaeeSqsfAAAAAABAXZIdrx+y4+WTHQcAAABIJD/+BflxAAAqK6u+BwAAAPiqTz/9NO6555645557onv37tG/f//o379/9OzZMzp06BDt27ePzMzMKCkpiVWrVsXixYvj5ZdfjpdffjlKS0tT9jzzzDNj+PDhOz07Jycn7rnnnlixYsWOWllZWdx0000xefLkOPPMM6Nfv37RoUOH2LBhQ6xatSpeeeWVmDRpUnz44YcJfV5//fVYuXJl9X8gteyUU06J3/zmNwk/u7lz5yatSzfsWlvat28fI0aMiH/+8587asXFxVFcXJywrk+fPnHwwQendcYRRxwRhx9+eMyePTuhfu+998bMmTPjnHPOiYEDB8aee+4ZmZmZsWbNmli8eHFMnz49JkyYEFu2bNmx59vf/vZOA98AAAAADdm9994b9957b43169OnT4wdOzb69etX5b177713/N//+38T/uJ8RMT27dvjrrvuioceeihOOeWUGDRoUBx88MHRoUOHaNOmTWzatCnWr18fn332WSxevDgWLVoUr7zyStJfWq9IixYt4tvf/nbcddddCfWXX345TjnllPjmN78Zw4cPj549e0bz5s1j7dq1sWzZsnjxxRdj/PjxsXr16h17DjnkkGjWrFm88cYbVf4ZRESMGDEiFixYkFD76U9/GgsXLoxjjz029tprr6RAcUREy5Yto2XLlmmdCQAAAAAAUJNkx+uW7Hj5ZMcBAAAAEsmPf0F+HACAynLJMwAA0KCtWLEiVqxYEU8++WTaPXJychI+2bAiLVu2jF/96lfx3e9+N8rKyhKezZkzJ+bMmbPTHgMGDIgbb7wxTjnllLTmrWsdOnSIY489Np555ply1zRp0iROO+20OpyqcnJzcxOCuuWtqY7rr78+vvnNbyZ9+ubixYvj+uuvr1SPQYMGxY9//GNBXQAAAICI6NGjR5x//vnxX//1X9GkSZO0+wwfPjzuuOOOuPzyy2Pjxo0Jz0pKSuLRRx+NRx99tLrjpvT9738/nnnmmViyZElCffXq1XHbbbfFbbfdttMenTp1iltvvTWuueaatOc488wz4+677074C+OlpaVx//33x/3331/uvksuuSTGjBmT9rkAAAAAAAC1QXa89smOV0x2HAAAAKDmyY9/QX4cAKDxy6zvAQAAAPbYY4/o1KlTjfdt27ZtXH/99fGb3/wmMjMr/58/Q4YMid/85jeRlVX1z8UZNmxY3HPPPV+7TzHMy8ur8PmwYcNq5d9RdR1zzDGx2267lfu8JgLGBxxwQPzxj3+MFi1apLV/2LBhceedd0azZs2qNQcAAADA11mnTp3irLPOinvvvTemTJkSZ599drUCul8aMWJEPP7443HQQQfVwJRfaNq0aTRt2rTCNa1bt46//OUv0a1bt7TO2GuvveKBBx6I7t27p7X/S127do2f/OQn1eoBAAAAAABQl2TH65/sePlkxwEAAABqhvx4IvlxAIBdQ9V/6wwAAFDDjjjiiJgxY0a89dZbMX369Jg5c2YsXrw4tm7dmla/7t27x2mnnRbnnntuhSHOiuTm5kbXrl1j7NixsWjRop2ub9u2bVx88cVx7rnn1sgvF+ra8OHDo2PHjlFYWJjyeW5ubt0OVElZWVlx6qmnxgMPPJDy+VFHHRWdO3eu9jkjRoyIxx57LK6++upYsGBBpfa0adMmLr744vj2t7/9tfwzAQAAAFBZmZmZ0bRp02jWrFm0bds2dt9999hjjz1in332iV69esWhhx4aPXv2rLXze/fuHU888USMHz8+7rvvvliyZEmVe2RmZka/fv3i1FNPjVNOOSU6duy40z177bVXjB8/Pn7+85/HlClToqysbKd7srKy4swzz4wrrrgi2rVrV+U5U/k//+f/RNu2bWPs2LGxdu3aGukJAAAAAABQW2TH65/seMVkxwEAAAAqJj8uPw4AQGoZZZX5X4kAAAB1rKSkJObOnRvz58+PpUuXxtKlS+OTTz6JkpKSKCkpiYgvPu2wVatW0aFDh9hvv/3igAMOiEMPPTQOPfTQyMjIqJE5tm/fHjNmzIjnn38+3njjjVizZk2sXbs2srKyolOnTnHAAQfE8OHDY9SoUdG6desaObO+TJkyJd59992kekZGRnz/+9+P5s2b18NUO7ds2bKYOHFiymfDhw+PQw89tEbPmzlzZkyZMiVee+21WLlyZZSWlkbEF7/I6datWxx00EFx9NFHx+jRo6NVq1YJe5977rmEr9u3bx/9+/ev0fkAAAAAdnVvvfVWPPfcczF37txYsmRJfPrpp7F9+/Ydz5s3bx6dO3eOfffdN3r16hX9+/ePwYMHVys0+95778UTTzwRs2bNiiVLlsTGjRt3POvUqVPst99+MWTIkDj99NOjS5cuCXvnzJmTFK4dMWJElWfYsmVLTJs2LV555ZVYvHhxrFy5MjZs2BAbNmxIChBfcsklMWbMmCqfAQAAAAAAUBtkx+ue7HjlyI4DAAAANHzy4/LjAAANhUueAQAAoBpKSkqirKwssrOzIzMzs77HAQAAAOArtm/fHhs2bIjt27dHdnZ2ZGVl1fqZmzdvjs2bN9fZeQAAAAAAAEDtkx0HAAAAaPjkxwEAqC8ueQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIg4+JBQAAAAAAAAAAAAAAAAAAAAAAAAAAAEiDS54BAAAAAAAAAAAAAAAAAAAAAAAAAAAA0uCSZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIA0uOQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIA0ueQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIg0ueAQAAAAAAAAAAAAAAAAAAAAAAAAAAANLgkmcAAAAAAAAAAAAAAAAAAAAAAAAAAACANLjkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACANLnkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASINLngEAAAAAAAAAAAAAAAAAAAAAAAAAAADS4JJnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgDS45BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgDS55BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEiDS54BAAAAAAAAAAAAAAAAAAAAAAAAAAAA0uCSZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIA0uOQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIA0ueQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIg0ueAQAAAAAAAAAAAAAAAAAAAAAAAAAAANLgkmcAAAAAAAAAAAAAAAAAAAAAAAAAAACANLjkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACANLnkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASINLngEAAAAAAAAAAAAAAAAAAAAAAAAAAADS4JJnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgDS45BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgDS55BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEiDS54BAAAAAAAAAAAAAAAAAAAAAAAAAAAA0uCSZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIA0uOQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIA1Z9T0AAAAAAABQv/bff/+k2oMPPhiDBw+uh2kaj9tvvz3uuOOOhNqgQYPioYceqqeJAAAAAAAAAAAAAIBdmex47ZAdBwAAAMAlzwAAAAAApLR27dp4++234/PPP4+ioqJYv359NGnSJFq2bBnZ2dmxxx57RPfu3WPPPfeMJk2a1Pe4AAAAAAAAAAAAAACkQXYcAAAAAKB6XPIMAAAAAMAOs2fPjieeeCJef/31WLZsWaX2ZGVlRa9evaJv375xyCGHxKBBg6JXr161PCkAAAAAAAAAAAAAAOmSHQcAAAAAqDkueQYAAAAAIF544YX4wx/+EO+8806V927dujUWL14cixcvjscffzwiIrp27RpHH310XHbZZdGxY8eaHhcAAAAAAAAAAAAAgDTIjgMAAAAA1LzM+h4AAAAAAID6U1xcHNdcc01ceOGFaYV0y/PRRx/FI488EqtWraqxngAAAAAAAAAAAAAApEd2HAAAAACg9mTV9wAAAAAAANSPkpKSuOCCC2LOnDn1PQoAAAAAAAAAAAAAALVEdhwAAAAAoHa55BkAAAAAYBe0bdu2uOiii8oN6WZlZcWRRx4ZI0aMiP333z/23nvvaNWqVbRo0SKKiopi3bp18fHHH8e8efNi/vz5MWvWrFi7dm3dfhMAAAAAAAAAAAAAAFRIdhwAAAAAoPa55BkAAAAAYBf0j3/8I2bNmpXy2ahRo+Kqq66Krl27pnzesWPH6NixY/Ts2TOGDh0aERFbt26NV155JaZOnRpTpkyJoqKiWpudmrd48eL6HgEAAAAAAAAAAAAAqAWy4/wn2XEAAAAAqB0ueQYAAAAA2MWsW7cubrvttpTPrrzyyrjggguq3DMrKyuGDRsWw4YNi6uvvjrGjx8fDz74YHVHBQAAAAAAAAAAAAAgTbLjAAAAAAB1wyXPAAAAAAC7mGeeeSbWrVuXVD/99NPTCul+VatWreLcc8+Nc845J7Zu3VrtfgAAAAAAAAAAAAAAVJ3sOAAAAABA3cis7wEAAAAAAKhb06dPT6plZGTEZZddVqPnZGZmRrNmzWq0JwAAAAAAAAAAAAAAlSM7DgAAAABQN1zyDAAAAACwi5k7d25S7cADD4xu3brVwzQAAAAAAAAAAAAAANQG2XEAAAAAgLqRVd8DAAAAAABQd7Zt2xaff/55Ul1IN32lpaUxc+bMmDFjRrz99tuxfPnyWL9+fWzbti06d+4cffv2jVtvvbW+x9xlLV++PF555ZVYvHhxfPDBB7F8+fIoKiqK4uLiaNKkSbRr1y7at28fXbt2jSOOOCIGDhwYffv2jcxMn5MJAAAAAAAAAAAAwNeX7HjNkx1v2GTHAQAAAKhPLnkGAAAAANiFFBYWxvbt25PqTZs2rYdp/m3//fdPqj344IMxePDgtHvefvvtcccddyTUBg0aFA899FCl9p977rkxa9ashNoll1wSY8aMiYgvQs/jxo2Lu+++Oz799NOUPVauXBlFRUUREbF69eo45phjYtu2bQlrLrvssvjBD35QqZl2ZsuWLXH00UfH2rVrE+pnn3123HjjjeXuq+rPf/LkyXHZZZcl1ceNGxeHH354lWYuz7x58+Kss85Kqt9yyy0xevTocveVlZXF7Nmz46mnnornn38+Pvroo3LXlpaWxqZNm2LVqlWxePHieO655yIiYp999okLL7wwTj/99Hr//w0AAAAAAAAAAAAASIfsuOx4KrLjsuMAAAAA1A4fJQYAAAAAsAvJykr92X+rVq2q40m+3j777LM455xz4le/+lW5Id2v6tSpUwwbNiypXlBQUGNzPffcc0kh3YiIvLy8GjsjIuL444+P9u3bJ9Vr8ntJ1att27Zx/PHHV7gvJycnzjnnnBg3blyFId2KLF26NK699to466yzYuXKlWn1AAAAAAAAAAAAAID6JDteM2THZccBAAAAoDJc8gwAAAAAsAtp3759NGnSJKn+1ltvxfr16+thoq+fwsLC+Na3vhVz5syp8t5UgdmlS5em1SuV/Pz8pFrPnj3jsMMOq5H+X2rWrFmMHj06qT558uTYvHlztfuXlpbGpEmTkuqjRo2K5s2bV7h3zZo11T7/S4sWLYqzzjor3nrrrRrrCQAAAAAAAAAAAAB1QXa8+mTHZccBAAAAoLJSf+weAAAAAACNUkZGRuy1116xdOnShHppaWncddddceWVV9bPYF8T27dvj0suuSSWLVuWUG/evHkMHDgwunXrFh07dozi4uL45JNPYt68eVFSUrJj3fHHHx/t2rWLdevWJezPz8+P/v37V2u2wsLCmDFjRlI9VTi4JuTl5cW4ceMSauvXr49p06bFKaecUq3ezz//fKxduzblmenq2rVr7L///tGpU6do3bp1ZGdnR0lJSRQWFsbbb78dS5YsiW3btiXtKywsjP/93/+NgoKCaNeuXdrnAwAAAAAAAAAAAEBdkh2vHtnxxL6y4wAAAABQMZc8AwAAAADsYgYPHpwU1I2I+Otf/xodOnSI888/PzIzM+t+sK+BJ554IlatWrXj69122y0uu+yyOO2006Jly5ZJ68vKyuLVV1/d8XWzZs1i9OjR8Y9//CNh3eTJk+O6666LZs2apT3bxIkTY+vWrQm1zMzMyMnJSbtnRfr16xe9e/eO9957L6Gen59f7aBufn5+Uq1nz55VCjO3adMmjjnmmDjxxBNjyJAh0bZt2wrXr169Op544om45557ori4OOHZRx99FNdff33ceuutlT4fAAAAAAAAAAAAAOqb7Hj6ZMf/TXYcAAAAAHbOm1YAAAAAgF3MqFGjyn32+9//Ps4444woKChICisSCSHdww47LCZPnhzf+MY3UoZ0IyIyMjLiyCOPTKidccYZSeuKiopi2rRp1ZqtoKAgqTZ06NDo0qVLtfpWJDc3N6n28ssvx6effpp2z8LCwnjxxReT6nl5eZXav+eee8Y111wTL774Ytx8881x0kkn7TSkGxHRqVOnuOiii+Kpp56KQw45JOn5lClTkkLJAAAAAAAAAAAAANCQyY6nT3Y8kew4AAAAAFTMJc8AAAAAALuYIUOGxOGHH17u87fffjt+8pOfxJFHHhnf+9734k9/+lO89NJLUVRUVIdTNmx9+vSJe++9N9q1a1flvf369YvevXsn1VMFbStr8eLF8fbbbyfVUwVpa1JOTk40adIkobZt27aYOHFi2j0nTZoUpaWlCbXMzMzIycmp1P5HH300zjvvvMjOzk7r/C5dusR9990X++yzT9Kzv//972n1BAAAAAAAAAAAAID6IDtefbLjX5AdBwAAAICKueQZAAAAAGAX9Mtf/jJat25d4ZrS0tKYOXNm3HbbbXH++efHoEGD4qSTToof//jH8fe//z0WL14cZWVldTRxw5GZmRljx47d6c+vIqkCtDNnzozVq1en1S8/Pz+p1rp16xg5cmRa/Sqrc+fOcdRRRyXVqxM6TrV3yJAh0aVLl0rtz8ys/q8+2rVrF9dee21SfdKkSbF9+/Zq9wcAAAAAAAAAAACAuiI7nj7Z8X+THQcAAACAirnkGQAAAABgF9SrV6+49dZbIzs7u9J7ysrKYunSpfHkk0/GL3/5yzj99NPjyCOPjCuuuCJmzpy5ywQYR44cGf369atWj9NPPz2aNGmSUNu2bVs8+eSTVe5V3r7Ro0dHixYt0p6xsvLy8pJq7777bsyfP7/Kvd55551YsGBBpc6obcccc0zsscceCbX169fH+++/X+ezAAAAAAAAAAAAAEC6ZMfTJzueSHYcAAAAAMrnkmcAAAAAgF3UsGHD4rHHHovevXun3WPt2rUxadKk+N73vhfHHXdcPPbYY40+sHvmmWdWu8cee+wRQ4cOTarn5+dXudeMGTPis88+S6rn5uamM1qVnXDCCdGuXbukekFBQZV7pfr+W7duHSNHjkxntGobOHBgUm3u3Ln1MAkAAAAAAAAAAAAApE92PD2y44lkxwEAAACgfC55BgAAAADYhfXu3TsmTJgQN954Y+yxxx7V6vXxxx/HddddF3l5efHuu+/W0IQNS9OmTWPIkCE10uuMM85Iqr3zzjuxcOHCKvVJFW7t0aNHHH744WnPVhXNmjWLUaNGJdUnTZoUpaWlle6zbdu2ePLJJ5Pqo0aNihYtWlRrxnR17tw5qfbee+/VwyQAAAAAAAAAAAAAUD2y41UjO55MdhwAAAAAypdV3wMAAAAAAFC/srKy4uyzz46zzjorZsyYERMmTIgXXnghNmzYkFa/RYsWxdlnnx1//OMf45hjjqnhaetXnz59olmzZjXS64QTToi2bdtGUVFRQj0/Pz8OOuigSvUoKiqKZ599Nqmem5tbEyNW2hlnnBEPP/xwQu3zzz+PF154IU444YRK9Zg5c2asXr06qV6T30thYWG88847sWbNmigpKYni4uLYvHlzlJWVpVy/YMGCpNq6detqbB4AAAAAAAAAAAAAqEuy45UnO56a7DgAAAAApOaSZwAAAAAAIuKLwO6IESNixIgRUVpaGvPmzYtZs2bFm2++GfPnz08ZoixPcXFxXHLJJfH//t//i379+tXi1HVr3333rbFezZo1i1GjRsUjjzySUJ80aVJcddVV0bRp0532eOqpp2LLli0JtYyMjDoP6h566KGx7777xvvvv59Qz8/Pr3RQt6CgIKnWo0ePOOKII9Keq7S0NF566aV48sknY9asWfHpp5+m3etL69evr3YPAAAAAAAAAAAAAKhPsuM7Jzuemuw4AAAAAKTmkmcAAAAAAJI0bdo0BgwYEAMGDNhRW716dcyfPz9ee+21ePXVV2P+/PkV9tiyZUtceumlMWnSpGjdunVtj1wn2rZtW6P9zjjjjKSgbmFhYbz44otx/PHH73R/qnDrkUceGV27dq2pESstLy8vbr755oTaCy+8EIWFhdGxY8cK965fvz6mT5+eVK9O4HjChAnx+9//vkoB88oQ1AUAAAAAAAAAAACgMZEdT012vHyy4wAAAACQLLO+BwAAAAAA4OuhU6dOMWLEiLjqqqviiSeeiGeeeSYuvPDCyM7OLnfPxx9/HH//+9/rcMra1aZNmxrtd9hhh8W+++6bVE8VwP2qDz74IN58882kenXCrdWRk5MTmZmJv3YoLS2Np556aqd7n3766di8eXNCLSMjI3Jycqo8R0lJSZx//vlx1VVX1XhINyJi27ZtNd4TAAAAAAAAAAAAABoS2XHZ8YrIjgMAAABAMpc8AwAAAACQlr333juuuOKKeOaZZ+Koo44qd939998fpaWldThZ7cnKyqrxnnl5eUm15557LtauXVvhvlRh3latWsVJJ51UQ5NVzR577BFDhw5Nqufn5+90b6o1gwcPjm7dulVphpKSkrjgggvipZdeqtI+AAAAAAAAAAAAAKB8suM1Q3ZcdhwAAACAxsslzwAAAAAAVMvuu+8e99xzTxx99NEpn3/++efx1ltv1fFUXx85OTmRmZn4ur60tDSeeuqpcveUlZXFxIkTk+onn3xytGzZssZnrKxUoeMFCxbEu+++W+6eZcuWxZw5cyrVa2duvvnmeOONN1I+y87OjhEjRsSVV14Zd955ZxQUFMTMmTPj9ddfj3nz5sXixYuT/rnkkkuqPAMAAAAAAAAAAAAANFay49UjOy47DgAAAEDj5ZJnAAAAAACqrUmTJvHrX/+63JDoq6++WscTfX3sscceMXTo0KR6fn5+uXteeeWV+Oijj5Lq6YRba9LIkSOjTZs2SfWKvpdUz7Kzs+PEE0+s0tmLFy+Ohx9+OKmelZUV//u//xsvvvhi3HXXXXHBBRfEcccdFwceeGB06tQp2rRpE82aNUvZs7S0tEozAAAAAAAAAAAAAEBjJzuePtlx2XEAAAAAGi+XPAMAAAAAUCM6deoUJ510UspnK1asqONpIrZu3VrnZ6YrVcB23rx5sWTJkpTrU4Vb99prrzjiiCNqfLaqaN68eYwePTqpPnHixNi2bVtSvaysLCZOnJhUP/nkkyM7O7tKZz/xxBNJZ2RkZMSf/vSn+OEPf5gyQLwza9eurfIeAAAAAAAAAAAAAGjsZMfTJzueSHYcAAAAgMbCJc8AAAAAANSY8oKin3/+eYX7MjIykmrbt2+v1izr1q2r1v66NHLkyJRB0lSB3JKSknjmmWeS6rm5uSl/jnUtVeh49erV8dJLLyXVX3311Vi5cmWleuzMc889l1TLzc2NY489tsq9viSoCwAAAAAAAAAAAACpyY6nR3Z85z12RnYcAAAAgIbIJc8AAAAAANSYzp07p6xv3Lixwn2tWrVKqm3YsKFasxQWFlZrf11q3rx5jBo1Kqk+ceLEpMDy1KlTk342GRkZkZubW5sjVlr//v1jn332SaoXFBRUqta9e/cYOHBglc4sKiqKDz/8MKmek5NTpT5ftWDBgmrtBwAAAAAAAAAAAIDGSnY8PbLj/yY7DgAAAEBj4pJnAAAAAABqzNatW1PWW7ZsWeG+1q1bJ9XWrVtXrVnmzZtXrf11LS8vL6m2atWqePnllxNq+fn5SesGDhwY3bt3r7XZqirV9zJt2rRYv379jq83bNgQU6dOTbk3IyOjSud99tlnKet9+vSpUp//tHr16lixYkXa+wEAAAAAAAAAAACgMZMdT5/s+L/3yo4DAAAA0Fi45BkAAAAAgBrzySefpKx36dKlwn3t2rVLqi1ZsiTtOZYvXx4fffRR2vvrw4ABA2KfffZJqv9nMHflypXx2muvJa1JFYytT7m5uZGZmfgriM2bN8fkyZN3fP3Pf/4zNmzYkLAmIyMjcnJyqnze2rVrU9bbtm1b5V5fmjBhQtp7AQAAAAAAAAAAAKCxkx1Pn+y47DgAAAAAjY9LngEAAAAAqDEzZsxIWd9vv/0q3Jfq+VtvvZX2HOPGjUt7b31KFbidNm1aFBcXR0REQUFBlJWVJTzPzs6Ok046qU7mq6wuXbrEkCFDkur/GTr+z//7SwMHDoy99tqryudlZ2enrJcX4N2Z0tLSr+2fIQAAAAAAAAAAAACoC7Lj1SM7LjsOAAAAQOPikmcAAAAAgF3MzJkza6Xv+++/X27vwYMHV7j3oIMOSqq9/vrrsWrVqirP8fHHH8ejjz5a5X0NQU5OTmRmJr6637RpU0yePDkiIiZMmJC056STTopWrVrVyXxVkZubm1R74403YtmyZfHxxx/HrFmzkp6nCipXRocOHVLWZ8+enVa/P//5z7Fy5cq09gIAAAAAAAAAAABAQyE73nDJjsuOAwAAANC4uOQZAAAAAGAXM2bMmPiv//qvmDFjRo31LCkpiauvvjpKS0uTnh100EHRq1evCvcPHDgwqbZ9+/a46667qjRHaWlpXHnllVFcXFylfQ3FnnvuGUceeWRSvaCgIGbPnh3Lli1LepYqENsQnHjiidG6deukekFBQRQUFMT27dsT6tnZ2XHSSSelddYee+wRnTp1Sqo/+OCDVe710ksvxd13353WHAAAAAAAAAAAAADQkMiON1yy47LjAAAAADQuLnkGAAAAANgFvfXWW3HBBRfEN77xjXjssceqFWxdvnx5fPe73425c+emfP79739/pz369euXMsz78MMPx7Rp0yo1R0lJSVx00UXx2muvVWp9Q5WXl5dUmz17dtxxxx1J9W7dusXgwYPrYqwqa9GiRYwaNSqpPmHChCgoKEiqn3jiidGqVau0zxs6dGhSbfbs2XHbbbdVusc///nPuOiii2Lr1q1pzwEAAAAAAAAAAAAADYnseMMlO54e2XEAAAAAGiKXPAMAAAAA7MLmzp0b1113XRx11FFxxRVXxNNPPx2FhYWV2rtgwYIYO3ZsjB49utyQ7lFHHRWjR4+uVL9vfOMbSbXt27fHZZddFn/+859j8+bNKfdt2bIl8vPz49RTT42ZM2fuqPfu3btS5zY0J554YrRu3TqhVlZWFi+//HLS2pycnMjIyKir0aosVeh45cqVsXTp0qR6bm5utc761re+lbL+pz/9KS6//PJYvnx5uXvfeeeduPTSS2PMmDGxZcuWHfWDDz64WjMBAAAAAAAAAAAAQEMhO97wyI6nR3YcAAAAgIYoq74HAAAAAACg/m3atCkmTZoUkyZNioiIbt26xX777Rd77rlntGvXLpo2bRobN26M4uLi+PDDD2Px4sWxZs2aCnv26NEj/vCHP1R6hnPOOSeeeOKJeOeddxLqpaWlceutt8Zf//rXGDJkSOy1116RnZ0da9eujRUrVsRrr70WGzZsSNiTl5cX3bp1izvuuKPS5zcULVq0iJNPPjkef/zxna5NFYRtSA4//PDYZ599UgZz/1O3bt3iyCOPrNZZhx12WBx//PExffr0pGdPP/10TJkyJfr27RsHH3xwtG/fPjZv3hyfffZZzJ07N+V8Z511VnTp0iUWLFhQrbkAAAAAAAAAAAAAoCGRHW84ZMfTIzsOAAAAQEPkkmcAAAAAAJKsXLkyVq5cmfb+Qw45JO68887o2LFjpfc0bdo0fv3rX8e5556bFLyNiCgpKYlp06bttM+wYcPiF7/4RfzlL3+p0swNSV5e3k6DukcccUTsvffedTRR+nJycuLWW2/d6ZqMjIxqn3XTTTfFN77xjVi2bFnSs+3bt8fcuXNj7ty5O+0zdOjQ+PnPfx533XVXtWcCAAAAAAAAAAAAgIZMdrx+yY6nR3YcAAAAgIYms74HAAAAAACgbp1zzjnRvXv3WumdnZ0dP/nJT+KRRx6Jzp07V3n/IYccEvfee2+0adMmrfPz8vLizjvvjGbNmqW1v6E44ogjokePHhWuyc3NrZthqikvLy8yMyv+dURNfS/t27ePe++9N/bff/+0e+Tm5sZf/vKXaNq0aY3MBAAAAAAAAAAAAAD1RXa84ZMdT4/sOAAAAAANjUueAQAAAAB2MT/+8Y9j+vTpUVBQEGPGjIlBgwZFixYtqtWzd+/ecfnll8fUqVPj/PPPj6ysrLR7DRgwIAoKCmLUqFGV3rP33nvHbbfdFr/5zW++9iHdL1UUXm3ZsmWVfj71ac8994zBgweX+/zwww/faSi5Kvbaa6945JFH4vzzz4/s7OxK79tvv/3iz3/+c/z2t79tNH+GAAAAAAAAAAAAANi1yY5/PciOp0d2HAAAAICGJKOsrKysvocAAAAAAKB+bdmyJRYsWBALFy6MDz74IJYtWxYrVqyI9evXR0lJSWzatClatGgRrVu3jtatW8fuu+8e+++/fxxwwAHRr1+/6NOnT63M9d5778Wzzz4bL7/8cqxcuTIKCwtj06ZN0apVq+jevXsccsghcfzxx8fRRx8dmZk+15BEhYWFMWXKlHj11Vdj0aJF8fnnn0dxcXE0b9482rdvHz169Ih+/frF8OHD44gjjki5//PPP0+otWzZMrp27VpX3wIAAAAAAAAAAAAA1CjZcRoj2XEAAAAA6ptLngEAAAAAAAAAAAAAAAAAAAAAAAAAAADS4KPpAAAAAAAAAAAAAAAAAAAAAAAAAAAAANLgkmcAAAAAAAAAAAAAAAAAAAAAAAAAAACANLjkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACANLnkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASINLngEAAAAAAAAAAAAAAAAAAAAAAAAAAADS4JJnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgDS45BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgDS55BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEiDS54BAAAAAAAAAAAAAAAAAAAAAAAAAAAA0uCSZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIA0uOQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIA0ueQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIg0ueAQAAAAAAAAAAAAAAAAAAAAAAAAAAANLgkmcAAAAAAAAAAAAAAAAAAAAAAAAAAACANLjkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACANLnkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASINLngEAAAAAAAAAAAAAAAAAAAAAAAAAAADS4JJnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgDS45BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgDS55BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEiDS54BAAAAAAAAAAAAAAAAAAAAAAAAAAAA0uCSZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIA0uOQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIA0ueQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIg0ueAQAAAAAAAAAAAAAAAAAAAAAAAAAAANLgkmcAAAAAAAAAAAAAAAAAAAAAAAAAAACANLjkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACANLnkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASINLngEAAAAAAAAAAAAAAAAAAAAAAAAAAADS4JJnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgDS45BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgDS55BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEiDS54BAAAAAAAAAAAAAAAAAAAAAAAAAAAA0uCSZwAAAAAAAAAAAAAAAAAAAAAAAAAAAIA0uOQZAAAAAAAAAAAAAAAAAAAAAAAAAAAAIA0ueQYAAAAAAAAAAAAAAAAAAAAAAAAAAABIg0ueAQAAAAAAAAAAAAAAAAAAAAAAAAAAANLgkmcAAAAAAAAAAAAAAAAAAAAAAAAAAACANLjkGQAAAAAAAAAAAAAAAAAAAAAAAAAAACANLnkGAAAAAAAAAAAAAAAAAAAAAAAAAAAASINLngEAAAAAAAAAAAAAAAAAAAAAAAAAAADS4JJnAAAAAAAAAAAAAAAAAAAAAAAAAAAAgDS45BkAAAAAAAAAAAAAAAAAAAAAAAAAAAAgDS55BgAAAAAAAAAAAAAAAAAAAAAAAAAAAEjD/weAhLBTZbCKNgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 7000x2450 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20,7), dpi = 350)\n",
    "sns.set(style=\"whitegrid\")\n",
    "# fig.subplots_adjust(top=0.15)\n",
    "ax1 = fig.add_subplot(121)\n",
    "x1 = \"survival\"\n",
    "y = \"sum_weights_abs\"\n",
    "order1 = [-1, 1]\n",
    "# ax1 = sns.boxplot(data=clinical_df, x=x1, y=y, order=order1, palette=sns.color_palette(['#FF5720', '#18C288']))\n",
    "ax1 = sns.boxplot(data=clinical_df, x=x1, y=y, order=order1, palette=sns.color_palette(['#FF5720', '#18C288']))\n",
    "\n",
    "add_stat_annotation(ax1, data=clinical_df, x=x1, y=y,  box_pairs=[(-1,1)],\n",
    "                    test='Mann-Whitney', text_format='star', loc='outside', verbose=2)\n",
    "\n",
    "ax1.set(xlabel='Survival', ylabel='sum of neighboors weights')\n",
    "ax1.set(xticklabels=[\"Survival\", \"Death\"])\n",
    "# fig.subplots_adjust(bottom = 0.5)\n",
    "# fig.savefig(\"box_plot.jpg\")\n",
    "ax2 = fig.add_subplot(122)\n",
    "x2 = \"survival\"\n",
    "y = \"mean_sum_weights_abs\"\n",
    "order2= [-1, 1]\n",
    "ax2 = sns.boxplot(data=clinical_df, x=x2, y=y, order=order2, palette=sns.color_palette(['#FF5720', '#18C288']))\n",
    "add_stat_annotation(ax2, data=clinical_df, x=x2, y=y, box_pairs=[(-1,1)],\n",
    "                    test='Mann-Whitney', text_format='star', loc='outside', verbose=2)\n",
    "ax2.set(xlabel='Survival', ylabel='mean of neighboors weights')\n",
    "ax2.set(xticklabels=[\"Survival\", \"Death\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694c9695",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "95f526d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_d = clinical_df[['survival', 'low_weights', 'high_weights']]\n",
    "d2 = clinical_d.melt(id_vars=\"survival\", var_name=\"neighboor risk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "696feb1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Text(0, 0, 'survival'), Text(1, 0, 'death')]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 7000x2450 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Draw a nested barplot to show survival for class and sex\n",
    "fig = plt.figure(figsize=(20,7), dpi = 350)\n",
    "sns.set(style=\"whitegrid\")\n",
    "# fig.subplots_adjust(top=0.15)\n",
    "ax1 = fig.add_subplot(121)\n",
    "x = \"survival\"\n",
    "y1 = \"value\"\n",
    "hue1 = 'neighboor risk'\n",
    "order1 = [-1, 1]\n",
    "# ax1 = sns.boxplot(data=clinical_df, x=x1, y=y, order=order1, palette=sns.color_palette(['#FF5720', '#18C288']))\n",
    "ax1 = sns.boxplot(data=d2, x=x, y=y1, hue = hue1,  order=order1)\n",
    "ax1.set(xlabel='survival', ylabel='sum of neighboor weights')\n",
    "ax1.set(xticklabels=[\"survival\", \"death\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e6b22ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_df.to_csv('data_ind/test_graph_feature.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57906b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6245c3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ad0a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8caa870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
