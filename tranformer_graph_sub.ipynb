{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9668e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, roc_curve\n",
    "from statannot import add_stat_annotation\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "import dgl.function as fn\n",
    "from dgl.nn.pytorch import GraphConv, SAGEConv, TAGConv\n",
    "\n",
    "from deepsurv_utils import c_index, adjust_learning_rate\n",
    "# from loss import NegativeLogLikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782cd24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# load SHPH data\n",
    "all_patient_info = pd.read_csv(\"\")\n",
    "all_patient_info = all_patient_info[['folder_name', 'Sex_1_male_2_female', 'Age',\n",
    "       'Location_1_LUL_2_LLL_3_RUL_4_RML_5_RLL','Histology_1_Adenocarcinoma_2_SquamousCellCarcinoma_3_Others',\n",
    "        'pT_Stage', 'pN_Stage', 'pM_Stage', 'pTNM', 'RFS_Status', 'RFS_Month',\n",
    "       'OS_Status', 'OS_Month']]\n",
    "stage1 = list(np.load(\"labels/name_stage1.npy\"))\n",
    "stage2 = list(np.load(\"labels/name_stage2.npy\"))\n",
    "patint_list = [*stage1, *stage2]\n",
    "patient_info = all_patient_info[all_patient_info['folder_name'].isin(patint_list)]\n",
    "\n",
    "patient_info['pT_Stage']=patient_info['pT_Stage'].replace({\"T1a\":0, \"T1b\":0, \"T1c\":0, \"T2a\":1,\"T2b\":1,\"T3\":2})\n",
    "patient_info['pM_Stage']=patient_info['pM_Stage'].replace({\"M1a\":1})\n",
    "\n",
    "feature_files = os.listdir(\"trans_feature\")\n",
    "\n",
    "data = []\n",
    "name = []\n",
    "for feature_name in feature_files:\n",
    "    path = \"trans_feature/\"+feature_name\n",
    "    name.append(int(feature_name[:-4]))\n",
    "    feature = list(np.load(path, allow_pickle=True))\n",
    "    data.append(feature)\n",
    "feature_data = pd.DataFrame(data)\n",
    "feature_data['folder_name']=name\n",
    "all_data = patient_info.merge(feature_data, how='left', on='folder_name')\n",
    "print(len(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aa6bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load external information\n",
    "external_info = pd.read_csv(\"/home/jielian/lung-graph-project/Tumor_tranformer/data_ind/External_label.csv\")\n",
    "external_patint_list = external_info['Patient']\n",
    "external_info=external_info.rename(columns={\"Patient\":\"folder_name\"})\n",
    "external_info=external_info.rename(columns={\"Histology\":\"Histology_1_Adenocarcinoma_2_SquamousCellCarcinoma_3_Others\"})\n",
    "external_info['pT_Stage']=external_info['pT_Stage'].replace({\"Tis\":0,\"T1a\":0, \"T1b\":0, \"T1c\":0, \"T2a\":1,\"T2b\":1,\"T3\":2,\"T4\":3 })\n",
    "\n",
    "external_feature_files = os.listdir(\"trans_feature_val\")\n",
    "\n",
    "external_data = []\n",
    "for feature_name in external_patint_list:\n",
    "    path = \"trans_feature_val/\"+feature_name+\".npy\"\n",
    "    feature = list(np.load(path, allow_pickle=True))\n",
    "    external_data.append(feature)\n",
    "external_feature_data = pd.DataFrame(external_data)\n",
    "external_feature_data['folder_name']=external_patint_list\n",
    "external_all_data = external_info.merge(external_feature_data, how='left', on='folder_name')\n",
    "print(len(external_all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186cfc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #merge the dataset\n",
    "frames = [all_data, external_all_data]\n",
    "final_data = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae9c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = np.load(\"data_ind/train_index.npy\",allow_pickle=True)\n",
    "val_id = np.load(\"data_ind/val_index.npy\", allow_pickle=True)\n",
    "test_id = np.load(\"data_ind/test_index.npy\",allow_pickle=True)\n",
    "external_id = np.array(range(len(all_data),len(final_data)))\n",
    "idx_train = torch.LongTensor(train_id)\n",
    "idx_val = torch.LongTensor(val_id)\n",
    "idx_test = torch.LongTensor(test_id)\n",
    "idx_external_val = torch.LongTensor(external_id)\n",
    "\n",
    "print(\"training OS distribution:\")\n",
    "print(all_data.iloc[train_id,:]['OS_Status'].value_counts())\n",
    "print(\"validation OS distribution:\")\n",
    "print(all_data.iloc[val_id,:]['OS_Status'].value_counts())\n",
    "print(\"test OS distribution:\")\n",
    "print(all_data.iloc[test_id,:]['OS_Status'].value_counts())\n",
    "print(\"External OS distribution:\")\n",
    "print(final_data.iloc[external_id,:]['OS_Status'].value_counts())\n",
    "\n",
    "\n",
    "print(\"training RFS_Status distribution:\")\n",
    "print(all_data.iloc[train_id,:]['RFS_Status'].value_counts())\n",
    "print(\"validation RFS_Status distribution:\")\n",
    "print(all_data.iloc[val_id,:]['RFS_Status'].value_counts())\n",
    "print(\"test RFS_Status distribution:\")\n",
    "print(all_data.iloc[test_id,:]['RFS_Status'].value_counts())\n",
    "print(\"External RFS_Status distribution:\")\n",
    "print(final_data.iloc[external_id,:]['RFS_Status'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e657d71f",
   "metadata": {},
   "source": [
    "# Start Graph Building!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e35452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define similarity of two patient\n",
    "def SimScore(a1,a2,s1,s2,l1,l2,h1,h2,t1,t2,n1,n2,m1,m2,tnm1,tnm2): \n",
    "    c_score = 0\n",
    "    h_score = 0\n",
    "    t_score = 0\n",
    "    # sex and age\n",
    "    if s1 == s2:\n",
    "        c_score +=1\n",
    "    if abs(a1-a2) <= 5:\n",
    "        c_score +=1\n",
    "    \n",
    "    if l1 == l2:\n",
    "        h_score +=1\n",
    "    if h1 == h2:\n",
    "        h_score +=1\n",
    "    \n",
    "    if t1 == t2:\n",
    "        t_score +=1\n",
    "    if n1 == n2:\n",
    "        t_score +=1\n",
    "    if m1 == m2:\n",
    "        t_score +=1\n",
    "#     if tnm1 == tnm2:\n",
    "#         t_score +=1\n",
    "\n",
    "    return c_score*t_score*h_score\n",
    "\n",
    "# def SimScore(a1,a2,s1,s2,l1,l2,h1,h2,t1,t2,n1,n2,m1,m2,tnm1,tnm2): \n",
    "\n",
    "#     return c_score*t_score*h_score\n",
    "\n",
    "\n",
    "def adj_matrix(patient_info):\n",
    "    age = patient_info['Age'].to_list()\n",
    "    sex = patient_info['Sex_1_male_2_female'].to_list()\n",
    "    loc = patient_info['Location_1_LUL_2_LLL_3_RUL_4_RML_5_RLL'].to_list()\n",
    "    his = patient_info['Histology_1_Adenocarcinoma_2_SquamousCellCarcinoma_3_Others'].to_list()\n",
    "    pts = patient_info['pT_Stage'].to_list()\n",
    "    pns = patient_info['pN_Stage'].to_list()\n",
    "    pms = patient_info['pM_Stage'].to_list()\n",
    "    tnm = patient_info['pTNM'].to_list()\n",
    "\n",
    "    edge_list=[]\n",
    "    edge_wight=[]\n",
    "    n_sample = len(age)\n",
    "    adj = np.zeros((n_sample, n_sample))\n",
    "    for i in range(n_sample):\n",
    "        for j in range(n_sample):\n",
    "            adj[i,j] = SimScore(age[i],age[j],sex[i],sex[j],loc[i],loc[j],his[i],his[j],\n",
    "                                pts[i],pts[j],pns[i],pns[j], pms[i],pms[j],tnm[i],tnm[j])\n",
    "            if adj[i,j] != 0:\n",
    "                edge_list.append([i,j])\n",
    "                edge_wight.append(adj[i,j])\n",
    "    return adj, edge_list,edge_wight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c77c681",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def graph_bulider(all_data, start_cloumn = 13, event = \"OS_Status\", label = \"OS_Month\"):\n",
    "\n",
    "    # save the labels\n",
    "    norm_label_sh = all_data[label]\n",
    "    # norm_label = (final_data['OS_Month']-np.min(final_data['OS_Month']))/(np.max(final_data['OS_Month'])-np.min(final_data['OS_Month']))\n",
    "    labels_sh = torch.from_numpy(norm_label_sh.to_numpy())\n",
    "    \n",
    "    events_sh = torch.from_numpy(all_data[event].to_numpy())\n",
    "    \n",
    "    adj_sh, edge_list_sh, edge_wight_sh = adj_matrix(all_data)\n",
    "    print(\"the number of nodes in this graph:\",len(norm_label_sh))\n",
    "    print(\"the number of edges in this graph:\",len(edge_list_sh))\n",
    "    print(\"Number of average degree: \",len(edge_list_sh)/len(norm_label_sh) )\n",
    "    \n",
    "    # build graph struture data\n",
    "    g_sh = dgl.DGLGraph()\n",
    "    g_sh.add_nodes(len(labels_sh))\n",
    "    # add nodes\n",
    "    # node_feature = (all_data.iloc[:, 15:]-all_data.iloc[:, 15:].min())/(all_data.iloc[:, 15:].max()- all_data.iloc[:, 15:].min())\n",
    "    node_feature_sh = all_data.iloc[:, start_cloumn:]\n",
    "    # print(node_feature)\n",
    "    node_feature_norm_sh = node_feature_sh.to_numpy()\n",
    "    g_sh.ndata['h'] = torch.from_numpy(node_feature_norm_sh).float()\n",
    "    g_sh.ndata['event'] = events_sh\n",
    "    g_sh.ndata['label'] = labels_sh\n",
    "    g_sh.ndata\n",
    "    # g.adj = adj\n",
    "    # add edges\n",
    "    src, dst = tuple(zip(*edge_list_sh))\n",
    "    g_sh.add_edges(src, dst)\n",
    "    # add edge weight\n",
    "    edge_wight_sh = np.array(edge_wight_sh)\n",
    "    g_sh.edata['w'] = torch.from_numpy(edge_wight_sh).float()\n",
    "    return adj_sh, g_sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7758f5f8",
   "metadata": {},
   "source": [
    "# Network and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ede423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats, dropout=0, activation = None,aggregator_type='mean'):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_feats, hid_feats) \n",
    "        self.conv1 = SAGEConv(in_feats=hid_feats, out_feats=hid_feats, aggregator_type=aggregator_type, activation=activation, feat_drop=dropout)\n",
    "        self.conv2 = SAGEConv(in_feats=hid_feats, out_feats= out_feats, aggregator_type=aggregator_type, activation=activation, feat_drop=dropout)\n",
    "        self.fc2 = nn.Linear(out_feats, 1) \n",
    "    def forward(self, graph, inputs, w_input):\n",
    "        # inputs are features of nodes\n",
    "        h = self.fc1(inputs)\n",
    "        h = self.conv1(graph, h, w_input)\n",
    "        h = self.conv2(graph,h)\n",
    "#         print(h.size())\n",
    "#         output=F.relu(self.fc2(h))\n",
    "        output= self.fc2(h)\n",
    "\n",
    "        return output\n",
    "\n",
    "class TAG(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats, activation = F.softmax):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_feats, hid_feats) \n",
    "        self.conv1 = TAGConv(in_feats=hid_feats, out_feats= hid_feats, activation=activation)\n",
    "        self.conv2 = TAGConv(in_feats=hid_feats, out_feats= out_feats,  activation=activation)\n",
    "        self.fc2 = nn.Linear(out_feats, 1) \n",
    "        \n",
    "    def forward(self, graph, inputs, w_input):\n",
    "        # inputs are features of nodes\n",
    "        h= self.fc1(inputs)\n",
    "        h = self.conv1(graph, h)\n",
    "        h = self.conv2(graph,h)\n",
    "#         output=F.relu(self.fc2(h))\n",
    "        h=self.fc2(h)\n",
    "        \n",
    "        return h\n",
    "\n",
    "    \n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats, activation = F.softmax, norm =\"both\"):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_feats, hid_feats) \n",
    "        self.conv1 = GraphConv(in_feats=hid_feats, out_feats= 32, activation=activation, norm=norm)\n",
    "        self.conv2 = GraphConv(in_feats=32, out_feats= out_feats,  activation=activation, norm=norm)\n",
    "        self.fc2 = nn.Linear(out_feats, 1) \n",
    "        \n",
    "    def forward(self, graph, inputs, w_input):\n",
    "        # inputs are features of nodes\n",
    "        h= self.fc1(inputs)\n",
    "        h = self.conv1(graph, h)\n",
    "        h = self.conv2(graph,h)\n",
    "#         output=F.relu(self.fc2(h))\n",
    "        h=self.fc2(h)\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    \n",
    "class SAGE1L(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats, dropout=0, activation = None,aggregator_type='mean'):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_feats, hid_feats) \n",
    "        self.conv1 = SAGEConv(in_feats=hid_feats, out_feats=out_feats, aggregator_type=aggregator_type, activation=activation, feat_drop=dropout)\n",
    "        self.fc2 = nn.Linear(out_feats, 1) \n",
    "    def forward(self, graph, inputs, w_input):\n",
    "        # inputs are features of nodes\n",
    "        h = self.fc1(inputs)\n",
    "        h = self.conv1(graph, h, w_input)\n",
    "\n",
    "        output= self.fc2(h)\n",
    "\n",
    "        return output\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18751f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regularization(object):\n",
    "    def __init__(self, order, weight_decay):\n",
    "        ''' The initialization of Regularization class\n",
    "        :param order: (int) norm order number\n",
    "        :param weight_decay: (float) weight decay rate\n",
    "        '''\n",
    "        super(Regularization, self).__init__()\n",
    "        self.order = order\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def __call__(self, model):\n",
    "        ''' Performs calculates regularization(self.order) loss for model.\n",
    "        :param model: (torch.nn.Module object)\n",
    "        :return reg_loss: (torch.Tensor) the regularization(self.order) loss\n",
    "        '''\n",
    "        reg_loss = 0\n",
    "        for name, w in model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                reg_loss = reg_loss + torch.norm(w, p=self.order)\n",
    "        reg_loss = self.weight_decay * reg_loss\n",
    "        return reg_loss\n",
    "\n",
    "    \n",
    "class NegativeLogLikelihood(nn.Module):\n",
    "    def __init__(self, l2_reg, device):\n",
    "        super(NegativeLogLikelihood, self).__init__()\n",
    "        self.L2_reg = l2_reg\n",
    "        self.device = device\n",
    "        self.reg = Regularization(order=2, weight_decay=self.L2_reg)\n",
    "\n",
    "    def forward(self, risk_pred, y, e, model):\n",
    "        mask = torch.ones(y.shape[0], y.shape[0]).to(self.device)\n",
    "        mask[(y.T - y) > 0] = 0\n",
    "        log_loss = torch.exp(risk_pred) * mask\n",
    "        log_loss = torch.sum(log_loss, dim=0) / torch.sum(mask, dim=0)\n",
    "        log_loss = torch.log(log_loss).reshape(-1, 1)\n",
    "        neg_log_loss = -torch.sum((risk_pred-log_loss) * e) / torch.sum(e)\n",
    "        l2_loss = self.reg(model)\n",
    "        return neg_log_loss + l2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ebd2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_all, g_all = graph_bulider(final_data)\n",
    "adj_sh, g_sh = graph_bulider(all_data)\n",
    "# g_sh = dgl.node_subgraph(g_all, list(range(len(all_data))))\n",
    "g_external = dgl.node_subgraph(g_all, list(range(len(all_data),len(final_data))))\n",
    "# g_val = graph_bulider(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d4e1cf",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a4aad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g, g_all, model,device, save_dic, idx_train,idx_val, idx_test, total_epoch=100, patience=5, lr=0.001, reg_l2=0, weight_decay=0.0001):\n",
    "    model_name = save_dic['model']+str(save_dic['hid_feats'])+str(save_dic['out_feats'])+str(save_dic['reg_l2'])+save_dic[\"aggregator_type\"]\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr, weight_decay=weight_decay)\n",
    "    best_cindex = 0\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.5, patience=patience, min_lr = 0.0001, verbose=True)\n",
    "    criterion = NegativeLogLikelihood(reg_l2, device).to(device) \n",
    "    model = model.to(device) \n",
    "    features = g.ndata['h'].to(device) \n",
    "    e_feature = g.edata['w'].to(device) \n",
    "    labels = g.ndata['label'].to(device) \n",
    "    events = g.ndata['event'].to(device) \n",
    "    g_all= g_all.to(device)  \n",
    "    g = g.to(device) \n",
    "    t_total = time.time()\n",
    "    with tqdm(range(total_epoch)) as t:\n",
    "        for epoch in t:\n",
    "            t.set_description('Epoch %d' % epoch)\n",
    "            start = time.time()\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(g, features,e_feature)\n",
    "            # Compute loss\n",
    "            # Note that you should only compute the losses of the nodes in the training set.\n",
    "            loss_train = criterion(output[idx_train], labels[idx_train],events[idx_train], model).clone()\n",
    "            auc_train = c_index(-output[idx_train], labels[idx_train],events[idx_train])\n",
    "            \n",
    "            loss_train.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            \n",
    "            model.eval()\n",
    "            val_output = model(g, features,e_feature)\n",
    "            loss_val = criterion(val_output[idx_val], labels[idx_val],events[idx_val], model).clone()\n",
    "            scheduler.step(loss_val)\n",
    "            \n",
    "            auc_val = c_index(-val_output[idx_val], labels[idx_val],events[idx_val])\n",
    "            auc_test = c_index(-val_output[idx_test], labels[idx_test], events[idx_test])\n",
    "            exter_val_output = model(g_all, g_all.ndata['h'],g_all.edata['w'])\n",
    "            \n",
    "            t.set_postfix(\n",
    "                  {\"train_loss\":loss_train.item(), \"val_loss\":loss_val.item(),\n",
    "                  \"train_cindex\":auc_train.item(), \"val_auc\":auc_val.item(),\n",
    "                \"lr\":optimizer.param_groups[0]['lr']}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93326ee3",
   "metadata": {},
   "source": [
    "# Subanalysis: High and Low Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d877bf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import KaplanMeierFitter, CoxPHFitter, calibration\n",
    "from lifelines.statistics import logrank_test\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58c171c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr):\n",
    "    plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "y_true = test['OS_Status']\n",
    "y_scores = test['risk']\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "print(\"AUC_value:\",roc_auc_score(y_true, y_scores))\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(\"Threshold value is:\", optimal_threshold)\n",
    "plot_roc_curve(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe76b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "thred = optimal_threshold\n",
    "risk_label = []\n",
    "for risk_score in test['risk']:\n",
    "    if risk_score <= thred:\n",
    "        risk_label.append(0)\n",
    "    else:\n",
    "        risk_label.append(1)\n",
    "test['risk_label'] = risk_label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e8c3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage as reference\n",
    "Ts1 = test[test['Stage']==0]['OS_Month']\n",
    "Es1 = test[test['Stage']==0]['OS_Status']\n",
    "kmfs1 = KaplanMeierFitter(label=\"TNM Low Risk\")\n",
    "kmfs1.fit(Ts1, Es1)\n",
    "kmfs1.survival_function_\n",
    "kmfs1.cumulative_density_\n",
    "kmfs1.plot_survival_function(ci_show =True)\n",
    "# kmf2.plot_cumulative_density()\n",
    "\n",
    "Ts2 = test[test['Stage']==1]['OS_Month']\n",
    "Es2 = test[test['Stage']==1]['OS_Status']\n",
    "kmfs2 = KaplanMeierFitter(label=\"TNM High Risk\")\n",
    "kmfs2.fit(Ts2, Es2)\n",
    "kmfs2.survival_function_\n",
    "kmfs2.cumulative_density_\n",
    "kmfs2.plot_survival_function(ci_show =True)\n",
    "# kmf2.plot_cumulative_density()\n",
    "stage_results=logrank_test(Ts1,Ts2,event_observed_A=Es1, event_observed_B=Es2)\n",
    "stage_results.print_summary()\n",
    "\n",
    "\n",
    "TGCN_low = test[test['risk_label']==0]['OS_Month']\n",
    "EGCN_low = test[test['risk_label']==0]['OS_Status']\n",
    "kmGCN_low = KaplanMeierFitter(label=\"GCN Low Risk\")\n",
    "kmGCN_low.fit(TGCN_low, EGCN_low)\n",
    "kmGCN_low.survival_function_\n",
    "kmGCN_low.cumulative_density_\n",
    "kmGCN_low.plot_survival_function(ci_show =True)\n",
    "# kmf2.plot_cumulative_density()\n",
    "\n",
    "TGCN_high = test[test['risk_label']==1]['OS_Month']\n",
    "EGCN_high = test[test['risk_label']==1]['OS_Status']\n",
    "kmGCN_high = KaplanMeierFitter(label=\"GCN High Risk\")\n",
    "kmGCN_high.fit(TGCN_high, EGCN_high)\n",
    "kmGCN_high.survival_function_\n",
    "kmGCN_high.cumulative_density_\n",
    "kmGCN_high.plot_survival_function(ci_show =True)\n",
    "# kmf2.plot_cumulative_density()\n",
    "gcn_results=logrank_test(TGCN_low,TGCN_high,event_observed_A=EGCN_low, event_observed_B=EGCN_high)\n",
    "gcn_results.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecb68e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.offsetbox import AnchoredText\n",
    "fig = plt.figure(figsize=(20,7))\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "kmGCN_low.plot_survival_function(ci_show =False)\n",
    "kmGCN_high.plot_survival_function(ci_show =False, color='r')\n",
    "ax1.title.set_text('Transformer Graph Model Kaplan-Meier Curve ')\n",
    "ax1.title.set_fontsize(15)\n",
    "ax1.set_ylim([0.4, 1.0])\n",
    "ax1.set_xlim([0.0, 65.0])\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "p1 = gcn_results.p_value\n",
    "ax1.add_artist(AnchoredText(\"p = %.3e\" % round(p1, 10), loc=4, frameon=False))\n",
    "# ax1.title(\"GCN Kaplan-Meier Curve\")\n",
    "ax1.legend(loc='lower left',prop={'size': 10})\n",
    "\n",
    "\n",
    "ax2 =  fig.add_subplot(132)\n",
    "kmfs1.plot_survival_function(ci_show =False)\n",
    "kmfs2.plot_survival_function(ci_show =False, color='r')\n",
    "ax2.title.set_text('TNM Model Kaplan-Meier Curve ')\n",
    "ax2.title.set_fontsize(15)\n",
    "ax2.set_ylim([0.4, 1.0])\n",
    "ax2.set_xlim([0.0, 65.0])\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "p2 = stage_results.p_value\n",
    "ax2.add_artist(AnchoredText(\"p = %.3e\" % round(p2,10 ), loc=4, frameon=False))\n",
    "ax2.legend(loc='lower left',prop={'size': 10})\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b044f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCN_cox = test[['OS_Month','OS_Status', 'risk']]\n",
    "cph_GCN = CoxPHFitter()\n",
    "cph_GCN.fit(GCN_cox, 'OS_Month', event_col='OS_Status')\n",
    "cph_GCN.print_summary()\n",
    "axGCN, ICI, E50 = calibration.survival_probability_calibration(cph_GCN, GCN_cox, t0=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4b981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stage = test[['OS_Month','OS_Status', 'Stage']]\n",
    "cph_stage = CoxPHFitter()\n",
    "cph_stage.fit(test_stage, 'OS_Month', event_col='OS_Status')\n",
    "cph_stage.print_summary()\n",
    "ax_stage, ICI, E50 = calibration.survival_probability_calibration(cph_stage, test_stage, t0=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eefc57",
   "metadata": {},
   "source": [
    "# Subanalysis: Inside the Graph\n",
    "### Turn to Networkx Plotting and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa3bb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thred(risk, best_thresh = optimal_threshold):\n",
    "    if risk < best_thresh:\n",
    "        label = -1\n",
    "    else:\n",
    "        label = 1\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab19bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the trained graph\n",
    "g_test = dgl.node_subgraph(g_sh, idx_test)\n",
    "g_test.ndata['h'] = torch.from_numpy(np.array(test['risk']))\n",
    "print(g_test.number_of_nodes())\n",
    "print(g_test.number_of_edges())\n",
    "print(g_test.number_of_edges()/g_test.number_of_nodes())\n",
    "nx_G = g_test.to_networkx()\n",
    "\n",
    "# add node and edge data to the graph\n",
    "for i in range(len(nx_G.nodes())):\n",
    "    nx_G.nodes[i][\"Risk\"] = g_test.ndata['h'][i].item()\n",
    "    nx_G.nodes[i][\"Risk_Label\"] = thred(g_test.ndata['h'][i].item())\n",
    "    nx_G.nodes[i][\"E\"] = g_test.ndata['event'][i].item()\n",
    "    nx_G.nodes[i][\"T\"] = g_test.ndata['label'][i].item()\n",
    "\n",
    "w = g_test.edata['w'].numpy()\n",
    "for i in range(len(nx_G.edges())):\n",
    "    edge = list(nx_G.edges())[i]\n",
    "    nx_G.edges[edge[0],edge[1],0][\"w\"] = g_test.edata['w'][i].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cb5d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.readwrite import json_graph\n",
    "import json\n",
    "# import json\n",
    "data = json_graph.node_link_data(nx_G)\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NpEncoder, self).default(obj)\n",
    "jsodata = json.dumps(data, cls=NpEncoder)\n",
    "with open('dense_data.json', 'w') as f:\n",
    "    json.dump(jsodata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88762d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict(nx_G.nodes(data=\"Risk_Label\"))\n",
    "low, *_, high = sorted(d.values())\n",
    "norm = mpl.colors.Normalize(vmin=low-2, vmax=high+2, clip=True)\n",
    "mapper = mpl.cm.ScalarMappable(norm=norm, cmap=mpl.cm.coolwarm)\n",
    "plt.figure(figsize=[20,10])\n",
    "nx.draw(nx_G, nodelist=d, node_size= 400,\n",
    "        node_color=[mapper.to_rgba(i) for i in d.values()], \n",
    "        edge_color = 'grey', with_labels=False, width= 0.2, font_color='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0f64a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict(nx_G.nodes(data=\"E\"))\n",
    "low, *_, high = sorted(d.values())\n",
    "norm = mpl.colors.Normalize(vmin=low-1, vmax=high+1, clip=True)\n",
    "mapper = mpl.cm.ScalarMappable(norm=norm, cmap=mpl.cm.coolwarm)\n",
    "plt.figure(figsize=[20,10])\n",
    "nx.draw_spring(nx_G, nodelist=d, node_size= 400,\n",
    "        node_color=[mapper.to_rgba(i) for i in d.values()], \n",
    "        edge_color = 'grey', with_labels=False, width= 0.2, font_color='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74639741",
   "metadata": {},
   "source": [
    "# Make analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72522d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# g_sh\n",
    "risk_label = [thred(i) for i in patient_info['risk']]\n",
    "g_sh.ndata['risk'] = torch.from_numpy(patient_info['risk'].to_numpy())\n",
    "g_sh.ndata['risk_label'] =torch.from_numpy(np.array(risk_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61606d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_np(values):\n",
    "    neg = 0\n",
    "    pos = 0\n",
    "    for i in values:\n",
    "        if i>0:\n",
    "            pos += i\n",
    "        else:\n",
    "            neg +=i\n",
    "    return neg, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1f0aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all neighboor of a target node: here is: dx_test[0]\n",
    "def neighboor_analysis(idx): \n",
    "    node_survival = g_sh.ndata['event'][idx]\n",
    "    node_survival_month = g_sh.ndata['label'][idx]\n",
    "    node_risk = g_sh.ndata['risk'][idx]\n",
    "    node_risk_label = g_sh.ndata['risk_label'][idx]\n",
    "    print(\"node risk:\", node_risk)\n",
    "    print(\"node risk label:\", node_risk_label)\n",
    "    print(\"node survival:\", node_survival)\n",
    "    neighboor = g_sh.in_edges(idx)\n",
    "    # find the neiboors's risk:\n",
    "    neighboor_risk = g_sh.ndata['risk'][neighboor[0]]\n",
    "    neighboor_risk_label = g_sh.ndata['risk_label'][neighboor[0]]\n",
    "    uni, count = torch.unique(neighboor_risk_label, return_counts=True)\n",
    "    print(\"node neriboor risk label:\", uni, count)\n",
    "    neighboor_survival_label = g_sh.ndata['event'][neighboor[0]]\n",
    "    print(\"node neriboor sum survival\",sum(neighboor_survival_label))\n",
    "    # find the neighboor with weight\n",
    "    neighboor_weight = g_sh.edges[neighboor][0]['w']\n",
    "    neighboor_weight_label = torch.mul(neighboor_risk_label, neighboor_weight)\n",
    "    neg, pos = sum_np(neighboor_weight_label)\n",
    "#     print(\"low risk imformation:\", neg)\n",
    "#     print(\"High risk imformation:\", pos)  \n",
    "    sum_weights = neg + pos\n",
    "    if sum_weights >0:\n",
    "        print(\"high risk provide more wights!\")\n",
    "    else:\n",
    "        print(\"==========low risk provide more wights!=============\")\n",
    "    return node_survival, node_survival_month, node_risk, node_risk_label, count[0], count[1], neighboor_risk, neighboor_risk_label, neighboor_weight, neighboor_weight_label, -neg, pos, sum_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ec13de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis from edge information\n",
    "\n",
    "idxs = [] \n",
    "survivals = [] \n",
    "survival_months = []\n",
    "risks = []\n",
    "risk_labels = []\n",
    "neighboor_risks = []\n",
    "low_counts  = []\n",
    "high_counts = []\n",
    "num_neighboors = []\n",
    "neighboor_risk_labels = []\n",
    "neighboor_weights = []\n",
    "neighboor_weight_labels = []\n",
    "negs = []\n",
    "poss = []\n",
    "sum_weightss = []\n",
    "for idx in idx_test:    \n",
    "    node_survival, node_survival_m, node_risk, node_risk_label, low_count, high_count, neighboor_risk, _, neighboor_weight, neighboor_weight_label, neg, pos,sum_weights = neighboor_analysis(idx)\n",
    "    idxs.append(idx.item())\n",
    "    survivals.append(node_survival.item())\n",
    "    survival_months.append(node_survival_m.item())\n",
    "    risks.append(node_risk.item())\n",
    "    risk_labels.append(node_risk_label.item())\n",
    "    neighboor_risks.append(neighboor_risk)\n",
    "    low_counts.append(low_count.item())\n",
    "    high_counts.append(high_count.item())\n",
    "    num_neighboors.append((low_count+high_count).item())\n",
    "    neighboor_weights.append(neighboor_weight)\n",
    "    neighboor_weight_labels.append(neighboor_weight_label)\n",
    "    negs.append(neg.item())\n",
    "    poss.append(pos.item())\n",
    "    sum_weightss.append(sum_weights.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc735b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_df = pd.DataFrame()\n",
    "clinical_df['node'] = idxs\n",
    "clinical_df['survival'] = survivals\n",
    "clinical_df['survival_month'] =\n",
    "clinical_df['num_neighboors'] = num_neighboors\n",
    "clinical_df['risks'] = risks\n",
    "clinical_df['risk_labels'] = risk_labels\n",
    "clinical_df['neighboor_risks'] = neighboor_risks\n",
    "clinical_df['neighboor_low_counts'] = [100*low_counts[i]/num_neighboors[i] for i in range(len(num_neighboors))]\n",
    "clinical_df['neighboor_high_counts'] = [100*high_counts[i]/num_neighboors[i] for i in range(len(num_neighboors))]\n",
    "\n",
    "\n",
    "clinical_df['low_weights'] = negs\n",
    "clinical_df['high_weights'] = poss\n",
    "clinical_df['sum_weights'] = sum_weightss\n",
    "clinical_df['sum_weights_abs'] = clinical_df['low_weights'] + clinical_df['low_weights']\n",
    "clinical_df['survival'] = clinical_df['survival'].replace({0:-1})\n",
    "clinical_df['mean_sum_weights_abs'] = clinical_df['sum_weights_abs']/clinical_df['num_neighboors']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae97c55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb0ccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,7), dpi = 350)\n",
    "sns.set(style=\"whitegrid\")\n",
    "# fig.subplots_adjust(top=0.15)\n",
    "ax1 = fig.add_subplot(121)\n",
    "x1 = \"risk_labels\"\n",
    "y = \"num_neighboors\"\n",
    "order1 = [-1, 1]\n",
    "# ax1 = sns.boxplot(data=clinical_df, x=x1, y=y, order=order1, palette=sns.color_palette(['#FF5720', '#18C288']))\n",
    "ax1 = sns.boxplot(data=clinical_df, x=x1, y=y, order=order1, palette=sns.color_palette(['#FF5720', '#18C288']))\n",
    "\n",
    "add_stat_annotation(ax1, data=clinical_df, x=x1, y=y,  box_pairs=[(-1,1)],\n",
    "                    test='Mann-Whitney', text_format='star', loc='outside', verbose=2)\n",
    "\n",
    "ax1.set(xlabel='risk', ylabel='number of neighboors')\n",
    "ax1.set(xticklabels=[\"low risk\", \"high risk\"])\n",
    "# fig.subplots_adjust(bottom = 0.5)\n",
    "# fig.savefig(\"box_plot.jpg\")\n",
    "                  \n",
    "ax2 = fig.add_subplot(122)\n",
    "x2 = \"survival\"\n",
    "y = \"num_neighboors\"\n",
    "order2= [-1, 1]\n",
    "ax2 = sns.boxplot(data=clinical_df, x=x2, y=y, order=order2, palette=sns.color_palette(['#FF5720', '#18C288']))\n",
    "add_stat_annotation(ax2, data=clinical_df, x=x2, y=y, box_pairs=[(-1,1)],\n",
    "                    test='Mann-Whitney', text_format='star', loc='outside', verbose=2)\n",
    "ax2.set(xlabel='Survival', ylabel='number of neighboors')\n",
    "ax2.set(xticklabels=[\"Survival\", \"Death\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719e7e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,7), dpi = 350)\n",
    "sns.set(style=\"whitegrid\")\n",
    "# fig.subplots_adjust(top=0.15)\n",
    "ax1 = fig.add_subplot(121)\n",
    "x1 = \"survival\"\n",
    "y = \"sum_weights_abs\"\n",
    "order1 = [-1, 1]\n",
    "# ax1 = sns.boxplot(data=clinical_df, x=x1, y=y, order=order1, palette=sns.color_palette(['#FF5720', '#18C288']))\n",
    "ax1 = sns.boxplot(data=clinical_df, x=x1, y=y, order=order1, palette=sns.color_palette(['#FF5720', '#18C288']))\n",
    "\n",
    "add_stat_annotation(ax1, data=clinical_df, x=x1, y=y,  box_pairs=[(-1,1)],\n",
    "                    test='Mann-Whitney', text_format='star', loc='outside', verbose=2)\n",
    "\n",
    "ax1.set(xlabel='Survival', ylabel='sum of neighboors weights')\n",
    "ax1.set(xticklabels=[\"Survival\", \"Death\"])\n",
    "# fig.subplots_adjust(bottom = 0.5)\n",
    "# fig.savefig(\"box_plot.jpg\")\n",
    "ax2 = fig.add_subplot(122)\n",
    "x2 = \"survival\"\n",
    "y = \"mean_sum_weights_abs\"\n",
    "order2= [-1, 1]\n",
    "ax2 = sns.boxplot(data=clinical_df, x=x2, y=y, order=order2, palette=sns.color_palette(['#FF5720', '#18C288']))\n",
    "add_stat_annotation(ax2, data=clinical_df, x=x2, y=y, box_pairs=[(-1,1)],\n",
    "                    test='Mann-Whitney', text_format='star', loc='outside', verbose=2)\n",
    "ax2.set(xlabel='Survival', ylabel='mean of neighboors weights')\n",
    "ax2.set(xticklabels=[\"Survival\", \"Death\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694c9695",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f526d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_d = clinical_df[['survival', 'low_weights', 'high_weights']]\n",
    "d2 = clinical_d.melt(id_vars=\"survival\", var_name=\"neighboor risk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696feb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a nested barplot to show survival for class and sex\n",
    "fig = plt.figure(figsize=(20,7), dpi = 350)\n",
    "sns.set(style=\"whitegrid\")\n",
    "# fig.subplots_adjust(top=0.15)\n",
    "ax1 = fig.add_subplot(121)\n",
    "x = \"survival\"\n",
    "y1 = \"value\"\n",
    "hue1 = 'neighboor risk'\n",
    "order1 = [-1, 1]\n",
    "# ax1 = sns.boxplot(data=clinical_df, x=x1, y=y, order=order1, palette=sns.color_palette(['#FF5720', '#18C288']))\n",
    "ax1 = sns.boxplot(data=d2, x=x, y=y1, hue = hue1,  order=order1)\n",
    "ax1.set(xlabel='survival', ylabel='sum of neighboor weights')\n",
    "ax1.set(xticklabels=[\"survival\", \"death\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6b22ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_df.to_csv('data_ind/test_graph_feature.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57906b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6245c3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ad0a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8caa870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
