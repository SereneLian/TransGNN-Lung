{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9668e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2163cda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "import dgl.function as fn\n",
    "from dgl.nn.pytorch import GraphConv, SAGEConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53a2ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepsurv_utils import c_index, adjust_learning_rate\n",
    "from loss import NegativeLogLikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782cd24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "all_patient_info = pd.read_csv(\"v\")\n",
    "stage1 = list(np.load(\"labels/name_stage1.npy\"))\n",
    "stage2 = list(np.load(\"labels/name_stage2.npy\"))\n",
    "patint_list = [*stage1, *stage2]\n",
    "patient_info = all_patient_info[all_patient_info['folder_name'].isin(patint_list)]\n",
    "feature_files = os.listdir(\"trans_feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d65ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "name = []\n",
    "for feature_name in feature_files:\n",
    "    path = \"trans_feature/\"+feature_name\n",
    "    name.append(int(feature_name[:-4]))\n",
    "    feature = list(np.load(path, allow_pickle=True))\n",
    "    data.append(feature)\n",
    "feature_data = pd.DataFrame(data)\n",
    "feature_data['folder_name']=name\n",
    "all_data = patient_info.merge(feature_data, how='left', on='folder_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae9c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = np.load(\"data_ind/train_index.npy\",allow_pickle=True)\n",
    "val_id = np.load(\"data_ind/val_index.npy\", allow_pickle=True)\n",
    "test_id = np.load(\"data_ind/test_index.npy\",allow_pickle=True)\n",
    "idx_train = torch.LongTensor(train_id)\n",
    "idx_val = torch.LongTensor(val_id)\n",
    "idx_test = torch.LongTensor(test_id)\n",
    "\n",
    "print(\"training survival distribution:\")\n",
    "print(all_data.iloc[train_id,:]['OS_Status'].value_counts())\n",
    "print(\"validation survival distribution:\")\n",
    "print(all_data.iloc[val_id,:]['OS_Status'].value_counts())\n",
    "print(\"test survival distribution:\")\n",
    "print(all_data.iloc[test_id,:]['OS_Status'].value_counts())\n",
    "\n",
    "\n",
    "# print(\"training survival distribution:\")\n",
    "# print(all_data.iloc[train_id,:]['RFS_Status'].value_counts())\n",
    "# print(\"validation survival distribution:\")\n",
    "# print(all_data.iloc[val_id,:]['RFS_Status'].value_counts())\n",
    "# print(\"test survival distribution:\")\n",
    "# print(all_data.iloc[test_id,:]['RFS_Status'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec90816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "data = all_data['RFS_Status'].to_list()\n",
    "print(st.t.interval(alpha=0.95, df=len(data)-1, loc=np.mean(data), scale=st.sem(data)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e35452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define similarity of two patient\n",
    "def SimScore(a1,a2,s1,s2,l1,l2,h1,h2,t1,t2,n1,n2,m1,m2,tnm1,tnm2): \n",
    "    c_score = 0\n",
    "    h_score = 0\n",
    "    t_score = 0\n",
    "    # sex and age\n",
    "    if s1 == s2:\n",
    "        c_score +=1\n",
    "    if abs(a1-a2) <= 5:\n",
    "        c_score +=1\n",
    "    \n",
    "    if l1 == l2:\n",
    "        h_score +=1\n",
    "    if h1 == h2:\n",
    "        h_score +=1\n",
    "    \n",
    "    if t1 == t2:\n",
    "        t_score +=1\n",
    "    if n1 == n2:\n",
    "        t_score +=1\n",
    "    if m1 == m2:\n",
    "        t_score +=1\n",
    "#     if tnm1 == tnm2:\n",
    "#         t_score +=1\n",
    "\n",
    "    return c_score*t_score*h_score\n",
    "\n",
    "# def SimScore(a1,a2,s1,s2,l1,l2,h1,h2,t1,t2,n1,n2,m1,m2,tnm1,tnm2): \n",
    "\n",
    "#     return c_score*t_score*h_score\n",
    "\n",
    "\n",
    "def adj_matrix(patient_info):\n",
    "    age = patient_info['Age'].to_list()\n",
    "    sex = patient_info['Sex_1_male_2_female'].to_list()\n",
    "    \n",
    "    loc = patient_info['Location_1_LUL_2_LLL_3_RUL_4_RML_5_RLL'].to_list()\n",
    "    his = patient_info['Histology_1_Adenocarcinoma_2_SquamousCellCarcinoma_3_Others'].to_list()\n",
    "    pts = patient_info['pT_Stage'].to_list()\n",
    "    pns = patient_info['pN_Stage'].to_list()\n",
    "    pms = patient_info['pM_Stage'].to_list()\n",
    "    tnm = patient_info['pTNM'].to_list()\n",
    "\n",
    "    edge_list=[]\n",
    "    edge_wight=[]\n",
    "    n_sample = len(age)\n",
    "    adj = np.zeros((n_sample, n_sample))\n",
    "    for i in tqdm(range(n_sample)):\n",
    "        for j in range(n_sample):\n",
    "            adj[i,j] = SimScore(age[i],age[j],sex[i],sex[j],loc[i],loc[j],his[i],his[j],\n",
    "                                pts[i],pts[j],pns[i],pns[j], pms[i],pms[j],tnm[i],tnm[j])\n",
    "            if adj[i,j] != 0:\n",
    "                edge_list.append([i,j])\n",
    "                edge_wight.append(adj[i,j])\n",
    "    return adj, edge_list,edge_wight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e8866c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adj, edge_list, edge_wight = adj_matrix(all_data)\n",
    "print(\"the number of edges in this graph:\",len(edge_list))\n",
    "print(\"Number of average degree: \",len(edge_list)/1705 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97603d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the labels\n",
    "norm_label = all_data['OS_Month']\n",
    "# norm_label = (all_data['OS_Month']-np.min(all_data['OS_Month']))/(np.max(all_data['OS_Month'])-np.min(all_data['OS_Month']))\n",
    "labels = torch.from_numpy(norm_label.to_numpy())\n",
    "events = torch.from_numpy(all_data['OS_Status'].to_numpy())\n",
    "# build graph struture data\n",
    "g = dgl.DGLGraph()\n",
    "g.add_nodes(len(labels))\n",
    "# add nodes\n",
    "# node_feature = (all_data.iloc[:, 15:]-all_data.iloc[:, 15:].min())/(all_data.iloc[:, 15:].max()- all_data.iloc[:, 15:].min())\n",
    "node_feature = all_data.iloc[:, 15:]\n",
    "# print(node_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c77c681",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_feature_norm = node_feature.to_numpy()\n",
    "g.ndata['h'] = torch.from_numpy(node_feature_norm).float()\n",
    "g.ndata['event'] = events\n",
    "g.ndata['label'] = labels\n",
    "g.ndata\n",
    "# g.adj = adj\n",
    "# add edges\n",
    "src, dst = tuple(zip(*edge_list))\n",
    "g.add_edges(src, dst)\n",
    "# add edge weight\n",
    "edge_wight = np.array(edge_wight)\n",
    "g.edata['w'] = torch.from_numpy(edge_wight).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637ca099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats, dropout=0, activation = None,aggregator_type='mean'):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_feats, hid_feats) \n",
    "        self.conv1 = SAGEConv(in_feats=hid_feats, out_feats=64, aggregator_type=aggregator_type, activation=activation, feat_drop=dropout)\n",
    "        self.conv2 = SAGEConv(in_feats=64, out_feats= out_feats, aggregator_type=aggregator_type, activation=activation, feat_drop=dropout)\n",
    "        self.fc2 = nn.Linear(out_feats, 1) \n",
    "    def forward(self, graph, inputs, w_input):\n",
    "        # inputs are features of nodes\n",
    "#         graph.ndata['h']= self.fc1(inputs)\n",
    "        h = self.fc1(inputs)\n",
    "        if w_input != None:\n",
    "            h = self.conv1(graph, h, w_input)\n",
    "        else:\n",
    "            h = self.conv1(graph, h)\n",
    "        h = self.conv2(graph,h)\n",
    "#         print(h.size())\n",
    "#         output=F.relu(self.fc2(h))\n",
    "        output=self.fc2(h)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b777c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats, activation = F.softmax, norm =\"both\"):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_feats, hid_feats) \n",
    "        self.conv1 = GraphConv(in_feats=hid_feats, out_feats= hid_feats, activation=activation, norm=norm)\n",
    "        self.conv2 = GraphConv(in_feats=hid_feats, out_feats= out_feats,  activation=activation, norm=norm)\n",
    "        self.fc2 = nn.Linear(out_feats, 1) \n",
    "        \n",
    "    def forward(self, graph, inputs, w_input):\n",
    "        # inputs are features of nodes\n",
    "        h= self.fc1(inputs)\n",
    "        h = self.conv1(graph, h)\n",
    "        h = self.conv2(graph,h)\n",
    "#         output=F.relu(self.fc2(h))\n",
    "        h=self.fc2(h)\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b814b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g, model, save_dic, idx_train,idx_val, idx_test, total_epoch=100, patience=5, lr=0.001, reg_l2=0, weight_decay=0.0001):\n",
    "    model_name = save_dic['model']+str(save_dic['hid_feats'])+str(save_dic['out_feats'])+str(save_dic['reg_l2'])+save_dic[\"aggregator_type\"]\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=lr, weight_decay=weight_decay)\n",
    "    best_cindex = 0\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',factor=0.5, patience=patience, min_lr = 0.0001, verbose=True)\n",
    "    criterion = NegativeLogLikelihood(reg_l2)\n",
    "    features = g.ndata['h']\n",
    "#     e_feature = g.edata['w']\n",
    "    e_feature = None\n",
    "    labels = g.ndata['label']\n",
    "    events = g.ndata['event']\n",
    "    t_total = time.time()\n",
    "    with tqdm(range(total_epoch)) as t:\n",
    "        for epoch in t:\n",
    "            t.set_description('Epoch %d' % epoch)\n",
    "            start = time.time()\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(g, features,e_feature)\n",
    "            # Compute loss\n",
    "            # Note that you should only compute the losses of the nodes in the training set.\n",
    "            loss_train = criterion(output[idx_train], labels[idx_train],events[idx_train], model).clone()\n",
    "            auc_train = c_index(-output[idx_train], labels[idx_train],events[idx_train])\n",
    "            \n",
    "            loss_train.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            \n",
    "            model.eval()\n",
    "            val_output = model(g, features,e_feature)\n",
    "            loss_val = criterion(val_output[idx_val], labels[idx_val],events[idx_val], model).clone()\n",
    "            scheduler.step(loss_val)\n",
    "            \n",
    "            auc_val = c_index(-val_output[idx_val], labels[idx_val],events[idx_val])\n",
    "            auc_test = c_index(-val_output[idx_test], labels[idx_test],events[idx_test])\n",
    "            t.set_postfix(\n",
    "                  {\"train_loss\":loss_train.item(), \"val_loss\":loss_val.item(),\n",
    "                  \"train_cindex\":auc_train.item(), \"val_auc\":auc_val.item(),\n",
    "                \"lr\":optimizer.param_groups[0]['lr']}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e447926f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7655eede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2cceb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08edffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab19bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
